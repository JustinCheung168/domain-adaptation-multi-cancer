{
  "best_metric": 0.013969456776976585,
  "best_model_checkpoint": "./D21_Experiment\\checkpoint-101567",
  "epoch": 50.0,
  "eval_steps": 500,
  "global_step": 108050,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004627487274409995,
      "grad_norm": 19.7022762298584,
      "learning_rate": 0.09999074502545119,
      "loss": 1.2007,
      "step": 10
    },
    {
      "epoch": 0.00925497454881999,
      "grad_norm": 10.680229187011719,
      "learning_rate": 0.09998149005090236,
      "loss": 1.102,
      "step": 20
    },
    {
      "epoch": 0.013882461823229986,
      "grad_norm": 15.5957612991333,
      "learning_rate": 0.09997223507635355,
      "loss": 1.091,
      "step": 30
    },
    {
      "epoch": 0.01850994909763998,
      "grad_norm": 13.87302017211914,
      "learning_rate": 0.09996298010180472,
      "loss": 1.1064,
      "step": 40
    },
    {
      "epoch": 0.02313743637204998,
      "grad_norm": 11.416851043701172,
      "learning_rate": 0.09995372512725591,
      "loss": 1.0842,
      "step": 50
    },
    {
      "epoch": 0.027764923646459973,
      "grad_norm": 9.775653839111328,
      "learning_rate": 0.09994447015270708,
      "loss": 1.0117,
      "step": 60
    },
    {
      "epoch": 0.03239241092086997,
      "grad_norm": 9.899078369140625,
      "learning_rate": 0.09993521517815827,
      "loss": 0.9644,
      "step": 70
    },
    {
      "epoch": 0.03701989819527996,
      "grad_norm": 8.873281478881836,
      "learning_rate": 0.09992596020360944,
      "loss": 0.9697,
      "step": 80
    },
    {
      "epoch": 0.041647385469689956,
      "grad_norm": 8.150521278381348,
      "learning_rate": 0.09991670522906063,
      "loss": 0.9793,
      "step": 90
    },
    {
      "epoch": 0.04627487274409996,
      "grad_norm": 12.412474632263184,
      "learning_rate": 0.09990745025451181,
      "loss": 1.0254,
      "step": 100
    },
    {
      "epoch": 0.05090236001850995,
      "grad_norm": 10.573280334472656,
      "learning_rate": 0.09989819527996298,
      "loss": 0.9183,
      "step": 110
    },
    {
      "epoch": 0.055529847292919945,
      "grad_norm": 6.864780902862549,
      "learning_rate": 0.09988894030541418,
      "loss": 0.9486,
      "step": 120
    },
    {
      "epoch": 0.06015733456732994,
      "grad_norm": 10.532772064208984,
      "learning_rate": 0.09987968533086534,
      "loss": 0.8243,
      "step": 130
    },
    {
      "epoch": 0.06478482184173993,
      "grad_norm": 6.583571910858154,
      "learning_rate": 0.09987043035631653,
      "loss": 0.982,
      "step": 140
    },
    {
      "epoch": 0.06941230911614993,
      "grad_norm": 8.225725173950195,
      "learning_rate": 0.0998611753817677,
      "loss": 0.9303,
      "step": 150
    },
    {
      "epoch": 0.07403979639055992,
      "grad_norm": 8.782870292663574,
      "learning_rate": 0.09985192040721888,
      "loss": 0.9778,
      "step": 160
    },
    {
      "epoch": 0.07866728366496992,
      "grad_norm": 8.92304801940918,
      "learning_rate": 0.09984266543267006,
      "loss": 0.893,
      "step": 170
    },
    {
      "epoch": 0.08329477093937991,
      "grad_norm": 6.097683906555176,
      "learning_rate": 0.09983341045812125,
      "loss": 0.9149,
      "step": 180
    },
    {
      "epoch": 0.0879222582137899,
      "grad_norm": 8.210267066955566,
      "learning_rate": 0.09982415548357243,
      "loss": 0.9436,
      "step": 190
    },
    {
      "epoch": 0.09254974548819991,
      "grad_norm": 10.065581321716309,
      "learning_rate": 0.0998149005090236,
      "loss": 0.9059,
      "step": 200
    },
    {
      "epoch": 0.09717723276260991,
      "grad_norm": 8.602108001708984,
      "learning_rate": 0.0998056455344748,
      "loss": 0.8127,
      "step": 210
    },
    {
      "epoch": 0.1018047200370199,
      "grad_norm": 9.417023658752441,
      "learning_rate": 0.09979639055992597,
      "loss": 0.8928,
      "step": 220
    },
    {
      "epoch": 0.1064322073114299,
      "grad_norm": 8.278359413146973,
      "learning_rate": 0.09978713558537715,
      "loss": 0.8781,
      "step": 230
    },
    {
      "epoch": 0.11105969458583989,
      "grad_norm": 6.9999237060546875,
      "learning_rate": 0.09977788061082832,
      "loss": 0.8919,
      "step": 240
    },
    {
      "epoch": 0.11568718186024989,
      "grad_norm": 10.031181335449219,
      "learning_rate": 0.0997686256362795,
      "loss": 0.8571,
      "step": 250
    },
    {
      "epoch": 0.12031466913465988,
      "grad_norm": 4.6185431480407715,
      "learning_rate": 0.09975937066173068,
      "loss": 0.9084,
      "step": 260
    },
    {
      "epoch": 0.12494215640906987,
      "grad_norm": 5.846721172332764,
      "learning_rate": 0.09975011568718187,
      "loss": 0.8222,
      "step": 270
    },
    {
      "epoch": 0.12956964368347987,
      "grad_norm": 7.026492118835449,
      "learning_rate": 0.09974086071263305,
      "loss": 0.8788,
      "step": 280
    },
    {
      "epoch": 0.13419713095788988,
      "grad_norm": 5.876639366149902,
      "learning_rate": 0.09973160573808422,
      "loss": 0.8963,
      "step": 290
    },
    {
      "epoch": 0.13882461823229986,
      "grad_norm": 6.666741847991943,
      "learning_rate": 0.09972235076353542,
      "loss": 0.9192,
      "step": 300
    },
    {
      "epoch": 0.14345210550670986,
      "grad_norm": 6.357951641082764,
      "learning_rate": 0.09971309578898659,
      "loss": 0.9365,
      "step": 310
    },
    {
      "epoch": 0.14807959278111985,
      "grad_norm": 8.49746322631836,
      "learning_rate": 0.09970384081443777,
      "loss": 0.7897,
      "step": 320
    },
    {
      "epoch": 0.15270708005552985,
      "grad_norm": 6.837855815887451,
      "learning_rate": 0.09969458583988894,
      "loss": 0.8414,
      "step": 330
    },
    {
      "epoch": 0.15733456732993983,
      "grad_norm": 5.618194580078125,
      "learning_rate": 0.09968533086534012,
      "loss": 0.7448,
      "step": 340
    },
    {
      "epoch": 0.16196205460434984,
      "grad_norm": 8.444693565368652,
      "learning_rate": 0.0996760758907913,
      "loss": 0.809,
      "step": 350
    },
    {
      "epoch": 0.16658954187875982,
      "grad_norm": 7.724545478820801,
      "learning_rate": 0.09966682091624249,
      "loss": 0.7833,
      "step": 360
    },
    {
      "epoch": 0.17121702915316983,
      "grad_norm": 9.44605827331543,
      "learning_rate": 0.09965756594169367,
      "loss": 0.8271,
      "step": 370
    },
    {
      "epoch": 0.1758445164275798,
      "grad_norm": 8.172614097595215,
      "learning_rate": 0.09964831096714484,
      "loss": 0.8387,
      "step": 380
    },
    {
      "epoch": 0.18047200370198982,
      "grad_norm": 10.395463943481445,
      "learning_rate": 0.09963905599259602,
      "loss": 0.9196,
      "step": 390
    },
    {
      "epoch": 0.18509949097639983,
      "grad_norm": 5.298125743865967,
      "learning_rate": 0.0996298010180472,
      "loss": 0.8055,
      "step": 400
    },
    {
      "epoch": 0.1897269782508098,
      "grad_norm": 6.008655548095703,
      "learning_rate": 0.09962054604349839,
      "loss": 0.777,
      "step": 410
    },
    {
      "epoch": 0.19435446552521982,
      "grad_norm": 5.71018648147583,
      "learning_rate": 0.09961129106894956,
      "loss": 0.7761,
      "step": 420
    },
    {
      "epoch": 0.1989819527996298,
      "grad_norm": 7.229270935058594,
      "learning_rate": 0.09960203609440074,
      "loss": 0.8437,
      "step": 430
    },
    {
      "epoch": 0.2036094400740398,
      "grad_norm": 5.851606845855713,
      "learning_rate": 0.09959278111985193,
      "loss": 0.879,
      "step": 440
    },
    {
      "epoch": 0.20823692734844979,
      "grad_norm": 9.484896659851074,
      "learning_rate": 0.09958352614530311,
      "loss": 0.8883,
      "step": 450
    },
    {
      "epoch": 0.2128644146228598,
      "grad_norm": 8.991230010986328,
      "learning_rate": 0.09957427117075429,
      "loss": 0.6752,
      "step": 460
    },
    {
      "epoch": 0.21749190189726977,
      "grad_norm": 9.841570854187012,
      "learning_rate": 0.09956501619620546,
      "loss": 0.8798,
      "step": 470
    },
    {
      "epoch": 0.22211938917167978,
      "grad_norm": 5.7275590896606445,
      "learning_rate": 0.09955576122165664,
      "loss": 0.6888,
      "step": 480
    },
    {
      "epoch": 0.22674687644608976,
      "grad_norm": 6.725393295288086,
      "learning_rate": 0.09954650624710783,
      "loss": 0.8213,
      "step": 490
    },
    {
      "epoch": 0.23137436372049977,
      "grad_norm": 6.176420211791992,
      "learning_rate": 0.09953725127255901,
      "loss": 0.7909,
      "step": 500
    },
    {
      "epoch": 0.23600185099490975,
      "grad_norm": 8.854714393615723,
      "learning_rate": 0.09952799629801018,
      "loss": 0.7515,
      "step": 510
    },
    {
      "epoch": 0.24062933826931976,
      "grad_norm": 3.9949300289154053,
      "learning_rate": 0.09951874132346136,
      "loss": 0.7348,
      "step": 520
    },
    {
      "epoch": 0.24525682554372977,
      "grad_norm": 7.110570430755615,
      "learning_rate": 0.09950948634891255,
      "loss": 0.7412,
      "step": 530
    },
    {
      "epoch": 0.24988431281813975,
      "grad_norm": 4.430490970611572,
      "learning_rate": 0.09950023137436373,
      "loss": 0.6681,
      "step": 540
    },
    {
      "epoch": 0.25451180009254976,
      "grad_norm": 6.749942302703857,
      "learning_rate": 0.09949097639981491,
      "loss": 0.7193,
      "step": 550
    },
    {
      "epoch": 0.25913928736695974,
      "grad_norm": 5.4171953201293945,
      "learning_rate": 0.09948172142526608,
      "loss": 0.634,
      "step": 560
    },
    {
      "epoch": 0.2637667746413697,
      "grad_norm": 6.247860431671143,
      "learning_rate": 0.09947246645071726,
      "loss": 0.6152,
      "step": 570
    },
    {
      "epoch": 0.26839426191577975,
      "grad_norm": 5.858383655548096,
      "learning_rate": 0.09946321147616845,
      "loss": 0.6434,
      "step": 580
    },
    {
      "epoch": 0.27302174919018973,
      "grad_norm": 6.110184669494629,
      "learning_rate": 0.09945395650161963,
      "loss": 0.636,
      "step": 590
    },
    {
      "epoch": 0.2776492364645997,
      "grad_norm": 6.507684707641602,
      "learning_rate": 0.0994447015270708,
      "loss": 0.622,
      "step": 600
    },
    {
      "epoch": 0.2822767237390097,
      "grad_norm": 9.796433448791504,
      "learning_rate": 0.09943544655252198,
      "loss": 0.678,
      "step": 610
    },
    {
      "epoch": 0.28690421101341973,
      "grad_norm": 6.946575164794922,
      "learning_rate": 0.09942619157797317,
      "loss": 0.6324,
      "step": 620
    },
    {
      "epoch": 0.2915316982878297,
      "grad_norm": 3.455223560333252,
      "learning_rate": 0.09941693660342435,
      "loss": 0.5986,
      "step": 630
    },
    {
      "epoch": 0.2961591855622397,
      "grad_norm": 5.6446757316589355,
      "learning_rate": 0.09940768162887553,
      "loss": 0.573,
      "step": 640
    },
    {
      "epoch": 0.3007866728366497,
      "grad_norm": 6.651237964630127,
      "learning_rate": 0.0993984266543267,
      "loss": 0.6556,
      "step": 650
    },
    {
      "epoch": 0.3054141601110597,
      "grad_norm": 4.839270114898682,
      "learning_rate": 0.09938917167977789,
      "loss": 0.6132,
      "step": 660
    },
    {
      "epoch": 0.3100416473854697,
      "grad_norm": 6.549077987670898,
      "learning_rate": 0.09937991670522907,
      "loss": 0.6021,
      "step": 670
    },
    {
      "epoch": 0.31466913465987967,
      "grad_norm": 3.4756245613098145,
      "learning_rate": 0.09937066173068025,
      "loss": 0.5728,
      "step": 680
    },
    {
      "epoch": 0.3192966219342897,
      "grad_norm": 6.229339122772217,
      "learning_rate": 0.09936140675613142,
      "loss": 0.5447,
      "step": 690
    },
    {
      "epoch": 0.3239241092086997,
      "grad_norm": 7.5437445640563965,
      "learning_rate": 0.0993521517815826,
      "loss": 0.5291,
      "step": 700
    },
    {
      "epoch": 0.32855159648310966,
      "grad_norm": 6.3096089363098145,
      "learning_rate": 0.09934289680703379,
      "loss": 0.601,
      "step": 710
    },
    {
      "epoch": 0.33317908375751965,
      "grad_norm": 8.350288391113281,
      "learning_rate": 0.09933364183248497,
      "loss": 0.5509,
      "step": 720
    },
    {
      "epoch": 0.3378065710319297,
      "grad_norm": 6.9311418533325195,
      "learning_rate": 0.09932438685793615,
      "loss": 0.5332,
      "step": 730
    },
    {
      "epoch": 0.34243405830633966,
      "grad_norm": 6.668572425842285,
      "learning_rate": 0.09931513188338732,
      "loss": 0.5608,
      "step": 740
    },
    {
      "epoch": 0.34706154558074964,
      "grad_norm": 5.205570697784424,
      "learning_rate": 0.0993058769088385,
      "loss": 0.5297,
      "step": 750
    },
    {
      "epoch": 0.3516890328551596,
      "grad_norm": 6.494523525238037,
      "learning_rate": 0.09929662193428969,
      "loss": 0.5148,
      "step": 760
    },
    {
      "epoch": 0.35631652012956966,
      "grad_norm": 4.003772258758545,
      "learning_rate": 0.09928736695974087,
      "loss": 0.5248,
      "step": 770
    },
    {
      "epoch": 0.36094400740397964,
      "grad_norm": 8.962814331054688,
      "learning_rate": 0.09927811198519204,
      "loss": 0.5722,
      "step": 780
    },
    {
      "epoch": 0.3655714946783896,
      "grad_norm": 4.731631278991699,
      "learning_rate": 0.09926885701064322,
      "loss": 0.5347,
      "step": 790
    },
    {
      "epoch": 0.37019898195279966,
      "grad_norm": 6.487951278686523,
      "learning_rate": 0.09925960203609441,
      "loss": 0.4909,
      "step": 800
    },
    {
      "epoch": 0.37482646922720964,
      "grad_norm": 5.214522838592529,
      "learning_rate": 0.09925034706154559,
      "loss": 0.5153,
      "step": 810
    },
    {
      "epoch": 0.3794539565016196,
      "grad_norm": 6.782802581787109,
      "learning_rate": 0.09924109208699677,
      "loss": 0.5047,
      "step": 820
    },
    {
      "epoch": 0.3840814437760296,
      "grad_norm": 4.644204139709473,
      "learning_rate": 0.09923183711244794,
      "loss": 0.4476,
      "step": 830
    },
    {
      "epoch": 0.38870893105043963,
      "grad_norm": 5.468560218811035,
      "learning_rate": 0.09922258213789913,
      "loss": 0.4667,
      "step": 840
    },
    {
      "epoch": 0.3933364183248496,
      "grad_norm": 2.8770999908447266,
      "learning_rate": 0.0992133271633503,
      "loss": 0.463,
      "step": 850
    },
    {
      "epoch": 0.3979639055992596,
      "grad_norm": 3.0838091373443604,
      "learning_rate": 0.09920407218880149,
      "loss": 0.4018,
      "step": 860
    },
    {
      "epoch": 0.4025913928736696,
      "grad_norm": 5.773324012756348,
      "learning_rate": 0.09919481721425266,
      "loss": 0.4239,
      "step": 870
    },
    {
      "epoch": 0.4072188801480796,
      "grad_norm": 4.814685344696045,
      "learning_rate": 0.09918556223970384,
      "loss": 0.447,
      "step": 880
    },
    {
      "epoch": 0.4118463674224896,
      "grad_norm": 5.463889122009277,
      "learning_rate": 0.09917630726515503,
      "loss": 0.4339,
      "step": 890
    },
    {
      "epoch": 0.41647385469689957,
      "grad_norm": 3.3241145610809326,
      "learning_rate": 0.09916705229060621,
      "loss": 0.4061,
      "step": 900
    },
    {
      "epoch": 0.4211013419713096,
      "grad_norm": 6.129632472991943,
      "learning_rate": 0.0991577973160574,
      "loss": 0.3974,
      "step": 910
    },
    {
      "epoch": 0.4257288292457196,
      "grad_norm": 6.7609686851501465,
      "learning_rate": 0.09914854234150856,
      "loss": 0.476,
      "step": 920
    },
    {
      "epoch": 0.43035631652012957,
      "grad_norm": 6.530712604522705,
      "learning_rate": 0.09913928736695975,
      "loss": 0.5187,
      "step": 930
    },
    {
      "epoch": 0.43498380379453955,
      "grad_norm": 5.936202526092529,
      "learning_rate": 0.09913003239241092,
      "loss": 0.4382,
      "step": 940
    },
    {
      "epoch": 0.4396112910689496,
      "grad_norm": 4.591488361358643,
      "learning_rate": 0.09912077741786211,
      "loss": 0.3963,
      "step": 950
    },
    {
      "epoch": 0.44423877834335956,
      "grad_norm": 4.184928894042969,
      "learning_rate": 0.09911152244331328,
      "loss": 0.3856,
      "step": 960
    },
    {
      "epoch": 0.44886626561776954,
      "grad_norm": 6.98768424987793,
      "learning_rate": 0.09910226746876447,
      "loss": 0.4167,
      "step": 970
    },
    {
      "epoch": 0.4534937528921795,
      "grad_norm": 3.419609308242798,
      "learning_rate": 0.09909301249421565,
      "loss": 0.3879,
      "step": 980
    },
    {
      "epoch": 0.45812124016658956,
      "grad_norm": 4.889385223388672,
      "learning_rate": 0.09908375751966683,
      "loss": 0.3838,
      "step": 990
    },
    {
      "epoch": 0.46274872744099954,
      "grad_norm": 3.542311906814575,
      "learning_rate": 0.09907450254511802,
      "loss": 0.373,
      "step": 1000
    },
    {
      "epoch": 0.4673762147154095,
      "grad_norm": 6.755397319793701,
      "learning_rate": 0.09906524757056918,
      "loss": 0.3521,
      "step": 1010
    },
    {
      "epoch": 0.4720037019898195,
      "grad_norm": 6.314610004425049,
      "learning_rate": 0.09905599259602037,
      "loss": 0.3858,
      "step": 1020
    },
    {
      "epoch": 0.47663118926422954,
      "grad_norm": 6.651208400726318,
      "learning_rate": 0.09904673762147154,
      "loss": 0.4117,
      "step": 1030
    },
    {
      "epoch": 0.4812586765386395,
      "grad_norm": 4.324338436126709,
      "learning_rate": 0.09903748264692273,
      "loss": 0.4149,
      "step": 1040
    },
    {
      "epoch": 0.4858861638130495,
      "grad_norm": 4.820389747619629,
      "learning_rate": 0.0990282276723739,
      "loss": 0.3948,
      "step": 1050
    },
    {
      "epoch": 0.49051365108745953,
      "grad_norm": 4.242310523986816,
      "learning_rate": 0.09901897269782509,
      "loss": 0.3077,
      "step": 1060
    },
    {
      "epoch": 0.4951411383618695,
      "grad_norm": 4.52276086807251,
      "learning_rate": 0.09900971772327627,
      "loss": 0.3618,
      "step": 1070
    },
    {
      "epoch": 0.4997686256362795,
      "grad_norm": 3.397413969039917,
      "learning_rate": 0.09900046274872744,
      "loss": 0.3019,
      "step": 1080
    },
    {
      "epoch": 0.5043961129106895,
      "grad_norm": 3.064455509185791,
      "learning_rate": 0.09899120777417864,
      "loss": 0.3731,
      "step": 1090
    },
    {
      "epoch": 0.5090236001850995,
      "grad_norm": 3.112165927886963,
      "learning_rate": 0.0989819527996298,
      "loss": 0.3235,
      "step": 1100
    },
    {
      "epoch": 0.5136510874595095,
      "grad_norm": 4.750683784484863,
      "learning_rate": 0.09897269782508099,
      "loss": 0.3444,
      "step": 1110
    },
    {
      "epoch": 0.5182785747339195,
      "grad_norm": 4.9301018714904785,
      "learning_rate": 0.09896344285053216,
      "loss": 0.3924,
      "step": 1120
    },
    {
      "epoch": 0.5229060620083295,
      "grad_norm": 4.25645112991333,
      "learning_rate": 0.09895418787598335,
      "loss": 0.3664,
      "step": 1130
    },
    {
      "epoch": 0.5275335492827394,
      "grad_norm": 4.228530406951904,
      "learning_rate": 0.09894493290143452,
      "loss": 0.3394,
      "step": 1140
    },
    {
      "epoch": 0.5321610365571494,
      "grad_norm": 6.205869674682617,
      "learning_rate": 0.0989356779268857,
      "loss": 0.3575,
      "step": 1150
    },
    {
      "epoch": 0.5367885238315595,
      "grad_norm": 8.202690124511719,
      "learning_rate": 0.09892642295233689,
      "loss": 0.3973,
      "step": 1160
    },
    {
      "epoch": 0.5414160111059695,
      "grad_norm": 5.175931930541992,
      "learning_rate": 0.09891716797778806,
      "loss": 0.3432,
      "step": 1170
    },
    {
      "epoch": 0.5460434983803795,
      "grad_norm": 3.617147445678711,
      "learning_rate": 0.09890791300323926,
      "loss": 0.3725,
      "step": 1180
    },
    {
      "epoch": 0.5506709856547894,
      "grad_norm": 2.614436626434326,
      "learning_rate": 0.09889865802869043,
      "loss": 0.2827,
      "step": 1190
    },
    {
      "epoch": 0.5552984729291994,
      "grad_norm": 6.005400657653809,
      "learning_rate": 0.09888940305414161,
      "loss": 0.3688,
      "step": 1200
    },
    {
      "epoch": 0.5599259602036094,
      "grad_norm": 8.294410705566406,
      "learning_rate": 0.09888014807959278,
      "loss": 0.3906,
      "step": 1210
    },
    {
      "epoch": 0.5645534474780194,
      "grad_norm": 4.710206031799316,
      "learning_rate": 0.09887089310504397,
      "loss": 0.2996,
      "step": 1220
    },
    {
      "epoch": 0.5691809347524295,
      "grad_norm": 4.508251190185547,
      "learning_rate": 0.09886163813049514,
      "loss": 0.3593,
      "step": 1230
    },
    {
      "epoch": 0.5738084220268395,
      "grad_norm": 5.00848913192749,
      "learning_rate": 0.09885238315594633,
      "loss": 0.3287,
      "step": 1240
    },
    {
      "epoch": 0.5784359093012494,
      "grad_norm": 4.642329216003418,
      "learning_rate": 0.09884312818139751,
      "loss": 0.378,
      "step": 1250
    },
    {
      "epoch": 0.5830633965756594,
      "grad_norm": 7.054576873779297,
      "learning_rate": 0.09883387320684868,
      "loss": 0.3408,
      "step": 1260
    },
    {
      "epoch": 0.5876908838500694,
      "grad_norm": 4.107532024383545,
      "learning_rate": 0.09882461823229988,
      "loss": 0.3554,
      "step": 1270
    },
    {
      "epoch": 0.5923183711244794,
      "grad_norm": 3.984400749206543,
      "learning_rate": 0.09881536325775105,
      "loss": 0.3283,
      "step": 1280
    },
    {
      "epoch": 0.5969458583988894,
      "grad_norm": 4.094479560852051,
      "learning_rate": 0.09880610828320223,
      "loss": 0.3422,
      "step": 1290
    },
    {
      "epoch": 0.6015733456732995,
      "grad_norm": 6.339057922363281,
      "learning_rate": 0.0987968533086534,
      "loss": 0.3305,
      "step": 1300
    },
    {
      "epoch": 0.6062008329477094,
      "grad_norm": 2.4121479988098145,
      "learning_rate": 0.09878759833410458,
      "loss": 0.2975,
      "step": 1310
    },
    {
      "epoch": 0.6108283202221194,
      "grad_norm": 5.246463775634766,
      "learning_rate": 0.09877834335955576,
      "loss": 0.3708,
      "step": 1320
    },
    {
      "epoch": 0.6154558074965294,
      "grad_norm": 5.797094821929932,
      "learning_rate": 0.09876908838500695,
      "loss": 0.3876,
      "step": 1330
    },
    {
      "epoch": 0.6200832947709394,
      "grad_norm": 4.258255958557129,
      "learning_rate": 0.09875983341045813,
      "loss": 0.3315,
      "step": 1340
    },
    {
      "epoch": 0.6247107820453494,
      "grad_norm": 5.62821626663208,
      "learning_rate": 0.0987505784359093,
      "loss": 0.3311,
      "step": 1350
    },
    {
      "epoch": 0.6293382693197593,
      "grad_norm": 3.4533627033233643,
      "learning_rate": 0.0987413234613605,
      "loss": 0.314,
      "step": 1360
    },
    {
      "epoch": 0.6339657565941693,
      "grad_norm": 4.852917671203613,
      "learning_rate": 0.09873206848681167,
      "loss": 0.2824,
      "step": 1370
    },
    {
      "epoch": 0.6385932438685794,
      "grad_norm": 2.848811388015747,
      "learning_rate": 0.09872281351226285,
      "loss": 0.3095,
      "step": 1380
    },
    {
      "epoch": 0.6432207311429894,
      "grad_norm": 3.33583927154541,
      "learning_rate": 0.09871355853771402,
      "loss": 0.2677,
      "step": 1390
    },
    {
      "epoch": 0.6478482184173994,
      "grad_norm": 4.770492076873779,
      "learning_rate": 0.0987043035631652,
      "loss": 0.3662,
      "step": 1400
    },
    {
      "epoch": 0.6524757056918093,
      "grad_norm": 4.113992214202881,
      "learning_rate": 0.09869504858861639,
      "loss": 0.2601,
      "step": 1410
    },
    {
      "epoch": 0.6571031929662193,
      "grad_norm": 3.1359522342681885,
      "learning_rate": 0.09868579361406757,
      "loss": 0.3136,
      "step": 1420
    },
    {
      "epoch": 0.6617306802406293,
      "grad_norm": 2.9299476146698,
      "learning_rate": 0.09867653863951875,
      "loss": 0.273,
      "step": 1430
    },
    {
      "epoch": 0.6663581675150393,
      "grad_norm": 1.525413155555725,
      "learning_rate": 0.09866728366496992,
      "loss": 0.3152,
      "step": 1440
    },
    {
      "epoch": 0.6709856547894494,
      "grad_norm": 6.0178303718566895,
      "learning_rate": 0.09865802869042112,
      "loss": 0.2864,
      "step": 1450
    },
    {
      "epoch": 0.6756131420638594,
      "grad_norm": 3.920273542404175,
      "learning_rate": 0.09864877371587229,
      "loss": 0.2594,
      "step": 1460
    },
    {
      "epoch": 0.6802406293382693,
      "grad_norm": 5.839955806732178,
      "learning_rate": 0.09863951874132347,
      "loss": 0.3,
      "step": 1470
    },
    {
      "epoch": 0.6848681166126793,
      "grad_norm": 5.4699387550354,
      "learning_rate": 0.09863026376677464,
      "loss": 0.3081,
      "step": 1480
    },
    {
      "epoch": 0.6894956038870893,
      "grad_norm": 5.295597553253174,
      "learning_rate": 0.09862100879222582,
      "loss": 0.2735,
      "step": 1490
    },
    {
      "epoch": 0.6941230911614993,
      "grad_norm": 4.298578262329102,
      "learning_rate": 0.098611753817677,
      "loss": 0.3194,
      "step": 1500
    },
    {
      "epoch": 0.6987505784359093,
      "grad_norm": 5.15764045715332,
      "learning_rate": 0.09860249884312819,
      "loss": 0.3537,
      "step": 1510
    },
    {
      "epoch": 0.7033780657103192,
      "grad_norm": 4.878505706787109,
      "learning_rate": 0.09859324386857937,
      "loss": 0.2995,
      "step": 1520
    },
    {
      "epoch": 0.7080055529847293,
      "grad_norm": 3.445225715637207,
      "learning_rate": 0.09858398889403054,
      "loss": 0.2807,
      "step": 1530
    },
    {
      "epoch": 0.7126330402591393,
      "grad_norm": 5.078899383544922,
      "learning_rate": 0.09857473391948172,
      "loss": 0.3013,
      "step": 1540
    },
    {
      "epoch": 0.7172605275335493,
      "grad_norm": 4.400111675262451,
      "learning_rate": 0.09856547894493291,
      "loss": 0.2489,
      "step": 1550
    },
    {
      "epoch": 0.7218880148079593,
      "grad_norm": 3.27813720703125,
      "learning_rate": 0.09855622397038409,
      "loss": 0.2259,
      "step": 1560
    },
    {
      "epoch": 0.7265155020823693,
      "grad_norm": 3.339237689971924,
      "learning_rate": 0.09854696899583526,
      "loss": 0.2764,
      "step": 1570
    },
    {
      "epoch": 0.7311429893567792,
      "grad_norm": 4.2251105308532715,
      "learning_rate": 0.09853771402128644,
      "loss": 0.2462,
      "step": 1580
    },
    {
      "epoch": 0.7357704766311892,
      "grad_norm": 4.613346576690674,
      "learning_rate": 0.09852845904673763,
      "loss": 0.3476,
      "step": 1590
    },
    {
      "epoch": 0.7403979639055993,
      "grad_norm": 6.679478645324707,
      "learning_rate": 0.09851920407218881,
      "loss": 0.289,
      "step": 1600
    },
    {
      "epoch": 0.7450254511800093,
      "grad_norm": 4.5380635261535645,
      "learning_rate": 0.09850994909763999,
      "loss": 0.2648,
      "step": 1610
    },
    {
      "epoch": 0.7496529384544193,
      "grad_norm": 3.709306240081787,
      "learning_rate": 0.09850069412309116,
      "loss": 0.2379,
      "step": 1620
    },
    {
      "epoch": 0.7542804257288293,
      "grad_norm": 6.267308235168457,
      "learning_rate": 0.09849143914854235,
      "loss": 0.2755,
      "step": 1630
    },
    {
      "epoch": 0.7589079130032392,
      "grad_norm": 4.13671875,
      "learning_rate": 0.09848218417399353,
      "loss": 0.2434,
      "step": 1640
    },
    {
      "epoch": 0.7635354002776492,
      "grad_norm": 6.198558330535889,
      "learning_rate": 0.09847292919944471,
      "loss": 0.2212,
      "step": 1650
    },
    {
      "epoch": 0.7681628875520592,
      "grad_norm": 3.9989402294158936,
      "learning_rate": 0.09846367422489588,
      "loss": 0.2593,
      "step": 1660
    },
    {
      "epoch": 0.7727903748264692,
      "grad_norm": 4.58512020111084,
      "learning_rate": 0.09845441925034706,
      "loss": 0.2617,
      "step": 1670
    },
    {
      "epoch": 0.7774178621008793,
      "grad_norm": 3.555952310562134,
      "learning_rate": 0.09844516427579825,
      "loss": 0.2491,
      "step": 1680
    },
    {
      "epoch": 0.7820453493752892,
      "grad_norm": 4.771616458892822,
      "learning_rate": 0.09843590930124943,
      "loss": 0.2616,
      "step": 1690
    },
    {
      "epoch": 0.7866728366496992,
      "grad_norm": 2.854241371154785,
      "learning_rate": 0.09842665432670061,
      "loss": 0.2205,
      "step": 1700
    },
    {
      "epoch": 0.7913003239241092,
      "grad_norm": 4.757652759552002,
      "learning_rate": 0.09841739935215178,
      "loss": 0.25,
      "step": 1710
    },
    {
      "epoch": 0.7959278111985192,
      "grad_norm": 2.957655429840088,
      "learning_rate": 0.09840814437760297,
      "loss": 0.3003,
      "step": 1720
    },
    {
      "epoch": 0.8005552984729292,
      "grad_norm": 5.686645984649658,
      "learning_rate": 0.09839888940305415,
      "loss": 0.2738,
      "step": 1730
    },
    {
      "epoch": 0.8051827857473391,
      "grad_norm": 3.4445555210113525,
      "learning_rate": 0.09838963442850533,
      "loss": 0.2275,
      "step": 1740
    },
    {
      "epoch": 0.8098102730217492,
      "grad_norm": 5.1112141609191895,
      "learning_rate": 0.0983803794539565,
      "loss": 0.2566,
      "step": 1750
    },
    {
      "epoch": 0.8144377602961592,
      "grad_norm": 4.324853420257568,
      "learning_rate": 0.09837112447940768,
      "loss": 0.2363,
      "step": 1760
    },
    {
      "epoch": 0.8190652475705692,
      "grad_norm": 3.7196991443634033,
      "learning_rate": 0.09836186950485887,
      "loss": 0.2658,
      "step": 1770
    },
    {
      "epoch": 0.8236927348449792,
      "grad_norm": 3.4979281425476074,
      "learning_rate": 0.09835261453031005,
      "loss": 0.2289,
      "step": 1780
    },
    {
      "epoch": 0.8283202221193892,
      "grad_norm": 3.6699225902557373,
      "learning_rate": 0.09834335955576123,
      "loss": 0.2427,
      "step": 1790
    },
    {
      "epoch": 0.8329477093937991,
      "grad_norm": 4.841946601867676,
      "learning_rate": 0.0983341045812124,
      "loss": 0.2363,
      "step": 1800
    },
    {
      "epoch": 0.8375751966682091,
      "grad_norm": 3.2988390922546387,
      "learning_rate": 0.09832484960666359,
      "loss": 0.2344,
      "step": 1810
    },
    {
      "epoch": 0.8422026839426192,
      "grad_norm": 4.748848915100098,
      "learning_rate": 0.09831559463211477,
      "loss": 0.2539,
      "step": 1820
    },
    {
      "epoch": 0.8468301712170292,
      "grad_norm": 2.709540605545044,
      "learning_rate": 0.09830633965756595,
      "loss": 0.2223,
      "step": 1830
    },
    {
      "epoch": 0.8514576584914392,
      "grad_norm": 4.589247226715088,
      "learning_rate": 0.09829708468301712,
      "loss": 0.2611,
      "step": 1840
    },
    {
      "epoch": 0.8560851457658492,
      "grad_norm": 4.83732795715332,
      "learning_rate": 0.0982878297084683,
      "loss": 0.2578,
      "step": 1850
    },
    {
      "epoch": 0.8607126330402591,
      "grad_norm": 3.328442096710205,
      "learning_rate": 0.09827857473391949,
      "loss": 0.2314,
      "step": 1860
    },
    {
      "epoch": 0.8653401203146691,
      "grad_norm": 8.811905860900879,
      "learning_rate": 0.09826931975937067,
      "loss": 0.2808,
      "step": 1870
    },
    {
      "epoch": 0.8699676075890791,
      "grad_norm": 5.45347785949707,
      "learning_rate": 0.09826006478482185,
      "loss": 0.2068,
      "step": 1880
    },
    {
      "epoch": 0.8745950948634891,
      "grad_norm": 4.398195743560791,
      "learning_rate": 0.09825080981027302,
      "loss": 0.2837,
      "step": 1890
    },
    {
      "epoch": 0.8792225821378992,
      "grad_norm": 4.255890369415283,
      "learning_rate": 0.0982415548357242,
      "loss": 0.2712,
      "step": 1900
    },
    {
      "epoch": 0.8838500694123091,
      "grad_norm": 2.905083417892456,
      "learning_rate": 0.09823229986117539,
      "loss": 0.1945,
      "step": 1910
    },
    {
      "epoch": 0.8884775566867191,
      "grad_norm": 3.208214282989502,
      "learning_rate": 0.09822304488662657,
      "loss": 0.2364,
      "step": 1920
    },
    {
      "epoch": 0.8931050439611291,
      "grad_norm": 3.406844139099121,
      "learning_rate": 0.09821378991207774,
      "loss": 0.2075,
      "step": 1930
    },
    {
      "epoch": 0.8977325312355391,
      "grad_norm": 6.871862888336182,
      "learning_rate": 0.09820453493752893,
      "loss": 0.2121,
      "step": 1940
    },
    {
      "epoch": 0.9023600185099491,
      "grad_norm": 2.9989922046661377,
      "learning_rate": 0.09819527996298011,
      "loss": 0.2245,
      "step": 1950
    },
    {
      "epoch": 0.906987505784359,
      "grad_norm": 4.143982410430908,
      "learning_rate": 0.09818602498843129,
      "loss": 0.1664,
      "step": 1960
    },
    {
      "epoch": 0.9116149930587691,
      "grad_norm": 4.002537250518799,
      "learning_rate": 0.09817677001388248,
      "loss": 0.2339,
      "step": 1970
    },
    {
      "epoch": 0.9162424803331791,
      "grad_norm": 2.813267469406128,
      "learning_rate": 0.09816751503933364,
      "loss": 0.2207,
      "step": 1980
    },
    {
      "epoch": 0.9208699676075891,
      "grad_norm": 4.974066257476807,
      "learning_rate": 0.09815826006478483,
      "loss": 0.2583,
      "step": 1990
    },
    {
      "epoch": 0.9254974548819991,
      "grad_norm": 4.538376808166504,
      "learning_rate": 0.098149005090236,
      "loss": 0.2122,
      "step": 2000
    },
    {
      "epoch": 0.9301249421564091,
      "grad_norm": 4.768242359161377,
      "learning_rate": 0.0981397501156872,
      "loss": 0.26,
      "step": 2010
    },
    {
      "epoch": 0.934752429430819,
      "grad_norm": 4.125535488128662,
      "learning_rate": 0.09813049514113836,
      "loss": 0.2304,
      "step": 2020
    },
    {
      "epoch": 0.939379916705229,
      "grad_norm": 3.018066883087158,
      "learning_rate": 0.09812124016658955,
      "loss": 0.2575,
      "step": 2030
    },
    {
      "epoch": 0.944007403979639,
      "grad_norm": 6.345213413238525,
      "learning_rate": 0.09811198519204073,
      "loss": 0.2365,
      "step": 2040
    },
    {
      "epoch": 0.9486348912540491,
      "grad_norm": 4.484162330627441,
      "learning_rate": 0.09810273021749191,
      "loss": 0.2102,
      "step": 2050
    },
    {
      "epoch": 0.9532623785284591,
      "grad_norm": 2.303513765335083,
      "learning_rate": 0.0980934752429431,
      "loss": 0.2203,
      "step": 2060
    },
    {
      "epoch": 0.9578898658028691,
      "grad_norm": 4.764201641082764,
      "learning_rate": 0.09808422026839426,
      "loss": 0.1771,
      "step": 2070
    },
    {
      "epoch": 0.962517353077279,
      "grad_norm": 4.035793304443359,
      "learning_rate": 0.09807496529384545,
      "loss": 0.2131,
      "step": 2080
    },
    {
      "epoch": 0.967144840351689,
      "grad_norm": 4.351743698120117,
      "learning_rate": 0.09806571031929662,
      "loss": 0.1925,
      "step": 2090
    },
    {
      "epoch": 0.971772327626099,
      "grad_norm": 3.2648396492004395,
      "learning_rate": 0.09805645534474781,
      "loss": 0.2487,
      "step": 2100
    },
    {
      "epoch": 0.976399814900509,
      "grad_norm": 4.702908992767334,
      "learning_rate": 0.09804720037019898,
      "loss": 0.228,
      "step": 2110
    },
    {
      "epoch": 0.9810273021749191,
      "grad_norm": 1.831469178199768,
      "learning_rate": 0.09803794539565017,
      "loss": 0.2903,
      "step": 2120
    },
    {
      "epoch": 0.985654789449329,
      "grad_norm": 4.367427349090576,
      "learning_rate": 0.09802869042110135,
      "loss": 0.2095,
      "step": 2130
    },
    {
      "epoch": 0.990282276723739,
      "grad_norm": 5.294801235198975,
      "learning_rate": 0.09801943544655253,
      "loss": 0.1873,
      "step": 2140
    },
    {
      "epoch": 0.994909763998149,
      "grad_norm": 6.532893657684326,
      "learning_rate": 0.09801018047200372,
      "loss": 0.2025,
      "step": 2150
    },
    {
      "epoch": 0.999537251272559,
      "grad_norm": 2.617418050765991,
      "learning_rate": 0.09800092549745489,
      "loss": 0.222,
      "step": 2160
    },
    {
      "epoch": 1.0,
      "eval_accuracy_branch1": 0.8684332152210753,
      "eval_accuracy_branch2": 0.5024649514712679,
      "eval_f1_branch1": 0.7802063142066443,
      "eval_f1_branch2": 0.49948918799613373,
      "eval_loss": 0.16134731471538544,
      "eval_precision_branch1": 0.8375975490699069,
      "eval_precision_branch2": 0.5025250005591878,
      "eval_recall_branch1": 0.8135978709530239,
      "eval_recall_branch2": 0.5024649514712679,
      "eval_runtime": 16.5291,
      "eval_samples_per_second": 785.403,
      "eval_steps_per_second": 98.191,
      "step": 2161
    },
    {
      "epoch": 1.004164738546969,
      "grad_norm": 4.296426773071289,
      "learning_rate": 0.09799167052290607,
      "loss": 0.396,
      "step": 2170
    },
    {
      "epoch": 1.008792225821379,
      "grad_norm": 3.416938543319702,
      "learning_rate": 0.09798241554835724,
      "loss": 0.1935,
      "step": 2180
    },
    {
      "epoch": 1.013419713095789,
      "grad_norm": 4.491221904754639,
      "learning_rate": 0.09797316057380843,
      "loss": 0.2182,
      "step": 2190
    },
    {
      "epoch": 1.018047200370199,
      "grad_norm": 3.5644431114196777,
      "learning_rate": 0.0979639055992596,
      "loss": 0.2156,
      "step": 2200
    },
    {
      "epoch": 1.022674687644609,
      "grad_norm": 2.869910955429077,
      "learning_rate": 0.09795465062471079,
      "loss": 0.1855,
      "step": 2210
    },
    {
      "epoch": 1.027302174919019,
      "grad_norm": 3.7604384422302246,
      "learning_rate": 0.09794539565016197,
      "loss": 0.2142,
      "step": 2220
    },
    {
      "epoch": 1.0319296621934289,
      "grad_norm": 5.920695781707764,
      "learning_rate": 0.09793614067561314,
      "loss": 0.1784,
      "step": 2230
    },
    {
      "epoch": 1.036557149467839,
      "grad_norm": 3.589383125305176,
      "learning_rate": 0.09792688570106434,
      "loss": 0.1836,
      "step": 2240
    },
    {
      "epoch": 1.041184636742249,
      "grad_norm": 8.573786735534668,
      "learning_rate": 0.0979176307265155,
      "loss": 0.2162,
      "step": 2250
    },
    {
      "epoch": 1.045812124016659,
      "grad_norm": 2.6508302688598633,
      "learning_rate": 0.09790837575196669,
      "loss": 0.1501,
      "step": 2260
    },
    {
      "epoch": 1.050439611291069,
      "grad_norm": 3.1871657371520996,
      "learning_rate": 0.09789912077741786,
      "loss": 0.2149,
      "step": 2270
    },
    {
      "epoch": 1.0550670985654789,
      "grad_norm": 4.410411357879639,
      "learning_rate": 0.09788986580286906,
      "loss": 0.205,
      "step": 2280
    },
    {
      "epoch": 1.059694585839889,
      "grad_norm": 1.7437721490859985,
      "learning_rate": 0.09788061082832022,
      "loss": 0.1918,
      "step": 2290
    },
    {
      "epoch": 1.0643220731142988,
      "grad_norm": 4.140513896942139,
      "learning_rate": 0.09787135585377141,
      "loss": 0.1827,
      "step": 2300
    },
    {
      "epoch": 1.068949560388709,
      "grad_norm": 4.869771957397461,
      "learning_rate": 0.09786210087922259,
      "loss": 0.1965,
      "step": 2310
    },
    {
      "epoch": 1.073577047663119,
      "grad_norm": 4.872440338134766,
      "learning_rate": 0.09785284590467376,
      "loss": 0.1888,
      "step": 2320
    },
    {
      "epoch": 1.0782045349375289,
      "grad_norm": 3.6826376914978027,
      "learning_rate": 0.09784359093012496,
      "loss": 0.2005,
      "step": 2330
    },
    {
      "epoch": 1.082832022211939,
      "grad_norm": 3.188537836074829,
      "learning_rate": 0.09783433595557613,
      "loss": 0.2392,
      "step": 2340
    },
    {
      "epoch": 1.0874595094863488,
      "grad_norm": 2.400789976119995,
      "learning_rate": 0.09782508098102731,
      "loss": 0.1498,
      "step": 2350
    },
    {
      "epoch": 1.092086996760759,
      "grad_norm": 4.505429744720459,
      "learning_rate": 0.09781582600647848,
      "loss": 0.1668,
      "step": 2360
    },
    {
      "epoch": 1.0967144840351688,
      "grad_norm": 5.239011287689209,
      "learning_rate": 0.09780657103192968,
      "loss": 0.1597,
      "step": 2370
    },
    {
      "epoch": 1.101341971309579,
      "grad_norm": 2.183818817138672,
      "learning_rate": 0.09779731605738085,
      "loss": 0.1698,
      "step": 2380
    },
    {
      "epoch": 1.105969458583989,
      "grad_norm": 2.095639944076538,
      "learning_rate": 0.09778806108283203,
      "loss": 0.2259,
      "step": 2390
    },
    {
      "epoch": 1.1105969458583989,
      "grad_norm": 4.575774192810059,
      "learning_rate": 0.09777880610828321,
      "loss": 0.2032,
      "step": 2400
    },
    {
      "epoch": 1.115224433132809,
      "grad_norm": 4.580210208892822,
      "learning_rate": 0.09776955113373438,
      "loss": 0.1754,
      "step": 2410
    },
    {
      "epoch": 1.1198519204072188,
      "grad_norm": 5.132493019104004,
      "learning_rate": 0.09776029615918558,
      "loss": 0.1528,
      "step": 2420
    },
    {
      "epoch": 1.124479407681629,
      "grad_norm": 3.985814094543457,
      "learning_rate": 0.09775104118463675,
      "loss": 0.1882,
      "step": 2430
    },
    {
      "epoch": 1.1291068949560388,
      "grad_norm": 2.514954090118408,
      "learning_rate": 0.09774178621008793,
      "loss": 0.1879,
      "step": 2440
    },
    {
      "epoch": 1.1337343822304489,
      "grad_norm": 4.751681327819824,
      "learning_rate": 0.0977325312355391,
      "loss": 0.1807,
      "step": 2450
    },
    {
      "epoch": 1.138361869504859,
      "grad_norm": 3.191465377807617,
      "learning_rate": 0.09772327626099028,
      "loss": 0.1925,
      "step": 2460
    },
    {
      "epoch": 1.1429893567792688,
      "grad_norm": 4.330513954162598,
      "learning_rate": 0.09771402128644147,
      "loss": 0.1304,
      "step": 2470
    },
    {
      "epoch": 1.147616844053679,
      "grad_norm": 4.524057865142822,
      "learning_rate": 0.09770476631189265,
      "loss": 0.1949,
      "step": 2480
    },
    {
      "epoch": 1.1522443313280888,
      "grad_norm": 4.018391132354736,
      "learning_rate": 0.09769551133734383,
      "loss": 0.1664,
      "step": 2490
    },
    {
      "epoch": 1.1568718186024989,
      "grad_norm": 4.961422920227051,
      "learning_rate": 0.097686256362795,
      "loss": 0.1419,
      "step": 2500
    },
    {
      "epoch": 1.1614993058769087,
      "grad_norm": 4.742071628570557,
      "learning_rate": 0.0976770013882462,
      "loss": 0.1469,
      "step": 2510
    },
    {
      "epoch": 1.1661267931513188,
      "grad_norm": 2.989645481109619,
      "learning_rate": 0.09766774641369737,
      "loss": 0.1701,
      "step": 2520
    },
    {
      "epoch": 1.170754280425729,
      "grad_norm": 3.6403064727783203,
      "learning_rate": 0.09765849143914855,
      "loss": 0.1455,
      "step": 2530
    },
    {
      "epoch": 1.1753817677001388,
      "grad_norm": 3.979269027709961,
      "learning_rate": 0.09764923646459972,
      "loss": 0.2124,
      "step": 2540
    },
    {
      "epoch": 1.180009254974549,
      "grad_norm": 7.703718185424805,
      "learning_rate": 0.0976399814900509,
      "loss": 0.1881,
      "step": 2550
    },
    {
      "epoch": 1.1846367422489588,
      "grad_norm": 4.528403282165527,
      "learning_rate": 0.09763072651550209,
      "loss": 0.2059,
      "step": 2560
    },
    {
      "epoch": 1.1892642295233689,
      "grad_norm": 3.057223320007324,
      "learning_rate": 0.09762147154095327,
      "loss": 0.1607,
      "step": 2570
    },
    {
      "epoch": 1.1938917167977787,
      "grad_norm": 5.718692779541016,
      "learning_rate": 0.09761221656640445,
      "loss": 0.1491,
      "step": 2580
    },
    {
      "epoch": 1.1985192040721888,
      "grad_norm": 6.497332572937012,
      "learning_rate": 0.09760296159185562,
      "loss": 0.1722,
      "step": 2590
    },
    {
      "epoch": 1.203146691346599,
      "grad_norm": 3.278903007507324,
      "learning_rate": 0.0975937066173068,
      "loss": 0.1552,
      "step": 2600
    },
    {
      "epoch": 1.2077741786210088,
      "grad_norm": 3.9454851150512695,
      "learning_rate": 0.09758445164275799,
      "loss": 0.1724,
      "step": 2610
    },
    {
      "epoch": 1.2124016658954189,
      "grad_norm": 4.857532024383545,
      "learning_rate": 0.09757519666820917,
      "loss": 0.182,
      "step": 2620
    },
    {
      "epoch": 1.2170291531698287,
      "grad_norm": 3.262946605682373,
      "learning_rate": 0.09756594169366034,
      "loss": 0.196,
      "step": 2630
    },
    {
      "epoch": 1.2216566404442388,
      "grad_norm": 2.276801824569702,
      "learning_rate": 0.09755668671911152,
      "loss": 0.2081,
      "step": 2640
    },
    {
      "epoch": 1.2262841277186487,
      "grad_norm": 6.593889236450195,
      "learning_rate": 0.09754743174456271,
      "loss": 0.1842,
      "step": 2650
    },
    {
      "epoch": 1.2309116149930588,
      "grad_norm": 4.886756420135498,
      "learning_rate": 0.09753817677001389,
      "loss": 0.1394,
      "step": 2660
    },
    {
      "epoch": 1.2355391022674689,
      "grad_norm": 3.7462093830108643,
      "learning_rate": 0.09752892179546507,
      "loss": 0.1552,
      "step": 2670
    },
    {
      "epoch": 1.2401665895418787,
      "grad_norm": 3.8007922172546387,
      "learning_rate": 0.09751966682091624,
      "loss": 0.1514,
      "step": 2680
    },
    {
      "epoch": 1.2447940768162888,
      "grad_norm": 2.7035579681396484,
      "learning_rate": 0.09751041184636743,
      "loss": 0.1268,
      "step": 2690
    },
    {
      "epoch": 1.2494215640906987,
      "grad_norm": 2.9642674922943115,
      "learning_rate": 0.09750115687181861,
      "loss": 0.1907,
      "step": 2700
    },
    {
      "epoch": 1.2540490513651088,
      "grad_norm": 7.2043914794921875,
      "learning_rate": 0.09749190189726979,
      "loss": 0.1897,
      "step": 2710
    },
    {
      "epoch": 1.2586765386395187,
      "grad_norm": 4.2486042976379395,
      "learning_rate": 0.09748264692272096,
      "loss": 0.1362,
      "step": 2720
    },
    {
      "epoch": 1.2633040259139288,
      "grad_norm": 4.2320661544799805,
      "learning_rate": 0.09747339194817214,
      "loss": 0.1575,
      "step": 2730
    },
    {
      "epoch": 1.2679315131883389,
      "grad_norm": 5.575689792633057,
      "learning_rate": 0.09746413697362333,
      "loss": 0.1615,
      "step": 2740
    },
    {
      "epoch": 1.2725590004627487,
      "grad_norm": 5.112056255340576,
      "learning_rate": 0.09745488199907451,
      "loss": 0.1792,
      "step": 2750
    },
    {
      "epoch": 1.2771864877371586,
      "grad_norm": 3.5198075771331787,
      "learning_rate": 0.0974456270245257,
      "loss": 0.1341,
      "step": 2760
    },
    {
      "epoch": 1.2818139750115687,
      "grad_norm": 7.85525369644165,
      "learning_rate": 0.09743637204997686,
      "loss": 0.1584,
      "step": 2770
    },
    {
      "epoch": 1.2864414622859788,
      "grad_norm": 2.470557928085327,
      "learning_rate": 0.09742711707542805,
      "loss": 0.2076,
      "step": 2780
    },
    {
      "epoch": 1.2910689495603886,
      "grad_norm": 3.7969911098480225,
      "learning_rate": 0.09741786210087923,
      "loss": 0.1209,
      "step": 2790
    },
    {
      "epoch": 1.2956964368347987,
      "grad_norm": 5.038512706756592,
      "learning_rate": 0.09740860712633041,
      "loss": 0.1573,
      "step": 2800
    },
    {
      "epoch": 1.3003239241092088,
      "grad_norm": 1.3753645420074463,
      "learning_rate": 0.09739935215178158,
      "loss": 0.1507,
      "step": 2810
    },
    {
      "epoch": 1.3049514113836187,
      "grad_norm": 3.041062116622925,
      "learning_rate": 0.09739009717723277,
      "loss": 0.1417,
      "step": 2820
    },
    {
      "epoch": 1.3095788986580286,
      "grad_norm": 5.008647441864014,
      "learning_rate": 0.09738084220268395,
      "loss": 0.2059,
      "step": 2830
    },
    {
      "epoch": 1.3142063859324387,
      "grad_norm": 3.6502487659454346,
      "learning_rate": 0.09737158722813513,
      "loss": 0.161,
      "step": 2840
    },
    {
      "epoch": 1.3188338732068488,
      "grad_norm": 2.4577720165252686,
      "learning_rate": 0.09736233225358631,
      "loss": 0.1495,
      "step": 2850
    },
    {
      "epoch": 1.3234613604812586,
      "grad_norm": 3.975057363510132,
      "learning_rate": 0.09735307727903748,
      "loss": 0.1664,
      "step": 2860
    },
    {
      "epoch": 1.3280888477556687,
      "grad_norm": 4.8035054206848145,
      "learning_rate": 0.09734382230448867,
      "loss": 0.186,
      "step": 2870
    },
    {
      "epoch": 1.3327163350300788,
      "grad_norm": 6.434144020080566,
      "learning_rate": 0.09733456732993985,
      "loss": 0.1549,
      "step": 2880
    },
    {
      "epoch": 1.3373438223044887,
      "grad_norm": 2.7800722122192383,
      "learning_rate": 0.09732531235539103,
      "loss": 0.1262,
      "step": 2890
    },
    {
      "epoch": 1.3419713095788985,
      "grad_norm": 1.6307858228683472,
      "learning_rate": 0.0973160573808422,
      "loss": 0.1398,
      "step": 2900
    },
    {
      "epoch": 1.3465987968533086,
      "grad_norm": 3.158078670501709,
      "learning_rate": 0.09730680240629339,
      "loss": 0.1271,
      "step": 2910
    },
    {
      "epoch": 1.3512262841277187,
      "grad_norm": 2.56510066986084,
      "learning_rate": 0.09729754743174457,
      "loss": 0.1308,
      "step": 2920
    },
    {
      "epoch": 1.3558537714021286,
      "grad_norm": 3.242190361022949,
      "learning_rate": 0.09728829245719575,
      "loss": 0.1494,
      "step": 2930
    },
    {
      "epoch": 1.3604812586765387,
      "grad_norm": 3.8279011249542236,
      "learning_rate": 0.09727903748264694,
      "loss": 0.1642,
      "step": 2940
    },
    {
      "epoch": 1.3651087459509486,
      "grad_norm": 3.1376562118530273,
      "learning_rate": 0.0972697825080981,
      "loss": 0.1521,
      "step": 2950
    },
    {
      "epoch": 1.3697362332253586,
      "grad_norm": 3.1366689205169678,
      "learning_rate": 0.09726052753354929,
      "loss": 0.1077,
      "step": 2960
    },
    {
      "epoch": 1.3743637204997685,
      "grad_norm": 2.347269058227539,
      "learning_rate": 0.09725127255900047,
      "loss": 0.1407,
      "step": 2970
    },
    {
      "epoch": 1.3789912077741786,
      "grad_norm": 1.5723929405212402,
      "learning_rate": 0.09724201758445165,
      "loss": 0.1012,
      "step": 2980
    },
    {
      "epoch": 1.3836186950485887,
      "grad_norm": 1.6227985620498657,
      "learning_rate": 0.09723276260990282,
      "loss": 0.1858,
      "step": 2990
    },
    {
      "epoch": 1.3882461823229986,
      "grad_norm": 3.747985601425171,
      "learning_rate": 0.097223507635354,
      "loss": 0.1215,
      "step": 3000
    },
    {
      "epoch": 1.3928736695974087,
      "grad_norm": 8.009430885314941,
      "learning_rate": 0.09721425266080519,
      "loss": 0.1281,
      "step": 3010
    },
    {
      "epoch": 1.3975011568718185,
      "grad_norm": 2.4799644947052,
      "learning_rate": 0.09720499768625637,
      "loss": 0.1306,
      "step": 3020
    },
    {
      "epoch": 1.4021286441462286,
      "grad_norm": 2.9297237396240234,
      "learning_rate": 0.09719574271170756,
      "loss": 0.1577,
      "step": 3030
    },
    {
      "epoch": 1.4067561314206385,
      "grad_norm": 1.8526662588119507,
      "learning_rate": 0.09718648773715872,
      "loss": 0.1286,
      "step": 3040
    },
    {
      "epoch": 1.4113836186950486,
      "grad_norm": 4.984649181365967,
      "learning_rate": 0.09717723276260991,
      "loss": 0.1566,
      "step": 3050
    },
    {
      "epoch": 1.4160111059694587,
      "grad_norm": 2.210902690887451,
      "learning_rate": 0.09716797778806108,
      "loss": 0.1002,
      "step": 3060
    },
    {
      "epoch": 1.4206385932438685,
      "grad_norm": 6.117252349853516,
      "learning_rate": 0.09715872281351227,
      "loss": 0.1192,
      "step": 3070
    },
    {
      "epoch": 1.4252660805182786,
      "grad_norm": 5.952739715576172,
      "learning_rate": 0.09714946783896344,
      "loss": 0.1991,
      "step": 3080
    },
    {
      "epoch": 1.4298935677926885,
      "grad_norm": 3.967966318130493,
      "learning_rate": 0.09714021286441463,
      "loss": 0.1214,
      "step": 3090
    },
    {
      "epoch": 1.4345210550670986,
      "grad_norm": 3.4102180004119873,
      "learning_rate": 0.09713095788986581,
      "loss": 0.1548,
      "step": 3100
    },
    {
      "epoch": 1.4391485423415085,
      "grad_norm": 4.654379367828369,
      "learning_rate": 0.097121702915317,
      "loss": 0.1231,
      "step": 3110
    },
    {
      "epoch": 1.4437760296159186,
      "grad_norm": 4.166568756103516,
      "learning_rate": 0.09711244794076818,
      "loss": 0.1296,
      "step": 3120
    },
    {
      "epoch": 1.4484035168903286,
      "grad_norm": 5.314073085784912,
      "learning_rate": 0.09710319296621935,
      "loss": 0.1595,
      "step": 3130
    },
    {
      "epoch": 1.4530310041647385,
      "grad_norm": 5.937757968902588,
      "learning_rate": 0.09709393799167053,
      "loss": 0.1833,
      "step": 3140
    },
    {
      "epoch": 1.4576584914391486,
      "grad_norm": 2.708341121673584,
      "learning_rate": 0.0970846830171217,
      "loss": 0.098,
      "step": 3150
    },
    {
      "epoch": 1.4622859787135585,
      "grad_norm": 2.1876065731048584,
      "learning_rate": 0.0970754280425729,
      "loss": 0.1247,
      "step": 3160
    },
    {
      "epoch": 1.4669134659879686,
      "grad_norm": 5.070809364318848,
      "learning_rate": 0.09706617306802406,
      "loss": 0.1532,
      "step": 3170
    },
    {
      "epoch": 1.4715409532623784,
      "grad_norm": 5.098237991333008,
      "learning_rate": 0.09705691809347525,
      "loss": 0.1749,
      "step": 3180
    },
    {
      "epoch": 1.4761684405367885,
      "grad_norm": 2.308793544769287,
      "learning_rate": 0.09704766311892643,
      "loss": 0.101,
      "step": 3190
    },
    {
      "epoch": 1.4807959278111986,
      "grad_norm": 3.026624917984009,
      "learning_rate": 0.09703840814437761,
      "loss": 0.1397,
      "step": 3200
    },
    {
      "epoch": 1.4854234150856085,
      "grad_norm": 3.0162570476531982,
      "learning_rate": 0.0970291531698288,
      "loss": 0.1101,
      "step": 3210
    },
    {
      "epoch": 1.4900509023600186,
      "grad_norm": 3.270226240158081,
      "learning_rate": 0.09701989819527997,
      "loss": 0.119,
      "step": 3220
    },
    {
      "epoch": 1.4946783896344285,
      "grad_norm": 2.4697916507720947,
      "learning_rate": 0.09701064322073115,
      "loss": 0.1223,
      "step": 3230
    },
    {
      "epoch": 1.4993058769088385,
      "grad_norm": 3.3055002689361572,
      "learning_rate": 0.09700138824618232,
      "loss": 0.1334,
      "step": 3240
    },
    {
      "epoch": 1.5039333641832484,
      "grad_norm": 3.284264326095581,
      "learning_rate": 0.09699213327163352,
      "loss": 0.135,
      "step": 3250
    },
    {
      "epoch": 1.5085608514576585,
      "grad_norm": 3.5055408477783203,
      "learning_rate": 0.09698287829708468,
      "loss": 0.1092,
      "step": 3260
    },
    {
      "epoch": 1.5131883387320686,
      "grad_norm": 3.0406908988952637,
      "learning_rate": 0.09697362332253587,
      "loss": 0.1463,
      "step": 3270
    },
    {
      "epoch": 1.5178158260064785,
      "grad_norm": 5.292878150939941,
      "learning_rate": 0.09696436834798705,
      "loss": 0.1612,
      "step": 3280
    },
    {
      "epoch": 1.5224433132808883,
      "grad_norm": 3.8211748600006104,
      "learning_rate": 0.09695511337343822,
      "loss": 0.1179,
      "step": 3290
    },
    {
      "epoch": 1.5270708005552984,
      "grad_norm": 3.653618574142456,
      "learning_rate": 0.09694585839888942,
      "loss": 0.0913,
      "step": 3300
    },
    {
      "epoch": 1.5316982878297085,
      "grad_norm": 5.4571967124938965,
      "learning_rate": 0.09693660342434059,
      "loss": 0.1251,
      "step": 3310
    },
    {
      "epoch": 1.5363257751041184,
      "grad_norm": 2.4358363151550293,
      "learning_rate": 0.09692734844979177,
      "loss": 0.0923,
      "step": 3320
    },
    {
      "epoch": 1.5409532623785285,
      "grad_norm": 3.676638603210449,
      "learning_rate": 0.09691809347524294,
      "loss": 0.1313,
      "step": 3330
    },
    {
      "epoch": 1.5455807496529386,
      "grad_norm": 2.813555955886841,
      "learning_rate": 0.09690883850069414,
      "loss": 0.122,
      "step": 3340
    },
    {
      "epoch": 1.5502082369273484,
      "grad_norm": 2.401794672012329,
      "learning_rate": 0.0968995835261453,
      "loss": 0.1152,
      "step": 3350
    },
    {
      "epoch": 1.5548357242017583,
      "grad_norm": 7.590295791625977,
      "learning_rate": 0.09689032855159649,
      "loss": 0.1303,
      "step": 3360
    },
    {
      "epoch": 1.5594632114761684,
      "grad_norm": 3.390906810760498,
      "learning_rate": 0.09688107357704767,
      "loss": 0.1292,
      "step": 3370
    },
    {
      "epoch": 1.5640906987505785,
      "grad_norm": 2.5421411991119385,
      "learning_rate": 0.09687181860249884,
      "loss": 0.0941,
      "step": 3380
    },
    {
      "epoch": 1.5687181860249884,
      "grad_norm": 2.2483835220336914,
      "learning_rate": 0.09686256362795004,
      "loss": 0.1507,
      "step": 3390
    },
    {
      "epoch": 1.5733456732993985,
      "grad_norm": 2.3423235416412354,
      "learning_rate": 0.09685330865340121,
      "loss": 0.1294,
      "step": 3400
    },
    {
      "epoch": 1.5779731605738085,
      "grad_norm": 2.8114616870880127,
      "learning_rate": 0.09684405367885239,
      "loss": 0.1364,
      "step": 3410
    },
    {
      "epoch": 1.5826006478482184,
      "grad_norm": 2.039083480834961,
      "learning_rate": 0.09683479870430356,
      "loss": 0.132,
      "step": 3420
    },
    {
      "epoch": 1.5872281351226283,
      "grad_norm": 4.345898151397705,
      "learning_rate": 0.09682554372975476,
      "loss": 0.1148,
      "step": 3430
    },
    {
      "epoch": 1.5918556223970384,
      "grad_norm": 4.14138650894165,
      "learning_rate": 0.09681628875520593,
      "loss": 0.119,
      "step": 3440
    },
    {
      "epoch": 1.5964831096714485,
      "grad_norm": 1.2959821224212646,
      "learning_rate": 0.09680703378065711,
      "loss": 0.0742,
      "step": 3450
    },
    {
      "epoch": 1.6011105969458583,
      "grad_norm": 5.427833080291748,
      "learning_rate": 0.09679777880610829,
      "loss": 0.124,
      "step": 3460
    },
    {
      "epoch": 1.6057380842202684,
      "grad_norm": 1.9189519882202148,
      "learning_rate": 0.09678852383155946,
      "loss": 0.1595,
      "step": 3470
    },
    {
      "epoch": 1.6103655714946785,
      "grad_norm": 3.9651522636413574,
      "learning_rate": 0.09677926885701066,
      "loss": 0.1101,
      "step": 3480
    },
    {
      "epoch": 1.6149930587690884,
      "grad_norm": 4.206417560577393,
      "learning_rate": 0.09677001388246183,
      "loss": 0.0893,
      "step": 3490
    },
    {
      "epoch": 1.6196205460434983,
      "grad_norm": 2.1402456760406494,
      "learning_rate": 0.09676075890791301,
      "loss": 0.1209,
      "step": 3500
    },
    {
      "epoch": 1.6242480333179083,
      "grad_norm": 2.8551547527313232,
      "learning_rate": 0.09675150393336418,
      "loss": 0.1231,
      "step": 3510
    },
    {
      "epoch": 1.6288755205923184,
      "grad_norm": 4.477281093597412,
      "learning_rate": 0.09674224895881536,
      "loss": 0.1697,
      "step": 3520
    },
    {
      "epoch": 1.6335030078667283,
      "grad_norm": 3.0826473236083984,
      "learning_rate": 0.09673299398426655,
      "loss": 0.1276,
      "step": 3530
    },
    {
      "epoch": 1.6381304951411384,
      "grad_norm": 3.1376407146453857,
      "learning_rate": 0.09672373900971773,
      "loss": 0.1279,
      "step": 3540
    },
    {
      "epoch": 1.6427579824155485,
      "grad_norm": 4.966737747192383,
      "learning_rate": 0.09671448403516891,
      "loss": 0.1177,
      "step": 3550
    },
    {
      "epoch": 1.6473854696899584,
      "grad_norm": 1.3299241065979004,
      "learning_rate": 0.09670522906062008,
      "loss": 0.0879,
      "step": 3560
    },
    {
      "epoch": 1.6520129569643682,
      "grad_norm": 5.125261306762695,
      "learning_rate": 0.09669597408607128,
      "loss": 0.1065,
      "step": 3570
    },
    {
      "epoch": 1.6566404442387783,
      "grad_norm": 2.5485177040100098,
      "learning_rate": 0.09668671911152245,
      "loss": 0.0986,
      "step": 3580
    },
    {
      "epoch": 1.6612679315131884,
      "grad_norm": 5.234419345855713,
      "learning_rate": 0.09667746413697363,
      "loss": 0.1166,
      "step": 3590
    },
    {
      "epoch": 1.6658954187875983,
      "grad_norm": 2.102033853530884,
      "learning_rate": 0.0966682091624248,
      "loss": 0.1069,
      "step": 3600
    },
    {
      "epoch": 1.6705229060620084,
      "grad_norm": 2.3665003776550293,
      "learning_rate": 0.09665895418787598,
      "loss": 0.0802,
      "step": 3610
    },
    {
      "epoch": 1.6751503933364185,
      "grad_norm": 3.229524850845337,
      "learning_rate": 0.09664969921332717,
      "loss": 0.0895,
      "step": 3620
    },
    {
      "epoch": 1.6797778806108283,
      "grad_norm": 4.892857074737549,
      "learning_rate": 0.09664044423877835,
      "loss": 0.097,
      "step": 3630
    },
    {
      "epoch": 1.6844053678852382,
      "grad_norm": 1.6632310152053833,
      "learning_rate": 0.09663118926422953,
      "loss": 0.0836,
      "step": 3640
    },
    {
      "epoch": 1.6890328551596483,
      "grad_norm": 3.3476622104644775,
      "learning_rate": 0.0966219342896807,
      "loss": 0.0831,
      "step": 3650
    },
    {
      "epoch": 1.6936603424340584,
      "grad_norm": 1.944352388381958,
      "learning_rate": 0.0966126793151319,
      "loss": 0.1135,
      "step": 3660
    },
    {
      "epoch": 1.6982878297084683,
      "grad_norm": 4.047804832458496,
      "learning_rate": 0.09660342434058307,
      "loss": 0.1107,
      "step": 3670
    },
    {
      "epoch": 1.7029153169828783,
      "grad_norm": 2.3656985759735107,
      "learning_rate": 0.09659416936603425,
      "loss": 0.1034,
      "step": 3680
    },
    {
      "epoch": 1.7075428042572884,
      "grad_norm": 5.018238067626953,
      "learning_rate": 0.09658491439148542,
      "loss": 0.1492,
      "step": 3690
    },
    {
      "epoch": 1.7121702915316983,
      "grad_norm": 3.732703924179077,
      "learning_rate": 0.0965756594169366,
      "loss": 0.0844,
      "step": 3700
    },
    {
      "epoch": 1.7167977788061082,
      "grad_norm": 4.79712438583374,
      "learning_rate": 0.09656640444238779,
      "loss": 0.1047,
      "step": 3710
    },
    {
      "epoch": 1.7214252660805183,
      "grad_norm": 6.159112930297852,
      "learning_rate": 0.09655714946783897,
      "loss": 0.1552,
      "step": 3720
    },
    {
      "epoch": 1.7260527533549284,
      "grad_norm": 1.368095874786377,
      "learning_rate": 0.09654789449329015,
      "loss": 0.0991,
      "step": 3730
    },
    {
      "epoch": 1.7306802406293382,
      "grad_norm": 0.7127138376235962,
      "learning_rate": 0.09653863951874132,
      "loss": 0.0996,
      "step": 3740
    },
    {
      "epoch": 1.735307727903748,
      "grad_norm": 2.680210590362549,
      "learning_rate": 0.0965293845441925,
      "loss": 0.0689,
      "step": 3750
    },
    {
      "epoch": 1.7399352151781584,
      "grad_norm": 2.9786806106567383,
      "learning_rate": 0.09652012956964369,
      "loss": 0.1698,
      "step": 3760
    },
    {
      "epoch": 1.7445627024525683,
      "grad_norm": 4.259860515594482,
      "learning_rate": 0.09651087459509487,
      "loss": 0.092,
      "step": 3770
    },
    {
      "epoch": 1.7491901897269782,
      "grad_norm": 2.7589950561523438,
      "learning_rate": 0.09650161962054604,
      "loss": 0.1041,
      "step": 3780
    },
    {
      "epoch": 1.7538176770013882,
      "grad_norm": 5.013893127441406,
      "learning_rate": 0.09649236464599723,
      "loss": 0.0994,
      "step": 3790
    },
    {
      "epoch": 1.7584451642757983,
      "grad_norm": 1.6035019159317017,
      "learning_rate": 0.09648310967144841,
      "loss": 0.0823,
      "step": 3800
    },
    {
      "epoch": 1.7630726515502082,
      "grad_norm": 4.079823970794678,
      "learning_rate": 0.09647385469689959,
      "loss": 0.1057,
      "step": 3810
    },
    {
      "epoch": 1.767700138824618,
      "grad_norm": 0.8521233201026917,
      "learning_rate": 0.09646459972235077,
      "loss": 0.1271,
      "step": 3820
    },
    {
      "epoch": 1.7723276260990284,
      "grad_norm": 0.4861210286617279,
      "learning_rate": 0.09645534474780194,
      "loss": 0.1084,
      "step": 3830
    },
    {
      "epoch": 1.7769551133734383,
      "grad_norm": 3.644803047180176,
      "learning_rate": 0.09644608977325313,
      "loss": 0.0834,
      "step": 3840
    },
    {
      "epoch": 1.7815826006478481,
      "grad_norm": 5.321638584136963,
      "learning_rate": 0.09643683479870431,
      "loss": 0.0891,
      "step": 3850
    },
    {
      "epoch": 1.7862100879222582,
      "grad_norm": 8.445669174194336,
      "learning_rate": 0.0964275798241555,
      "loss": 0.1162,
      "step": 3860
    },
    {
      "epoch": 1.7908375751966683,
      "grad_norm": 1.000528335571289,
      "learning_rate": 0.09641832484960666,
      "loss": 0.0773,
      "step": 3870
    },
    {
      "epoch": 1.7954650624710782,
      "grad_norm": 2.7206475734710693,
      "learning_rate": 0.09640906987505785,
      "loss": 0.0798,
      "step": 3880
    },
    {
      "epoch": 1.800092549745488,
      "grad_norm": 4.358487129211426,
      "learning_rate": 0.09639981490050903,
      "loss": 0.1029,
      "step": 3890
    },
    {
      "epoch": 1.8047200370198984,
      "grad_norm": 2.889360189437866,
      "learning_rate": 0.09639055992596021,
      "loss": 0.1079,
      "step": 3900
    },
    {
      "epoch": 1.8093475242943082,
      "grad_norm": 3.7917838096618652,
      "learning_rate": 0.0963813049514114,
      "loss": 0.1033,
      "step": 3910
    },
    {
      "epoch": 1.813975011568718,
      "grad_norm": 3.8476064205169678,
      "learning_rate": 0.09637204997686256,
      "loss": 0.1182,
      "step": 3920
    },
    {
      "epoch": 1.8186024988431282,
      "grad_norm": 1.8438324928283691,
      "learning_rate": 0.09636279500231375,
      "loss": 0.1006,
      "step": 3930
    },
    {
      "epoch": 1.8232299861175383,
      "grad_norm": 3.8009047508239746,
      "learning_rate": 0.09635354002776493,
      "loss": 0.0767,
      "step": 3940
    },
    {
      "epoch": 1.8278574733919482,
      "grad_norm": 4.178678035736084,
      "learning_rate": 0.09634428505321611,
      "loss": 0.1445,
      "step": 3950
    },
    {
      "epoch": 1.832484960666358,
      "grad_norm": 6.302729606628418,
      "learning_rate": 0.09633503007866728,
      "loss": 0.1268,
      "step": 3960
    },
    {
      "epoch": 1.8371124479407681,
      "grad_norm": 4.093457221984863,
      "learning_rate": 0.09632577510411847,
      "loss": 0.0991,
      "step": 3970
    },
    {
      "epoch": 1.8417399352151782,
      "grad_norm": 2.4840433597564697,
      "learning_rate": 0.09631652012956965,
      "loss": 0.0789,
      "step": 3980
    },
    {
      "epoch": 1.846367422489588,
      "grad_norm": 2.4643476009368896,
      "learning_rate": 0.09630726515502083,
      "loss": 0.0894,
      "step": 3990
    },
    {
      "epoch": 1.8509949097639982,
      "grad_norm": 4.392586708068848,
      "learning_rate": 0.09629801018047202,
      "loss": 0.1025,
      "step": 4000
    },
    {
      "epoch": 1.8556223970384083,
      "grad_norm": 0.9173155426979065,
      "learning_rate": 0.09628875520592318,
      "loss": 0.0895,
      "step": 4010
    },
    {
      "epoch": 1.8602498843128181,
      "grad_norm": 0.9238767623901367,
      "learning_rate": 0.09627950023137437,
      "loss": 0.0892,
      "step": 4020
    },
    {
      "epoch": 1.864877371587228,
      "grad_norm": 2.845728874206543,
      "learning_rate": 0.09627024525682555,
      "loss": 0.0669,
      "step": 4030
    },
    {
      "epoch": 1.869504858861638,
      "grad_norm": 1.648269534111023,
      "learning_rate": 0.09626099028227673,
      "loss": 0.073,
      "step": 4040
    },
    {
      "epoch": 1.8741323461360482,
      "grad_norm": 0.7319415211677551,
      "learning_rate": 0.0962517353077279,
      "loss": 0.0718,
      "step": 4050
    },
    {
      "epoch": 1.878759833410458,
      "grad_norm": 4.477851390838623,
      "learning_rate": 0.09624248033317909,
      "loss": 0.1107,
      "step": 4060
    },
    {
      "epoch": 1.8833873206848681,
      "grad_norm": 4.7089738845825195,
      "learning_rate": 0.09623322535863027,
      "loss": 0.1094,
      "step": 4070
    },
    {
      "epoch": 1.8880148079592782,
      "grad_norm": 4.65480899810791,
      "learning_rate": 0.09622397038408145,
      "loss": 0.1068,
      "step": 4080
    },
    {
      "epoch": 1.892642295233688,
      "grad_norm": 6.39229154586792,
      "learning_rate": 0.09621471540953264,
      "loss": 0.0604,
      "step": 4090
    },
    {
      "epoch": 1.897269782508098,
      "grad_norm": 3.535193681716919,
      "learning_rate": 0.0962054604349838,
      "loss": 0.0901,
      "step": 4100
    },
    {
      "epoch": 1.901897269782508,
      "grad_norm": 2.916599750518799,
      "learning_rate": 0.09619620546043499,
      "loss": 0.0804,
      "step": 4110
    },
    {
      "epoch": 1.9065247570569182,
      "grad_norm": 3.042659044265747,
      "learning_rate": 0.09618695048588617,
      "loss": 0.0817,
      "step": 4120
    },
    {
      "epoch": 1.911152244331328,
      "grad_norm": 4.737796783447266,
      "learning_rate": 0.09617769551133735,
      "loss": 0.0454,
      "step": 4130
    },
    {
      "epoch": 1.9157797316057381,
      "grad_norm": 4.057162761688232,
      "learning_rate": 0.09616844053678852,
      "loss": 0.0658,
      "step": 4140
    },
    {
      "epoch": 1.9204072188801482,
      "grad_norm": 6.436202049255371,
      "learning_rate": 0.09615918556223971,
      "loss": 0.162,
      "step": 4150
    },
    {
      "epoch": 1.925034706154558,
      "grad_norm": 4.236896514892578,
      "learning_rate": 0.09614993058769089,
      "loss": 0.1393,
      "step": 4160
    },
    {
      "epoch": 1.929662193428968,
      "grad_norm": 3.8916866779327393,
      "learning_rate": 0.09614067561314207,
      "loss": 0.1006,
      "step": 4170
    },
    {
      "epoch": 1.934289680703378,
      "grad_norm": 2.9338080883026123,
      "learning_rate": 0.09613142063859326,
      "loss": 0.0573,
      "step": 4180
    },
    {
      "epoch": 1.9389171679777881,
      "grad_norm": 4.189176082611084,
      "learning_rate": 0.09612216566404443,
      "loss": 0.0909,
      "step": 4190
    },
    {
      "epoch": 1.943544655252198,
      "grad_norm": 3.6637916564941406,
      "learning_rate": 0.09611291068949561,
      "loss": 0.0669,
      "step": 4200
    },
    {
      "epoch": 1.948172142526608,
      "grad_norm": 5.674232006072998,
      "learning_rate": 0.09610365571494678,
      "loss": 0.0814,
      "step": 4210
    },
    {
      "epoch": 1.9527996298010182,
      "grad_norm": 2.6031112670898438,
      "learning_rate": 0.09609440074039798,
      "loss": 0.0905,
      "step": 4220
    },
    {
      "epoch": 1.957427117075428,
      "grad_norm": 2.131920099258423,
      "learning_rate": 0.09608514576584914,
      "loss": 0.0916,
      "step": 4230
    },
    {
      "epoch": 1.962054604349838,
      "grad_norm": 5.746319770812988,
      "learning_rate": 0.09607589079130033,
      "loss": 0.0747,
      "step": 4240
    },
    {
      "epoch": 1.966682091624248,
      "grad_norm": 1.5836058855056763,
      "learning_rate": 0.09606663581675151,
      "loss": 0.0618,
      "step": 4250
    },
    {
      "epoch": 1.971309578898658,
      "grad_norm": 3.470141649246216,
      "learning_rate": 0.0960573808422027,
      "loss": 0.095,
      "step": 4260
    },
    {
      "epoch": 1.975937066173068,
      "grad_norm": 7.206092357635498,
      "learning_rate": 0.09604812586765388,
      "loss": 0.0852,
      "step": 4270
    },
    {
      "epoch": 1.980564553447478,
      "grad_norm": 4.388459205627441,
      "learning_rate": 0.09603887089310505,
      "loss": 0.1017,
      "step": 4280
    },
    {
      "epoch": 1.9851920407218882,
      "grad_norm": 4.719480514526367,
      "learning_rate": 0.09602961591855623,
      "loss": 0.0839,
      "step": 4290
    },
    {
      "epoch": 1.989819527996298,
      "grad_norm": 1.7422949075698853,
      "learning_rate": 0.0960203609440074,
      "loss": 0.0452,
      "step": 4300
    },
    {
      "epoch": 1.994447015270708,
      "grad_norm": 5.397147178649902,
      "learning_rate": 0.0960111059694586,
      "loss": 0.0743,
      "step": 4310
    },
    {
      "epoch": 1.999074502545118,
      "grad_norm": 3.366739511489868,
      "learning_rate": 0.09600185099490977,
      "loss": 0.0897,
      "step": 4320
    },
    {
      "epoch": 2.0,
      "eval_accuracy_branch1": 0.9661069172700663,
      "eval_accuracy_branch2": 0.500616237867817,
      "eval_f1_branch1": 0.9650725749937518,
      "eval_f1_branch2": 0.4942457092662542,
      "eval_loss": 0.062231723219156265,
      "eval_precision_branch1": 0.9630372380014606,
      "eval_precision_branch2": 0.5006489340026821,
      "eval_recall_branch1": 0.9678855010188271,
      "eval_recall_branch2": 0.500616237867817,
      "eval_runtime": 29.1578,
      "eval_samples_per_second": 445.233,
      "eval_steps_per_second": 55.663,
      "step": 4322
    },
    {
      "epoch": 2.003701989819528,
      "grad_norm": 2.8655025959014893,
      "learning_rate": 0.09599259602036095,
      "loss": 0.1499,
      "step": 4330
    },
    {
      "epoch": 2.008329477093938,
      "grad_norm": 0.9871217012405396,
      "learning_rate": 0.09598334104581213,
      "loss": 0.0944,
      "step": 4340
    },
    {
      "epoch": 2.012956964368348,
      "grad_norm": 1.0555797815322876,
      "learning_rate": 0.09597408607126331,
      "loss": 0.1113,
      "step": 4350
    },
    {
      "epoch": 2.017584451642758,
      "grad_norm": 2.780998706817627,
      "learning_rate": 0.0959648310967145,
      "loss": 0.0619,
      "step": 4360
    },
    {
      "epoch": 2.022211938917168,
      "grad_norm": 3.6736841201782227,
      "learning_rate": 0.09595557612216567,
      "loss": 0.0821,
      "step": 4370
    },
    {
      "epoch": 2.026839426191578,
      "grad_norm": 2.5564301013946533,
      "learning_rate": 0.09594632114761685,
      "loss": 0.0409,
      "step": 4380
    },
    {
      "epoch": 2.031466913465988,
      "grad_norm": 7.1816182136535645,
      "learning_rate": 0.09593706617306802,
      "loss": 0.0756,
      "step": 4390
    },
    {
      "epoch": 2.036094400740398,
      "grad_norm": 1.1055444478988647,
      "learning_rate": 0.09592781119851922,
      "loss": 0.1045,
      "step": 4400
    },
    {
      "epoch": 2.040721888014808,
      "grad_norm": 2.7740585803985596,
      "learning_rate": 0.09591855622397039,
      "loss": 0.0941,
      "step": 4410
    },
    {
      "epoch": 2.045349375289218,
      "grad_norm": 2.507610559463501,
      "learning_rate": 0.09590930124942157,
      "loss": 0.0553,
      "step": 4420
    },
    {
      "epoch": 2.049976862563628,
      "grad_norm": 4.726449966430664,
      "learning_rate": 0.09590004627487275,
      "loss": 0.0888,
      "step": 4430
    },
    {
      "epoch": 2.054604349838038,
      "grad_norm": 0.6065084934234619,
      "learning_rate": 0.09589079130032392,
      "loss": 0.0802,
      "step": 4440
    },
    {
      "epoch": 2.059231837112448,
      "grad_norm": 1.6166261434555054,
      "learning_rate": 0.09588153632577512,
      "loss": 0.0775,
      "step": 4450
    },
    {
      "epoch": 2.0638593243868577,
      "grad_norm": 2.769381046295166,
      "learning_rate": 0.09587228135122629,
      "loss": 0.0723,
      "step": 4460
    },
    {
      "epoch": 2.068486811661268,
      "grad_norm": 2.360661506652832,
      "learning_rate": 0.09586302637667747,
      "loss": 0.0605,
      "step": 4470
    },
    {
      "epoch": 2.073114298935678,
      "grad_norm": 5.306640625,
      "learning_rate": 0.09585377140212864,
      "loss": 0.0754,
      "step": 4480
    },
    {
      "epoch": 2.0777417862100878,
      "grad_norm": 3.6764063835144043,
      "learning_rate": 0.09584451642757984,
      "loss": 0.0618,
      "step": 4490
    },
    {
      "epoch": 2.082369273484498,
      "grad_norm": 4.142328262329102,
      "learning_rate": 0.095835261453031,
      "loss": 0.0624,
      "step": 4500
    },
    {
      "epoch": 2.086996760758908,
      "grad_norm": 2.118925094604492,
      "learning_rate": 0.09582600647848219,
      "loss": 0.0673,
      "step": 4510
    },
    {
      "epoch": 2.091624248033318,
      "grad_norm": 6.45722770690918,
      "learning_rate": 0.09581675150393337,
      "loss": 0.0972,
      "step": 4520
    },
    {
      "epoch": 2.096251735307728,
      "grad_norm": 3.067247152328491,
      "learning_rate": 0.09580749652938454,
      "loss": 0.0552,
      "step": 4530
    },
    {
      "epoch": 2.100879222582138,
      "grad_norm": 3.2666542530059814,
      "learning_rate": 0.09579824155483574,
      "loss": 0.0579,
      "step": 4540
    },
    {
      "epoch": 2.105506709856548,
      "grad_norm": 5.38088846206665,
      "learning_rate": 0.09578898658028691,
      "loss": 0.1132,
      "step": 4550
    },
    {
      "epoch": 2.1101341971309577,
      "grad_norm": 2.451371908187866,
      "learning_rate": 0.09577973160573809,
      "loss": 0.0924,
      "step": 4560
    },
    {
      "epoch": 2.114761684405368,
      "grad_norm": 6.100115776062012,
      "learning_rate": 0.09577047663118926,
      "loss": 0.0921,
      "step": 4570
    },
    {
      "epoch": 2.119389171679778,
      "grad_norm": 10.137146949768066,
      "learning_rate": 0.09576122165664046,
      "loss": 0.0822,
      "step": 4580
    },
    {
      "epoch": 2.124016658954188,
      "grad_norm": 7.2314558029174805,
      "learning_rate": 0.09575196668209163,
      "loss": 0.0896,
      "step": 4590
    },
    {
      "epoch": 2.1286441462285977,
      "grad_norm": 1.0759031772613525,
      "learning_rate": 0.09574271170754281,
      "loss": 0.1129,
      "step": 4600
    },
    {
      "epoch": 2.133271633503008,
      "grad_norm": 0.9956883788108826,
      "learning_rate": 0.095733456732994,
      "loss": 0.0667,
      "step": 4610
    },
    {
      "epoch": 2.137899120777418,
      "grad_norm": 3.267305374145508,
      "learning_rate": 0.09572420175844516,
      "loss": 0.1012,
      "step": 4620
    },
    {
      "epoch": 2.1425266080518277,
      "grad_norm": 5.754367828369141,
      "learning_rate": 0.09571494678389636,
      "loss": 0.0926,
      "step": 4630
    },
    {
      "epoch": 2.147154095326238,
      "grad_norm": 5.351354122161865,
      "learning_rate": 0.09570569180934753,
      "loss": 0.0661,
      "step": 4640
    },
    {
      "epoch": 2.151781582600648,
      "grad_norm": 3.289447069168091,
      "learning_rate": 0.09569643683479871,
      "loss": 0.0719,
      "step": 4650
    },
    {
      "epoch": 2.1564090698750578,
      "grad_norm": 3.8061363697052,
      "learning_rate": 0.09568718186024988,
      "loss": 0.0809,
      "step": 4660
    },
    {
      "epoch": 2.161036557149468,
      "grad_norm": 3.669797420501709,
      "learning_rate": 0.09567792688570106,
      "loss": 0.0716,
      "step": 4670
    },
    {
      "epoch": 2.165664044423878,
      "grad_norm": 3.4959006309509277,
      "learning_rate": 0.09566867191115225,
      "loss": 0.0878,
      "step": 4680
    },
    {
      "epoch": 2.170291531698288,
      "grad_norm": 2.172741413116455,
      "learning_rate": 0.09565941693660343,
      "loss": 0.0779,
      "step": 4690
    },
    {
      "epoch": 2.1749190189726977,
      "grad_norm": 3.6695094108581543,
      "learning_rate": 0.09565016196205461,
      "loss": 0.074,
      "step": 4700
    },
    {
      "epoch": 2.179546506247108,
      "grad_norm": 0.25316011905670166,
      "learning_rate": 0.09564090698750578,
      "loss": 0.0669,
      "step": 4710
    },
    {
      "epoch": 2.184173993521518,
      "grad_norm": 2.110579252243042,
      "learning_rate": 0.09563165201295698,
      "loss": 0.0621,
      "step": 4720
    },
    {
      "epoch": 2.1888014807959277,
      "grad_norm": 4.015868186950684,
      "learning_rate": 0.09562239703840815,
      "loss": 0.1032,
      "step": 4730
    },
    {
      "epoch": 2.1934289680703376,
      "grad_norm": 8.513344764709473,
      "learning_rate": 0.09561314206385933,
      "loss": 0.1264,
      "step": 4740
    },
    {
      "epoch": 2.198056455344748,
      "grad_norm": 0.7579340934753418,
      "learning_rate": 0.0956038870893105,
      "loss": 0.0565,
      "step": 4750
    },
    {
      "epoch": 2.202683942619158,
      "grad_norm": 4.8363356590271,
      "learning_rate": 0.09559463211476169,
      "loss": 0.0627,
      "step": 4760
    },
    {
      "epoch": 2.2073114298935677,
      "grad_norm": 6.493096351623535,
      "learning_rate": 0.09558537714021287,
      "loss": 0.069,
      "step": 4770
    },
    {
      "epoch": 2.211938917167978,
      "grad_norm": 2.847322463989258,
      "learning_rate": 0.09557612216566405,
      "loss": 0.0651,
      "step": 4780
    },
    {
      "epoch": 2.216566404442388,
      "grad_norm": 2.029299259185791,
      "learning_rate": 0.09556686719111523,
      "loss": 0.0816,
      "step": 4790
    },
    {
      "epoch": 2.2211938917167977,
      "grad_norm": 3.2517378330230713,
      "learning_rate": 0.0955576122165664,
      "loss": 0.0521,
      "step": 4800
    },
    {
      "epoch": 2.2258213789912076,
      "grad_norm": 0.978894054889679,
      "learning_rate": 0.0955483572420176,
      "loss": 0.0936,
      "step": 4810
    },
    {
      "epoch": 2.230448866265618,
      "grad_norm": 3.4712164402008057,
      "learning_rate": 0.09553910226746877,
      "loss": 0.1031,
      "step": 4820
    },
    {
      "epoch": 2.2350763535400278,
      "grad_norm": 2.873673915863037,
      "learning_rate": 0.09552984729291995,
      "loss": 0.0589,
      "step": 4830
    },
    {
      "epoch": 2.2397038408144376,
      "grad_norm": 4.298067092895508,
      "learning_rate": 0.09552059231837112,
      "loss": 0.0951,
      "step": 4840
    },
    {
      "epoch": 2.244331328088848,
      "grad_norm": 4.609220027923584,
      "learning_rate": 0.0955113373438223,
      "loss": 0.0556,
      "step": 4850
    },
    {
      "epoch": 2.248958815363258,
      "grad_norm": 5.0674638748168945,
      "learning_rate": 0.09550208236927349,
      "loss": 0.069,
      "step": 4860
    },
    {
      "epoch": 2.2535863026376677,
      "grad_norm": 1.6179077625274658,
      "learning_rate": 0.09549282739472467,
      "loss": 0.0591,
      "step": 4870
    },
    {
      "epoch": 2.2582137899120776,
      "grad_norm": 3.0621819496154785,
      "learning_rate": 0.09548357242017586,
      "loss": 0.0717,
      "step": 4880
    },
    {
      "epoch": 2.262841277186488,
      "grad_norm": 5.720998764038086,
      "learning_rate": 0.09547431744562702,
      "loss": 0.0891,
      "step": 4890
    },
    {
      "epoch": 2.2674687644608977,
      "grad_norm": 10.69343090057373,
      "learning_rate": 0.09546506247107821,
      "loss": 0.1215,
      "step": 4900
    },
    {
      "epoch": 2.2720962517353076,
      "grad_norm": 4.401549816131592,
      "learning_rate": 0.09545580749652939,
      "loss": 0.0907,
      "step": 4910
    },
    {
      "epoch": 2.276723739009718,
      "grad_norm": 3.88519024848938,
      "learning_rate": 0.09544655252198057,
      "loss": 0.0632,
      "step": 4920
    },
    {
      "epoch": 2.281351226284128,
      "grad_norm": 2.0684914588928223,
      "learning_rate": 0.09543729754743174,
      "loss": 0.0674,
      "step": 4930
    },
    {
      "epoch": 2.2859787135585377,
      "grad_norm": 3.615838050842285,
      "learning_rate": 0.09542804257288293,
      "loss": 0.0675,
      "step": 4940
    },
    {
      "epoch": 2.2906062008329475,
      "grad_norm": 5.388014316558838,
      "learning_rate": 0.09541878759833411,
      "loss": 0.0654,
      "step": 4950
    },
    {
      "epoch": 2.295233688107358,
      "grad_norm": 1.0490859746932983,
      "learning_rate": 0.09540953262378529,
      "loss": 0.0682,
      "step": 4960
    },
    {
      "epoch": 2.2998611753817677,
      "grad_norm": 2.201291561126709,
      "learning_rate": 0.09540027764923648,
      "loss": 0.0468,
      "step": 4970
    },
    {
      "epoch": 2.3044886626561776,
      "grad_norm": 2.425501823425293,
      "learning_rate": 0.09539102267468764,
      "loss": 0.102,
      "step": 4980
    },
    {
      "epoch": 2.3091161499305874,
      "grad_norm": 0.5163938403129578,
      "learning_rate": 0.09538176770013883,
      "loss": 0.0976,
      "step": 4990
    },
    {
      "epoch": 2.3137436372049978,
      "grad_norm": 1.0209676027297974,
      "learning_rate": 0.09537251272559001,
      "loss": 0.0337,
      "step": 5000
    },
    {
      "epoch": 2.3183711244794076,
      "grad_norm": 1.06680428981781,
      "learning_rate": 0.0953632577510412,
      "loss": 0.07,
      "step": 5010
    },
    {
      "epoch": 2.3229986117538175,
      "grad_norm": 2.9170148372650146,
      "learning_rate": 0.09535400277649236,
      "loss": 0.0775,
      "step": 5020
    },
    {
      "epoch": 2.327626099028228,
      "grad_norm": 5.16068172454834,
      "learning_rate": 0.09534474780194355,
      "loss": 0.1062,
      "step": 5030
    },
    {
      "epoch": 2.3322535863026377,
      "grad_norm": 0.6094755530357361,
      "learning_rate": 0.09533549282739473,
      "loss": 0.0833,
      "step": 5040
    },
    {
      "epoch": 2.3368810735770476,
      "grad_norm": 4.16580867767334,
      "learning_rate": 0.09532623785284591,
      "loss": 0.0591,
      "step": 5050
    },
    {
      "epoch": 2.341508560851458,
      "grad_norm": 3.7913718223571777,
      "learning_rate": 0.0953169828782971,
      "loss": 0.0491,
      "step": 5060
    },
    {
      "epoch": 2.3461360481258677,
      "grad_norm": 2.520615339279175,
      "learning_rate": 0.09530772790374827,
      "loss": 0.0464,
      "step": 5070
    },
    {
      "epoch": 2.3507635354002776,
      "grad_norm": 5.951999664306641,
      "learning_rate": 0.09529847292919945,
      "loss": 0.0555,
      "step": 5080
    },
    {
      "epoch": 2.3553910226746875,
      "grad_norm": 1.2486798763275146,
      "learning_rate": 0.09528921795465063,
      "loss": 0.0639,
      "step": 5090
    },
    {
      "epoch": 2.360018509949098,
      "grad_norm": 5.0196123123168945,
      "learning_rate": 0.09527996298010181,
      "loss": 0.0628,
      "step": 5100
    },
    {
      "epoch": 2.3646459972235077,
      "grad_norm": 4.565728187561035,
      "learning_rate": 0.09527070800555298,
      "loss": 0.0657,
      "step": 5110
    },
    {
      "epoch": 2.3692734844979175,
      "grad_norm": 3.063019275665283,
      "learning_rate": 0.09526145303100417,
      "loss": 0.062,
      "step": 5120
    },
    {
      "epoch": 2.3739009717723274,
      "grad_norm": 1.1593036651611328,
      "learning_rate": 0.09525219805645535,
      "loss": 0.0715,
      "step": 5130
    },
    {
      "epoch": 2.3785284590467377,
      "grad_norm": 9.429797172546387,
      "learning_rate": 0.09524294308190653,
      "loss": 0.075,
      "step": 5140
    },
    {
      "epoch": 2.3831559463211476,
      "grad_norm": 1.3709850311279297,
      "learning_rate": 0.09523368810735772,
      "loss": 0.0737,
      "step": 5150
    },
    {
      "epoch": 2.3877834335955574,
      "grad_norm": 4.349929332733154,
      "learning_rate": 0.09522443313280889,
      "loss": 0.0597,
      "step": 5160
    },
    {
      "epoch": 2.3924109208699678,
      "grad_norm": 2.644517183303833,
      "learning_rate": 0.09521517815826007,
      "loss": 0.04,
      "step": 5170
    },
    {
      "epoch": 2.3970384081443776,
      "grad_norm": 5.447585105895996,
      "learning_rate": 0.09520592318371125,
      "loss": 0.0634,
      "step": 5180
    },
    {
      "epoch": 2.4016658954187875,
      "grad_norm": 2.659075975418091,
      "learning_rate": 0.09519666820916244,
      "loss": 0.0596,
      "step": 5190
    },
    {
      "epoch": 2.406293382693198,
      "grad_norm": 3.559718608856201,
      "learning_rate": 0.0951874132346136,
      "loss": 0.0668,
      "step": 5200
    },
    {
      "epoch": 2.4109208699676077,
      "grad_norm": 2.8716607093811035,
      "learning_rate": 0.09517815826006479,
      "loss": 0.1009,
      "step": 5210
    },
    {
      "epoch": 2.4155483572420176,
      "grad_norm": 6.056154251098633,
      "learning_rate": 0.09516890328551597,
      "loss": 0.083,
      "step": 5220
    },
    {
      "epoch": 2.4201758445164274,
      "grad_norm": 2.344132661819458,
      "learning_rate": 0.09515964831096715,
      "loss": 0.0527,
      "step": 5230
    },
    {
      "epoch": 2.4248033317908377,
      "grad_norm": 5.427602291107178,
      "learning_rate": 0.09515039333641834,
      "loss": 0.0813,
      "step": 5240
    },
    {
      "epoch": 2.4294308190652476,
      "grad_norm": 1.1944315433502197,
      "learning_rate": 0.0951411383618695,
      "loss": 0.0633,
      "step": 5250
    },
    {
      "epoch": 2.4340583063396575,
      "grad_norm": 4.455111026763916,
      "learning_rate": 0.09513188338732069,
      "loss": 0.1027,
      "step": 5260
    },
    {
      "epoch": 2.4386857936140673,
      "grad_norm": 2.8275089263916016,
      "learning_rate": 0.09512262841277187,
      "loss": 0.0553,
      "step": 5270
    },
    {
      "epoch": 2.4433132808884777,
      "grad_norm": 2.8115367889404297,
      "learning_rate": 0.09511337343822306,
      "loss": 0.0559,
      "step": 5280
    },
    {
      "epoch": 2.4479407681628875,
      "grad_norm": 1.6052452325820923,
      "learning_rate": 0.09510411846367423,
      "loss": 0.0424,
      "step": 5290
    },
    {
      "epoch": 2.4525682554372974,
      "grad_norm": 5.089514255523682,
      "learning_rate": 0.09509486348912541,
      "loss": 0.0501,
      "step": 5300
    },
    {
      "epoch": 2.4571957427117077,
      "grad_norm": 2.6295294761657715,
      "learning_rate": 0.09508560851457659,
      "loss": 0.0392,
      "step": 5310
    },
    {
      "epoch": 2.4618232299861176,
      "grad_norm": 0.9338929057121277,
      "learning_rate": 0.09507635354002777,
      "loss": 0.0609,
      "step": 5320
    },
    {
      "epoch": 2.4664507172605274,
      "grad_norm": 0.8376808762550354,
      "learning_rate": 0.09506709856547896,
      "loss": 0.0749,
      "step": 5330
    },
    {
      "epoch": 2.4710782045349378,
      "grad_norm": 3.520575761795044,
      "learning_rate": 0.09505784359093013,
      "loss": 0.0307,
      "step": 5340
    },
    {
      "epoch": 2.4757056918093476,
      "grad_norm": 2.3387796878814697,
      "learning_rate": 0.09504858861638131,
      "loss": 0.065,
      "step": 5350
    },
    {
      "epoch": 2.4803331790837575,
      "grad_norm": 0.928411602973938,
      "learning_rate": 0.09503933364183248,
      "loss": 0.0758,
      "step": 5360
    },
    {
      "epoch": 2.4849606663581674,
      "grad_norm": 1.6720175743103027,
      "learning_rate": 0.09503007866728368,
      "loss": 0.0692,
      "step": 5370
    },
    {
      "epoch": 2.4895881536325777,
      "grad_norm": 6.391281604766846,
      "learning_rate": 0.09502082369273485,
      "loss": 0.0989,
      "step": 5380
    },
    {
      "epoch": 2.4942156409069876,
      "grad_norm": 0.9706639647483826,
      "learning_rate": 0.09501156871818603,
      "loss": 0.0823,
      "step": 5390
    },
    {
      "epoch": 2.4988431281813974,
      "grad_norm": 2.9129714965820312,
      "learning_rate": 0.09500231374363721,
      "loss": 0.0577,
      "step": 5400
    },
    {
      "epoch": 2.5034706154558073,
      "grad_norm": 1.9163986444473267,
      "learning_rate": 0.0949930587690884,
      "loss": 0.0765,
      "step": 5410
    },
    {
      "epoch": 2.5080981027302176,
      "grad_norm": 3.1321868896484375,
      "learning_rate": 0.09498380379453958,
      "loss": 0.0808,
      "step": 5420
    },
    {
      "epoch": 2.5127255900046275,
      "grad_norm": 1.4946690797805786,
      "learning_rate": 0.09497454881999075,
      "loss": 0.061,
      "step": 5430
    },
    {
      "epoch": 2.5173530772790373,
      "grad_norm": 4.223419666290283,
      "learning_rate": 0.09496529384544193,
      "loss": 0.0465,
      "step": 5440
    },
    {
      "epoch": 2.5219805645534477,
      "grad_norm": 0.49110278487205505,
      "learning_rate": 0.0949560388708931,
      "loss": 0.0829,
      "step": 5450
    },
    {
      "epoch": 2.5266080518278575,
      "grad_norm": 3.794424295425415,
      "learning_rate": 0.0949467838963443,
      "loss": 0.0458,
      "step": 5460
    },
    {
      "epoch": 2.5312355391022674,
      "grad_norm": 2.7380921840667725,
      "learning_rate": 0.09493752892179547,
      "loss": 0.0538,
      "step": 5470
    },
    {
      "epoch": 2.5358630263766777,
      "grad_norm": 0.9932998418807983,
      "learning_rate": 0.09492827394724665,
      "loss": 0.0429,
      "step": 5480
    },
    {
      "epoch": 2.5404905136510876,
      "grad_norm": 4.10062837600708,
      "learning_rate": 0.09491901897269783,
      "loss": 0.0498,
      "step": 5490
    },
    {
      "epoch": 2.5451180009254974,
      "grad_norm": 4.443046569824219,
      "learning_rate": 0.09490976399814902,
      "loss": 0.0502,
      "step": 5500
    },
    {
      "epoch": 2.5497454881999073,
      "grad_norm": 2.9293923377990723,
      "learning_rate": 0.0949005090236002,
      "loss": 0.0509,
      "step": 5510
    },
    {
      "epoch": 2.554372975474317,
      "grad_norm": 4.908831596374512,
      "learning_rate": 0.09489125404905137,
      "loss": 0.0493,
      "step": 5520
    },
    {
      "epoch": 2.5590004627487275,
      "grad_norm": 1.3495055437088013,
      "learning_rate": 0.09488199907450255,
      "loss": 0.0719,
      "step": 5530
    },
    {
      "epoch": 2.5636279500231374,
      "grad_norm": 5.315603256225586,
      "learning_rate": 0.09487274409995372,
      "loss": 0.0665,
      "step": 5540
    },
    {
      "epoch": 2.5682554372975472,
      "grad_norm": 2.0966124534606934,
      "learning_rate": 0.09486348912540492,
      "loss": 0.0792,
      "step": 5550
    },
    {
      "epoch": 2.5728829245719576,
      "grad_norm": 1.2362148761749268,
      "learning_rate": 0.09485423415085609,
      "loss": 0.0583,
      "step": 5560
    },
    {
      "epoch": 2.5775104118463674,
      "grad_norm": 1.5119048357009888,
      "learning_rate": 0.09484497917630727,
      "loss": 0.0588,
      "step": 5570
    },
    {
      "epoch": 2.5821378991207773,
      "grad_norm": 2.5138723850250244,
      "learning_rate": 0.09483572420175845,
      "loss": 0.0412,
      "step": 5580
    },
    {
      "epoch": 2.5867653863951876,
      "grad_norm": 3.0853264331817627,
      "learning_rate": 0.09482646922720962,
      "loss": 0.0754,
      "step": 5590
    },
    {
      "epoch": 2.5913928736695975,
      "grad_norm": 0.6354638934135437,
      "learning_rate": 0.09481721425266082,
      "loss": 0.0538,
      "step": 5600
    },
    {
      "epoch": 2.5960203609440073,
      "grad_norm": 1.7815430164337158,
      "learning_rate": 0.09480795927811199,
      "loss": 0.069,
      "step": 5610
    },
    {
      "epoch": 2.6006478482184177,
      "grad_norm": 2.5937092304229736,
      "learning_rate": 0.09479870430356317,
      "loss": 0.0517,
      "step": 5620
    },
    {
      "epoch": 2.6052753354928275,
      "grad_norm": 0.3706989884376526,
      "learning_rate": 0.09478944932901434,
      "loss": 0.0521,
      "step": 5630
    },
    {
      "epoch": 2.6099028227672374,
      "grad_norm": 2.638157606124878,
      "learning_rate": 0.09478019435446554,
      "loss": 0.0479,
      "step": 5640
    },
    {
      "epoch": 2.6145303100416473,
      "grad_norm": 4.029910564422607,
      "learning_rate": 0.09477093937991671,
      "loss": 0.0632,
      "step": 5650
    },
    {
      "epoch": 2.619157797316057,
      "grad_norm": 1.1909663677215576,
      "learning_rate": 0.09476168440536789,
      "loss": 0.0536,
      "step": 5660
    },
    {
      "epoch": 2.6237852845904674,
      "grad_norm": 6.886475563049316,
      "learning_rate": 0.09475242943081907,
      "loss": 0.0797,
      "step": 5670
    },
    {
      "epoch": 2.6284127718648773,
      "grad_norm": 0.6302635669708252,
      "learning_rate": 0.09474317445627024,
      "loss": 0.0499,
      "step": 5680
    },
    {
      "epoch": 2.633040259139287,
      "grad_norm": 3.5990777015686035,
      "learning_rate": 0.09473391948172144,
      "loss": 0.0437,
      "step": 5690
    },
    {
      "epoch": 2.6376677464136975,
      "grad_norm": 0.9482247829437256,
      "learning_rate": 0.09472466450717261,
      "loss": 0.0505,
      "step": 5700
    },
    {
      "epoch": 2.6422952336881074,
      "grad_norm": 0.6168421506881714,
      "learning_rate": 0.09471540953262379,
      "loss": 0.071,
      "step": 5710
    },
    {
      "epoch": 2.6469227209625172,
      "grad_norm": 1.0986087322235107,
      "learning_rate": 0.09470615455807496,
      "loss": 0.0652,
      "step": 5720
    },
    {
      "epoch": 2.6515502082369276,
      "grad_norm": 0.4392297863960266,
      "learning_rate": 0.09469689958352616,
      "loss": 0.0575,
      "step": 5730
    },
    {
      "epoch": 2.6561776955113374,
      "grad_norm": 4.560585021972656,
      "learning_rate": 0.09468764460897733,
      "loss": 0.0793,
      "step": 5740
    },
    {
      "epoch": 2.6608051827857473,
      "grad_norm": 3.8308396339416504,
      "learning_rate": 0.09467838963442851,
      "loss": 0.0449,
      "step": 5750
    },
    {
      "epoch": 2.6654326700601576,
      "grad_norm": 2.308870315551758,
      "learning_rate": 0.0946691346598797,
      "loss": 0.0606,
      "step": 5760
    },
    {
      "epoch": 2.6700601573345675,
      "grad_norm": 1.689775824546814,
      "learning_rate": 0.09465987968533086,
      "loss": 0.0333,
      "step": 5770
    },
    {
      "epoch": 2.6746876446089773,
      "grad_norm": 3.528292655944824,
      "learning_rate": 0.09465062471078206,
      "loss": 0.041,
      "step": 5780
    },
    {
      "epoch": 2.679315131883387,
      "grad_norm": 2.1252188682556152,
      "learning_rate": 0.09464136973623323,
      "loss": 0.0524,
      "step": 5790
    },
    {
      "epoch": 2.683942619157797,
      "grad_norm": 0.22602324187755585,
      "learning_rate": 0.09463211476168441,
      "loss": 0.0431,
      "step": 5800
    },
    {
      "epoch": 2.6885701064322074,
      "grad_norm": 6.489712238311768,
      "learning_rate": 0.09462285978713558,
      "loss": 0.0588,
      "step": 5810
    },
    {
      "epoch": 2.6931975937066173,
      "grad_norm": 0.6614479422569275,
      "learning_rate": 0.09461360481258677,
      "loss": 0.0493,
      "step": 5820
    },
    {
      "epoch": 2.697825080981027,
      "grad_norm": 0.1898851841688156,
      "learning_rate": 0.09460434983803795,
      "loss": 0.0523,
      "step": 5830
    },
    {
      "epoch": 2.7024525682554374,
      "grad_norm": 2.9966864585876465,
      "learning_rate": 0.09459509486348913,
      "loss": 0.0441,
      "step": 5840
    },
    {
      "epoch": 2.7070800555298473,
      "grad_norm": 3.794279098510742,
      "learning_rate": 0.09458583988894032,
      "loss": 0.0634,
      "step": 5850
    },
    {
      "epoch": 2.711707542804257,
      "grad_norm": 2.3587896823883057,
      "learning_rate": 0.09457658491439148,
      "loss": 0.0558,
      "step": 5860
    },
    {
      "epoch": 2.7163350300786675,
      "grad_norm": 1.7225127220153809,
      "learning_rate": 0.09456732993984268,
      "loss": 0.0668,
      "step": 5870
    },
    {
      "epoch": 2.7209625173530774,
      "grad_norm": 4.5490851402282715,
      "learning_rate": 0.09455807496529385,
      "loss": 0.0636,
      "step": 5880
    },
    {
      "epoch": 2.7255900046274872,
      "grad_norm": 0.7446686029434204,
      "learning_rate": 0.09454881999074503,
      "loss": 0.086,
      "step": 5890
    },
    {
      "epoch": 2.730217491901897,
      "grad_norm": 2.3975682258605957,
      "learning_rate": 0.0945395650161962,
      "loss": 0.0448,
      "step": 5900
    },
    {
      "epoch": 2.7348449791763074,
      "grad_norm": 2.162142038345337,
      "learning_rate": 0.09453031004164739,
      "loss": 0.029,
      "step": 5910
    },
    {
      "epoch": 2.7394724664507173,
      "grad_norm": 0.825396716594696,
      "learning_rate": 0.09452105506709857,
      "loss": 0.0346,
      "step": 5920
    },
    {
      "epoch": 2.744099953725127,
      "grad_norm": 5.181827068328857,
      "learning_rate": 0.09451180009254975,
      "loss": 0.0367,
      "step": 5930
    },
    {
      "epoch": 2.748727440999537,
      "grad_norm": 0.24751253426074982,
      "learning_rate": 0.09450254511800094,
      "loss": 0.0767,
      "step": 5940
    },
    {
      "epoch": 2.7533549282739473,
      "grad_norm": 2.1523163318634033,
      "learning_rate": 0.0944932901434521,
      "loss": 0.0193,
      "step": 5950
    },
    {
      "epoch": 2.757982415548357,
      "grad_norm": 2.348729133605957,
      "learning_rate": 0.0944840351689033,
      "loss": 0.0427,
      "step": 5960
    },
    {
      "epoch": 2.762609902822767,
      "grad_norm": 2.9915454387664795,
      "learning_rate": 0.09447478019435447,
      "loss": 0.0833,
      "step": 5970
    },
    {
      "epoch": 2.7672373900971774,
      "grad_norm": 2.7679946422576904,
      "learning_rate": 0.09446552521980565,
      "loss": 0.0379,
      "step": 5980
    },
    {
      "epoch": 2.7718648773715873,
      "grad_norm": 4.846217155456543,
      "learning_rate": 0.09445627024525682,
      "loss": 0.0615,
      "step": 5990
    },
    {
      "epoch": 2.776492364645997,
      "grad_norm": 1.5291565656661987,
      "learning_rate": 0.094447015270708,
      "loss": 0.0343,
      "step": 6000
    },
    {
      "epoch": 2.7811198519204074,
      "grad_norm": 0.9668816328048706,
      "learning_rate": 0.09443776029615919,
      "loss": 0.0509,
      "step": 6010
    },
    {
      "epoch": 2.7857473391948173,
      "grad_norm": 6.5371527671813965,
      "learning_rate": 0.09442850532161037,
      "loss": 0.0805,
      "step": 6020
    },
    {
      "epoch": 2.790374826469227,
      "grad_norm": 4.063841342926025,
      "learning_rate": 0.09441925034706156,
      "loss": 0.0729,
      "step": 6030
    },
    {
      "epoch": 2.795002313743637,
      "grad_norm": 1.09793221950531,
      "learning_rate": 0.09440999537251273,
      "loss": 0.0418,
      "step": 6040
    },
    {
      "epoch": 2.799629801018047,
      "grad_norm": 4.184850215911865,
      "learning_rate": 0.09440074039796391,
      "loss": 0.038,
      "step": 6050
    },
    {
      "epoch": 2.8042572882924572,
      "grad_norm": 7.0165324211120605,
      "learning_rate": 0.09439148542341509,
      "loss": 0.0347,
      "step": 6060
    },
    {
      "epoch": 2.808884775566867,
      "grad_norm": 5.228413105010986,
      "learning_rate": 0.09438223044886627,
      "loss": 0.0586,
      "step": 6070
    },
    {
      "epoch": 2.813512262841277,
      "grad_norm": 5.1453142166137695,
      "learning_rate": 0.09437297547431744,
      "loss": 0.0536,
      "step": 6080
    },
    {
      "epoch": 2.8181397501156873,
      "grad_norm": 6.648736953735352,
      "learning_rate": 0.09436372049976863,
      "loss": 0.0566,
      "step": 6090
    },
    {
      "epoch": 2.822767237390097,
      "grad_norm": 1.8920292854309082,
      "learning_rate": 0.09435446552521981,
      "loss": 0.045,
      "step": 6100
    },
    {
      "epoch": 2.827394724664507,
      "grad_norm": 1.6397467851638794,
      "learning_rate": 0.094345210550671,
      "loss": 0.0361,
      "step": 6110
    },
    {
      "epoch": 2.8320222119389173,
      "grad_norm": 0.4603458046913147,
      "learning_rate": 0.09433595557612218,
      "loss": 0.0361,
      "step": 6120
    },
    {
      "epoch": 2.836649699213327,
      "grad_norm": 1.8218519687652588,
      "learning_rate": 0.09432670060157335,
      "loss": 0.0378,
      "step": 6130
    },
    {
      "epoch": 2.841277186487737,
      "grad_norm": 0.15469534695148468,
      "learning_rate": 0.09431744562702453,
      "loss": 0.0481,
      "step": 6140
    },
    {
      "epoch": 2.8459046737621474,
      "grad_norm": 4.983644962310791,
      "learning_rate": 0.09430819065247571,
      "loss": 0.05,
      "step": 6150
    },
    {
      "epoch": 2.8505321610365573,
      "grad_norm": 0.802599310874939,
      "learning_rate": 0.0942989356779269,
      "loss": 0.0397,
      "step": 6160
    },
    {
      "epoch": 2.855159648310967,
      "grad_norm": 0.8344515562057495,
      "learning_rate": 0.09428968070337806,
      "loss": 0.0463,
      "step": 6170
    },
    {
      "epoch": 2.859787135585377,
      "grad_norm": 3.316866159439087,
      "learning_rate": 0.09428042572882925,
      "loss": 0.0541,
      "step": 6180
    },
    {
      "epoch": 2.864414622859787,
      "grad_norm": 2.3833088874816895,
      "learning_rate": 0.09427117075428043,
      "loss": 0.0406,
      "step": 6190
    },
    {
      "epoch": 2.869042110134197,
      "grad_norm": 2.0345897674560547,
      "learning_rate": 0.09426191577973161,
      "loss": 0.0223,
      "step": 6200
    },
    {
      "epoch": 2.873669597408607,
      "grad_norm": 1.4756852388381958,
      "learning_rate": 0.0942526608051828,
      "loss": 0.0372,
      "step": 6210
    },
    {
      "epoch": 2.878297084683017,
      "grad_norm": 6.789320468902588,
      "learning_rate": 0.09424340583063397,
      "loss": 0.0686,
      "step": 6220
    },
    {
      "epoch": 2.8829245719574272,
      "grad_norm": 5.571910381317139,
      "learning_rate": 0.09423415085608515,
      "loss": 0.0976,
      "step": 6230
    },
    {
      "epoch": 2.887552059231837,
      "grad_norm": 3.1435678005218506,
      "learning_rate": 0.09422489588153633,
      "loss": 0.0481,
      "step": 6240
    },
    {
      "epoch": 2.892179546506247,
      "grad_norm": 6.82468843460083,
      "learning_rate": 0.09421564090698752,
      "loss": 0.06,
      "step": 6250
    },
    {
      "epoch": 2.8968070337806573,
      "grad_norm": 2.07535457611084,
      "learning_rate": 0.09420638593243869,
      "loss": 0.055,
      "step": 6260
    },
    {
      "epoch": 2.901434521055067,
      "grad_norm": 1.8969765901565552,
      "learning_rate": 0.09419713095788987,
      "loss": 0.0448,
      "step": 6270
    },
    {
      "epoch": 2.906062008329477,
      "grad_norm": 0.6212610006332397,
      "learning_rate": 0.09418787598334105,
      "loss": 0.0479,
      "step": 6280
    },
    {
      "epoch": 2.9106894956038873,
      "grad_norm": 1.3858615159988403,
      "learning_rate": 0.09417862100879223,
      "loss": 0.0551,
      "step": 6290
    },
    {
      "epoch": 2.915316982878297,
      "grad_norm": 2.162715196609497,
      "learning_rate": 0.09416936603424342,
      "loss": 0.0386,
      "step": 6300
    },
    {
      "epoch": 2.919944470152707,
      "grad_norm": 1.0874881744384766,
      "learning_rate": 0.09416011105969459,
      "loss": 0.0322,
      "step": 6310
    },
    {
      "epoch": 2.924571957427117,
      "grad_norm": 3.4820969104766846,
      "learning_rate": 0.09415085608514577,
      "loss": 0.05,
      "step": 6320
    },
    {
      "epoch": 2.929199444701527,
      "grad_norm": 5.76046085357666,
      "learning_rate": 0.09414160111059695,
      "loss": 0.03,
      "step": 6330
    },
    {
      "epoch": 2.933826931975937,
      "grad_norm": 1.211641550064087,
      "learning_rate": 0.09413234613604814,
      "loss": 0.0396,
      "step": 6340
    },
    {
      "epoch": 2.938454419250347,
      "grad_norm": 0.4611397981643677,
      "learning_rate": 0.0941230911614993,
      "loss": 0.0445,
      "step": 6350
    },
    {
      "epoch": 2.943081906524757,
      "grad_norm": 6.105392932891846,
      "learning_rate": 0.09411383618695049,
      "loss": 0.0688,
      "step": 6360
    },
    {
      "epoch": 2.947709393799167,
      "grad_norm": 4.2238078117370605,
      "learning_rate": 0.09410458121240167,
      "loss": 0.0452,
      "step": 6370
    },
    {
      "epoch": 2.952336881073577,
      "grad_norm": 4.601557731628418,
      "learning_rate": 0.09409532623785286,
      "loss": 0.0782,
      "step": 6380
    },
    {
      "epoch": 2.956964368347987,
      "grad_norm": 2.6670472621917725,
      "learning_rate": 0.09408607126330404,
      "loss": 0.0412,
      "step": 6390
    },
    {
      "epoch": 2.9615918556223972,
      "grad_norm": 2.2732107639312744,
      "learning_rate": 0.09407681628875521,
      "loss": 0.0409,
      "step": 6400
    },
    {
      "epoch": 2.966219342896807,
      "grad_norm": 4.172773838043213,
      "learning_rate": 0.09406756131420639,
      "loss": 0.0499,
      "step": 6410
    },
    {
      "epoch": 2.970846830171217,
      "grad_norm": 2.715712308883667,
      "learning_rate": 0.09405830633965757,
      "loss": 0.0356,
      "step": 6420
    },
    {
      "epoch": 2.9754743174456273,
      "grad_norm": 0.2533354163169861,
      "learning_rate": 0.09404905136510876,
      "loss": 0.0293,
      "step": 6430
    },
    {
      "epoch": 2.980101804720037,
      "grad_norm": 3.6657214164733887,
      "learning_rate": 0.09403979639055993,
      "loss": 0.0549,
      "step": 6440
    },
    {
      "epoch": 2.984729291994447,
      "grad_norm": 1.1099328994750977,
      "learning_rate": 0.09403054141601111,
      "loss": 0.048,
      "step": 6450
    },
    {
      "epoch": 2.989356779268857,
      "grad_norm": 2.4981415271759033,
      "learning_rate": 0.09402128644146229,
      "loss": 0.0272,
      "step": 6460
    },
    {
      "epoch": 2.9939842665432668,
      "grad_norm": 0.7112832069396973,
      "learning_rate": 0.09401203146691348,
      "loss": 0.0313,
      "step": 6470
    },
    {
      "epoch": 2.998611753817677,
      "grad_norm": 1.2201169729232788,
      "learning_rate": 0.09400277649236466,
      "loss": 0.0529,
      "step": 6480
    },
    {
      "epoch": 3.0,
      "eval_accuracy_branch1": 0.9667231551378832,
      "eval_accuracy_branch2": 0.4980742566630719,
      "eval_f1_branch1": 0.9672303367604225,
      "eval_f1_branch2": 0.4810399138848845,
      "eval_loss": 0.06565187871456146,
      "eval_precision_branch1": 0.9698186419316823,
      "eval_precision_branch2": 0.49778319970420626,
      "eval_recall_branch1": 0.9663335945242928,
      "eval_recall_branch2": 0.498074256663072,
      "eval_runtime": 28.9727,
      "eval_samples_per_second": 448.077,
      "eval_steps_per_second": 56.018,
      "step": 6483
    },
    {
      "epoch": 3.003239241092087,
      "grad_norm": 3.950934648513794,
      "learning_rate": 0.09399352151781583,
      "loss": 0.0479,
      "step": 6490
    },
    {
      "epoch": 3.007866728366497,
      "grad_norm": 4.167917251586914,
      "learning_rate": 0.09398426654326701,
      "loss": 0.017,
      "step": 6500
    },
    {
      "epoch": 3.012494215640907,
      "grad_norm": 2.785784959793091,
      "learning_rate": 0.09397501156871818,
      "loss": 0.0529,
      "step": 6510
    },
    {
      "epoch": 3.017121702915317,
      "grad_norm": 9.652353286743164,
      "learning_rate": 0.09396575659416938,
      "loss": 0.0511,
      "step": 6520
    },
    {
      "epoch": 3.021749190189727,
      "grad_norm": 3.3274033069610596,
      "learning_rate": 0.09395650161962055,
      "loss": 0.0242,
      "step": 6530
    },
    {
      "epoch": 3.026376677464137,
      "grad_norm": 0.34794390201568604,
      "learning_rate": 0.09394724664507173,
      "loss": 0.0275,
      "step": 6540
    },
    {
      "epoch": 3.031004164738547,
      "grad_norm": 1.9102882146835327,
      "learning_rate": 0.09393799167052291,
      "loss": 0.0227,
      "step": 6550
    },
    {
      "epoch": 3.035631652012957,
      "grad_norm": 2.832420587539673,
      "learning_rate": 0.0939287366959741,
      "loss": 0.0288,
      "step": 6560
    },
    {
      "epoch": 3.040259139287367,
      "grad_norm": 4.058744430541992,
      "learning_rate": 0.09391948172142528,
      "loss": 0.0202,
      "step": 6570
    },
    {
      "epoch": 3.044886626561777,
      "grad_norm": 1.6262327432632446,
      "learning_rate": 0.09391022674687645,
      "loss": 0.0439,
      "step": 6580
    },
    {
      "epoch": 3.049514113836187,
      "grad_norm": 3.4550986289978027,
      "learning_rate": 0.09390097177232763,
      "loss": 0.0254,
      "step": 6590
    },
    {
      "epoch": 3.054141601110597,
      "grad_norm": 4.108205318450928,
      "learning_rate": 0.0938917167977788,
      "loss": 0.0419,
      "step": 6600
    },
    {
      "epoch": 3.0587690883850067,
      "grad_norm": 0.34808072447776794,
      "learning_rate": 0.09388246182323,
      "loss": 0.0314,
      "step": 6610
    },
    {
      "epoch": 3.063396575659417,
      "grad_norm": 4.171520709991455,
      "learning_rate": 0.09387320684868117,
      "loss": 0.0336,
      "step": 6620
    },
    {
      "epoch": 3.068024062933827,
      "grad_norm": 3.6950700283050537,
      "learning_rate": 0.09386395187413235,
      "loss": 0.0651,
      "step": 6630
    },
    {
      "epoch": 3.0726515502082368,
      "grad_norm": 8.380570411682129,
      "learning_rate": 0.09385469689958353,
      "loss": 0.0695,
      "step": 6640
    },
    {
      "epoch": 3.077279037482647,
      "grad_norm": 1.2148550748825073,
      "learning_rate": 0.09384544192503472,
      "loss": 0.0262,
      "step": 6650
    },
    {
      "epoch": 3.081906524757057,
      "grad_norm": 2.3502707481384277,
      "learning_rate": 0.0938361869504859,
      "loss": 0.0342,
      "step": 6660
    },
    {
      "epoch": 3.086534012031467,
      "grad_norm": 0.743019700050354,
      "learning_rate": 0.09382693197593707,
      "loss": 0.0552,
      "step": 6670
    },
    {
      "epoch": 3.091161499305877,
      "grad_norm": 6.04484748840332,
      "learning_rate": 0.09381767700138825,
      "loss": 0.0423,
      "step": 6680
    },
    {
      "epoch": 3.095788986580287,
      "grad_norm": 0.7196928262710571,
      "learning_rate": 0.09380842202683942,
      "loss": 0.0295,
      "step": 6690
    },
    {
      "epoch": 3.100416473854697,
      "grad_norm": 4.708559513092041,
      "learning_rate": 0.09379916705229062,
      "loss": 0.0578,
      "step": 6700
    },
    {
      "epoch": 3.1050439611291067,
      "grad_norm": 2.438432455062866,
      "learning_rate": 0.09378991207774179,
      "loss": 0.0386,
      "step": 6710
    },
    {
      "epoch": 3.109671448403517,
      "grad_norm": 6.468174457550049,
      "learning_rate": 0.09378065710319297,
      "loss": 0.0679,
      "step": 6720
    },
    {
      "epoch": 3.114298935677927,
      "grad_norm": 1.4088495969772339,
      "learning_rate": 0.09377140212864415,
      "loss": 0.0333,
      "step": 6730
    },
    {
      "epoch": 3.118926422952337,
      "grad_norm": 0.7899837493896484,
      "learning_rate": 0.09376214715409532,
      "loss": 0.0186,
      "step": 6740
    },
    {
      "epoch": 3.1235539102267467,
      "grad_norm": 3.933786153793335,
      "learning_rate": 0.09375289217954652,
      "loss": 0.0322,
      "step": 6750
    },
    {
      "epoch": 3.128181397501157,
      "grad_norm": 0.22765205800533295,
      "learning_rate": 0.09374363720499769,
      "loss": 0.0474,
      "step": 6760
    },
    {
      "epoch": 3.132808884775567,
      "grad_norm": 6.617018222808838,
      "learning_rate": 0.09373438223044887,
      "loss": 0.0501,
      "step": 6770
    },
    {
      "epoch": 3.1374363720499767,
      "grad_norm": 3.304112672805786,
      "learning_rate": 0.09372512725590004,
      "loss": 0.0341,
      "step": 6780
    },
    {
      "epoch": 3.142063859324387,
      "grad_norm": 6.9920878410339355,
      "learning_rate": 0.09371587228135124,
      "loss": 0.0387,
      "step": 6790
    },
    {
      "epoch": 3.146691346598797,
      "grad_norm": 2.9784979820251465,
      "learning_rate": 0.09370661730680241,
      "loss": 0.0427,
      "step": 6800
    },
    {
      "epoch": 3.1513188338732068,
      "grad_norm": 3.0729868412017822,
      "learning_rate": 0.09369736233225359,
      "loss": 0.0238,
      "step": 6810
    },
    {
      "epoch": 3.1559463211476166,
      "grad_norm": 3.14790415763855,
      "learning_rate": 0.09368810735770478,
      "loss": 0.0247,
      "step": 6820
    },
    {
      "epoch": 3.160573808422027,
      "grad_norm": 0.9507494568824768,
      "learning_rate": 0.09367885238315594,
      "loss": 0.0403,
      "step": 6830
    },
    {
      "epoch": 3.165201295696437,
      "grad_norm": 0.45068567991256714,
      "learning_rate": 0.09366959740860714,
      "loss": 0.0218,
      "step": 6840
    },
    {
      "epoch": 3.1698287829708467,
      "grad_norm": 0.19971828162670135,
      "learning_rate": 0.09366034243405831,
      "loss": 0.0271,
      "step": 6850
    },
    {
      "epoch": 3.174456270245257,
      "grad_norm": 4.040306091308594,
      "learning_rate": 0.0936510874595095,
      "loss": 0.0666,
      "step": 6860
    },
    {
      "epoch": 3.179083757519667,
      "grad_norm": 0.914659321308136,
      "learning_rate": 0.09364183248496066,
      "loss": 0.0451,
      "step": 6870
    },
    {
      "epoch": 3.1837112447940767,
      "grad_norm": 0.14591743052005768,
      "learning_rate": 0.09363257751041186,
      "loss": 0.0321,
      "step": 6880
    },
    {
      "epoch": 3.1883387320684866,
      "grad_norm": 7.51956033706665,
      "learning_rate": 0.09362332253586303,
      "loss": 0.0776,
      "step": 6890
    },
    {
      "epoch": 3.192966219342897,
      "grad_norm": 1.0183231830596924,
      "learning_rate": 0.09361406756131421,
      "loss": 0.0282,
      "step": 6900
    },
    {
      "epoch": 3.197593706617307,
      "grad_norm": 1.8558049201965332,
      "learning_rate": 0.0936048125867654,
      "loss": 0.0292,
      "step": 6910
    },
    {
      "epoch": 3.2022211938917167,
      "grad_norm": 4.13284158706665,
      "learning_rate": 0.09359555761221657,
      "loss": 0.0328,
      "step": 6920
    },
    {
      "epoch": 3.206848681166127,
      "grad_norm": 1.983862280845642,
      "learning_rate": 0.09358630263766776,
      "loss": 0.0382,
      "step": 6930
    },
    {
      "epoch": 3.211476168440537,
      "grad_norm": 3.5686452388763428,
      "learning_rate": 0.09357704766311893,
      "loss": 0.0211,
      "step": 6940
    },
    {
      "epoch": 3.2161036557149467,
      "grad_norm": 3.674637794494629,
      "learning_rate": 0.09356779268857011,
      "loss": 0.0357,
      "step": 6950
    },
    {
      "epoch": 3.2207311429893566,
      "grad_norm": 3.8142268657684326,
      "learning_rate": 0.09355853771402128,
      "loss": 0.0301,
      "step": 6960
    },
    {
      "epoch": 3.225358630263767,
      "grad_norm": 4.332625389099121,
      "learning_rate": 0.09354928273947247,
      "loss": 0.0382,
      "step": 6970
    },
    {
      "epoch": 3.2299861175381768,
      "grad_norm": 5.321276664733887,
      "learning_rate": 0.09354002776492365,
      "loss": 0.0322,
      "step": 6980
    },
    {
      "epoch": 3.2346136048125866,
      "grad_norm": 0.06704152375459671,
      "learning_rate": 0.09353077279037483,
      "loss": 0.0839,
      "step": 6990
    },
    {
      "epoch": 3.239241092086997,
      "grad_norm": 3.23226261138916,
      "learning_rate": 0.09352151781582602,
      "loss": 0.0536,
      "step": 7000
    },
    {
      "epoch": 3.243868579361407,
      "grad_norm": 1.9606157541275024,
      "learning_rate": 0.09351226284127719,
      "loss": 0.019,
      "step": 7010
    },
    {
      "epoch": 3.2484960666358167,
      "grad_norm": 3.6141669750213623,
      "learning_rate": 0.09350300786672838,
      "loss": 0.0328,
      "step": 7020
    },
    {
      "epoch": 3.2531235539102266,
      "grad_norm": 4.680494785308838,
      "learning_rate": 0.09349375289217955,
      "loss": 0.0548,
      "step": 7030
    },
    {
      "epoch": 3.257751041184637,
      "grad_norm": 1.4184544086456299,
      "learning_rate": 0.09348449791763074,
      "loss": 0.0442,
      "step": 7040
    },
    {
      "epoch": 3.2623785284590467,
      "grad_norm": 0.1998327672481537,
      "learning_rate": 0.0934752429430819,
      "loss": 0.0309,
      "step": 7050
    },
    {
      "epoch": 3.2670060157334566,
      "grad_norm": 4.023176670074463,
      "learning_rate": 0.09346598796853309,
      "loss": 0.0383,
      "step": 7060
    },
    {
      "epoch": 3.271633503007867,
      "grad_norm": 2.0534956455230713,
      "learning_rate": 0.09345673299398427,
      "loss": 0.0364,
      "step": 7070
    },
    {
      "epoch": 3.276260990282277,
      "grad_norm": 4.074929237365723,
      "learning_rate": 0.09344747801943545,
      "loss": 0.0441,
      "step": 7080
    },
    {
      "epoch": 3.2808884775566867,
      "grad_norm": 4.634137153625488,
      "learning_rate": 0.09343822304488664,
      "loss": 0.0452,
      "step": 7090
    },
    {
      "epoch": 3.2855159648310965,
      "grad_norm": 3.599552869796753,
      "learning_rate": 0.0934289680703378,
      "loss": 0.0411,
      "step": 7100
    },
    {
      "epoch": 3.290143452105507,
      "grad_norm": 3.8814969062805176,
      "learning_rate": 0.093419713095789,
      "loss": 0.0472,
      "step": 7110
    },
    {
      "epoch": 3.2947709393799167,
      "grad_norm": 2.8312184810638428,
      "learning_rate": 0.09341045812124017,
      "loss": 0.0435,
      "step": 7120
    },
    {
      "epoch": 3.2993984266543266,
      "grad_norm": 2.8109755516052246,
      "learning_rate": 0.09340120314669136,
      "loss": 0.0412,
      "step": 7130
    },
    {
      "epoch": 3.3040259139287365,
      "grad_norm": 1.088598370552063,
      "learning_rate": 0.09339194817214252,
      "loss": 0.031,
      "step": 7140
    },
    {
      "epoch": 3.3086534012031468,
      "grad_norm": 12.105844497680664,
      "learning_rate": 0.09338269319759371,
      "loss": 0.0646,
      "step": 7150
    },
    {
      "epoch": 3.3132808884775566,
      "grad_norm": 1.1592369079589844,
      "learning_rate": 0.09337343822304489,
      "loss": 0.0279,
      "step": 7160
    },
    {
      "epoch": 3.3179083757519665,
      "grad_norm": 2.456573486328125,
      "learning_rate": 0.09336418324849607,
      "loss": 0.0223,
      "step": 7170
    },
    {
      "epoch": 3.322535863026377,
      "grad_norm": 1.8357462882995605,
      "learning_rate": 0.09335492827394726,
      "loss": 0.0278,
      "step": 7180
    },
    {
      "epoch": 3.3271633503007867,
      "grad_norm": 1.3480720520019531,
      "learning_rate": 0.09334567329939843,
      "loss": 0.0144,
      "step": 7190
    },
    {
      "epoch": 3.3317908375751966,
      "grad_norm": 0.659300684928894,
      "learning_rate": 0.09333641832484961,
      "loss": 0.0242,
      "step": 7200
    },
    {
      "epoch": 3.336418324849607,
      "grad_norm": 1.9884755611419678,
      "learning_rate": 0.09332716335030079,
      "loss": 0.0334,
      "step": 7210
    },
    {
      "epoch": 3.3410458121240167,
      "grad_norm": 1.1024807691574097,
      "learning_rate": 0.09331790837575198,
      "loss": 0.0292,
      "step": 7220
    },
    {
      "epoch": 3.3456732993984266,
      "grad_norm": 0.7618456482887268,
      "learning_rate": 0.09330865340120315,
      "loss": 0.0366,
      "step": 7230
    },
    {
      "epoch": 3.3503007866728365,
      "grad_norm": 1.9659168720245361,
      "learning_rate": 0.09329939842665433,
      "loss": 0.0179,
      "step": 7240
    },
    {
      "epoch": 3.354928273947247,
      "grad_norm": 0.40870198607444763,
      "learning_rate": 0.09329014345210551,
      "loss": 0.0102,
      "step": 7250
    },
    {
      "epoch": 3.3595557612216567,
      "grad_norm": 5.573237895965576,
      "learning_rate": 0.0932808884775567,
      "loss": 0.0516,
      "step": 7260
    },
    {
      "epoch": 3.3641832484960665,
      "grad_norm": 5.165126800537109,
      "learning_rate": 0.09327163350300788,
      "loss": 0.0359,
      "step": 7270
    },
    {
      "epoch": 3.3688107357704764,
      "grad_norm": 1.2739886045455933,
      "learning_rate": 0.09326237852845905,
      "loss": 0.025,
      "step": 7280
    },
    {
      "epoch": 3.3734382230448867,
      "grad_norm": 3.318601608276367,
      "learning_rate": 0.09325312355391023,
      "loss": 0.0342,
      "step": 7290
    },
    {
      "epoch": 3.3780657103192966,
      "grad_norm": 4.176680088043213,
      "learning_rate": 0.09324386857936141,
      "loss": 0.0324,
      "step": 7300
    },
    {
      "epoch": 3.3826931975937065,
      "grad_norm": 2.379472017288208,
      "learning_rate": 0.0932346136048126,
      "loss": 0.041,
      "step": 7310
    },
    {
      "epoch": 3.3873206848681168,
      "grad_norm": 8.677924156188965,
      "learning_rate": 0.09322535863026377,
      "loss": 0.0648,
      "step": 7320
    },
    {
      "epoch": 3.3919481721425266,
      "grad_norm": 3.4466865062713623,
      "learning_rate": 0.09321610365571495,
      "loss": 0.0609,
      "step": 7330
    },
    {
      "epoch": 3.3965756594169365,
      "grad_norm": 3.9346063137054443,
      "learning_rate": 0.09320684868116613,
      "loss": 0.0254,
      "step": 7340
    },
    {
      "epoch": 3.401203146691347,
      "grad_norm": 0.025456920266151428,
      "learning_rate": 0.09319759370661732,
      "loss": 0.0593,
      "step": 7350
    },
    {
      "epoch": 3.4058306339657567,
      "grad_norm": 2.511284351348877,
      "learning_rate": 0.0931883387320685,
      "loss": 0.0487,
      "step": 7360
    },
    {
      "epoch": 3.4104581212401666,
      "grad_norm": 7.530548095703125,
      "learning_rate": 0.09317908375751967,
      "loss": 0.0539,
      "step": 7370
    },
    {
      "epoch": 3.4150856085145764,
      "grad_norm": 1.454124927520752,
      "learning_rate": 0.09316982878297085,
      "loss": 0.039,
      "step": 7380
    },
    {
      "epoch": 3.4197130957889867,
      "grad_norm": 0.4934121072292328,
      "learning_rate": 0.09316057380842203,
      "loss": 0.0404,
      "step": 7390
    },
    {
      "epoch": 3.4243405830633966,
      "grad_norm": 0.7692146301269531,
      "learning_rate": 0.09315131883387322,
      "loss": 0.0121,
      "step": 7400
    },
    {
      "epoch": 3.4289680703378065,
      "grad_norm": 2.987640142440796,
      "learning_rate": 0.09314206385932439,
      "loss": 0.0332,
      "step": 7410
    },
    {
      "epoch": 3.4335955576122164,
      "grad_norm": 1.0998659133911133,
      "learning_rate": 0.09313280888477557,
      "loss": 0.0201,
      "step": 7420
    },
    {
      "epoch": 3.4382230448866267,
      "grad_norm": 5.645959854125977,
      "learning_rate": 0.09312355391022675,
      "loss": 0.0769,
      "step": 7430
    },
    {
      "epoch": 3.4428505321610365,
      "grad_norm": 3.1339986324310303,
      "learning_rate": 0.09311429893567794,
      "loss": 0.0261,
      "step": 7440
    },
    {
      "epoch": 3.4474780194354464,
      "grad_norm": 0.13082477450370789,
      "learning_rate": 0.09310504396112912,
      "loss": 0.0167,
      "step": 7450
    },
    {
      "epoch": 3.4521055067098567,
      "grad_norm": 0.6816484928131104,
      "learning_rate": 0.09309578898658029,
      "loss": 0.0386,
      "step": 7460
    },
    {
      "epoch": 3.4567329939842666,
      "grad_norm": 2.0154106616973877,
      "learning_rate": 0.09308653401203147,
      "loss": 0.0287,
      "step": 7470
    },
    {
      "epoch": 3.4613604812586765,
      "grad_norm": 1.5565422773361206,
      "learning_rate": 0.09307727903748265,
      "loss": 0.0294,
      "step": 7480
    },
    {
      "epoch": 3.4659879685330868,
      "grad_norm": 0.0684395432472229,
      "learning_rate": 0.09306802406293384,
      "loss": 0.0128,
      "step": 7490
    },
    {
      "epoch": 3.4706154558074966,
      "grad_norm": 1.485243558883667,
      "learning_rate": 0.09305876908838501,
      "loss": 0.0775,
      "step": 7500
    },
    {
      "epoch": 3.4752429430819065,
      "grad_norm": 7.01202392578125,
      "learning_rate": 0.09304951411383619,
      "loss": 0.0811,
      "step": 7510
    },
    {
      "epoch": 3.4798704303563164,
      "grad_norm": 0.28162604570388794,
      "learning_rate": 0.09304025913928737,
      "loss": 0.0495,
      "step": 7520
    },
    {
      "epoch": 3.4844979176307267,
      "grad_norm": 3.6662237644195557,
      "learning_rate": 0.09303100416473856,
      "loss": 0.0393,
      "step": 7530
    },
    {
      "epoch": 3.4891254049051366,
      "grad_norm": 3.099647283554077,
      "learning_rate": 0.09302174919018974,
      "loss": 0.0354,
      "step": 7540
    },
    {
      "epoch": 3.4937528921795464,
      "grad_norm": 5.3410162925720215,
      "learning_rate": 0.09301249421564091,
      "loss": 0.0419,
      "step": 7550
    },
    {
      "epoch": 3.4983803794539563,
      "grad_norm": 4.24848747253418,
      "learning_rate": 0.09300323924109209,
      "loss": 0.0331,
      "step": 7560
    },
    {
      "epoch": 3.5030078667283666,
      "grad_norm": 0.2712726891040802,
      "learning_rate": 0.09299398426654328,
      "loss": 0.029,
      "step": 7570
    },
    {
      "epoch": 3.5076353540027765,
      "grad_norm": 2.2700917720794678,
      "learning_rate": 0.09298472929199446,
      "loss": 0.044,
      "step": 7580
    },
    {
      "epoch": 3.5122628412771864,
      "grad_norm": 0.7089502215385437,
      "learning_rate": 0.09297547431744563,
      "loss": 0.0175,
      "step": 7590
    },
    {
      "epoch": 3.5168903285515967,
      "grad_norm": 2.832460641860962,
      "learning_rate": 0.09296621934289681,
      "loss": 0.0271,
      "step": 7600
    },
    {
      "epoch": 3.5215178158260065,
      "grad_norm": 1.4740257263183594,
      "learning_rate": 0.092956964368348,
      "loss": 0.0191,
      "step": 7610
    },
    {
      "epoch": 3.5261453031004164,
      "grad_norm": 0.8757106065750122,
      "learning_rate": 0.09294770939379918,
      "loss": 0.0334,
      "step": 7620
    },
    {
      "epoch": 3.5307727903748267,
      "grad_norm": 0.3717500567436218,
      "learning_rate": 0.09293845441925036,
      "loss": 0.0237,
      "step": 7630
    },
    {
      "epoch": 3.5354002776492366,
      "grad_norm": 4.527213096618652,
      "learning_rate": 0.09292919944470153,
      "loss": 0.0262,
      "step": 7640
    },
    {
      "epoch": 3.5400277649236465,
      "grad_norm": 4.7268385887146,
      "learning_rate": 0.09291994447015271,
      "loss": 0.051,
      "step": 7650
    },
    {
      "epoch": 3.5446552521980563,
      "grad_norm": 1.4629548788070679,
      "learning_rate": 0.09291068949560388,
      "loss": 0.0186,
      "step": 7660
    },
    {
      "epoch": 3.549282739472466,
      "grad_norm": 4.372270584106445,
      "learning_rate": 0.09290143452105508,
      "loss": 0.0209,
      "step": 7670
    },
    {
      "epoch": 3.5539102267468765,
      "grad_norm": 3.5963644981384277,
      "learning_rate": 0.09289217954650625,
      "loss": 0.0365,
      "step": 7680
    },
    {
      "epoch": 3.5585377140212864,
      "grad_norm": 3.898101568222046,
      "learning_rate": 0.09288292457195743,
      "loss": 0.0256,
      "step": 7690
    },
    {
      "epoch": 3.5631652012956962,
      "grad_norm": 0.07769893109798431,
      "learning_rate": 0.09287366959740861,
      "loss": 0.0349,
      "step": 7700
    },
    {
      "epoch": 3.5677926885701066,
      "grad_norm": 0.20014244318008423,
      "learning_rate": 0.0928644146228598,
      "loss": 0.014,
      "step": 7710
    },
    {
      "epoch": 3.5724201758445164,
      "grad_norm": 4.80953311920166,
      "learning_rate": 0.09285515964831098,
      "loss": 0.0574,
      "step": 7720
    },
    {
      "epoch": 3.5770476631189263,
      "grad_norm": 2.7041361331939697,
      "learning_rate": 0.09284590467376215,
      "loss": 0.0267,
      "step": 7730
    },
    {
      "epoch": 3.5816751503933366,
      "grad_norm": 0.21804139018058777,
      "learning_rate": 0.09283664969921333,
      "loss": 0.0312,
      "step": 7740
    },
    {
      "epoch": 3.5863026376677465,
      "grad_norm": 2.8405425548553467,
      "learning_rate": 0.0928273947246645,
      "loss": 0.0207,
      "step": 7750
    },
    {
      "epoch": 3.5909301249421564,
      "grad_norm": 2.1344499588012695,
      "learning_rate": 0.0928181397501157,
      "loss": 0.0343,
      "step": 7760
    },
    {
      "epoch": 3.5955576122165667,
      "grad_norm": 8.636149406433105,
      "learning_rate": 0.09280888477556687,
      "loss": 0.0337,
      "step": 7770
    },
    {
      "epoch": 3.6001850994909765,
      "grad_norm": 1.1184704303741455,
      "learning_rate": 0.09279962980101805,
      "loss": 0.0713,
      "step": 7780
    },
    {
      "epoch": 3.6048125867653864,
      "grad_norm": 2.201272487640381,
      "learning_rate": 0.09279037482646924,
      "loss": 0.0433,
      "step": 7790
    },
    {
      "epoch": 3.6094400740397963,
      "grad_norm": 6.169801712036133,
      "learning_rate": 0.0927811198519204,
      "loss": 0.0163,
      "step": 7800
    },
    {
      "epoch": 3.614067561314206,
      "grad_norm": 4.939520359039307,
      "learning_rate": 0.0927718648773716,
      "loss": 0.0338,
      "step": 7810
    },
    {
      "epoch": 3.6186950485886165,
      "grad_norm": 0.08616441488265991,
      "learning_rate": 0.09276260990282277,
      "loss": 0.0284,
      "step": 7820
    },
    {
      "epoch": 3.6233225358630263,
      "grad_norm": 2.7649242877960205,
      "learning_rate": 0.09275335492827395,
      "loss": 0.0443,
      "step": 7830
    },
    {
      "epoch": 3.627950023137436,
      "grad_norm": 1.8698046207427979,
      "learning_rate": 0.09274409995372512,
      "loss": 0.0197,
      "step": 7840
    },
    {
      "epoch": 3.6325775104118465,
      "grad_norm": 0.8964669108390808,
      "learning_rate": 0.09273484497917632,
      "loss": 0.0288,
      "step": 7850
    },
    {
      "epoch": 3.6372049976862564,
      "grad_norm": 2.907565116882324,
      "learning_rate": 0.09272559000462749,
      "loss": 0.0237,
      "step": 7860
    },
    {
      "epoch": 3.6418324849606662,
      "grad_norm": 5.349301815032959,
      "learning_rate": 0.09271633503007867,
      "loss": 0.0166,
      "step": 7870
    },
    {
      "epoch": 3.6464599722350766,
      "grad_norm": 2.5386135578155518,
      "learning_rate": 0.09270708005552986,
      "loss": 0.0413,
      "step": 7880
    },
    {
      "epoch": 3.6510874595094864,
      "grad_norm": 0.018713505938649178,
      "learning_rate": 0.09269782508098103,
      "loss": 0.0325,
      "step": 7890
    },
    {
      "epoch": 3.6557149467838963,
      "grad_norm": 6.520620822906494,
      "learning_rate": 0.09268857010643222,
      "loss": 0.0319,
      "step": 7900
    },
    {
      "epoch": 3.6603424340583066,
      "grad_norm": 3.7815895080566406,
      "learning_rate": 0.09267931513188339,
      "loss": 0.024,
      "step": 7910
    },
    {
      "epoch": 3.6649699213327165,
      "grad_norm": 3.7125933170318604,
      "learning_rate": 0.09267006015733457,
      "loss": 0.0408,
      "step": 7920
    },
    {
      "epoch": 3.6695974086071264,
      "grad_norm": 3.442483901977539,
      "learning_rate": 0.09266080518278574,
      "loss": 0.026,
      "step": 7930
    },
    {
      "epoch": 3.6742248958815362,
      "grad_norm": 3.1664228439331055,
      "learning_rate": 0.09265155020823694,
      "loss": 0.0541,
      "step": 7940
    },
    {
      "epoch": 3.678852383155946,
      "grad_norm": 4.963663101196289,
      "learning_rate": 0.09264229523368811,
      "loss": 0.0399,
      "step": 7950
    },
    {
      "epoch": 3.6834798704303564,
      "grad_norm": 4.425302982330322,
      "learning_rate": 0.0926330402591393,
      "loss": 0.0268,
      "step": 7960
    },
    {
      "epoch": 3.6881073577047663,
      "grad_norm": 0.43528836965560913,
      "learning_rate": 0.09262378528459048,
      "loss": 0.032,
      "step": 7970
    },
    {
      "epoch": 3.692734844979176,
      "grad_norm": 5.382338523864746,
      "learning_rate": 0.09261453031004165,
      "loss": 0.0567,
      "step": 7980
    },
    {
      "epoch": 3.6973623322535865,
      "grad_norm": 0.19399778544902802,
      "learning_rate": 0.09260527533549284,
      "loss": 0.0298,
      "step": 7990
    },
    {
      "epoch": 3.7019898195279963,
      "grad_norm": 0.5232776999473572,
      "learning_rate": 0.09259602036094401,
      "loss": 0.0345,
      "step": 8000
    },
    {
      "epoch": 3.706617306802406,
      "grad_norm": 0.4751971364021301,
      "learning_rate": 0.0925867653863952,
      "loss": 0.0597,
      "step": 8010
    },
    {
      "epoch": 3.7112447940768165,
      "grad_norm": 0.5395204424858093,
      "learning_rate": 0.09257751041184636,
      "loss": 0.0205,
      "step": 8020
    },
    {
      "epoch": 3.7158722813512264,
      "grad_norm": 2.216606378555298,
      "learning_rate": 0.09256825543729755,
      "loss": 0.0283,
      "step": 8030
    },
    {
      "epoch": 3.7204997686256362,
      "grad_norm": 0.5162019729614258,
      "learning_rate": 0.09255900046274873,
      "loss": 0.0291,
      "step": 8040
    },
    {
      "epoch": 3.725127255900046,
      "grad_norm": 1.8096182346343994,
      "learning_rate": 0.09254974548819991,
      "loss": 0.0227,
      "step": 8050
    },
    {
      "epoch": 3.7297547431744564,
      "grad_norm": 2.8129684925079346,
      "learning_rate": 0.0925404905136511,
      "loss": 0.0435,
      "step": 8060
    },
    {
      "epoch": 3.7343822304488663,
      "grad_norm": 3.280251979827881,
      "learning_rate": 0.09253123553910227,
      "loss": 0.0395,
      "step": 8070
    },
    {
      "epoch": 3.739009717723276,
      "grad_norm": 1.0740082263946533,
      "learning_rate": 0.09252198056455346,
      "loss": 0.0465,
      "step": 8080
    },
    {
      "epoch": 3.743637204997686,
      "grad_norm": 1.9522908926010132,
      "learning_rate": 0.09251272559000463,
      "loss": 0.0165,
      "step": 8090
    },
    {
      "epoch": 3.7482646922720964,
      "grad_norm": 0.048352569341659546,
      "learning_rate": 0.09250347061545582,
      "loss": 0.0133,
      "step": 8100
    },
    {
      "epoch": 3.7528921795465062,
      "grad_norm": 0.47339388728141785,
      "learning_rate": 0.09249421564090698,
      "loss": 0.0138,
      "step": 8110
    },
    {
      "epoch": 3.757519666820916,
      "grad_norm": 0.036115046590566635,
      "learning_rate": 0.09248496066635817,
      "loss": 0.029,
      "step": 8120
    },
    {
      "epoch": 3.7621471540953264,
      "grad_norm": 2.0026936531066895,
      "learning_rate": 0.09247570569180935,
      "loss": 0.0206,
      "step": 8130
    },
    {
      "epoch": 3.7667746413697363,
      "grad_norm": 2.003486394882202,
      "learning_rate": 0.09246645071726053,
      "loss": 0.0454,
      "step": 8140
    },
    {
      "epoch": 3.771402128644146,
      "grad_norm": 0.9458946585655212,
      "learning_rate": 0.09245719574271172,
      "loss": 0.027,
      "step": 8150
    },
    {
      "epoch": 3.7760296159185565,
      "grad_norm": 2.816945791244507,
      "learning_rate": 0.09244794076816289,
      "loss": 0.0581,
      "step": 8160
    },
    {
      "epoch": 3.7806571031929663,
      "grad_norm": 0.16864901781082153,
      "learning_rate": 0.09243868579361408,
      "loss": 0.0115,
      "step": 8170
    },
    {
      "epoch": 3.785284590467376,
      "grad_norm": 0.7306394577026367,
      "learning_rate": 0.09242943081906525,
      "loss": 0.0134,
      "step": 8180
    },
    {
      "epoch": 3.789912077741786,
      "grad_norm": 0.4793134033679962,
      "learning_rate": 0.09242017584451644,
      "loss": 0.0254,
      "step": 8190
    },
    {
      "epoch": 3.794539565016196,
      "grad_norm": 0.21064455807209015,
      "learning_rate": 0.0924109208699676,
      "loss": 0.0159,
      "step": 8200
    },
    {
      "epoch": 3.7991670522906063,
      "grad_norm": 0.8186827898025513,
      "learning_rate": 0.09240166589541879,
      "loss": 0.013,
      "step": 8210
    },
    {
      "epoch": 3.803794539565016,
      "grad_norm": 0.2094316929578781,
      "learning_rate": 0.09239241092086997,
      "loss": 0.0328,
      "step": 8220
    },
    {
      "epoch": 3.808422026839426,
      "grad_norm": 0.049863431602716446,
      "learning_rate": 0.09238315594632115,
      "loss": 0.0275,
      "step": 8230
    },
    {
      "epoch": 3.8130495141138363,
      "grad_norm": 4.884315013885498,
      "learning_rate": 0.09237390097177234,
      "loss": 0.0401,
      "step": 8240
    },
    {
      "epoch": 3.817677001388246,
      "grad_norm": 4.600937366485596,
      "learning_rate": 0.09236464599722351,
      "loss": 0.0416,
      "step": 8250
    },
    {
      "epoch": 3.822304488662656,
      "grad_norm": 8.083589553833008,
      "learning_rate": 0.09235539102267469,
      "loss": 0.0416,
      "step": 8260
    },
    {
      "epoch": 3.8269319759370664,
      "grad_norm": 5.620134353637695,
      "learning_rate": 0.09234613604812587,
      "loss": 0.0305,
      "step": 8270
    },
    {
      "epoch": 3.8315594632114762,
      "grad_norm": 8.499994277954102,
      "learning_rate": 0.09233688107357706,
      "loss": 0.0281,
      "step": 8280
    },
    {
      "epoch": 3.836186950485886,
      "grad_norm": 0.5871559381484985,
      "learning_rate": 0.09232762609902823,
      "loss": 0.0319,
      "step": 8290
    },
    {
      "epoch": 3.8408144377602964,
      "grad_norm": 2.8156137466430664,
      "learning_rate": 0.09231837112447941,
      "loss": 0.0388,
      "step": 8300
    },
    {
      "epoch": 3.8454419250347063,
      "grad_norm": 2.630136728286743,
      "learning_rate": 0.09230911614993059,
      "loss": 0.0274,
      "step": 8310
    },
    {
      "epoch": 3.850069412309116,
      "grad_norm": 5.643703460693359,
      "learning_rate": 0.09229986117538178,
      "loss": 0.0284,
      "step": 8320
    },
    {
      "epoch": 3.854696899583526,
      "grad_norm": 0.7887156009674072,
      "learning_rate": 0.09229060620083296,
      "loss": 0.0506,
      "step": 8330
    },
    {
      "epoch": 3.859324386857936,
      "grad_norm": 1.6144399642944336,
      "learning_rate": 0.09228135122628413,
      "loss": 0.0168,
      "step": 8340
    },
    {
      "epoch": 3.863951874132346,
      "grad_norm": 0.12288811802864075,
      "learning_rate": 0.09227209625173531,
      "loss": 0.0221,
      "step": 8350
    },
    {
      "epoch": 3.868579361406756,
      "grad_norm": 0.816361665725708,
      "learning_rate": 0.0922628412771865,
      "loss": 0.0095,
      "step": 8360
    },
    {
      "epoch": 3.873206848681166,
      "grad_norm": 7.5465898513793945,
      "learning_rate": 0.09225358630263768,
      "loss": 0.0432,
      "step": 8370
    },
    {
      "epoch": 3.8778343359555763,
      "grad_norm": 0.5684595704078674,
      "learning_rate": 0.09224433132808885,
      "loss": 0.0404,
      "step": 8380
    },
    {
      "epoch": 3.882461823229986,
      "grad_norm": 2.9829468727111816,
      "learning_rate": 0.09223507635354003,
      "loss": 0.0247,
      "step": 8390
    },
    {
      "epoch": 3.887089310504396,
      "grad_norm": 0.13792796432971954,
      "learning_rate": 0.09222582137899121,
      "loss": 0.0436,
      "step": 8400
    },
    {
      "epoch": 3.8917167977788063,
      "grad_norm": 8.657479286193848,
      "learning_rate": 0.0922165664044424,
      "loss": 0.0299,
      "step": 8410
    },
    {
      "epoch": 3.896344285053216,
      "grad_norm": 3.2597179412841797,
      "learning_rate": 0.09220731142989358,
      "loss": 0.0409,
      "step": 8420
    },
    {
      "epoch": 3.900971772327626,
      "grad_norm": 3.2979116439819336,
      "learning_rate": 0.09219805645534475,
      "loss": 0.021,
      "step": 8430
    },
    {
      "epoch": 3.9055992596020364,
      "grad_norm": 2.3401057720184326,
      "learning_rate": 0.09218880148079593,
      "loss": 0.0264,
      "step": 8440
    },
    {
      "epoch": 3.9102267468764462,
      "grad_norm": 6.047923564910889,
      "learning_rate": 0.09217954650624711,
      "loss": 0.0325,
      "step": 8450
    },
    {
      "epoch": 3.914854234150856,
      "grad_norm": 1.737840175628662,
      "learning_rate": 0.0921702915316983,
      "loss": 0.0584,
      "step": 8460
    },
    {
      "epoch": 3.919481721425266,
      "grad_norm": 0.47332337498664856,
      "learning_rate": 0.09216103655714947,
      "loss": 0.0108,
      "step": 8470
    },
    {
      "epoch": 3.924109208699676,
      "grad_norm": 2.1457903385162354,
      "learning_rate": 0.09215178158260065,
      "loss": 0.0492,
      "step": 8480
    },
    {
      "epoch": 3.928736695974086,
      "grad_norm": 8.843616485595703,
      "learning_rate": 0.09214252660805183,
      "loss": 0.0343,
      "step": 8490
    },
    {
      "epoch": 3.933364183248496,
      "grad_norm": 2.309263229370117,
      "learning_rate": 0.09213327163350302,
      "loss": 0.0323,
      "step": 8500
    },
    {
      "epoch": 3.937991670522906,
      "grad_norm": 1.5148833990097046,
      "learning_rate": 0.0921240166589542,
      "loss": 0.0311,
      "step": 8510
    },
    {
      "epoch": 3.942619157797316,
      "grad_norm": 5.0368571281433105,
      "learning_rate": 0.09211476168440537,
      "loss": 0.0532,
      "step": 8520
    },
    {
      "epoch": 3.947246645071726,
      "grad_norm": 7.773936748504639,
      "learning_rate": 0.09210550670985655,
      "loss": 0.0225,
      "step": 8530
    },
    {
      "epoch": 3.951874132346136,
      "grad_norm": 0.419840544462204,
      "learning_rate": 0.09209625173530774,
      "loss": 0.0599,
      "step": 8540
    },
    {
      "epoch": 3.9565016196205463,
      "grad_norm": 3.5496740341186523,
      "learning_rate": 0.09208699676075892,
      "loss": 0.0143,
      "step": 8550
    },
    {
      "epoch": 3.961129106894956,
      "grad_norm": 1.3381887674331665,
      "learning_rate": 0.09207774178621009,
      "loss": 0.038,
      "step": 8560
    },
    {
      "epoch": 3.965756594169366,
      "grad_norm": 0.059486355632543564,
      "learning_rate": 0.09206848681166127,
      "loss": 0.0251,
      "step": 8570
    },
    {
      "epoch": 3.9703840814437763,
      "grad_norm": 0.37467092275619507,
      "learning_rate": 0.09205923183711245,
      "loss": 0.0153,
      "step": 8580
    },
    {
      "epoch": 3.975011568718186,
      "grad_norm": 1.0898030996322632,
      "learning_rate": 0.09204997686256364,
      "loss": 0.0409,
      "step": 8590
    },
    {
      "epoch": 3.979639055992596,
      "grad_norm": 4.396594524383545,
      "learning_rate": 0.09204072188801482,
      "loss": 0.0412,
      "step": 8600
    },
    {
      "epoch": 3.984266543267006,
      "grad_norm": 0.22139005362987518,
      "learning_rate": 0.09203146691346599,
      "loss": 0.0415,
      "step": 8610
    },
    {
      "epoch": 3.988894030541416,
      "grad_norm": 0.046416569501161575,
      "learning_rate": 0.09202221193891717,
      "loss": 0.0214,
      "step": 8620
    },
    {
      "epoch": 3.993521517815826,
      "grad_norm": 0.5143066048622131,
      "learning_rate": 0.09201295696436836,
      "loss": 0.0302,
      "step": 8630
    },
    {
      "epoch": 3.998149005090236,
      "grad_norm": 2.719592332839966,
      "learning_rate": 0.09200370198981954,
      "loss": 0.0295,
      "step": 8640
    },
    {
      "epoch": 4.0,
      "eval_accuracy_branch1": 0.9782776151594516,
      "eval_accuracy_branch2": 0.49946079186566017,
      "eval_f1_branch1": 0.9796217925019822,
      "eval_f1_branch2": 0.49653287801181323,
      "eval_loss": 0.043937310576438904,
      "eval_precision_branch1": 0.979088554710981,
      "eval_precision_branch2": 0.4994479500768122,
      "eval_recall_branch1": 0.9803695545119108,
      "eval_recall_branch2": 0.4994607918656601,
      "eval_runtime": 29.1355,
      "eval_samples_per_second": 445.574,
      "eval_steps_per_second": 55.705,
      "step": 8644
    },
    {
      "epoch": 4.002776492364646,
      "grad_norm": 3.977815628051758,
      "learning_rate": 0.09199444701527071,
      "loss": 0.0893,
      "step": 8650
    },
    {
      "epoch": 4.007403979639056,
      "grad_norm": 0.436026394367218,
      "learning_rate": 0.09198519204072189,
      "loss": 0.0068,
      "step": 8660
    },
    {
      "epoch": 4.012031466913466,
      "grad_norm": 2.092294454574585,
      "learning_rate": 0.09197593706617307,
      "loss": 0.0191,
      "step": 8670
    },
    {
      "epoch": 4.016658954187876,
      "grad_norm": 0.35975411534309387,
      "learning_rate": 0.09196668209162426,
      "loss": 0.0232,
      "step": 8680
    },
    {
      "epoch": 4.021286441462286,
      "grad_norm": 3.3431079387664795,
      "learning_rate": 0.09195742711707544,
      "loss": 0.0376,
      "step": 8690
    },
    {
      "epoch": 4.025913928736696,
      "grad_norm": 0.03109307959675789,
      "learning_rate": 0.09194817214252661,
      "loss": 0.0311,
      "step": 8700
    },
    {
      "epoch": 4.030541416011106,
      "grad_norm": 5.424489974975586,
      "learning_rate": 0.0919389171679778,
      "loss": 0.0271,
      "step": 8710
    },
    {
      "epoch": 4.035168903285516,
      "grad_norm": 1.4273195266723633,
      "learning_rate": 0.09192966219342896,
      "loss": 0.0215,
      "step": 8720
    },
    {
      "epoch": 4.039796390559926,
      "grad_norm": 0.23958411812782288,
      "learning_rate": 0.09192040721888016,
      "loss": 0.0143,
      "step": 8730
    },
    {
      "epoch": 4.044423877834336,
      "grad_norm": 0.6357892751693726,
      "learning_rate": 0.09191115224433133,
      "loss": 0.0334,
      "step": 8740
    },
    {
      "epoch": 4.049051365108746,
      "grad_norm": 0.0730515718460083,
      "learning_rate": 0.09190189726978251,
      "loss": 0.0215,
      "step": 8750
    },
    {
      "epoch": 4.053678852383156,
      "grad_norm": 3.9982824325561523,
      "learning_rate": 0.0918926422952337,
      "loss": 0.018,
      "step": 8760
    },
    {
      "epoch": 4.058306339657566,
      "grad_norm": 4.659097194671631,
      "learning_rate": 0.09188338732068488,
      "loss": 0.033,
      "step": 8770
    },
    {
      "epoch": 4.062933826931976,
      "grad_norm": 1.5015146732330322,
      "learning_rate": 0.09187413234613606,
      "loss": 0.0431,
      "step": 8780
    },
    {
      "epoch": 4.067561314206386,
      "grad_norm": 0.6903004050254822,
      "learning_rate": 0.09186487737158723,
      "loss": 0.0069,
      "step": 8790
    },
    {
      "epoch": 4.072188801480796,
      "grad_norm": 0.435776025056839,
      "learning_rate": 0.09185562239703841,
      "loss": 0.0156,
      "step": 8800
    },
    {
      "epoch": 4.0768162887552055,
      "grad_norm": 0.9439722299575806,
      "learning_rate": 0.09184636742248958,
      "loss": 0.0256,
      "step": 8810
    },
    {
      "epoch": 4.081443776029616,
      "grad_norm": 1.2044398784637451,
      "learning_rate": 0.09183711244794078,
      "loss": 0.0056,
      "step": 8820
    },
    {
      "epoch": 4.086071263304026,
      "grad_norm": 1.600117802619934,
      "learning_rate": 0.09182785747339195,
      "loss": 0.0291,
      "step": 8830
    },
    {
      "epoch": 4.090698750578436,
      "grad_norm": 1.5767927169799805,
      "learning_rate": 0.09181860249884313,
      "loss": 0.0102,
      "step": 8840
    },
    {
      "epoch": 4.095326237852846,
      "grad_norm": 5.991331100463867,
      "learning_rate": 0.09180934752429432,
      "loss": 0.0282,
      "step": 8850
    },
    {
      "epoch": 4.099953725127256,
      "grad_norm": 0.0777682363986969,
      "learning_rate": 0.0918000925497455,
      "loss": 0.0271,
      "step": 8860
    },
    {
      "epoch": 4.104581212401666,
      "grad_norm": 0.3771457076072693,
      "learning_rate": 0.09179083757519668,
      "loss": 0.019,
      "step": 8870
    },
    {
      "epoch": 4.109208699676076,
      "grad_norm": 0.048065222799777985,
      "learning_rate": 0.09178158260064785,
      "loss": 0.0165,
      "step": 8880
    },
    {
      "epoch": 4.113836186950486,
      "grad_norm": 0.7262923121452332,
      "learning_rate": 0.09177232762609903,
      "loss": 0.013,
      "step": 8890
    },
    {
      "epoch": 4.118463674224896,
      "grad_norm": 5.6731367111206055,
      "learning_rate": 0.0917630726515502,
      "loss": 0.0436,
      "step": 8900
    },
    {
      "epoch": 4.123091161499306,
      "grad_norm": 9.853696823120117,
      "learning_rate": 0.0917538176770014,
      "loss": 0.0464,
      "step": 8910
    },
    {
      "epoch": 4.127718648773715,
      "grad_norm": 1.0063306093215942,
      "learning_rate": 0.09174456270245257,
      "loss": 0.0067,
      "step": 8920
    },
    {
      "epoch": 4.132346136048126,
      "grad_norm": 4.822841644287109,
      "learning_rate": 0.09173530772790375,
      "loss": 0.0178,
      "step": 8930
    },
    {
      "epoch": 4.136973623322536,
      "grad_norm": 6.2579522132873535,
      "learning_rate": 0.09172605275335494,
      "loss": 0.0298,
      "step": 8940
    },
    {
      "epoch": 4.1416011105969455,
      "grad_norm": 1.8255317211151123,
      "learning_rate": 0.0917167977788061,
      "loss": 0.0289,
      "step": 8950
    },
    {
      "epoch": 4.146228597871356,
      "grad_norm": 1.460131287574768,
      "learning_rate": 0.0917075428042573,
      "loss": 0.0357,
      "step": 8960
    },
    {
      "epoch": 4.150856085145766,
      "grad_norm": 1.5751564502716064,
      "learning_rate": 0.09169828782970847,
      "loss": 0.0233,
      "step": 8970
    },
    {
      "epoch": 4.1554835724201755,
      "grad_norm": 0.0850958451628685,
      "learning_rate": 0.09168903285515966,
      "loss": 0.0096,
      "step": 8980
    },
    {
      "epoch": 4.160111059694586,
      "grad_norm": 0.2915058732032776,
      "learning_rate": 0.09167977788061082,
      "loss": 0.011,
      "step": 8990
    },
    {
      "epoch": 4.164738546968996,
      "grad_norm": 0.13480938971042633,
      "learning_rate": 0.09167052290606202,
      "loss": 0.0252,
      "step": 9000
    },
    {
      "epoch": 4.169366034243406,
      "grad_norm": 2.3956139087677,
      "learning_rate": 0.09166126793151319,
      "loss": 0.0111,
      "step": 9010
    },
    {
      "epoch": 4.173993521517816,
      "grad_norm": 8.060750961303711,
      "learning_rate": 0.09165201295696437,
      "loss": 0.0249,
      "step": 9020
    },
    {
      "epoch": 4.178621008792226,
      "grad_norm": 7.75682258605957,
      "learning_rate": 0.09164275798241556,
      "loss": 0.0256,
      "step": 9030
    },
    {
      "epoch": 4.183248496066636,
      "grad_norm": 0.042282652109861374,
      "learning_rate": 0.09163350300786673,
      "loss": 0.0288,
      "step": 9040
    },
    {
      "epoch": 4.187875983341046,
      "grad_norm": 2.1464195251464844,
      "learning_rate": 0.09162424803331792,
      "loss": 0.0242,
      "step": 9050
    },
    {
      "epoch": 4.192503470615456,
      "grad_norm": 0.31098878383636475,
      "learning_rate": 0.09161499305876909,
      "loss": 0.0188,
      "step": 9060
    },
    {
      "epoch": 4.197130957889866,
      "grad_norm": 0.06650268286466599,
      "learning_rate": 0.09160573808422028,
      "loss": 0.0186,
      "step": 9070
    },
    {
      "epoch": 4.201758445164276,
      "grad_norm": 5.382957458496094,
      "learning_rate": 0.09159648310967144,
      "loss": 0.0269,
      "step": 9080
    },
    {
      "epoch": 4.206385932438685,
      "grad_norm": 1.1674482822418213,
      "learning_rate": 0.09158722813512264,
      "loss": 0.0195,
      "step": 9090
    },
    {
      "epoch": 4.211013419713096,
      "grad_norm": 2.5521180629730225,
      "learning_rate": 0.09157797316057381,
      "loss": 0.0327,
      "step": 9100
    },
    {
      "epoch": 4.215640906987506,
      "grad_norm": 0.03536469489336014,
      "learning_rate": 0.091568718186025,
      "loss": 0.0276,
      "step": 9110
    },
    {
      "epoch": 4.2202683942619155,
      "grad_norm": 2.4674735069274902,
      "learning_rate": 0.09155946321147618,
      "loss": 0.0259,
      "step": 9120
    },
    {
      "epoch": 4.224895881536326,
      "grad_norm": 0.05056065693497658,
      "learning_rate": 0.09155020823692735,
      "loss": 0.0548,
      "step": 9130
    },
    {
      "epoch": 4.229523368810736,
      "grad_norm": 0.6520084738731384,
      "learning_rate": 0.09154095326237854,
      "loss": 0.0233,
      "step": 9140
    },
    {
      "epoch": 4.2341508560851455,
      "grad_norm": 5.080922603607178,
      "learning_rate": 0.09153169828782971,
      "loss": 0.0265,
      "step": 9150
    },
    {
      "epoch": 4.238778343359556,
      "grad_norm": 2.3679468631744385,
      "learning_rate": 0.0915224433132809,
      "loss": 0.0221,
      "step": 9160
    },
    {
      "epoch": 4.243405830633966,
      "grad_norm": 0.8893410563468933,
      "learning_rate": 0.09151318833873207,
      "loss": 0.053,
      "step": 9170
    },
    {
      "epoch": 4.248033317908376,
      "grad_norm": 3.239063262939453,
      "learning_rate": 0.09150393336418325,
      "loss": 0.0103,
      "step": 9180
    },
    {
      "epoch": 4.252660805182786,
      "grad_norm": 3.3612284660339355,
      "learning_rate": 0.09149467838963443,
      "loss": 0.0372,
      "step": 9190
    },
    {
      "epoch": 4.257288292457195,
      "grad_norm": 4.185802936553955,
      "learning_rate": 0.09148542341508561,
      "loss": 0.0158,
      "step": 9200
    },
    {
      "epoch": 4.261915779731606,
      "grad_norm": 3.9275341033935547,
      "learning_rate": 0.0914761684405368,
      "loss": 0.0401,
      "step": 9210
    },
    {
      "epoch": 4.266543267006016,
      "grad_norm": 4.330700874328613,
      "learning_rate": 0.09146691346598797,
      "loss": 0.0313,
      "step": 9220
    },
    {
      "epoch": 4.271170754280425,
      "grad_norm": 6.300172805786133,
      "learning_rate": 0.09145765849143916,
      "loss": 0.0147,
      "step": 9230
    },
    {
      "epoch": 4.275798241554836,
      "grad_norm": 3.309767723083496,
      "learning_rate": 0.09144840351689033,
      "loss": 0.0248,
      "step": 9240
    },
    {
      "epoch": 4.280425728829246,
      "grad_norm": 0.47480282187461853,
      "learning_rate": 0.09143914854234152,
      "loss": 0.0068,
      "step": 9250
    },
    {
      "epoch": 4.285053216103655,
      "grad_norm": 4.423295021057129,
      "learning_rate": 0.09142989356779269,
      "loss": 0.0607,
      "step": 9260
    },
    {
      "epoch": 4.289680703378066,
      "grad_norm": 0.7398043274879456,
      "learning_rate": 0.09142063859324387,
      "loss": 0.017,
      "step": 9270
    },
    {
      "epoch": 4.294308190652476,
      "grad_norm": 4.292009353637695,
      "learning_rate": 0.09141138361869505,
      "loss": 0.0216,
      "step": 9280
    },
    {
      "epoch": 4.2989356779268855,
      "grad_norm": 4.798901081085205,
      "learning_rate": 0.09140212864414624,
      "loss": 0.0196,
      "step": 9290
    },
    {
      "epoch": 4.303563165201296,
      "grad_norm": 0.3017035722732544,
      "learning_rate": 0.09139287366959742,
      "loss": 0.006,
      "step": 9300
    },
    {
      "epoch": 4.308190652475706,
      "grad_norm": 9.039767265319824,
      "learning_rate": 0.09138361869504859,
      "loss": 0.0463,
      "step": 9310
    },
    {
      "epoch": 4.3128181397501155,
      "grad_norm": 1.9790802001953125,
      "learning_rate": 0.09137436372049978,
      "loss": 0.0094,
      "step": 9320
    },
    {
      "epoch": 4.317445627024526,
      "grad_norm": 3.059816360473633,
      "learning_rate": 0.09136510874595095,
      "loss": 0.0334,
      "step": 9330
    },
    {
      "epoch": 4.322073114298936,
      "grad_norm": 0.0868295356631279,
      "learning_rate": 0.09135585377140214,
      "loss": 0.0145,
      "step": 9340
    },
    {
      "epoch": 4.326700601573346,
      "grad_norm": 1.0887678861618042,
      "learning_rate": 0.0913465987968533,
      "loss": 0.0555,
      "step": 9350
    },
    {
      "epoch": 4.331328088847756,
      "grad_norm": 2.525303363800049,
      "learning_rate": 0.09133734382230449,
      "loss": 0.0306,
      "step": 9360
    },
    {
      "epoch": 4.335955576122165,
      "grad_norm": 0.0973297506570816,
      "learning_rate": 0.09132808884775567,
      "loss": 0.0211,
      "step": 9370
    },
    {
      "epoch": 4.340583063396576,
      "grad_norm": 0.09295464307069778,
      "learning_rate": 0.09131883387320686,
      "loss": 0.0099,
      "step": 9380
    },
    {
      "epoch": 4.345210550670986,
      "grad_norm": 0.04429999738931656,
      "learning_rate": 0.09130957889865804,
      "loss": 0.0221,
      "step": 9390
    },
    {
      "epoch": 4.349838037945395,
      "grad_norm": 0.07646644115447998,
      "learning_rate": 0.09130032392410921,
      "loss": 0.0224,
      "step": 9400
    },
    {
      "epoch": 4.354465525219806,
      "grad_norm": 4.397680759429932,
      "learning_rate": 0.09129106894956039,
      "loss": 0.0238,
      "step": 9410
    },
    {
      "epoch": 4.359093012494216,
      "grad_norm": 0.8842735290527344,
      "learning_rate": 0.09128181397501157,
      "loss": 0.0697,
      "step": 9420
    },
    {
      "epoch": 4.363720499768625,
      "grad_norm": 5.429032325744629,
      "learning_rate": 0.09127255900046276,
      "loss": 0.0296,
      "step": 9430
    },
    {
      "epoch": 4.368347987043036,
      "grad_norm": 2.9458844661712646,
      "learning_rate": 0.09126330402591393,
      "loss": 0.0181,
      "step": 9440
    },
    {
      "epoch": 4.372975474317446,
      "grad_norm": 3.979536771774292,
      "learning_rate": 0.09125404905136511,
      "loss": 0.0169,
      "step": 9450
    },
    {
      "epoch": 4.3776029615918555,
      "grad_norm": 0.35038530826568604,
      "learning_rate": 0.0912447940768163,
      "loss": 0.0212,
      "step": 9460
    },
    {
      "epoch": 4.382230448866266,
      "grad_norm": 0.015654178336262703,
      "learning_rate": 0.09123553910226748,
      "loss": 0.0115,
      "step": 9470
    },
    {
      "epoch": 4.386857936140675,
      "grad_norm": 0.8668747544288635,
      "learning_rate": 0.09122628412771866,
      "loss": 0.0418,
      "step": 9480
    },
    {
      "epoch": 4.3914854234150855,
      "grad_norm": 3.294093370437622,
      "learning_rate": 0.09121702915316983,
      "loss": 0.0326,
      "step": 9490
    },
    {
      "epoch": 4.396112910689496,
      "grad_norm": 1.2898039817810059,
      "learning_rate": 0.09120777417862101,
      "loss": 0.0266,
      "step": 9500
    },
    {
      "epoch": 4.400740397963905,
      "grad_norm": 2.8643600940704346,
      "learning_rate": 0.0911985192040722,
      "loss": 0.0213,
      "step": 9510
    },
    {
      "epoch": 4.405367885238316,
      "grad_norm": 1.0709147453308105,
      "learning_rate": 0.09118926422952338,
      "loss": 0.01,
      "step": 9520
    },
    {
      "epoch": 4.409995372512726,
      "grad_norm": 0.5586109161376953,
      "learning_rate": 0.09118000925497455,
      "loss": 0.0317,
      "step": 9530
    },
    {
      "epoch": 4.414622859787135,
      "grad_norm": 0.3960045576095581,
      "learning_rate": 0.09117075428042573,
      "loss": 0.033,
      "step": 9540
    },
    {
      "epoch": 4.419250347061546,
      "grad_norm": 0.06127778813242912,
      "learning_rate": 0.09116149930587691,
      "loss": 0.0164,
      "step": 9550
    },
    {
      "epoch": 4.423877834335956,
      "grad_norm": 8.597823143005371,
      "learning_rate": 0.0911522443313281,
      "loss": 0.0742,
      "step": 9560
    },
    {
      "epoch": 4.428505321610365,
      "grad_norm": 0.1982940137386322,
      "learning_rate": 0.09114298935677928,
      "loss": 0.0084,
      "step": 9570
    },
    {
      "epoch": 4.433132808884776,
      "grad_norm": 0.06985611468553543,
      "learning_rate": 0.09113373438223045,
      "loss": 0.0089,
      "step": 9580
    },
    {
      "epoch": 4.437760296159185,
      "grad_norm": 6.421295642852783,
      "learning_rate": 0.09112447940768163,
      "loss": 0.0156,
      "step": 9590
    },
    {
      "epoch": 4.442387783433595,
      "grad_norm": 5.889334678649902,
      "learning_rate": 0.09111522443313282,
      "loss": 0.0248,
      "step": 9600
    },
    {
      "epoch": 4.447015270708006,
      "grad_norm": 0.7142423987388611,
      "learning_rate": 0.091105969458584,
      "loss": 0.0397,
      "step": 9610
    },
    {
      "epoch": 4.451642757982415,
      "grad_norm": 1.4376230239868164,
      "learning_rate": 0.09109671448403517,
      "loss": 0.0242,
      "step": 9620
    },
    {
      "epoch": 4.4562702452568255,
      "grad_norm": 0.46535494923591614,
      "learning_rate": 0.09108745950948635,
      "loss": 0.0319,
      "step": 9630
    },
    {
      "epoch": 4.460897732531236,
      "grad_norm": 3.504230499267578,
      "learning_rate": 0.09107820453493753,
      "loss": 0.0206,
      "step": 9640
    },
    {
      "epoch": 4.465525219805645,
      "grad_norm": 4.735328674316406,
      "learning_rate": 0.09106894956038872,
      "loss": 0.0169,
      "step": 9650
    },
    {
      "epoch": 4.4701527070800555,
      "grad_norm": 4.60107421875,
      "learning_rate": 0.0910596945858399,
      "loss": 0.0212,
      "step": 9660
    },
    {
      "epoch": 4.474780194354466,
      "grad_norm": 3.303623914718628,
      "learning_rate": 0.09105043961129107,
      "loss": 0.0239,
      "step": 9670
    },
    {
      "epoch": 4.479407681628875,
      "grad_norm": 0.6326615810394287,
      "learning_rate": 0.09104118463674225,
      "loss": 0.0109,
      "step": 9680
    },
    {
      "epoch": 4.484035168903286,
      "grad_norm": 1.244436264038086,
      "learning_rate": 0.09103192966219344,
      "loss": 0.0092,
      "step": 9690
    },
    {
      "epoch": 4.488662656177696,
      "grad_norm": 1.3846900463104248,
      "learning_rate": 0.09102267468764462,
      "loss": 0.0227,
      "step": 9700
    },
    {
      "epoch": 4.493290143452105,
      "grad_norm": 1.3171160221099854,
      "learning_rate": 0.09101341971309579,
      "loss": 0.0214,
      "step": 9710
    },
    {
      "epoch": 4.497917630726516,
      "grad_norm": 0.018614383414387703,
      "learning_rate": 0.09100416473854697,
      "loss": 0.022,
      "step": 9720
    },
    {
      "epoch": 4.502545118000926,
      "grad_norm": 2.0462002754211426,
      "learning_rate": 0.09099490976399816,
      "loss": 0.0259,
      "step": 9730
    },
    {
      "epoch": 4.507172605275335,
      "grad_norm": 0.26689261198043823,
      "learning_rate": 0.09098565478944934,
      "loss": 0.0383,
      "step": 9740
    },
    {
      "epoch": 4.511800092549746,
      "grad_norm": 0.1993722766637802,
      "learning_rate": 0.09097639981490052,
      "loss": 0.015,
      "step": 9750
    },
    {
      "epoch": 4.516427579824155,
      "grad_norm": 1.6421046257019043,
      "learning_rate": 0.09096714484035169,
      "loss": 0.0126,
      "step": 9760
    },
    {
      "epoch": 4.521055067098565,
      "grad_norm": 0.5891650915145874,
      "learning_rate": 0.09095788986580287,
      "loss": 0.0343,
      "step": 9770
    },
    {
      "epoch": 4.525682554372976,
      "grad_norm": 3.7665042877197266,
      "learning_rate": 0.09094863489125406,
      "loss": 0.0054,
      "step": 9780
    },
    {
      "epoch": 4.530310041647385,
      "grad_norm": 0.19143396615982056,
      "learning_rate": 0.09093937991670524,
      "loss": 0.0225,
      "step": 9790
    },
    {
      "epoch": 4.5349375289217955,
      "grad_norm": 0.26194626092910767,
      "learning_rate": 0.09093012494215641,
      "loss": 0.0171,
      "step": 9800
    },
    {
      "epoch": 4.539565016196206,
      "grad_norm": 3.4518611431121826,
      "learning_rate": 0.09092086996760759,
      "loss": 0.057,
      "step": 9810
    },
    {
      "epoch": 4.544192503470615,
      "grad_norm": 0.16016414761543274,
      "learning_rate": 0.09091161499305878,
      "loss": 0.0075,
      "step": 9820
    },
    {
      "epoch": 4.5488199907450255,
      "grad_norm": 0.47583580017089844,
      "learning_rate": 0.09090236001850996,
      "loss": 0.0284,
      "step": 9830
    },
    {
      "epoch": 4.553447478019436,
      "grad_norm": 0.4096711277961731,
      "learning_rate": 0.09089310504396114,
      "loss": 0.0191,
      "step": 9840
    },
    {
      "epoch": 4.558074965293845,
      "grad_norm": 0.0514444075524807,
      "learning_rate": 0.09088385006941231,
      "loss": 0.0082,
      "step": 9850
    },
    {
      "epoch": 4.562702452568256,
      "grad_norm": 3.3082382678985596,
      "learning_rate": 0.0908745950948635,
      "loss": 0.0166,
      "step": 9860
    },
    {
      "epoch": 4.567329939842665,
      "grad_norm": 0.0943099707365036,
      "learning_rate": 0.09086534012031466,
      "loss": 0.0119,
      "step": 9870
    },
    {
      "epoch": 4.571957427117075,
      "grad_norm": 0.02202226035296917,
      "learning_rate": 0.09085608514576586,
      "loss": 0.0413,
      "step": 9880
    },
    {
      "epoch": 4.576584914391486,
      "grad_norm": 0.49026837944984436,
      "learning_rate": 0.09084683017121703,
      "loss": 0.0207,
      "step": 9890
    },
    {
      "epoch": 4.581212401665895,
      "grad_norm": 4.340357303619385,
      "learning_rate": 0.09083757519666821,
      "loss": 0.0308,
      "step": 9900
    },
    {
      "epoch": 4.585839888940305,
      "grad_norm": 0.24688607454299927,
      "learning_rate": 0.0908283202221194,
      "loss": 0.0295,
      "step": 9910
    },
    {
      "epoch": 4.590467376214716,
      "grad_norm": 0.02052350528538227,
      "learning_rate": 0.09081906524757058,
      "loss": 0.0147,
      "step": 9920
    },
    {
      "epoch": 4.595094863489125,
      "grad_norm": 0.12291600555181503,
      "learning_rate": 0.09080981027302176,
      "loss": 0.0215,
      "step": 9930
    },
    {
      "epoch": 4.599722350763535,
      "grad_norm": 0.037439923733472824,
      "learning_rate": 0.09080055529847293,
      "loss": 0.0225,
      "step": 9940
    },
    {
      "epoch": 4.604349838037946,
      "grad_norm": 0.0939272791147232,
      "learning_rate": 0.09079130032392412,
      "loss": 0.006,
      "step": 9950
    },
    {
      "epoch": 4.608977325312355,
      "grad_norm": 2.159637928009033,
      "learning_rate": 0.09078204534937528,
      "loss": 0.0251,
      "step": 9960
    },
    {
      "epoch": 4.6136048125867655,
      "grad_norm": 0.5099025964736938,
      "learning_rate": 0.09077279037482648,
      "loss": 0.0438,
      "step": 9970
    },
    {
      "epoch": 4.618232299861175,
      "grad_norm": 0.28630736470222473,
      "learning_rate": 0.09076353540027765,
      "loss": 0.0204,
      "step": 9980
    },
    {
      "epoch": 4.622859787135585,
      "grad_norm": 0.09560825675725937,
      "learning_rate": 0.09075428042572883,
      "loss": 0.0096,
      "step": 9990
    },
    {
      "epoch": 4.6274872744099955,
      "grad_norm": 1.3530117273330688,
      "learning_rate": 0.09074502545118002,
      "loss": 0.0226,
      "step": 10000
    },
    {
      "epoch": 4.632114761684406,
      "grad_norm": 1.2275217771530151,
      "learning_rate": 0.0907357704766312,
      "loss": 0.0151,
      "step": 10010
    },
    {
      "epoch": 4.636742248958815,
      "grad_norm": 0.5107289552688599,
      "learning_rate": 0.09072651550208238,
      "loss": 0.0395,
      "step": 10020
    },
    {
      "epoch": 4.641369736233226,
      "grad_norm": 0.33043596148490906,
      "learning_rate": 0.09071726052753355,
      "loss": 0.0188,
      "step": 10030
    },
    {
      "epoch": 4.645997223507635,
      "grad_norm": 1.2899388074874878,
      "learning_rate": 0.09070800555298474,
      "loss": 0.0443,
      "step": 10040
    },
    {
      "epoch": 4.650624710782045,
      "grad_norm": 1.9473257064819336,
      "learning_rate": 0.0906987505784359,
      "loss": 0.006,
      "step": 10050
    },
    {
      "epoch": 4.655252198056456,
      "grad_norm": 4.356199264526367,
      "learning_rate": 0.0906894956038871,
      "loss": 0.0184,
      "step": 10060
    },
    {
      "epoch": 4.659879685330865,
      "grad_norm": 0.8985303640365601,
      "learning_rate": 0.09068024062933827,
      "loss": 0.0093,
      "step": 10070
    },
    {
      "epoch": 4.664507172605275,
      "grad_norm": 0.2231159657239914,
      "learning_rate": 0.09067098565478945,
      "loss": 0.025,
      "step": 10080
    },
    {
      "epoch": 4.669134659879686,
      "grad_norm": 0.07551005482673645,
      "learning_rate": 0.09066173068024064,
      "loss": 0.0292,
      "step": 10090
    },
    {
      "epoch": 4.673762147154095,
      "grad_norm": 0.26885274052619934,
      "learning_rate": 0.0906524757056918,
      "loss": 0.0037,
      "step": 10100
    },
    {
      "epoch": 4.678389634428505,
      "grad_norm": 0.6960651874542236,
      "learning_rate": 0.090643220731143,
      "loss": 0.0078,
      "step": 10110
    },
    {
      "epoch": 4.683017121702916,
      "grad_norm": 4.756002902984619,
      "learning_rate": 0.09063396575659417,
      "loss": 0.0224,
      "step": 10120
    },
    {
      "epoch": 4.687644608977325,
      "grad_norm": 0.08218220621347427,
      "learning_rate": 0.09062471078204536,
      "loss": 0.0285,
      "step": 10130
    },
    {
      "epoch": 4.6922720962517355,
      "grad_norm": 5.417265892028809,
      "learning_rate": 0.09061545580749653,
      "loss": 0.0273,
      "step": 10140
    },
    {
      "epoch": 4.696899583526145,
      "grad_norm": 0.5311005711555481,
      "learning_rate": 0.09060620083294772,
      "loss": 0.035,
      "step": 10150
    },
    {
      "epoch": 4.701527070800555,
      "grad_norm": 0.5260069966316223,
      "learning_rate": 0.09059694585839889,
      "loss": 0.0139,
      "step": 10160
    },
    {
      "epoch": 4.7061545580749655,
      "grad_norm": 2.926518201828003,
      "learning_rate": 0.09058769088385007,
      "loss": 0.0238,
      "step": 10170
    },
    {
      "epoch": 4.710782045349375,
      "grad_norm": 0.41007429361343384,
      "learning_rate": 0.09057843590930126,
      "loss": 0.0082,
      "step": 10180
    },
    {
      "epoch": 4.715409532623785,
      "grad_norm": 0.15233318507671356,
      "learning_rate": 0.09056918093475243,
      "loss": 0.0029,
      "step": 10190
    },
    {
      "epoch": 4.720037019898196,
      "grad_norm": 4.82659912109375,
      "learning_rate": 0.09055992596020362,
      "loss": 0.018,
      "step": 10200
    },
    {
      "epoch": 4.724664507172605,
      "grad_norm": 0.015165450982749462,
      "learning_rate": 0.0905506709856548,
      "loss": 0.0346,
      "step": 10210
    },
    {
      "epoch": 4.729291994447015,
      "grad_norm": 2.627523422241211,
      "learning_rate": 0.09054141601110598,
      "loss": 0.0109,
      "step": 10220
    },
    {
      "epoch": 4.733919481721426,
      "grad_norm": 1.341055989265442,
      "learning_rate": 0.09053216103655715,
      "loss": 0.019,
      "step": 10230
    },
    {
      "epoch": 4.738546968995835,
      "grad_norm": 0.7834033370018005,
      "learning_rate": 0.09052290606200834,
      "loss": 0.0239,
      "step": 10240
    },
    {
      "epoch": 4.743174456270245,
      "grad_norm": 0.204194113612175,
      "learning_rate": 0.09051365108745951,
      "loss": 0.0279,
      "step": 10250
    },
    {
      "epoch": 4.747801943544655,
      "grad_norm": 5.686503887176514,
      "learning_rate": 0.0905043961129107,
      "loss": 0.0309,
      "step": 10260
    },
    {
      "epoch": 4.752429430819065,
      "grad_norm": 0.22256964445114136,
      "learning_rate": 0.09049514113836188,
      "loss": 0.0105,
      "step": 10270
    },
    {
      "epoch": 4.757056918093475,
      "grad_norm": 0.10668143630027771,
      "learning_rate": 0.09048588616381305,
      "loss": 0.0284,
      "step": 10280
    },
    {
      "epoch": 4.761684405367886,
      "grad_norm": 6.2273149490356445,
      "learning_rate": 0.09047663118926424,
      "loss": 0.0298,
      "step": 10290
    },
    {
      "epoch": 4.766311892642295,
      "grad_norm": 0.36366698145866394,
      "learning_rate": 0.09046737621471541,
      "loss": 0.0228,
      "step": 10300
    },
    {
      "epoch": 4.7709393799167055,
      "grad_norm": 0.06526807695627213,
      "learning_rate": 0.0904581212401666,
      "loss": 0.025,
      "step": 10310
    },
    {
      "epoch": 4.775566867191115,
      "grad_norm": 0.7695848345756531,
      "learning_rate": 0.09044886626561777,
      "loss": 0.0062,
      "step": 10320
    },
    {
      "epoch": 4.780194354465525,
      "grad_norm": 3.0148346424102783,
      "learning_rate": 0.09043961129106895,
      "loss": 0.0256,
      "step": 10330
    },
    {
      "epoch": 4.7848218417399355,
      "grad_norm": 0.0767793357372284,
      "learning_rate": 0.09043035631652013,
      "loss": 0.012,
      "step": 10340
    },
    {
      "epoch": 4.789449329014345,
      "grad_norm": 0.10662366449832916,
      "learning_rate": 0.09042110134197132,
      "loss": 0.0261,
      "step": 10350
    },
    {
      "epoch": 4.794076816288755,
      "grad_norm": 1.5058810710906982,
      "learning_rate": 0.0904118463674225,
      "loss": 0.0352,
      "step": 10360
    },
    {
      "epoch": 4.798704303563166,
      "grad_norm": 3.5093185901641846,
      "learning_rate": 0.09040259139287367,
      "loss": 0.0082,
      "step": 10370
    },
    {
      "epoch": 4.803331790837575,
      "grad_norm": 0.23087526857852936,
      "learning_rate": 0.09039333641832487,
      "loss": 0.0171,
      "step": 10380
    },
    {
      "epoch": 4.807959278111985,
      "grad_norm": 0.12244853377342224,
      "learning_rate": 0.09038408144377603,
      "loss": 0.0212,
      "step": 10390
    },
    {
      "epoch": 4.812586765386396,
      "grad_norm": 0.7047516703605652,
      "learning_rate": 0.09037482646922722,
      "loss": 0.0318,
      "step": 10400
    },
    {
      "epoch": 4.817214252660805,
      "grad_norm": 4.997812747955322,
      "learning_rate": 0.09036557149467839,
      "loss": 0.0164,
      "step": 10410
    },
    {
      "epoch": 4.821841739935215,
      "grad_norm": 0.3201513886451721,
      "learning_rate": 0.09035631652012957,
      "loss": 0.0104,
      "step": 10420
    },
    {
      "epoch": 4.826469227209625,
      "grad_norm": 0.6302543878555298,
      "learning_rate": 0.09034706154558075,
      "loss": 0.0123,
      "step": 10430
    },
    {
      "epoch": 4.831096714484035,
      "grad_norm": 0.10875947773456573,
      "learning_rate": 0.09033780657103194,
      "loss": 0.0052,
      "step": 10440
    },
    {
      "epoch": 4.835724201758445,
      "grad_norm": 0.4024742841720581,
      "learning_rate": 0.09032855159648312,
      "loss": 0.0227,
      "step": 10450
    },
    {
      "epoch": 4.840351689032855,
      "grad_norm": 2.248798131942749,
      "learning_rate": 0.09031929662193429,
      "loss": 0.0178,
      "step": 10460
    },
    {
      "epoch": 4.844979176307265,
      "grad_norm": 3.4362213611602783,
      "learning_rate": 0.09031004164738549,
      "loss": 0.0146,
      "step": 10470
    },
    {
      "epoch": 4.8496066635816755,
      "grad_norm": 2.4831106662750244,
      "learning_rate": 0.09030078667283666,
      "loss": 0.0256,
      "step": 10480
    },
    {
      "epoch": 4.854234150856085,
      "grad_norm": 0.14171890914440155,
      "learning_rate": 0.09029153169828784,
      "loss": 0.004,
      "step": 10490
    },
    {
      "epoch": 4.858861638130495,
      "grad_norm": 0.014171826653182507,
      "learning_rate": 0.09028227672373901,
      "loss": 0.036,
      "step": 10500
    },
    {
      "epoch": 4.8634891254049055,
      "grad_norm": 4.710233211517334,
      "learning_rate": 0.09027302174919019,
      "loss": 0.0193,
      "step": 10510
    },
    {
      "epoch": 4.868116612679315,
      "grad_norm": 0.07266247272491455,
      "learning_rate": 0.09026376677464137,
      "loss": 0.0067,
      "step": 10520
    },
    {
      "epoch": 4.872744099953725,
      "grad_norm": 2.108464002609253,
      "learning_rate": 0.09025451180009256,
      "loss": 0.0178,
      "step": 10530
    },
    {
      "epoch": 4.877371587228135,
      "grad_norm": 1.2862329483032227,
      "learning_rate": 0.09024525682554374,
      "loss": 0.0273,
      "step": 10540
    },
    {
      "epoch": 4.881999074502545,
      "grad_norm": 0.17627008259296417,
      "learning_rate": 0.09023600185099491,
      "loss": 0.0127,
      "step": 10550
    },
    {
      "epoch": 4.886626561776955,
      "grad_norm": 7.825480937957764,
      "learning_rate": 0.09022674687644609,
      "loss": 0.05,
      "step": 10560
    },
    {
      "epoch": 4.891254049051365,
      "grad_norm": 0.07123754918575287,
      "learning_rate": 0.09021749190189728,
      "loss": 0.0215,
      "step": 10570
    },
    {
      "epoch": 4.895881536325775,
      "grad_norm": 5.2579193115234375,
      "learning_rate": 0.09020823692734846,
      "loss": 0.0132,
      "step": 10580
    },
    {
      "epoch": 4.900509023600185,
      "grad_norm": 4.7822747230529785,
      "learning_rate": 0.09019898195279963,
      "loss": 0.0214,
      "step": 10590
    },
    {
      "epoch": 4.905136510874595,
      "grad_norm": 0.01841108687222004,
      "learning_rate": 0.09018972697825081,
      "loss": 0.0084,
      "step": 10600
    },
    {
      "epoch": 4.909763998149005,
      "grad_norm": 1.0053913593292236,
      "learning_rate": 0.090180472003702,
      "loss": 0.0291,
      "step": 10610
    },
    {
      "epoch": 4.914391485423415,
      "grad_norm": 0.536287248134613,
      "learning_rate": 0.09017121702915318,
      "loss": 0.0124,
      "step": 10620
    },
    {
      "epoch": 4.919018972697825,
      "grad_norm": 0.03668820858001709,
      "learning_rate": 0.09016196205460436,
      "loss": 0.0266,
      "step": 10630
    },
    {
      "epoch": 4.923646459972235,
      "grad_norm": 0.2835097312927246,
      "learning_rate": 0.09015270708005553,
      "loss": 0.0135,
      "step": 10640
    },
    {
      "epoch": 4.928273947246645,
      "grad_norm": 2.161862373352051,
      "learning_rate": 0.09014345210550671,
      "loss": 0.0121,
      "step": 10650
    },
    {
      "epoch": 4.932901434521055,
      "grad_norm": 2.535583019256592,
      "learning_rate": 0.0901341971309579,
      "loss": 0.0165,
      "step": 10660
    },
    {
      "epoch": 4.937528921795465,
      "grad_norm": 8.602861404418945,
      "learning_rate": 0.09012494215640908,
      "loss": 0.025,
      "step": 10670
    },
    {
      "epoch": 4.9421564090698755,
      "grad_norm": 0.03591713309288025,
      "learning_rate": 0.09011568718186025,
      "loss": 0.0303,
      "step": 10680
    },
    {
      "epoch": 4.946783896344285,
      "grad_norm": 1.928389072418213,
      "learning_rate": 0.09010643220731143,
      "loss": 0.0139,
      "step": 10690
    },
    {
      "epoch": 4.951411383618695,
      "grad_norm": 2.0256948471069336,
      "learning_rate": 0.09009717723276262,
      "loss": 0.0069,
      "step": 10700
    },
    {
      "epoch": 4.956038870893105,
      "grad_norm": 1.5658437013626099,
      "learning_rate": 0.0900879222582138,
      "loss": 0.0148,
      "step": 10710
    },
    {
      "epoch": 4.960666358167515,
      "grad_norm": 8.921109199523926,
      "learning_rate": 0.09007866728366498,
      "loss": 0.0304,
      "step": 10720
    },
    {
      "epoch": 4.965293845441925,
      "grad_norm": 2.7563223838806152,
      "learning_rate": 0.09006941230911615,
      "loss": 0.0172,
      "step": 10730
    },
    {
      "epoch": 4.969921332716335,
      "grad_norm": 0.14566993713378906,
      "learning_rate": 0.09006015733456733,
      "loss": 0.0083,
      "step": 10740
    },
    {
      "epoch": 4.974548819990745,
      "grad_norm": 4.133203983306885,
      "learning_rate": 0.09005090236001852,
      "loss": 0.0227,
      "step": 10750
    },
    {
      "epoch": 4.979176307265155,
      "grad_norm": 0.10322694480419159,
      "learning_rate": 0.0900416473854697,
      "loss": 0.04,
      "step": 10760
    },
    {
      "epoch": 4.983803794539565,
      "grad_norm": 5.222519874572754,
      "learning_rate": 0.09003239241092087,
      "loss": 0.0123,
      "step": 10770
    },
    {
      "epoch": 4.988431281813975,
      "grad_norm": 0.0516502670943737,
      "learning_rate": 0.09002313743637205,
      "loss": 0.0188,
      "step": 10780
    },
    {
      "epoch": 4.993058769088385,
      "grad_norm": 1.271926999092102,
      "learning_rate": 0.09001388246182324,
      "loss": 0.022,
      "step": 10790
    },
    {
      "epoch": 4.997686256362795,
      "grad_norm": 2.5590877532958984,
      "learning_rate": 0.09000462748727442,
      "loss": 0.0407,
      "step": 10800
    },
    {
      "epoch": 5.0,
      "eval_accuracy_branch1": 0.9731936527499615,
      "eval_accuracy_branch2": 0.49976891079956864,
      "eval_f1_branch1": 0.9717891605077557,
      "eval_f1_branch2": 0.48483945661396244,
      "eval_loss": 0.0625557005405426,
      "eval_precision_branch1": 0.9743161039523581,
      "eval_precision_branch2": 0.499738610299165,
      "eval_recall_branch1": 0.9703671504327344,
      "eval_recall_branch2": 0.49976891079956864,
      "eval_runtime": 30.0959,
      "eval_samples_per_second": 431.355,
      "eval_steps_per_second": 53.928,
      "step": 10805
    },
    {
      "epoch": 5.002313743637205,
      "grad_norm": 6.52254056930542,
      "learning_rate": 0.0899953725127256,
      "loss": 0.0936,
      "step": 10810
    },
    {
      "epoch": 5.006941230911615,
      "grad_norm": 4.482895851135254,
      "learning_rate": 0.08998611753817677,
      "loss": 0.0121,
      "step": 10820
    },
    {
      "epoch": 5.011568718186025,
      "grad_norm": 2.969670057296753,
      "learning_rate": 0.08997686256362795,
      "loss": 0.0171,
      "step": 10830
    },
    {
      "epoch": 5.016196205460435,
      "grad_norm": 8.00521469116211,
      "learning_rate": 0.08996760758907914,
      "loss": 0.0188,
      "step": 10840
    },
    {
      "epoch": 5.020823692734845,
      "grad_norm": 0.5380814671516418,
      "learning_rate": 0.08995835261453032,
      "loss": 0.0385,
      "step": 10850
    },
    {
      "epoch": 5.025451180009255,
      "grad_norm": 6.058333873748779,
      "learning_rate": 0.08994909763998149,
      "loss": 0.0119,
      "step": 10860
    },
    {
      "epoch": 5.030078667283665,
      "grad_norm": 0.11129307001829147,
      "learning_rate": 0.08993984266543267,
      "loss": 0.0052,
      "step": 10870
    },
    {
      "epoch": 5.034706154558075,
      "grad_norm": 7.815112113952637,
      "learning_rate": 0.08993058769088386,
      "loss": 0.0181,
      "step": 10880
    },
    {
      "epoch": 5.039333641832485,
      "grad_norm": 0.0480571910738945,
      "learning_rate": 0.08992133271633504,
      "loss": 0.0206,
      "step": 10890
    },
    {
      "epoch": 5.043961129106895,
      "grad_norm": 1.8026174306869507,
      "learning_rate": 0.08991207774178622,
      "loss": 0.0157,
      "step": 10900
    },
    {
      "epoch": 5.048588616381305,
      "grad_norm": 0.364542156457901,
      "learning_rate": 0.08990282276723739,
      "loss": 0.0136,
      "step": 10910
    },
    {
      "epoch": 5.053216103655715,
      "grad_norm": 0.36128389835357666,
      "learning_rate": 0.08989356779268858,
      "loss": 0.0071,
      "step": 10920
    },
    {
      "epoch": 5.057843590930125,
      "grad_norm": 0.11789747327566147,
      "learning_rate": 0.08988431281813976,
      "loss": 0.0161,
      "step": 10930
    },
    {
      "epoch": 5.062471078204535,
      "grad_norm": 2.359255313873291,
      "learning_rate": 0.08987505784359094,
      "loss": 0.0096,
      "step": 10940
    },
    {
      "epoch": 5.067098565478945,
      "grad_norm": 0.2834584712982178,
      "learning_rate": 0.08986580286904211,
      "loss": 0.0388,
      "step": 10950
    },
    {
      "epoch": 5.0717260527533545,
      "grad_norm": 0.14086021482944489,
      "learning_rate": 0.0898565478944933,
      "loss": 0.0348,
      "step": 10960
    },
    {
      "epoch": 5.076353540027765,
      "grad_norm": 0.8565623164176941,
      "learning_rate": 0.08984729291994448,
      "loss": 0.0191,
      "step": 10970
    },
    {
      "epoch": 5.080981027302175,
      "grad_norm": 1.2434078454971313,
      "learning_rate": 0.08983803794539566,
      "loss": 0.013,
      "step": 10980
    },
    {
      "epoch": 5.085608514576585,
      "grad_norm": 0.5282365083694458,
      "learning_rate": 0.08982878297084684,
      "loss": 0.0014,
      "step": 10990
    },
    {
      "epoch": 5.090236001850995,
      "grad_norm": 0.47210872173309326,
      "learning_rate": 0.08981952799629801,
      "loss": 0.0112,
      "step": 11000
    },
    {
      "epoch": 5.094863489125405,
      "grad_norm": 0.2553764283657074,
      "learning_rate": 0.0898102730217492,
      "loss": 0.0266,
      "step": 11010
    },
    {
      "epoch": 5.099490976399815,
      "grad_norm": 1.9299079179763794,
      "learning_rate": 0.08980101804720036,
      "loss": 0.0167,
      "step": 11020
    },
    {
      "epoch": 5.104118463674225,
      "grad_norm": 0.27863049507141113,
      "learning_rate": 0.08979176307265156,
      "loss": 0.025,
      "step": 11030
    },
    {
      "epoch": 5.108745950948635,
      "grad_norm": 9.5460786819458,
      "learning_rate": 0.08978250809810273,
      "loss": 0.0176,
      "step": 11040
    },
    {
      "epoch": 5.113373438223045,
      "grad_norm": 0.004153181798756123,
      "learning_rate": 0.08977325312355391,
      "loss": 0.0114,
      "step": 11050
    },
    {
      "epoch": 5.118000925497455,
      "grad_norm": 8.690698623657227,
      "learning_rate": 0.0897639981490051,
      "loss": 0.0176,
      "step": 11060
    },
    {
      "epoch": 5.122628412771865,
      "grad_norm": 0.6970808506011963,
      "learning_rate": 0.08975474317445628,
      "loss": 0.0237,
      "step": 11070
    },
    {
      "epoch": 5.127255900046275,
      "grad_norm": 0.06351584196090698,
      "learning_rate": 0.08974548819990746,
      "loss": 0.0049,
      "step": 11080
    },
    {
      "epoch": 5.131883387320685,
      "grad_norm": 5.559455394744873,
      "learning_rate": 0.08973623322535863,
      "loss": 0.0121,
      "step": 11090
    },
    {
      "epoch": 5.1365108745950945,
      "grad_norm": 0.3608175814151764,
      "learning_rate": 0.08972697825080982,
      "loss": 0.0051,
      "step": 11100
    },
    {
      "epoch": 5.141138361869505,
      "grad_norm": 0.08253417164087296,
      "learning_rate": 0.08971772327626099,
      "loss": 0.0206,
      "step": 11110
    },
    {
      "epoch": 5.145765849143915,
      "grad_norm": 4.706130504608154,
      "learning_rate": 0.08970846830171218,
      "loss": 0.0108,
      "step": 11120
    },
    {
      "epoch": 5.1503933364183245,
      "grad_norm": 3.8991215229034424,
      "learning_rate": 0.08969921332716335,
      "loss": 0.0208,
      "step": 11130
    },
    {
      "epoch": 5.155020823692735,
      "grad_norm": 0.1897214651107788,
      "learning_rate": 0.08968995835261453,
      "loss": 0.1331,
      "step": 11140
    },
    {
      "epoch": 5.159648310967145,
      "grad_norm": 2.375377655029297,
      "learning_rate": 0.08968070337806572,
      "loss": 0.0189,
      "step": 11150
    },
    {
      "epoch": 5.164275798241555,
      "grad_norm": 4.390075206756592,
      "learning_rate": 0.0896714484035169,
      "loss": 0.0119,
      "step": 11160
    },
    {
      "epoch": 5.168903285515965,
      "grad_norm": 0.0182009469717741,
      "learning_rate": 0.08966219342896808,
      "loss": 0.0216,
      "step": 11170
    },
    {
      "epoch": 5.173530772790375,
      "grad_norm": 1.3423534631729126,
      "learning_rate": 0.08965293845441925,
      "loss": 0.0062,
      "step": 11180
    },
    {
      "epoch": 5.178158260064785,
      "grad_norm": 0.2618800699710846,
      "learning_rate": 0.08964368347987044,
      "loss": 0.0022,
      "step": 11190
    },
    {
      "epoch": 5.182785747339195,
      "grad_norm": 0.20342741906642914,
      "learning_rate": 0.0896344285053216,
      "loss": 0.0135,
      "step": 11200
    },
    {
      "epoch": 5.187413234613604,
      "grad_norm": 1.3023642301559448,
      "learning_rate": 0.0896251735307728,
      "loss": 0.025,
      "step": 11210
    },
    {
      "epoch": 5.192040721888015,
      "grad_norm": 0.9998198747634888,
      "learning_rate": 0.08961591855622397,
      "loss": 0.0045,
      "step": 11220
    },
    {
      "epoch": 5.196668209162425,
      "grad_norm": 0.23274490237236023,
      "learning_rate": 0.08960666358167516,
      "loss": 0.0141,
      "step": 11230
    },
    {
      "epoch": 5.201295696436834,
      "grad_norm": 0.08855386078357697,
      "learning_rate": 0.08959740860712634,
      "loss": 0.0109,
      "step": 11240
    },
    {
      "epoch": 5.205923183711245,
      "grad_norm": 2.169796943664551,
      "learning_rate": 0.08958815363257751,
      "loss": 0.0317,
      "step": 11250
    },
    {
      "epoch": 5.210550670985655,
      "grad_norm": 0.6346469521522522,
      "learning_rate": 0.0895788986580287,
      "loss": 0.006,
      "step": 11260
    },
    {
      "epoch": 5.2151781582600645,
      "grad_norm": 0.02253180742263794,
      "learning_rate": 0.08956964368347987,
      "loss": 0.0078,
      "step": 11270
    },
    {
      "epoch": 5.219805645534475,
      "grad_norm": 0.03251100331544876,
      "learning_rate": 0.08956038870893106,
      "loss": 0.0076,
      "step": 11280
    },
    {
      "epoch": 5.224433132808885,
      "grad_norm": 0.3731876313686371,
      "learning_rate": 0.08955113373438223,
      "loss": 0.0092,
      "step": 11290
    },
    {
      "epoch": 5.2290606200832945,
      "grad_norm": 3.141803503036499,
      "learning_rate": 0.08954187875983342,
      "loss": 0.0181,
      "step": 11300
    },
    {
      "epoch": 5.233688107357705,
      "grad_norm": 0.6780280470848083,
      "learning_rate": 0.08953262378528459,
      "loss": 0.0176,
      "step": 11310
    },
    {
      "epoch": 5.238315594632115,
      "grad_norm": 3.501528024673462,
      "learning_rate": 0.08952336881073578,
      "loss": 0.0086,
      "step": 11320
    },
    {
      "epoch": 5.242943081906525,
      "grad_norm": 0.16595280170440674,
      "learning_rate": 0.08951411383618696,
      "loss": 0.0276,
      "step": 11330
    },
    {
      "epoch": 5.247570569180935,
      "grad_norm": 2.380138874053955,
      "learning_rate": 0.08950485886163813,
      "loss": 0.0132,
      "step": 11340
    },
    {
      "epoch": 5.252198056455345,
      "grad_norm": 0.0708036795258522,
      "learning_rate": 0.08949560388708933,
      "loss": 0.0083,
      "step": 11350
    },
    {
      "epoch": 5.256825543729755,
      "grad_norm": 0.7010241746902466,
      "learning_rate": 0.0894863489125405,
      "loss": 0.0168,
      "step": 11360
    },
    {
      "epoch": 5.261453031004165,
      "grad_norm": 3.9070568084716797,
      "learning_rate": 0.08947709393799168,
      "loss": 0.0147,
      "step": 11370
    },
    {
      "epoch": 5.266080518278574,
      "grad_norm": 0.04454077035188675,
      "learning_rate": 0.08946783896344285,
      "loss": 0.0018,
      "step": 11380
    },
    {
      "epoch": 5.270708005552985,
      "grad_norm": 3.2595629692077637,
      "learning_rate": 0.08945858398889404,
      "loss": 0.0229,
      "step": 11390
    },
    {
      "epoch": 5.275335492827395,
      "grad_norm": 0.8053179979324341,
      "learning_rate": 0.08944932901434521,
      "loss": 0.0032,
      "step": 11400
    },
    {
      "epoch": 5.279962980101804,
      "grad_norm": 0.09264815598726273,
      "learning_rate": 0.0894400740397964,
      "loss": 0.0184,
      "step": 11410
    },
    {
      "epoch": 5.284590467376215,
      "grad_norm": 0.007713015191257,
      "learning_rate": 0.08943081906524758,
      "loss": 0.0051,
      "step": 11420
    },
    {
      "epoch": 5.289217954650625,
      "grad_norm": 0.6074944138526917,
      "learning_rate": 0.08942156409069875,
      "loss": 0.0175,
      "step": 11430
    },
    {
      "epoch": 5.2938454419250345,
      "grad_norm": 3.6671507358551025,
      "learning_rate": 0.08941230911614995,
      "loss": 0.017,
      "step": 11440
    },
    {
      "epoch": 5.298472929199445,
      "grad_norm": 0.3683464229106903,
      "learning_rate": 0.08940305414160112,
      "loss": 0.0178,
      "step": 11450
    },
    {
      "epoch": 5.303100416473855,
      "grad_norm": 0.06352844089269638,
      "learning_rate": 0.0893937991670523,
      "loss": 0.0125,
      "step": 11460
    },
    {
      "epoch": 5.3077279037482645,
      "grad_norm": 0.7484846115112305,
      "learning_rate": 0.08938454419250347,
      "loss": 0.0098,
      "step": 11470
    },
    {
      "epoch": 5.312355391022675,
      "grad_norm": 0.19177071750164032,
      "learning_rate": 0.08937528921795465,
      "loss": 0.0094,
      "step": 11480
    },
    {
      "epoch": 5.316982878297084,
      "grad_norm": 5.459676742553711,
      "learning_rate": 0.08936603424340583,
      "loss": 0.0095,
      "step": 11490
    },
    {
      "epoch": 5.321610365571495,
      "grad_norm": 0.02411031164228916,
      "learning_rate": 0.08935677926885702,
      "loss": 0.0176,
      "step": 11500
    },
    {
      "epoch": 5.326237852845905,
      "grad_norm": 0.2184518277645111,
      "learning_rate": 0.0893475242943082,
      "loss": 0.0105,
      "step": 11510
    },
    {
      "epoch": 5.330865340120314,
      "grad_norm": 0.30695009231567383,
      "learning_rate": 0.08933826931975937,
      "loss": 0.0167,
      "step": 11520
    },
    {
      "epoch": 5.335492827394725,
      "grad_norm": 0.1953418254852295,
      "learning_rate": 0.08932901434521057,
      "loss": 0.0086,
      "step": 11530
    },
    {
      "epoch": 5.340120314669135,
      "grad_norm": 0.05936240032315254,
      "learning_rate": 0.08931975937066174,
      "loss": 0.0272,
      "step": 11540
    },
    {
      "epoch": 5.344747801943544,
      "grad_norm": 0.23622778058052063,
      "learning_rate": 0.08931050439611292,
      "loss": 0.0028,
      "step": 11550
    },
    {
      "epoch": 5.349375289217955,
      "grad_norm": 0.13732899725437164,
      "learning_rate": 0.08930124942156409,
      "loss": 0.0038,
      "step": 11560
    },
    {
      "epoch": 5.354002776492365,
      "grad_norm": 3.249157667160034,
      "learning_rate": 0.08929199444701527,
      "loss": 0.0201,
      "step": 11570
    },
    {
      "epoch": 5.358630263766774,
      "grad_norm": 0.012116965837776661,
      "learning_rate": 0.08928273947246645,
      "loss": 0.011,
      "step": 11580
    },
    {
      "epoch": 5.363257751041185,
      "grad_norm": 0.893259584903717,
      "learning_rate": 0.08927348449791764,
      "loss": 0.0091,
      "step": 11590
    },
    {
      "epoch": 5.367885238315594,
      "grad_norm": 0.4648866057395935,
      "learning_rate": 0.08926422952336882,
      "loss": 0.0169,
      "step": 11600
    },
    {
      "epoch": 5.3725127255900045,
      "grad_norm": 6.409658432006836,
      "learning_rate": 0.08925497454881999,
      "loss": 0.0052,
      "step": 11610
    },
    {
      "epoch": 5.377140212864415,
      "grad_norm": 1.0453450679779053,
      "learning_rate": 0.08924571957427119,
      "loss": 0.0207,
      "step": 11620
    },
    {
      "epoch": 5.381767700138824,
      "grad_norm": 1.5056097507476807,
      "learning_rate": 0.08923646459972236,
      "loss": 0.0591,
      "step": 11630
    },
    {
      "epoch": 5.3863951874132345,
      "grad_norm": 3.466933012008667,
      "learning_rate": 0.08922720962517354,
      "loss": 0.0168,
      "step": 11640
    },
    {
      "epoch": 5.391022674687645,
      "grad_norm": 0.7460373640060425,
      "learning_rate": 0.08921795465062471,
      "loss": 0.0032,
      "step": 11650
    },
    {
      "epoch": 5.395650161962054,
      "grad_norm": 0.04086289554834366,
      "learning_rate": 0.08920869967607589,
      "loss": 0.0109,
      "step": 11660
    },
    {
      "epoch": 5.400277649236465,
      "grad_norm": 0.8397194743156433,
      "learning_rate": 0.08919944470152708,
      "loss": 0.0051,
      "step": 11670
    },
    {
      "epoch": 5.404905136510875,
      "grad_norm": 3.017366647720337,
      "learning_rate": 0.08919018972697826,
      "loss": 0.0135,
      "step": 11680
    },
    {
      "epoch": 5.409532623785284,
      "grad_norm": 3.5837273597717285,
      "learning_rate": 0.08918093475242944,
      "loss": 0.0169,
      "step": 11690
    },
    {
      "epoch": 5.414160111059695,
      "grad_norm": 0.2719627916812897,
      "learning_rate": 0.08917167977788061,
      "loss": 0.0013,
      "step": 11700
    },
    {
      "epoch": 5.418787598334105,
      "grad_norm": 7.678188800811768,
      "learning_rate": 0.0891624248033318,
      "loss": 0.0156,
      "step": 11710
    },
    {
      "epoch": 5.423415085608514,
      "grad_norm": 1.299738883972168,
      "learning_rate": 0.08915316982878298,
      "loss": 0.0081,
      "step": 11720
    },
    {
      "epoch": 5.428042572882925,
      "grad_norm": 0.1346587836742401,
      "learning_rate": 0.08914391485423416,
      "loss": 0.0073,
      "step": 11730
    },
    {
      "epoch": 5.432670060157335,
      "grad_norm": 0.08878283947706223,
      "learning_rate": 0.08913465987968533,
      "loss": 0.0019,
      "step": 11740
    },
    {
      "epoch": 5.437297547431744,
      "grad_norm": 0.0025343887973576784,
      "learning_rate": 0.08912540490513651,
      "loss": 0.0046,
      "step": 11750
    },
    {
      "epoch": 5.441925034706155,
      "grad_norm": 0.49486100673675537,
      "learning_rate": 0.0891161499305877,
      "loss": 0.025,
      "step": 11760
    },
    {
      "epoch": 5.446552521980564,
      "grad_norm": 2.713024616241455,
      "learning_rate": 0.08910689495603888,
      "loss": 0.0178,
      "step": 11770
    },
    {
      "epoch": 5.4511800092549745,
      "grad_norm": 2.223456859588623,
      "learning_rate": 0.08909763998149006,
      "loss": 0.0075,
      "step": 11780
    },
    {
      "epoch": 5.455807496529385,
      "grad_norm": 5.464870452880859,
      "learning_rate": 0.08908838500694123,
      "loss": 0.0069,
      "step": 11790
    },
    {
      "epoch": 5.460434983803794,
      "grad_norm": 1.1881204843521118,
      "learning_rate": 0.08907913003239241,
      "loss": 0.0134,
      "step": 11800
    },
    {
      "epoch": 5.4650624710782045,
      "grad_norm": 3.1849844455718994,
      "learning_rate": 0.0890698750578436,
      "loss": 0.015,
      "step": 11810
    },
    {
      "epoch": 5.469689958352615,
      "grad_norm": 6.6108245849609375,
      "learning_rate": 0.08906062008329478,
      "loss": 0.0053,
      "step": 11820
    },
    {
      "epoch": 5.474317445627024,
      "grad_norm": 0.7466368079185486,
      "learning_rate": 0.08905136510874595,
      "loss": 0.0133,
      "step": 11830
    },
    {
      "epoch": 5.478944932901435,
      "grad_norm": 0.2546054422855377,
      "learning_rate": 0.08904211013419713,
      "loss": 0.0236,
      "step": 11840
    },
    {
      "epoch": 5.483572420175845,
      "grad_norm": 0.5617857575416565,
      "learning_rate": 0.08903285515964832,
      "loss": 0.0218,
      "step": 11850
    },
    {
      "epoch": 5.488199907450254,
      "grad_norm": 1.8720908164978027,
      "learning_rate": 0.0890236001850995,
      "loss": 0.015,
      "step": 11860
    },
    {
      "epoch": 5.492827394724665,
      "grad_norm": 2.25858736038208,
      "learning_rate": 0.08901434521055068,
      "loss": 0.0125,
      "step": 11870
    },
    {
      "epoch": 5.497454881999074,
      "grad_norm": 5.0884480476379395,
      "learning_rate": 0.08900509023600185,
      "loss": 0.0155,
      "step": 11880
    },
    {
      "epoch": 5.502082369273484,
      "grad_norm": 5.84944486618042,
      "learning_rate": 0.08899583526145304,
      "loss": 0.0128,
      "step": 11890
    },
    {
      "epoch": 5.506709856547895,
      "grad_norm": 4.497133255004883,
      "learning_rate": 0.08898658028690422,
      "loss": 0.0313,
      "step": 11900
    },
    {
      "epoch": 5.511337343822304,
      "grad_norm": 0.8561303019523621,
      "learning_rate": 0.0889773253123554,
      "loss": 0.0078,
      "step": 11910
    },
    {
      "epoch": 5.515964831096714,
      "grad_norm": 1.0901626348495483,
      "learning_rate": 0.08896807033780657,
      "loss": 0.0124,
      "step": 11920
    },
    {
      "epoch": 5.520592318371125,
      "grad_norm": 0.06308019161224365,
      "learning_rate": 0.08895881536325775,
      "loss": 0.0085,
      "step": 11930
    },
    {
      "epoch": 5.525219805645534,
      "grad_norm": 3.0678327083587646,
      "learning_rate": 0.08894956038870894,
      "loss": 0.0117,
      "step": 11940
    },
    {
      "epoch": 5.5298472929199445,
      "grad_norm": 0.46174299716949463,
      "learning_rate": 0.08894030541416012,
      "loss": 0.0373,
      "step": 11950
    },
    {
      "epoch": 5.534474780194355,
      "grad_norm": 2.0656795501708984,
      "learning_rate": 0.0889310504396113,
      "loss": 0.0064,
      "step": 11960
    },
    {
      "epoch": 5.539102267468764,
      "grad_norm": 9.762907028198242,
      "learning_rate": 0.08892179546506247,
      "loss": 0.0189,
      "step": 11970
    },
    {
      "epoch": 5.5437297547431745,
      "grad_norm": 0.7887096405029297,
      "learning_rate": 0.08891254049051366,
      "loss": 0.0045,
      "step": 11980
    },
    {
      "epoch": 5.548357242017584,
      "grad_norm": 0.020706960931420326,
      "learning_rate": 0.08890328551596484,
      "loss": 0.0123,
      "step": 11990
    },
    {
      "epoch": 5.552984729291994,
      "grad_norm": 1.4830344915390015,
      "learning_rate": 0.08889403054141602,
      "loss": 0.0088,
      "step": 12000
    },
    {
      "epoch": 5.557612216566405,
      "grad_norm": 1.6796752214431763,
      "learning_rate": 0.08888477556686719,
      "loss": 0.0058,
      "step": 12010
    },
    {
      "epoch": 5.562239703840815,
      "grad_norm": 0.11465860158205032,
      "learning_rate": 0.08887552059231837,
      "loss": 0.0215,
      "step": 12020
    },
    {
      "epoch": 5.566867191115224,
      "grad_norm": 0.012188929133117199,
      "learning_rate": 0.08886626561776956,
      "loss": 0.0009,
      "step": 12030
    },
    {
      "epoch": 5.571494678389635,
      "grad_norm": 2.5796661376953125,
      "learning_rate": 0.08885701064322074,
      "loss": 0.0112,
      "step": 12040
    },
    {
      "epoch": 5.576122165664044,
      "grad_norm": 3.062685251235962,
      "learning_rate": 0.08884775566867192,
      "loss": 0.0123,
      "step": 12050
    },
    {
      "epoch": 5.580749652938454,
      "grad_norm": 0.022998685017228127,
      "learning_rate": 0.0888385006941231,
      "loss": 0.0045,
      "step": 12060
    },
    {
      "epoch": 5.585377140212865,
      "grad_norm": 0.04756738618016243,
      "learning_rate": 0.08882924571957428,
      "loss": 0.0149,
      "step": 12070
    },
    {
      "epoch": 5.590004627487274,
      "grad_norm": 1.7860418558120728,
      "learning_rate": 0.08881999074502546,
      "loss": 0.0086,
      "step": 12080
    },
    {
      "epoch": 5.594632114761684,
      "grad_norm": 1.8026819229125977,
      "learning_rate": 0.08881073577047664,
      "loss": 0.0068,
      "step": 12090
    },
    {
      "epoch": 5.599259602036095,
      "grad_norm": 1.929648995399475,
      "learning_rate": 0.08880148079592781,
      "loss": 0.0037,
      "step": 12100
    },
    {
      "epoch": 5.603887089310504,
      "grad_norm": 0.36240068078041077,
      "learning_rate": 0.088792225821379,
      "loss": 0.0114,
      "step": 12110
    },
    {
      "epoch": 5.6085145765849145,
      "grad_norm": 0.4981960952281952,
      "learning_rate": 0.08878297084683018,
      "loss": 0.0077,
      "step": 12120
    },
    {
      "epoch": 5.613142063859325,
      "grad_norm": 2.620450019836426,
      "learning_rate": 0.08877371587228136,
      "loss": 0.0102,
      "step": 12130
    },
    {
      "epoch": 5.617769551133734,
      "grad_norm": 3.336601495742798,
      "learning_rate": 0.08876446089773254,
      "loss": 0.0156,
      "step": 12140
    },
    {
      "epoch": 5.6223970384081445,
      "grad_norm": 6.805750370025635,
      "learning_rate": 0.08875520592318371,
      "loss": 0.0182,
      "step": 12150
    },
    {
      "epoch": 5.627024525682554,
      "grad_norm": 7.15728235244751,
      "learning_rate": 0.0887459509486349,
      "loss": 0.0157,
      "step": 12160
    },
    {
      "epoch": 5.631652012956964,
      "grad_norm": 0.21364456415176392,
      "learning_rate": 0.08873669597408607,
      "loss": 0.003,
      "step": 12170
    },
    {
      "epoch": 5.636279500231375,
      "grad_norm": 4.668857097625732,
      "learning_rate": 0.08872744099953726,
      "loss": 0.0214,
      "step": 12180
    },
    {
      "epoch": 5.640906987505784,
      "grad_norm": 0.06061572954058647,
      "learning_rate": 0.08871818602498843,
      "loss": 0.0114,
      "step": 12190
    },
    {
      "epoch": 5.645534474780194,
      "grad_norm": 0.925430953502655,
      "learning_rate": 0.08870893105043962,
      "loss": 0.0073,
      "step": 12200
    },
    {
      "epoch": 5.650161962054605,
      "grad_norm": 1.44376802444458,
      "learning_rate": 0.0886996760758908,
      "loss": 0.0269,
      "step": 12210
    },
    {
      "epoch": 5.654789449329014,
      "grad_norm": 0.02418454922735691,
      "learning_rate": 0.08869042110134198,
      "loss": 0.0117,
      "step": 12220
    },
    {
      "epoch": 5.659416936603424,
      "grad_norm": 1.1550939083099365,
      "learning_rate": 0.08868116612679317,
      "loss": 0.0167,
      "step": 12230
    },
    {
      "epoch": 5.664044423877835,
      "grad_norm": 3.9594316482543945,
      "learning_rate": 0.08867191115224433,
      "loss": 0.0216,
      "step": 12240
    },
    {
      "epoch": 5.668671911152244,
      "grad_norm": 0.1986110359430313,
      "learning_rate": 0.08866265617769552,
      "loss": 0.0134,
      "step": 12250
    },
    {
      "epoch": 5.673299398426654,
      "grad_norm": 0.2433183789253235,
      "learning_rate": 0.08865340120314669,
      "loss": 0.0034,
      "step": 12260
    },
    {
      "epoch": 5.677926885701064,
      "grad_norm": 0.5009862780570984,
      "learning_rate": 0.08864414622859788,
      "loss": 0.0061,
      "step": 12270
    },
    {
      "epoch": 5.682554372975474,
      "grad_norm": 0.8852177262306213,
      "learning_rate": 0.08863489125404905,
      "loss": 0.0045,
      "step": 12280
    },
    {
      "epoch": 5.6871818602498845,
      "grad_norm": 9.797248840332031,
      "learning_rate": 0.08862563627950024,
      "loss": 0.0085,
      "step": 12290
    },
    {
      "epoch": 5.691809347524295,
      "grad_norm": 1.9213812351226807,
      "learning_rate": 0.08861638130495142,
      "loss": 0.0125,
      "step": 12300
    },
    {
      "epoch": 5.696436834798704,
      "grad_norm": 5.5599894523620605,
      "learning_rate": 0.0886071263304026,
      "loss": 0.0027,
      "step": 12310
    },
    {
      "epoch": 5.7010643220731145,
      "grad_norm": 0.15295085310935974,
      "learning_rate": 0.08859787135585379,
      "loss": 0.0041,
      "step": 12320
    },
    {
      "epoch": 5.705691809347524,
      "grad_norm": 1.4038960933685303,
      "learning_rate": 0.08858861638130495,
      "loss": 0.0226,
      "step": 12330
    },
    {
      "epoch": 5.710319296621934,
      "grad_norm": 0.004427138715982437,
      "learning_rate": 0.08857936140675614,
      "loss": 0.0099,
      "step": 12340
    },
    {
      "epoch": 5.714946783896345,
      "grad_norm": 0.040731217712163925,
      "learning_rate": 0.08857010643220731,
      "loss": 0.0041,
      "step": 12350
    },
    {
      "epoch": 5.719574271170754,
      "grad_norm": 0.018696004524827003,
      "learning_rate": 0.0885608514576585,
      "loss": 0.0093,
      "step": 12360
    },
    {
      "epoch": 5.724201758445164,
      "grad_norm": 3.157574415206909,
      "learning_rate": 0.08855159648310967,
      "loss": 0.0099,
      "step": 12370
    },
    {
      "epoch": 5.728829245719575,
      "grad_norm": 3.1489710807800293,
      "learning_rate": 0.08854234150856086,
      "loss": 0.0038,
      "step": 12380
    },
    {
      "epoch": 5.733456732993984,
      "grad_norm": 3.6180546283721924,
      "learning_rate": 0.08853308653401204,
      "loss": 0.0181,
      "step": 12390
    },
    {
      "epoch": 5.738084220268394,
      "grad_norm": 3.2759459018707275,
      "learning_rate": 0.08852383155946321,
      "loss": 0.0106,
      "step": 12400
    },
    {
      "epoch": 5.742711707542805,
      "grad_norm": 0.027540041133761406,
      "learning_rate": 0.0885145765849144,
      "loss": 0.0045,
      "step": 12410
    },
    {
      "epoch": 5.747339194817214,
      "grad_norm": 5.705811500549316,
      "learning_rate": 0.08850532161036558,
      "loss": 0.0209,
      "step": 12420
    },
    {
      "epoch": 5.751966682091624,
      "grad_norm": 0.2820480167865753,
      "learning_rate": 0.08849606663581676,
      "loss": 0.0086,
      "step": 12430
    },
    {
      "epoch": 5.756594169366034,
      "grad_norm": 2.197481393814087,
      "learning_rate": 0.08848681166126793,
      "loss": 0.0061,
      "step": 12440
    },
    {
      "epoch": 5.761221656640444,
      "grad_norm": 0.06427836418151855,
      "learning_rate": 0.08847755668671912,
      "loss": 0.0229,
      "step": 12450
    },
    {
      "epoch": 5.7658491439148545,
      "grad_norm": 0.16848306357860565,
      "learning_rate": 0.0884683017121703,
      "loss": 0.0121,
      "step": 12460
    },
    {
      "epoch": 5.770476631189264,
      "grad_norm": 0.12002964317798615,
      "learning_rate": 0.08845904673762148,
      "loss": 0.016,
      "step": 12470
    },
    {
      "epoch": 5.775104118463674,
      "grad_norm": 0.8272868990898132,
      "learning_rate": 0.08844979176307266,
      "loss": 0.0253,
      "step": 12480
    },
    {
      "epoch": 5.7797316057380845,
      "grad_norm": 0.06169668212532997,
      "learning_rate": 0.08844053678852383,
      "loss": 0.0048,
      "step": 12490
    },
    {
      "epoch": 5.784359093012494,
      "grad_norm": 0.09193474054336548,
      "learning_rate": 0.08843128181397503,
      "loss": 0.0063,
      "step": 12500
    },
    {
      "epoch": 5.788986580286904,
      "grad_norm": 0.2937438189983368,
      "learning_rate": 0.0884220268394262,
      "loss": 0.0045,
      "step": 12510
    },
    {
      "epoch": 5.793614067561315,
      "grad_norm": 16.404722213745117,
      "learning_rate": 0.08841277186487738,
      "loss": 0.0184,
      "step": 12520
    },
    {
      "epoch": 5.798241554835724,
      "grad_norm": 0.18106704950332642,
      "learning_rate": 0.08840351689032855,
      "loss": 0.0092,
      "step": 12530
    },
    {
      "epoch": 5.802869042110134,
      "grad_norm": 0.707416832447052,
      "learning_rate": 0.08839426191577975,
      "loss": 0.002,
      "step": 12540
    },
    {
      "epoch": 5.807496529384544,
      "grad_norm": 0.021638086065649986,
      "learning_rate": 0.08838500694123091,
      "loss": 0.0505,
      "step": 12550
    },
    {
      "epoch": 5.812124016658954,
      "grad_norm": 0.20873984694480896,
      "learning_rate": 0.0883757519666821,
      "loss": 0.0095,
      "step": 12560
    },
    {
      "epoch": 5.816751503933364,
      "grad_norm": 3.33042311668396,
      "learning_rate": 0.08836649699213328,
      "loss": 0.0095,
      "step": 12570
    },
    {
      "epoch": 5.821378991207774,
      "grad_norm": 0.13843601942062378,
      "learning_rate": 0.08835724201758445,
      "loss": 0.0105,
      "step": 12580
    },
    {
      "epoch": 5.826006478482184,
      "grad_norm": 1.7973227500915527,
      "learning_rate": 0.08834798704303565,
      "loss": 0.0079,
      "step": 12590
    },
    {
      "epoch": 5.830633965756594,
      "grad_norm": 0.35348254442214966,
      "learning_rate": 0.08833873206848682,
      "loss": 0.0031,
      "step": 12600
    },
    {
      "epoch": 5.835261453031004,
      "grad_norm": 3.48154354095459,
      "learning_rate": 0.088329477093938,
      "loss": 0.0172,
      "step": 12610
    },
    {
      "epoch": 5.839888940305414,
      "grad_norm": 13.808919906616211,
      "learning_rate": 0.08832022211938917,
      "loss": 0.0295,
      "step": 12620
    },
    {
      "epoch": 5.8445164275798245,
      "grad_norm": 0.22177839279174805,
      "learning_rate": 0.08831096714484035,
      "loss": 0.0022,
      "step": 12630
    },
    {
      "epoch": 5.849143914854234,
      "grad_norm": 2.350264072418213,
      "learning_rate": 0.08830171217029154,
      "loss": 0.0154,
      "step": 12640
    },
    {
      "epoch": 5.853771402128644,
      "grad_norm": 0.8708748817443848,
      "learning_rate": 0.08829245719574272,
      "loss": 0.0052,
      "step": 12650
    },
    {
      "epoch": 5.858398889403054,
      "grad_norm": 0.07292398065328598,
      "learning_rate": 0.0882832022211939,
      "loss": 0.0131,
      "step": 12660
    },
    {
      "epoch": 5.863026376677464,
      "grad_norm": 0.15709717571735382,
      "learning_rate": 0.08827394724664507,
      "loss": 0.0058,
      "step": 12670
    },
    {
      "epoch": 5.867653863951874,
      "grad_norm": 0.09222088754177094,
      "learning_rate": 0.08826469227209627,
      "loss": 0.0044,
      "step": 12680
    },
    {
      "epoch": 5.872281351226285,
      "grad_norm": 0.32619625329971313,
      "learning_rate": 0.08825543729754744,
      "loss": 0.0131,
      "step": 12690
    },
    {
      "epoch": 5.876908838500694,
      "grad_norm": 0.004099203739315271,
      "learning_rate": 0.08824618232299862,
      "loss": 0.0046,
      "step": 12700
    },
    {
      "epoch": 5.881536325775104,
      "grad_norm": 1.8216220140457153,
      "learning_rate": 0.08823692734844979,
      "loss": 0.0188,
      "step": 12710
    },
    {
      "epoch": 5.886163813049514,
      "grad_norm": 0.037425365298986435,
      "learning_rate": 0.08822767237390097,
      "loss": 0.0087,
      "step": 12720
    },
    {
      "epoch": 5.890791300323924,
      "grad_norm": 0.15068547427654266,
      "learning_rate": 0.08821841739935216,
      "loss": 0.0118,
      "step": 12730
    },
    {
      "epoch": 5.895418787598334,
      "grad_norm": 4.699113845825195,
      "learning_rate": 0.08820916242480334,
      "loss": 0.01,
      "step": 12740
    },
    {
      "epoch": 5.900046274872744,
      "grad_norm": 0.9702417254447937,
      "learning_rate": 0.08819990745025452,
      "loss": 0.0092,
      "step": 12750
    },
    {
      "epoch": 5.904673762147154,
      "grad_norm": 0.12848223745822906,
      "learning_rate": 0.08819065247570569,
      "loss": 0.0181,
      "step": 12760
    },
    {
      "epoch": 5.909301249421564,
      "grad_norm": 7.078546524047852,
      "learning_rate": 0.08818139750115689,
      "loss": 0.0209,
      "step": 12770
    },
    {
      "epoch": 5.913928736695974,
      "grad_norm": 0.31806930899620056,
      "learning_rate": 0.08817214252660806,
      "loss": 0.0312,
      "step": 12780
    },
    {
      "epoch": 5.918556223970384,
      "grad_norm": 0.25209009647369385,
      "learning_rate": 0.08816288755205924,
      "loss": 0.0082,
      "step": 12790
    },
    {
      "epoch": 5.9231837112447945,
      "grad_norm": 5.936461925506592,
      "learning_rate": 0.08815363257751041,
      "loss": 0.0171,
      "step": 12800
    },
    {
      "epoch": 5.927811198519204,
      "grad_norm": 0.7107470631599426,
      "learning_rate": 0.0881443776029616,
      "loss": 0.0054,
      "step": 12810
    },
    {
      "epoch": 5.932438685793614,
      "grad_norm": 0.494514137506485,
      "learning_rate": 0.08813512262841278,
      "loss": 0.012,
      "step": 12820
    },
    {
      "epoch": 5.937066173068024,
      "grad_norm": 0.006808031816035509,
      "learning_rate": 0.08812586765386396,
      "loss": 0.0071,
      "step": 12830
    },
    {
      "epoch": 5.941693660342434,
      "grad_norm": 3.350297689437866,
      "learning_rate": 0.08811661267931514,
      "loss": 0.0279,
      "step": 12840
    },
    {
      "epoch": 5.946321147616844,
      "grad_norm": 1.6372106075286865,
      "learning_rate": 0.08810735770476631,
      "loss": 0.0272,
      "step": 12850
    },
    {
      "epoch": 5.950948634891254,
      "grad_norm": 0.050718460232019424,
      "learning_rate": 0.0880981027302175,
      "loss": 0.0023,
      "step": 12860
    },
    {
      "epoch": 5.955576122165664,
      "grad_norm": 0.14427295327186584,
      "learning_rate": 0.08808884775566868,
      "loss": 0.0051,
      "step": 12870
    },
    {
      "epoch": 5.960203609440074,
      "grad_norm": 0.06563170999288559,
      "learning_rate": 0.08807959278111986,
      "loss": 0.0085,
      "step": 12880
    },
    {
      "epoch": 5.964831096714484,
      "grad_norm": 0.20819352567195892,
      "learning_rate": 0.08807033780657103,
      "loss": 0.0071,
      "step": 12890
    },
    {
      "epoch": 5.969458583988894,
      "grad_norm": 0.012110017240047455,
      "learning_rate": 0.08806108283202221,
      "loss": 0.0255,
      "step": 12900
    },
    {
      "epoch": 5.974086071263304,
      "grad_norm": 0.029219256713986397,
      "learning_rate": 0.0880518278574734,
      "loss": 0.0063,
      "step": 12910
    },
    {
      "epoch": 5.978713558537714,
      "grad_norm": 0.24928317964076996,
      "learning_rate": 0.08804257288292458,
      "loss": 0.0084,
      "step": 12920
    },
    {
      "epoch": 5.983341045812124,
      "grad_norm": 0.5780519247055054,
      "learning_rate": 0.08803331790837576,
      "loss": 0.0027,
      "step": 12930
    },
    {
      "epoch": 5.9879685330865335,
      "grad_norm": 7.720078945159912,
      "learning_rate": 0.08802406293382693,
      "loss": 0.0197,
      "step": 12940
    },
    {
      "epoch": 5.992596020360944,
      "grad_norm": 0.6167384386062622,
      "learning_rate": 0.08801480795927812,
      "loss": 0.0158,
      "step": 12950
    },
    {
      "epoch": 5.997223507635354,
      "grad_norm": 0.16508479416370392,
      "learning_rate": 0.0880055529847293,
      "loss": 0.0073,
      "step": 12960
    },
    {
      "epoch": 6.0,
      "eval_accuracy_branch1": 0.980434447696811,
      "eval_accuracy_branch2": 0.4975350485287321,
      "eval_f1_branch1": 0.9808785331697204,
      "eval_f1_branch2": 0.49580249446031704,
      "eval_loss": 0.04431172460317612,
      "eval_precision_branch1": 0.9819868098844634,
      "eval_precision_branch2": 0.497500695481249,
      "eval_recall_branch1": 0.9806719879342279,
      "eval_recall_branch2": 0.4975350485287321,
      "eval_runtime": 28.9569,
      "eval_samples_per_second": 448.321,
      "eval_steps_per_second": 56.049,
      "step": 12966
    },
    {
      "epoch": 6.001850994909764,
      "grad_norm": 2.2111663818359375,
      "learning_rate": 0.08799629801018048,
      "loss": 0.2218,
      "step": 12970
    },
    {
      "epoch": 6.006478482184174,
      "grad_norm": 5.275379180908203,
      "learning_rate": 0.08798704303563165,
      "loss": 0.0053,
      "step": 12980
    },
    {
      "epoch": 6.011105969458584,
      "grad_norm": 1.1120003461837769,
      "learning_rate": 0.08797778806108283,
      "loss": 0.0084,
      "step": 12990
    },
    {
      "epoch": 6.015733456732994,
      "grad_norm": 0.03130129352211952,
      "learning_rate": 0.08796853308653402,
      "loss": 0.01,
      "step": 13000
    },
    {
      "epoch": 6.020360944007404,
      "grad_norm": 0.10748705267906189,
      "learning_rate": 0.0879592781119852,
      "loss": 0.0048,
      "step": 13010
    },
    {
      "epoch": 6.024988431281814,
      "grad_norm": 0.863086462020874,
      "learning_rate": 0.08795002313743638,
      "loss": 0.0073,
      "step": 13020
    },
    {
      "epoch": 6.029615918556224,
      "grad_norm": 0.7263628244400024,
      "learning_rate": 0.08794076816288755,
      "loss": 0.0065,
      "step": 13030
    },
    {
      "epoch": 6.034243405830634,
      "grad_norm": 0.06185988709330559,
      "learning_rate": 0.08793151318833874,
      "loss": 0.0037,
      "step": 13040
    },
    {
      "epoch": 6.038870893105044,
      "grad_norm": 0.018126197159290314,
      "learning_rate": 0.08792225821378992,
      "loss": 0.0017,
      "step": 13050
    },
    {
      "epoch": 6.043498380379454,
      "grad_norm": 0.03481176495552063,
      "learning_rate": 0.0879130032392411,
      "loss": 0.0073,
      "step": 13060
    },
    {
      "epoch": 6.048125867653864,
      "grad_norm": 0.044569019228219986,
      "learning_rate": 0.08790374826469227,
      "loss": 0.0067,
      "step": 13070
    },
    {
      "epoch": 6.052753354928274,
      "grad_norm": 2.3032467365264893,
      "learning_rate": 0.08789449329014346,
      "loss": 0.0046,
      "step": 13080
    },
    {
      "epoch": 6.057380842202684,
      "grad_norm": 2.612147331237793,
      "learning_rate": 0.08788523831559464,
      "loss": 0.0126,
      "step": 13090
    },
    {
      "epoch": 6.062008329477094,
      "grad_norm": 0.24926234781742096,
      "learning_rate": 0.08787598334104582,
      "loss": 0.003,
      "step": 13100
    },
    {
      "epoch": 6.0666358167515035,
      "grad_norm": 0.17245152592658997,
      "learning_rate": 0.087866728366497,
      "loss": 0.0029,
      "step": 13110
    },
    {
      "epoch": 6.071263304025914,
      "grad_norm": 0.05160791054368019,
      "learning_rate": 0.08785747339194817,
      "loss": 0.0082,
      "step": 13120
    },
    {
      "epoch": 6.075890791300324,
      "grad_norm": 0.5849901437759399,
      "learning_rate": 0.08784821841739936,
      "loss": 0.0024,
      "step": 13130
    },
    {
      "epoch": 6.080518278574734,
      "grad_norm": 1.5898984670639038,
      "learning_rate": 0.08783896344285054,
      "loss": 0.0174,
      "step": 13140
    },
    {
      "epoch": 6.085145765849144,
      "grad_norm": 0.4207735061645508,
      "learning_rate": 0.08782970846830172,
      "loss": 0.0063,
      "step": 13150
    },
    {
      "epoch": 6.089773253123554,
      "grad_norm": 0.028784839436411858,
      "learning_rate": 0.08782045349375289,
      "loss": 0.004,
      "step": 13160
    },
    {
      "epoch": 6.094400740397964,
      "grad_norm": 0.4051749110221863,
      "learning_rate": 0.08781119851920408,
      "loss": 0.0156,
      "step": 13170
    },
    {
      "epoch": 6.099028227672374,
      "grad_norm": 0.12532182037830353,
      "learning_rate": 0.08780194354465526,
      "loss": 0.0027,
      "step": 13180
    },
    {
      "epoch": 6.103655714946784,
      "grad_norm": 3.786329507827759,
      "learning_rate": 0.08779268857010644,
      "loss": 0.0097,
      "step": 13190
    },
    {
      "epoch": 6.108283202221194,
      "grad_norm": 2.4750876426696777,
      "learning_rate": 0.08778343359555763,
      "loss": 0.0018,
      "step": 13200
    },
    {
      "epoch": 6.112910689495604,
      "grad_norm": 0.007570765446871519,
      "learning_rate": 0.0877741786210088,
      "loss": 0.0169,
      "step": 13210
    },
    {
      "epoch": 6.117538176770013,
      "grad_norm": 0.14037059247493744,
      "learning_rate": 0.08776492364645998,
      "loss": 0.0056,
      "step": 13220
    },
    {
      "epoch": 6.122165664044424,
      "grad_norm": 1.292157530784607,
      "learning_rate": 0.08775566867191115,
      "loss": 0.0059,
      "step": 13230
    },
    {
      "epoch": 6.126793151318834,
      "grad_norm": 0.017849648371338844,
      "learning_rate": 0.08774641369736234,
      "loss": 0.0039,
      "step": 13240
    },
    {
      "epoch": 6.1314206385932435,
      "grad_norm": 0.04500207304954529,
      "learning_rate": 0.08773715872281351,
      "loss": 0.0041,
      "step": 13250
    },
    {
      "epoch": 6.136048125867654,
      "grad_norm": 1.76240873336792,
      "learning_rate": 0.0877279037482647,
      "loss": 0.0102,
      "step": 13260
    },
    {
      "epoch": 6.140675613142064,
      "grad_norm": 3.3928730487823486,
      "learning_rate": 0.08771864877371588,
      "loss": 0.0072,
      "step": 13270
    },
    {
      "epoch": 6.1453031004164735,
      "grad_norm": 0.3544762134552002,
      "learning_rate": 0.08770939379916706,
      "loss": 0.0086,
      "step": 13280
    },
    {
      "epoch": 6.149930587690884,
      "grad_norm": 0.016126368194818497,
      "learning_rate": 0.08770013882461825,
      "loss": 0.0053,
      "step": 13290
    },
    {
      "epoch": 6.154558074965294,
      "grad_norm": 0.012100933119654655,
      "learning_rate": 0.08769088385006941,
      "loss": 0.0007,
      "step": 13300
    },
    {
      "epoch": 6.159185562239704,
      "grad_norm": 0.023985743522644043,
      "learning_rate": 0.0876816288755206,
      "loss": 0.0028,
      "step": 13310
    },
    {
      "epoch": 6.163813049514114,
      "grad_norm": 6.60952091217041,
      "learning_rate": 0.08767237390097177,
      "loss": 0.0021,
      "step": 13320
    },
    {
      "epoch": 6.168440536788524,
      "grad_norm": 0.018726816400885582,
      "learning_rate": 0.08766311892642296,
      "loss": 0.0014,
      "step": 13330
    },
    {
      "epoch": 6.173068024062934,
      "grad_norm": 1.1895767450332642,
      "learning_rate": 0.08765386395187413,
      "loss": 0.0163,
      "step": 13340
    },
    {
      "epoch": 6.177695511337344,
      "grad_norm": 0.30732592940330505,
      "learning_rate": 0.08764460897732532,
      "loss": 0.0144,
      "step": 13350
    },
    {
      "epoch": 6.182322998611754,
      "grad_norm": 4.7448625564575195,
      "learning_rate": 0.0876353540027765,
      "loss": 0.0145,
      "step": 13360
    },
    {
      "epoch": 6.186950485886164,
      "grad_norm": 0.17387904226779938,
      "learning_rate": 0.08762609902822768,
      "loss": 0.0101,
      "step": 13370
    },
    {
      "epoch": 6.191577973160574,
      "grad_norm": 0.013482537120580673,
      "learning_rate": 0.08761684405367887,
      "loss": 0.0093,
      "step": 13380
    },
    {
      "epoch": 6.196205460434983,
      "grad_norm": 0.3291361629962921,
      "learning_rate": 0.08760758907913004,
      "loss": 0.0026,
      "step": 13390
    },
    {
      "epoch": 6.200832947709394,
      "grad_norm": 0.5366425514221191,
      "learning_rate": 0.08759833410458122,
      "loss": 0.0005,
      "step": 13400
    },
    {
      "epoch": 6.205460434983804,
      "grad_norm": 1.9554537534713745,
      "learning_rate": 0.08758907913003239,
      "loss": 0.0056,
      "step": 13410
    },
    {
      "epoch": 6.2100879222582135,
      "grad_norm": 0.03921828046441078,
      "learning_rate": 0.08757982415548358,
      "loss": 0.0052,
      "step": 13420
    },
    {
      "epoch": 6.214715409532624,
      "grad_norm": 1.473901629447937,
      "learning_rate": 0.08757056918093475,
      "loss": 0.015,
      "step": 13430
    },
    {
      "epoch": 6.219342896807034,
      "grad_norm": 1.5627611875534058,
      "learning_rate": 0.08756131420638594,
      "loss": 0.0066,
      "step": 13440
    },
    {
      "epoch": 6.2239703840814435,
      "grad_norm": 0.004695875104516745,
      "learning_rate": 0.08755205923183712,
      "loss": 0.0081,
      "step": 13450
    },
    {
      "epoch": 6.228597871355854,
      "grad_norm": 0.006264778785407543,
      "learning_rate": 0.08754280425728829,
      "loss": 0.0085,
      "step": 13460
    },
    {
      "epoch": 6.233225358630264,
      "grad_norm": 5.153824329376221,
      "learning_rate": 0.08753354928273949,
      "loss": 0.0038,
      "step": 13470
    },
    {
      "epoch": 6.237852845904674,
      "grad_norm": 0.03967856243252754,
      "learning_rate": 0.08752429430819066,
      "loss": 0.0142,
      "step": 13480
    },
    {
      "epoch": 6.242480333179084,
      "grad_norm": 1.7383954524993896,
      "learning_rate": 0.08751503933364184,
      "loss": 0.0066,
      "step": 13490
    },
    {
      "epoch": 6.247107820453493,
      "grad_norm": 0.8534297943115234,
      "learning_rate": 0.08750578435909301,
      "loss": 0.0007,
      "step": 13500
    },
    {
      "epoch": 6.251735307727904,
      "grad_norm": 0.007560428697615862,
      "learning_rate": 0.0874965293845442,
      "loss": 0.0021,
      "step": 13510
    },
    {
      "epoch": 6.256362795002314,
      "grad_norm": 4.374828815460205,
      "learning_rate": 0.08748727440999537,
      "loss": 0.0101,
      "step": 13520
    },
    {
      "epoch": 6.260990282276723,
      "grad_norm": 1.0596747398376465,
      "learning_rate": 0.08747801943544656,
      "loss": 0.001,
      "step": 13530
    },
    {
      "epoch": 6.265617769551134,
      "grad_norm": 5.448459148406982,
      "learning_rate": 0.08746876446089774,
      "loss": 0.0095,
      "step": 13540
    },
    {
      "epoch": 6.270245256825544,
      "grad_norm": 0.11106220632791519,
      "learning_rate": 0.08745950948634891,
      "loss": 0.0085,
      "step": 13550
    },
    {
      "epoch": 6.274872744099953,
      "grad_norm": 7.1161112785339355,
      "learning_rate": 0.08745025451180011,
      "loss": 0.0133,
      "step": 13560
    },
    {
      "epoch": 6.279500231374364,
      "grad_norm": 1.707031011581421,
      "learning_rate": 0.08744099953725128,
      "loss": 0.0122,
      "step": 13570
    },
    {
      "epoch": 6.284127718648774,
      "grad_norm": 1.6018412113189697,
      "learning_rate": 0.08743174456270246,
      "loss": 0.0097,
      "step": 13580
    },
    {
      "epoch": 6.2887552059231835,
      "grad_norm": 0.19708110392093658,
      "learning_rate": 0.08742248958815363,
      "loss": 0.0013,
      "step": 13590
    },
    {
      "epoch": 6.293382693197594,
      "grad_norm": 0.4749126732349396,
      "learning_rate": 0.08741323461360483,
      "loss": 0.0053,
      "step": 13600
    },
    {
      "epoch": 6.298010180472004,
      "grad_norm": 0.005112162791192532,
      "learning_rate": 0.087403979639056,
      "loss": 0.0006,
      "step": 13610
    },
    {
      "epoch": 6.3026376677464135,
      "grad_norm": 0.07291748374700546,
      "learning_rate": 0.08739472466450718,
      "loss": 0.0008,
      "step": 13620
    },
    {
      "epoch": 6.307265155020824,
      "grad_norm": 3.416649341583252,
      "learning_rate": 0.08738546968995836,
      "loss": 0.0067,
      "step": 13630
    },
    {
      "epoch": 6.311892642295233,
      "grad_norm": 1.0645654201507568,
      "learning_rate": 0.08737621471540953,
      "loss": 0.0047,
      "step": 13640
    },
    {
      "epoch": 6.316520129569644,
      "grad_norm": 0.11496265232563019,
      "learning_rate": 0.08736695974086073,
      "loss": 0.0009,
      "step": 13650
    },
    {
      "epoch": 6.321147616844054,
      "grad_norm": 0.4874809980392456,
      "learning_rate": 0.0873577047663119,
      "loss": 0.0076,
      "step": 13660
    },
    {
      "epoch": 6.325775104118463,
      "grad_norm": 0.021425845101475716,
      "learning_rate": 0.08734844979176308,
      "loss": 0.0043,
      "step": 13670
    },
    {
      "epoch": 6.330402591392874,
      "grad_norm": 0.0472441166639328,
      "learning_rate": 0.08733919481721425,
      "loss": 0.0015,
      "step": 13680
    },
    {
      "epoch": 6.335030078667284,
      "grad_norm": 2.244805335998535,
      "learning_rate": 0.08732993984266543,
      "loss": 0.0028,
      "step": 13690
    },
    {
      "epoch": 6.339657565941693,
      "grad_norm": 0.6628231406211853,
      "learning_rate": 0.08732068486811662,
      "loss": 0.0024,
      "step": 13700
    },
    {
      "epoch": 6.344285053216104,
      "grad_norm": 3.0403075218200684,
      "learning_rate": 0.0873114298935678,
      "loss": 0.0067,
      "step": 13710
    },
    {
      "epoch": 6.348912540490514,
      "grad_norm": 0.17910997569561005,
      "learning_rate": 0.08730217491901898,
      "loss": 0.0046,
      "step": 13720
    },
    {
      "epoch": 6.353540027764923,
      "grad_norm": 4.467287540435791,
      "learning_rate": 0.08729291994447015,
      "loss": 0.0421,
      "step": 13730
    },
    {
      "epoch": 6.358167515039334,
      "grad_norm": 0.007131490390747786,
      "learning_rate": 0.08728366496992135,
      "loss": 0.0017,
      "step": 13740
    },
    {
      "epoch": 6.362795002313744,
      "grad_norm": 0.10125642269849777,
      "learning_rate": 0.08727440999537252,
      "loss": 0.0037,
      "step": 13750
    },
    {
      "epoch": 6.3674224895881535,
      "grad_norm": 1.0794780254364014,
      "learning_rate": 0.0872651550208237,
      "loss": 0.0062,
      "step": 13760
    },
    {
      "epoch": 6.372049976862564,
      "grad_norm": 12.597784042358398,
      "learning_rate": 0.08725590004627487,
      "loss": 0.0084,
      "step": 13770
    },
    {
      "epoch": 6.376677464136973,
      "grad_norm": 6.470056533813477,
      "learning_rate": 0.08724664507172605,
      "loss": 0.0064,
      "step": 13780
    },
    {
      "epoch": 6.3813049514113835,
      "grad_norm": 0.06274177134037018,
      "learning_rate": 0.08723739009717724,
      "loss": 0.0009,
      "step": 13790
    },
    {
      "epoch": 6.385932438685794,
      "grad_norm": 10.276010513305664,
      "learning_rate": 0.08722813512262842,
      "loss": 0.0119,
      "step": 13800
    },
    {
      "epoch": 6.390559925960203,
      "grad_norm": 2.4025931358337402,
      "learning_rate": 0.0872188801480796,
      "loss": 0.0019,
      "step": 13810
    },
    {
      "epoch": 6.395187413234614,
      "grad_norm": 5.10391902923584,
      "learning_rate": 0.08720962517353077,
      "loss": 0.005,
      "step": 13820
    },
    {
      "epoch": 6.399814900509024,
      "grad_norm": 0.24863556027412415,
      "learning_rate": 0.08720037019898197,
      "loss": 0.0071,
      "step": 13830
    },
    {
      "epoch": 6.404442387783433,
      "grad_norm": 0.950717568397522,
      "learning_rate": 0.08719111522443314,
      "loss": 0.0016,
      "step": 13840
    },
    {
      "epoch": 6.409069875057844,
      "grad_norm": 0.04455152153968811,
      "learning_rate": 0.08718186024988432,
      "loss": 0.0016,
      "step": 13850
    },
    {
      "epoch": 6.413697362332254,
      "grad_norm": 0.4591379463672638,
      "learning_rate": 0.08717260527533549,
      "loss": 0.0042,
      "step": 13860
    },
    {
      "epoch": 6.418324849606663,
      "grad_norm": 4.825181484222412,
      "learning_rate": 0.08716335030078667,
      "loss": 0.0037,
      "step": 13870
    },
    {
      "epoch": 6.422952336881074,
      "grad_norm": 0.014434557408094406,
      "learning_rate": 0.08715409532623786,
      "loss": 0.0073,
      "step": 13880
    },
    {
      "epoch": 6.427579824155483,
      "grad_norm": 0.10105259716510773,
      "learning_rate": 0.08714484035168904,
      "loss": 0.0116,
      "step": 13890
    },
    {
      "epoch": 6.432207311429893,
      "grad_norm": 0.15340431034564972,
      "learning_rate": 0.08713558537714022,
      "loss": 0.0026,
      "step": 13900
    },
    {
      "epoch": 6.436834798704304,
      "grad_norm": 0.1830032765865326,
      "learning_rate": 0.08712633040259139,
      "loss": 0.0014,
      "step": 13910
    },
    {
      "epoch": 6.441462285978713,
      "grad_norm": 1.2686580419540405,
      "learning_rate": 0.08711707542804258,
      "loss": 0.0098,
      "step": 13920
    },
    {
      "epoch": 6.4460897732531235,
      "grad_norm": 0.02762758359313011,
      "learning_rate": 0.08710782045349376,
      "loss": 0.0009,
      "step": 13930
    },
    {
      "epoch": 6.450717260527534,
      "grad_norm": 0.05697751045227051,
      "learning_rate": 0.08709856547894494,
      "loss": 0.0034,
      "step": 13940
    },
    {
      "epoch": 6.455344747801943,
      "grad_norm": 0.006704336032271385,
      "learning_rate": 0.08708931050439611,
      "loss": 0.0087,
      "step": 13950
    },
    {
      "epoch": 6.4599722350763535,
      "grad_norm": 0.1436438113451004,
      "learning_rate": 0.0870800555298473,
      "loss": 0.0036,
      "step": 13960
    },
    {
      "epoch": 6.464599722350764,
      "grad_norm": 0.023436246439814568,
      "learning_rate": 0.08707080055529848,
      "loss": 0.0006,
      "step": 13970
    },
    {
      "epoch": 6.469227209625173,
      "grad_norm": 0.011158441193401814,
      "learning_rate": 0.08706154558074966,
      "loss": 0.0137,
      "step": 13980
    },
    {
      "epoch": 6.473854696899584,
      "grad_norm": 0.6066400408744812,
      "learning_rate": 0.08705229060620084,
      "loss": 0.0064,
      "step": 13990
    },
    {
      "epoch": 6.478482184173994,
      "grad_norm": 0.07688917964696884,
      "learning_rate": 0.08704303563165201,
      "loss": 0.0141,
      "step": 14000
    },
    {
      "epoch": 6.483109671448403,
      "grad_norm": 1.2254842519760132,
      "learning_rate": 0.0870337806571032,
      "loss": 0.0128,
      "step": 14010
    },
    {
      "epoch": 6.487737158722814,
      "grad_norm": 0.2259216457605362,
      "learning_rate": 0.08702452568255438,
      "loss": 0.0016,
      "step": 14020
    },
    {
      "epoch": 6.492364645997224,
      "grad_norm": 0.005795673467218876,
      "learning_rate": 0.08701527070800556,
      "loss": 0.003,
      "step": 14030
    },
    {
      "epoch": 6.496992133271633,
      "grad_norm": 0.0050466014072299,
      "learning_rate": 0.08700601573345673,
      "loss": 0.0045,
      "step": 14040
    },
    {
      "epoch": 6.501619620546044,
      "grad_norm": 4.673590183258057,
      "learning_rate": 0.08699676075890792,
      "loss": 0.0094,
      "step": 14050
    },
    {
      "epoch": 6.506247107820453,
      "grad_norm": 4.513769626617432,
      "learning_rate": 0.0869875057843591,
      "loss": 0.017,
      "step": 14060
    },
    {
      "epoch": 6.510874595094863,
      "grad_norm": 4.371377468109131,
      "learning_rate": 0.08697825080981028,
      "loss": 0.0118,
      "step": 14070
    },
    {
      "epoch": 6.515502082369274,
      "grad_norm": 0.686248242855072,
      "learning_rate": 0.08696899583526146,
      "loss": 0.007,
      "step": 14080
    },
    {
      "epoch": 6.520129569643683,
      "grad_norm": 0.5146421790122986,
      "learning_rate": 0.08695974086071263,
      "loss": 0.0012,
      "step": 14090
    },
    {
      "epoch": 6.5247570569180935,
      "grad_norm": 7.002684116363525,
      "learning_rate": 0.08695048588616382,
      "loss": 0.0119,
      "step": 14100
    },
    {
      "epoch": 6.529384544192504,
      "grad_norm": 1.002315878868103,
      "learning_rate": 0.086941230911615,
      "loss": 0.0016,
      "step": 14110
    },
    {
      "epoch": 6.534012031466913,
      "grad_norm": 1.7227864265441895,
      "learning_rate": 0.08693197593706618,
      "loss": 0.0021,
      "step": 14120
    },
    {
      "epoch": 6.5386395187413235,
      "grad_norm": 0.02869255281984806,
      "learning_rate": 0.08692272096251735,
      "loss": 0.0005,
      "step": 14130
    },
    {
      "epoch": 6.543267006015734,
      "grad_norm": 0.06782808899879456,
      "learning_rate": 0.08691346598796854,
      "loss": 0.006,
      "step": 14140
    },
    {
      "epoch": 6.547894493290143,
      "grad_norm": 3.5262649059295654,
      "learning_rate": 0.08690421101341972,
      "loss": 0.0025,
      "step": 14150
    },
    {
      "epoch": 6.552521980564554,
      "grad_norm": 2.710158109664917,
      "learning_rate": 0.0868949560388709,
      "loss": 0.0097,
      "step": 14160
    },
    {
      "epoch": 6.557149467838963,
      "grad_norm": 0.007700111251324415,
      "learning_rate": 0.08688570106432209,
      "loss": 0.0157,
      "step": 14170
    },
    {
      "epoch": 6.561776955113373,
      "grad_norm": 0.215608149766922,
      "learning_rate": 0.08687644608977325,
      "loss": 0.004,
      "step": 14180
    },
    {
      "epoch": 6.566404442387784,
      "grad_norm": 3.1441755294799805,
      "learning_rate": 0.08686719111522444,
      "loss": 0.0029,
      "step": 14190
    },
    {
      "epoch": 6.571031929662193,
      "grad_norm": 0.09964294731616974,
      "learning_rate": 0.08685793614067562,
      "loss": 0.0341,
      "step": 14200
    },
    {
      "epoch": 6.575659416936603,
      "grad_norm": 0.13168853521347046,
      "learning_rate": 0.0868486811661268,
      "loss": 0.0024,
      "step": 14210
    },
    {
      "epoch": 6.580286904211014,
      "grad_norm": 0.06270076334476471,
      "learning_rate": 0.08683942619157797,
      "loss": 0.0007,
      "step": 14220
    },
    {
      "epoch": 6.584914391485423,
      "grad_norm": 0.07487055659294128,
      "learning_rate": 0.08683017121702916,
      "loss": 0.0032,
      "step": 14230
    },
    {
      "epoch": 6.589541878759833,
      "grad_norm": 2.035917282104492,
      "learning_rate": 0.08682091624248034,
      "loss": 0.004,
      "step": 14240
    },
    {
      "epoch": 6.594169366034244,
      "grad_norm": 0.013458401896059513,
      "learning_rate": 0.08681166126793152,
      "loss": 0.0011,
      "step": 14250
    },
    {
      "epoch": 6.598796853308653,
      "grad_norm": 9.802684783935547,
      "learning_rate": 0.0868024062933827,
      "loss": 0.0061,
      "step": 14260
    },
    {
      "epoch": 6.6034243405830635,
      "grad_norm": 0.1879638284444809,
      "learning_rate": 0.08679315131883387,
      "loss": 0.0107,
      "step": 14270
    },
    {
      "epoch": 6.608051827857473,
      "grad_norm": 1.0095738172531128,
      "learning_rate": 0.08678389634428506,
      "loss": 0.0013,
      "step": 14280
    },
    {
      "epoch": 6.612679315131883,
      "grad_norm": 1.2143679857254028,
      "learning_rate": 0.08677464136973624,
      "loss": 0.0119,
      "step": 14290
    },
    {
      "epoch": 6.6173068024062935,
      "grad_norm": 0.07780050486326218,
      "learning_rate": 0.08676538639518742,
      "loss": 0.017,
      "step": 14300
    },
    {
      "epoch": 6.621934289680704,
      "grad_norm": 1.2019059658050537,
      "learning_rate": 0.0867561314206386,
      "loss": 0.0088,
      "step": 14310
    },
    {
      "epoch": 6.626561776955113,
      "grad_norm": 0.05935274809598923,
      "learning_rate": 0.08674687644608978,
      "loss": 0.0155,
      "step": 14320
    },
    {
      "epoch": 6.631189264229524,
      "grad_norm": 5.043586730957031,
      "learning_rate": 0.08673762147154096,
      "loss": 0.0237,
      "step": 14330
    },
    {
      "epoch": 6.635816751503933,
      "grad_norm": 0.07132158428430557,
      "learning_rate": 0.08672836649699214,
      "loss": 0.0057,
      "step": 14340
    },
    {
      "epoch": 6.640444238778343,
      "grad_norm": 0.04642940312623978,
      "learning_rate": 0.08671911152244333,
      "loss": 0.015,
      "step": 14350
    },
    {
      "epoch": 6.645071726052754,
      "grad_norm": 7.0690460205078125,
      "learning_rate": 0.0867098565478945,
      "loss": 0.0057,
      "step": 14360
    },
    {
      "epoch": 6.649699213327163,
      "grad_norm": 0.17213410139083862,
      "learning_rate": 0.08670060157334568,
      "loss": 0.0132,
      "step": 14370
    },
    {
      "epoch": 6.654326700601573,
      "grad_norm": 0.029554417356848717,
      "learning_rate": 0.08669134659879685,
      "loss": 0.0047,
      "step": 14380
    },
    {
      "epoch": 6.658954187875984,
      "grad_norm": 8.935968399047852,
      "learning_rate": 0.08668209162424804,
      "loss": 0.0199,
      "step": 14390
    },
    {
      "epoch": 6.663581675150393,
      "grad_norm": 1.5071836709976196,
      "learning_rate": 0.08667283664969921,
      "loss": 0.0016,
      "step": 14400
    },
    {
      "epoch": 6.668209162424803,
      "grad_norm": 0.030742235481739044,
      "learning_rate": 0.0866635816751504,
      "loss": 0.0016,
      "step": 14410
    },
    {
      "epoch": 6.672836649699214,
      "grad_norm": 1.1475881338119507,
      "learning_rate": 0.08665432670060158,
      "loss": 0.0008,
      "step": 14420
    },
    {
      "epoch": 6.677464136973623,
      "grad_norm": 2.150702476501465,
      "learning_rate": 0.08664507172605276,
      "loss": 0.0072,
      "step": 14430
    },
    {
      "epoch": 6.6820916242480335,
      "grad_norm": 0.009912961162626743,
      "learning_rate": 0.08663581675150395,
      "loss": 0.0048,
      "step": 14440
    },
    {
      "epoch": 6.686719111522443,
      "grad_norm": 6.22808837890625,
      "learning_rate": 0.08662656177695512,
      "loss": 0.0244,
      "step": 14450
    },
    {
      "epoch": 6.691346598796853,
      "grad_norm": 5.212578296661377,
      "learning_rate": 0.0866173068024063,
      "loss": 0.0071,
      "step": 14460
    },
    {
      "epoch": 6.6959740860712635,
      "grad_norm": 0.26101627945899963,
      "learning_rate": 0.08660805182785747,
      "loss": 0.0048,
      "step": 14470
    },
    {
      "epoch": 6.700601573345673,
      "grad_norm": 0.1221613883972168,
      "learning_rate": 0.08659879685330867,
      "loss": 0.0168,
      "step": 14480
    },
    {
      "epoch": 6.705229060620083,
      "grad_norm": 9.260049819946289,
      "learning_rate": 0.08658954187875983,
      "loss": 0.0124,
      "step": 14490
    },
    {
      "epoch": 6.709856547894494,
      "grad_norm": 3.4073920249938965,
      "learning_rate": 0.08658028690421102,
      "loss": 0.01,
      "step": 14500
    },
    {
      "epoch": 6.714484035168903,
      "grad_norm": 0.07077162712812424,
      "learning_rate": 0.0865710319296622,
      "loss": 0.0115,
      "step": 14510
    },
    {
      "epoch": 6.719111522443313,
      "grad_norm": 0.14033760130405426,
      "learning_rate": 0.08656177695511338,
      "loss": 0.0063,
      "step": 14520
    },
    {
      "epoch": 6.723739009717724,
      "grad_norm": 0.0354275107383728,
      "learning_rate": 0.08655252198056457,
      "loss": 0.0046,
      "step": 14530
    },
    {
      "epoch": 6.728366496992133,
      "grad_norm": 0.0304856039583683,
      "learning_rate": 0.08654326700601574,
      "loss": 0.0082,
      "step": 14540
    },
    {
      "epoch": 6.732993984266543,
      "grad_norm": 0.7227042317390442,
      "learning_rate": 0.08653401203146692,
      "loss": 0.0149,
      "step": 14550
    },
    {
      "epoch": 6.737621471540953,
      "grad_norm": 0.0022635236382484436,
      "learning_rate": 0.08652475705691809,
      "loss": 0.0028,
      "step": 14560
    },
    {
      "epoch": 6.742248958815363,
      "grad_norm": 10.389018058776855,
      "learning_rate": 0.08651550208236929,
      "loss": 0.019,
      "step": 14570
    },
    {
      "epoch": 6.746876446089773,
      "grad_norm": 1.7060296535491943,
      "learning_rate": 0.08650624710782046,
      "loss": 0.0104,
      "step": 14580
    },
    {
      "epoch": 6.751503933364183,
      "grad_norm": 0.005088936071842909,
      "learning_rate": 0.08649699213327164,
      "loss": 0.0071,
      "step": 14590
    },
    {
      "epoch": 6.756131420638593,
      "grad_norm": 10.576950073242188,
      "learning_rate": 0.08648773715872282,
      "loss": 0.0114,
      "step": 14600
    },
    {
      "epoch": 6.7607589079130035,
      "grad_norm": 6.602554798126221,
      "learning_rate": 0.08647848218417399,
      "loss": 0.0092,
      "step": 14610
    },
    {
      "epoch": 6.765386395187413,
      "grad_norm": 0.002138962969183922,
      "learning_rate": 0.08646922720962519,
      "loss": 0.0065,
      "step": 14620
    },
    {
      "epoch": 6.770013882461823,
      "grad_norm": 0.0061852289363741875,
      "learning_rate": 0.08645997223507636,
      "loss": 0.0005,
      "step": 14630
    },
    {
      "epoch": 6.7746413697362335,
      "grad_norm": 0.02082364447414875,
      "learning_rate": 0.08645071726052754,
      "loss": 0.0065,
      "step": 14640
    },
    {
      "epoch": 6.779268857010643,
      "grad_norm": 0.12626077234745026,
      "learning_rate": 0.08644146228597871,
      "loss": 0.0039,
      "step": 14650
    },
    {
      "epoch": 6.783896344285053,
      "grad_norm": 0.3357131779193878,
      "learning_rate": 0.0864322073114299,
      "loss": 0.0072,
      "step": 14660
    },
    {
      "epoch": 6.788523831559464,
      "grad_norm": 11.476263046264648,
      "learning_rate": 0.08642295233688108,
      "loss": 0.0051,
      "step": 14670
    },
    {
      "epoch": 6.793151318833873,
      "grad_norm": 0.19111880660057068,
      "learning_rate": 0.08641369736233226,
      "loss": 0.0135,
      "step": 14680
    },
    {
      "epoch": 6.797778806108283,
      "grad_norm": 7.938490390777588,
      "learning_rate": 0.08640444238778344,
      "loss": 0.0198,
      "step": 14690
    },
    {
      "epoch": 6.802406293382694,
      "grad_norm": 0.06669361144304276,
      "learning_rate": 0.08639518741323461,
      "loss": 0.0007,
      "step": 14700
    },
    {
      "epoch": 6.807033780657103,
      "grad_norm": 0.01732657477259636,
      "learning_rate": 0.08638593243868581,
      "loss": 0.0173,
      "step": 14710
    },
    {
      "epoch": 6.811661267931513,
      "grad_norm": 4.026866912841797,
      "learning_rate": 0.08637667746413698,
      "loss": 0.0054,
      "step": 14720
    },
    {
      "epoch": 6.816288755205923,
      "grad_norm": 0.14266569912433624,
      "learning_rate": 0.08636742248958816,
      "loss": 0.0107,
      "step": 14730
    },
    {
      "epoch": 6.820916242480333,
      "grad_norm": 0.0021523451432585716,
      "learning_rate": 0.08635816751503933,
      "loss": 0.0042,
      "step": 14740
    },
    {
      "epoch": 6.825543729754743,
      "grad_norm": 0.020639151334762573,
      "learning_rate": 0.08634891254049053,
      "loss": 0.001,
      "step": 14750
    },
    {
      "epoch": 6.830171217029153,
      "grad_norm": 0.001999875297769904,
      "learning_rate": 0.0863396575659417,
      "loss": 0.0009,
      "step": 14760
    },
    {
      "epoch": 6.834798704303563,
      "grad_norm": 0.002544363494962454,
      "learning_rate": 0.08633040259139288,
      "loss": 0.002,
      "step": 14770
    },
    {
      "epoch": 6.8394261915779735,
      "grad_norm": 1.1617240905761719,
      "learning_rate": 0.08632114761684406,
      "loss": 0.0031,
      "step": 14780
    },
    {
      "epoch": 6.844053678852383,
      "grad_norm": 0.14993242919445038,
      "learning_rate": 0.08631189264229523,
      "loss": 0.0134,
      "step": 14790
    },
    {
      "epoch": 6.848681166126793,
      "grad_norm": 6.1291351318359375,
      "learning_rate": 0.08630263766774643,
      "loss": 0.0108,
      "step": 14800
    },
    {
      "epoch": 6.8533086534012035,
      "grad_norm": 0.34167924523353577,
      "learning_rate": 0.0862933826931976,
      "loss": 0.0072,
      "step": 14810
    },
    {
      "epoch": 6.857936140675613,
      "grad_norm": 0.12422119081020355,
      "learning_rate": 0.08628412771864878,
      "loss": 0.0016,
      "step": 14820
    },
    {
      "epoch": 6.862563627950023,
      "grad_norm": 0.21971286833286285,
      "learning_rate": 0.08627487274409995,
      "loss": 0.0014,
      "step": 14830
    },
    {
      "epoch": 6.867191115224433,
      "grad_norm": 0.9057438969612122,
      "learning_rate": 0.08626561776955113,
      "loss": 0.0021,
      "step": 14840
    },
    {
      "epoch": 6.871818602498843,
      "grad_norm": 12.002800941467285,
      "learning_rate": 0.08625636279500232,
      "loss": 0.0094,
      "step": 14850
    },
    {
      "epoch": 6.876446089773253,
      "grad_norm": 0.14364278316497803,
      "learning_rate": 0.0862471078204535,
      "loss": 0.0009,
      "step": 14860
    },
    {
      "epoch": 6.881073577047663,
      "grad_norm": 0.012773488648235798,
      "learning_rate": 0.08623785284590468,
      "loss": 0.0137,
      "step": 14870
    },
    {
      "epoch": 6.885701064322073,
      "grad_norm": 7.875519275665283,
      "learning_rate": 0.08622859787135585,
      "loss": 0.0305,
      "step": 14880
    },
    {
      "epoch": 6.890328551596483,
      "grad_norm": 1.426055669784546,
      "learning_rate": 0.08621934289680705,
      "loss": 0.0058,
      "step": 14890
    },
    {
      "epoch": 6.894956038870893,
      "grad_norm": 0.45612451434135437,
      "learning_rate": 0.08621008792225822,
      "loss": 0.0031,
      "step": 14900
    },
    {
      "epoch": 6.899583526145303,
      "grad_norm": 7.374046802520752,
      "learning_rate": 0.0862008329477094,
      "loss": 0.0272,
      "step": 14910
    },
    {
      "epoch": 6.904211013419713,
      "grad_norm": 0.4501414895057678,
      "learning_rate": 0.08619157797316057,
      "loss": 0.0087,
      "step": 14920
    },
    {
      "epoch": 6.908838500694123,
      "grad_norm": 0.220485657453537,
      "learning_rate": 0.08618232299861175,
      "loss": 0.0216,
      "step": 14930
    },
    {
      "epoch": 6.913465987968533,
      "grad_norm": 0.0296182781457901,
      "learning_rate": 0.08617306802406294,
      "loss": 0.001,
      "step": 14940
    },
    {
      "epoch": 6.918093475242943,
      "grad_norm": 0.40348565578460693,
      "learning_rate": 0.08616381304951412,
      "loss": 0.0038,
      "step": 14950
    },
    {
      "epoch": 6.922720962517353,
      "grad_norm": 2.218831777572632,
      "learning_rate": 0.0861545580749653,
      "loss": 0.0011,
      "step": 14960
    },
    {
      "epoch": 6.927348449791763,
      "grad_norm": 0.0211289431899786,
      "learning_rate": 0.08614530310041647,
      "loss": 0.0187,
      "step": 14970
    },
    {
      "epoch": 6.9319759370661735,
      "grad_norm": 1.040772557258606,
      "learning_rate": 0.08613604812586767,
      "loss": 0.0041,
      "step": 14980
    },
    {
      "epoch": 6.936603424340583,
      "grad_norm": 0.22232921421527863,
      "learning_rate": 0.08612679315131884,
      "loss": 0.0085,
      "step": 14990
    },
    {
      "epoch": 6.941230911614993,
      "grad_norm": 0.0036739560309797525,
      "learning_rate": 0.08611753817677002,
      "loss": 0.0117,
      "step": 15000
    },
    {
      "epoch": 6.945858398889403,
      "grad_norm": 0.5582143068313599,
      "learning_rate": 0.08610828320222119,
      "loss": 0.0245,
      "step": 15010
    },
    {
      "epoch": 6.950485886163813,
      "grad_norm": 0.23097540438175201,
      "learning_rate": 0.08609902822767238,
      "loss": 0.0301,
      "step": 15020
    },
    {
      "epoch": 6.955113373438223,
      "grad_norm": 0.010588003322482109,
      "learning_rate": 0.08608977325312356,
      "loss": 0.0137,
      "step": 15030
    },
    {
      "epoch": 6.959740860712633,
      "grad_norm": 0.05933371186256409,
      "learning_rate": 0.08608051827857474,
      "loss": 0.0012,
      "step": 15040
    },
    {
      "epoch": 6.964368347987043,
      "grad_norm": 0.009380988776683807,
      "learning_rate": 0.08607126330402592,
      "loss": 0.0037,
      "step": 15050
    },
    {
      "epoch": 6.968995835261453,
      "grad_norm": 0.06056259945034981,
      "learning_rate": 0.0860620083294771,
      "loss": 0.01,
      "step": 15060
    },
    {
      "epoch": 6.973623322535863,
      "grad_norm": 2.010387659072876,
      "learning_rate": 0.08605275335492828,
      "loss": 0.0055,
      "step": 15070
    },
    {
      "epoch": 6.978250809810273,
      "grad_norm": 0.23039314150810242,
      "learning_rate": 0.08604349838037946,
      "loss": 0.0037,
      "step": 15080
    },
    {
      "epoch": 6.982878297084683,
      "grad_norm": 0.008636213839054108,
      "learning_rate": 0.08603424340583064,
      "loss": 0.0074,
      "step": 15090
    },
    {
      "epoch": 6.987505784359093,
      "grad_norm": 0.22671720385551453,
      "learning_rate": 0.08602498843128181,
      "loss": 0.0119,
      "step": 15100
    },
    {
      "epoch": 6.992133271633503,
      "grad_norm": 0.3844964802265167,
      "learning_rate": 0.086015733456733,
      "loss": 0.0005,
      "step": 15110
    },
    {
      "epoch": 6.996760758907913,
      "grad_norm": 0.12534546852111816,
      "learning_rate": 0.08600647848218418,
      "loss": 0.0054,
      "step": 15120
    },
    {
      "epoch": 7.0,
      "eval_accuracy_branch1": 0.985518410106301,
      "eval_accuracy_branch2": 0.4950700970574642,
      "eval_f1_branch1": 0.9856364358813656,
      "eval_f1_branch2": 0.49367681569336674,
      "eval_loss": 0.034941285848617554,
      "eval_precision_branch1": 0.98488431556923,
      "eval_precision_branch2": 0.4950152294301899,
      "eval_recall_branch1": 0.9865294057869666,
      "eval_recall_branch2": 0.4950700970574642,
      "eval_runtime": 29.0696,
      "eval_samples_per_second": 446.583,
      "eval_steps_per_second": 55.831,
      "step": 15127
    },
    {
      "epoch": 7.001388246182323,
      "grad_norm": 0.4270620346069336,
      "learning_rate": 0.08599722350763536,
      "loss": 0.4889,
      "step": 15130
    },
    {
      "epoch": 7.006015733456733,
      "grad_norm": 5.7130045890808105,
      "learning_rate": 0.08598796853308655,
      "loss": 0.0068,
      "step": 15140
    },
    {
      "epoch": 7.010643220731143,
      "grad_norm": 0.26431557536125183,
      "learning_rate": 0.08597871355853771,
      "loss": 0.0094,
      "step": 15150
    },
    {
      "epoch": 7.015270708005553,
      "grad_norm": 2.8849785327911377,
      "learning_rate": 0.0859694585839889,
      "loss": 0.0076,
      "step": 15160
    },
    {
      "epoch": 7.019898195279963,
      "grad_norm": 0.14165206253528595,
      "learning_rate": 0.08596020360944008,
      "loss": 0.0012,
      "step": 15170
    },
    {
      "epoch": 7.024525682554373,
      "grad_norm": 0.007910034619271755,
      "learning_rate": 0.08595094863489126,
      "loss": 0.0036,
      "step": 15180
    },
    {
      "epoch": 7.029153169828783,
      "grad_norm": 1.1374036073684692,
      "learning_rate": 0.08594169366034243,
      "loss": 0.003,
      "step": 15190
    },
    {
      "epoch": 7.033780657103193,
      "grad_norm": 1.983191728591919,
      "learning_rate": 0.08593243868579362,
      "loss": 0.0017,
      "step": 15200
    },
    {
      "epoch": 7.038408144377603,
      "grad_norm": 0.5924525260925293,
      "learning_rate": 0.0859231837112448,
      "loss": 0.0011,
      "step": 15210
    },
    {
      "epoch": 7.043035631652013,
      "grad_norm": 0.9877549409866333,
      "learning_rate": 0.08591392873669598,
      "loss": 0.0115,
      "step": 15220
    },
    {
      "epoch": 7.047663118926423,
      "grad_norm": 0.6123111844062805,
      "learning_rate": 0.08590467376214717,
      "loss": 0.0107,
      "step": 15230
    },
    {
      "epoch": 7.052290606200833,
      "grad_norm": 0.02143256738781929,
      "learning_rate": 0.08589541878759833,
      "loss": 0.0013,
      "step": 15240
    },
    {
      "epoch": 7.056918093475243,
      "grad_norm": 0.04627196118235588,
      "learning_rate": 0.08588616381304952,
      "loss": 0.0003,
      "step": 15250
    },
    {
      "epoch": 7.0615455807496526,
      "grad_norm": 0.0014092600904405117,
      "learning_rate": 0.0858769088385007,
      "loss": 0.0003,
      "step": 15260
    },
    {
      "epoch": 7.066173068024063,
      "grad_norm": 1.141099452972412,
      "learning_rate": 0.08586765386395188,
      "loss": 0.0017,
      "step": 15270
    },
    {
      "epoch": 7.070800555298473,
      "grad_norm": 3.119267463684082,
      "learning_rate": 0.08585839888940305,
      "loss": 0.0046,
      "step": 15280
    },
    {
      "epoch": 7.075428042572883,
      "grad_norm": 1.346731424331665,
      "learning_rate": 0.08584914391485424,
      "loss": 0.0059,
      "step": 15290
    },
    {
      "epoch": 7.080055529847293,
      "grad_norm": 0.25489646196365356,
      "learning_rate": 0.08583988894030542,
      "loss": 0.0018,
      "step": 15300
    },
    {
      "epoch": 7.084683017121703,
      "grad_norm": 0.17919741570949554,
      "learning_rate": 0.0858306339657566,
      "loss": 0.0062,
      "step": 15310
    },
    {
      "epoch": 7.089310504396113,
      "grad_norm": 1.2084832191467285,
      "learning_rate": 0.08582137899120779,
      "loss": 0.0234,
      "step": 15320
    },
    {
      "epoch": 7.093937991670523,
      "grad_norm": 0.5190354585647583,
      "learning_rate": 0.08581212401665896,
      "loss": 0.0086,
      "step": 15330
    },
    {
      "epoch": 7.098565478944933,
      "grad_norm": 0.0012848005862906575,
      "learning_rate": 0.08580286904211014,
      "loss": 0.0019,
      "step": 15340
    },
    {
      "epoch": 7.103192966219343,
      "grad_norm": 0.008743812330067158,
      "learning_rate": 0.08579361406756132,
      "loss": 0.0208,
      "step": 15350
    },
    {
      "epoch": 7.107820453493753,
      "grad_norm": 0.5886369943618774,
      "learning_rate": 0.0857843590930125,
      "loss": 0.0024,
      "step": 15360
    },
    {
      "epoch": 7.112447940768163,
      "grad_norm": 0.019996652379631996,
      "learning_rate": 0.08577510411846367,
      "loss": 0.0015,
      "step": 15370
    },
    {
      "epoch": 7.117075428042573,
      "grad_norm": 0.3713461458683014,
      "learning_rate": 0.08576584914391486,
      "loss": 0.0014,
      "step": 15380
    },
    {
      "epoch": 7.121702915316983,
      "grad_norm": 0.04825451970100403,
      "learning_rate": 0.08575659416936604,
      "loss": 0.0004,
      "step": 15390
    },
    {
      "epoch": 7.1263304025913925,
      "grad_norm": 0.6318044066429138,
      "learning_rate": 0.08574733919481722,
      "loss": 0.0078,
      "step": 15400
    },
    {
      "epoch": 7.130957889865803,
      "grad_norm": 0.6725894212722778,
      "learning_rate": 0.0857380842202684,
      "loss": 0.0034,
      "step": 15410
    },
    {
      "epoch": 7.135585377140213,
      "grad_norm": 0.055287208408117294,
      "learning_rate": 0.08572882924571958,
      "loss": 0.0133,
      "step": 15420
    },
    {
      "epoch": 7.1402128644146226,
      "grad_norm": 0.1383155733346939,
      "learning_rate": 0.08571957427117076,
      "loss": 0.0179,
      "step": 15430
    },
    {
      "epoch": 7.144840351689033,
      "grad_norm": 0.015539285726845264,
      "learning_rate": 0.08571031929662194,
      "loss": 0.0029,
      "step": 15440
    },
    {
      "epoch": 7.149467838963443,
      "grad_norm": 0.04764574021100998,
      "learning_rate": 0.08570106432207313,
      "loss": 0.0033,
      "step": 15450
    },
    {
      "epoch": 7.154095326237853,
      "grad_norm": 1.4427645206451416,
      "learning_rate": 0.0856918093475243,
      "loss": 0.0098,
      "step": 15460
    },
    {
      "epoch": 7.158722813512263,
      "grad_norm": 0.6112180948257446,
      "learning_rate": 0.08568255437297548,
      "loss": 0.0015,
      "step": 15470
    },
    {
      "epoch": 7.163350300786673,
      "grad_norm": 0.017866021022200584,
      "learning_rate": 0.08567329939842666,
      "loss": 0.0145,
      "step": 15480
    },
    {
      "epoch": 7.167977788061083,
      "grad_norm": 0.008234073407948017,
      "learning_rate": 0.08566404442387784,
      "loss": 0.0014,
      "step": 15490
    },
    {
      "epoch": 7.172605275335493,
      "grad_norm": 0.22651030123233795,
      "learning_rate": 0.08565478944932903,
      "loss": 0.0064,
      "step": 15500
    },
    {
      "epoch": 7.177232762609902,
      "grad_norm": 0.15777873992919922,
      "learning_rate": 0.0856455344747802,
      "loss": 0.0005,
      "step": 15510
    },
    {
      "epoch": 7.181860249884313,
      "grad_norm": 0.07092102617025375,
      "learning_rate": 0.08563627950023138,
      "loss": 0.0049,
      "step": 15520
    },
    {
      "epoch": 7.186487737158723,
      "grad_norm": 1.767149567604065,
      "learning_rate": 0.08562702452568255,
      "loss": 0.0065,
      "step": 15530
    },
    {
      "epoch": 7.1911152244331324,
      "grad_norm": 0.0035171136260032654,
      "learning_rate": 0.08561776955113375,
      "loss": 0.001,
      "step": 15540
    },
    {
      "epoch": 7.195742711707543,
      "grad_norm": 0.02219576947391033,
      "learning_rate": 0.08560851457658492,
      "loss": 0.002,
      "step": 15550
    },
    {
      "epoch": 7.200370198981953,
      "grad_norm": 0.1387426257133484,
      "learning_rate": 0.0855992596020361,
      "loss": 0.0005,
      "step": 15560
    },
    {
      "epoch": 7.2049976862563625,
      "grad_norm": 0.2390720695257187,
      "learning_rate": 0.08559000462748728,
      "loss": 0.0052,
      "step": 15570
    },
    {
      "epoch": 7.209625173530773,
      "grad_norm": 0.0032994684297591448,
      "learning_rate": 0.08558074965293846,
      "loss": 0.001,
      "step": 15580
    },
    {
      "epoch": 7.214252660805183,
      "grad_norm": 0.13531683385372162,
      "learning_rate": 0.08557149467838965,
      "loss": 0.0089,
      "step": 15590
    },
    {
      "epoch": 7.2188801480795926,
      "grad_norm": 0.0013054019073024392,
      "learning_rate": 0.08556223970384082,
      "loss": 0.0006,
      "step": 15600
    },
    {
      "epoch": 7.223507635354003,
      "grad_norm": 0.024985050782561302,
      "learning_rate": 0.085552984729292,
      "loss": 0.0102,
      "step": 15610
    },
    {
      "epoch": 7.228135122628413,
      "grad_norm": 0.0238024964928627,
      "learning_rate": 0.08554372975474317,
      "loss": 0.0002,
      "step": 15620
    },
    {
      "epoch": 7.232762609902823,
      "grad_norm": 2.5159354209899902,
      "learning_rate": 0.08553447478019437,
      "loss": 0.0034,
      "step": 15630
    },
    {
      "epoch": 7.237390097177233,
      "grad_norm": 0.024997595697641373,
      "learning_rate": 0.08552521980564554,
      "loss": 0.0027,
      "step": 15640
    },
    {
      "epoch": 7.242017584451642,
      "grad_norm": 0.00420145271345973,
      "learning_rate": 0.08551596483109672,
      "loss": 0.0003,
      "step": 15650
    },
    {
      "epoch": 7.246645071726053,
      "grad_norm": 3.6813738346099854,
      "learning_rate": 0.0855067098565479,
      "loss": 0.0025,
      "step": 15660
    },
    {
      "epoch": 7.251272559000463,
      "grad_norm": 2.093858003616333,
      "learning_rate": 0.08549745488199909,
      "loss": 0.0077,
      "step": 15670
    },
    {
      "epoch": 7.255900046274872,
      "grad_norm": 0.02110915631055832,
      "learning_rate": 0.08548819990745027,
      "loss": 0.0015,
      "step": 15680
    },
    {
      "epoch": 7.260527533549283,
      "grad_norm": 0.34876301884651184,
      "learning_rate": 0.08547894493290144,
      "loss": 0.0021,
      "step": 15690
    },
    {
      "epoch": 7.265155020823693,
      "grad_norm": 0.6495866775512695,
      "learning_rate": 0.08546968995835262,
      "loss": 0.0117,
      "step": 15700
    },
    {
      "epoch": 7.2697825080981024,
      "grad_norm": 0.10680177807807922,
      "learning_rate": 0.08546043498380379,
      "loss": 0.0129,
      "step": 15710
    },
    {
      "epoch": 7.274409995372513,
      "grad_norm": 6.834840774536133,
      "learning_rate": 0.08545118000925499,
      "loss": 0.0092,
      "step": 15720
    },
    {
      "epoch": 7.279037482646923,
      "grad_norm": 0.02492046356201172,
      "learning_rate": 0.08544192503470616,
      "loss": 0.0089,
      "step": 15730
    },
    {
      "epoch": 7.2836649699213325,
      "grad_norm": 1.6870797872543335,
      "learning_rate": 0.08543267006015734,
      "loss": 0.0032,
      "step": 15740
    },
    {
      "epoch": 7.288292457195743,
      "grad_norm": 0.080238476395607,
      "learning_rate": 0.08542341508560852,
      "loss": 0.0015,
      "step": 15750
    },
    {
      "epoch": 7.292919944470153,
      "grad_norm": 0.005025884602218866,
      "learning_rate": 0.08541416011105969,
      "loss": 0.0037,
      "step": 15760
    },
    {
      "epoch": 7.2975474317445626,
      "grad_norm": 0.0020021400414407253,
      "learning_rate": 0.08540490513651089,
      "loss": 0.0013,
      "step": 15770
    },
    {
      "epoch": 7.302174919018973,
      "grad_norm": 2.727808952331543,
      "learning_rate": 0.08539565016196206,
      "loss": 0.0027,
      "step": 15780
    },
    {
      "epoch": 7.306802406293382,
      "grad_norm": 1.7708922624588013,
      "learning_rate": 0.08538639518741324,
      "loss": 0.0017,
      "step": 15790
    },
    {
      "epoch": 7.311429893567793,
      "grad_norm": 0.03501537814736366,
      "learning_rate": 0.08537714021286441,
      "loss": 0.0056,
      "step": 15800
    },
    {
      "epoch": 7.316057380842203,
      "grad_norm": 0.03017742931842804,
      "learning_rate": 0.08536788523831561,
      "loss": 0.0044,
      "step": 15810
    },
    {
      "epoch": 7.320684868116612,
      "grad_norm": 6.076996326446533,
      "learning_rate": 0.08535863026376678,
      "loss": 0.0163,
      "step": 15820
    },
    {
      "epoch": 7.325312355391023,
      "grad_norm": 7.124014854431152,
      "learning_rate": 0.08534937528921796,
      "loss": 0.0045,
      "step": 15830
    },
    {
      "epoch": 7.329939842665433,
      "grad_norm": 0.07478158921003342,
      "learning_rate": 0.08534012031466914,
      "loss": 0.0161,
      "step": 15840
    },
    {
      "epoch": 7.334567329939842,
      "grad_norm": 0.05802082642912865,
      "learning_rate": 0.08533086534012031,
      "loss": 0.0061,
      "step": 15850
    },
    {
      "epoch": 7.339194817214253,
      "grad_norm": 0.10744373500347137,
      "learning_rate": 0.08532161036557151,
      "loss": 0.0021,
      "step": 15860
    },
    {
      "epoch": 7.343822304488663,
      "grad_norm": 3.6034650802612305,
      "learning_rate": 0.08531235539102268,
      "loss": 0.0058,
      "step": 15870
    },
    {
      "epoch": 7.3484497917630724,
      "grad_norm": 0.045902326703071594,
      "learning_rate": 0.08530310041647386,
      "loss": 0.0013,
      "step": 15880
    },
    {
      "epoch": 7.353077279037483,
      "grad_norm": 0.04770791158080101,
      "learning_rate": 0.08529384544192503,
      "loss": 0.0118,
      "step": 15890
    },
    {
      "epoch": 7.357704766311892,
      "grad_norm": 0.004719981458038092,
      "learning_rate": 0.08528459046737623,
      "loss": 0.0008,
      "step": 15900
    },
    {
      "epoch": 7.3623322535863025,
      "grad_norm": 0.05983331799507141,
      "learning_rate": 0.0852753354928274,
      "loss": 0.0038,
      "step": 15910
    },
    {
      "epoch": 7.366959740860713,
      "grad_norm": 0.004781065043061972,
      "learning_rate": 0.08526608051827858,
      "loss": 0.001,
      "step": 15920
    },
    {
      "epoch": 7.371587228135122,
      "grad_norm": 0.03277517110109329,
      "learning_rate": 0.08525682554372976,
      "loss": 0.0024,
      "step": 15930
    },
    {
      "epoch": 7.3762147154095326,
      "grad_norm": 3.7127935886383057,
      "learning_rate": 0.08524757056918093,
      "loss": 0.0086,
      "step": 15940
    },
    {
      "epoch": 7.380842202683943,
      "grad_norm": 0.07238295674324036,
      "learning_rate": 0.08523831559463213,
      "loss": 0.0021,
      "step": 15950
    },
    {
      "epoch": 7.385469689958352,
      "grad_norm": 2.265585422515869,
      "learning_rate": 0.0852290606200833,
      "loss": 0.0025,
      "step": 15960
    },
    {
      "epoch": 7.390097177232763,
      "grad_norm": 0.3613971471786499,
      "learning_rate": 0.08521980564553448,
      "loss": 0.0007,
      "step": 15970
    },
    {
      "epoch": 7.394724664507173,
      "grad_norm": 1.0448709726333618,
      "learning_rate": 0.08521055067098565,
      "loss": 0.0017,
      "step": 15980
    },
    {
      "epoch": 7.399352151781582,
      "grad_norm": 0.06107138469815254,
      "learning_rate": 0.08520129569643684,
      "loss": 0.0014,
      "step": 15990
    },
    {
      "epoch": 7.403979639055993,
      "grad_norm": 1.7845016717910767,
      "learning_rate": 0.08519204072188802,
      "loss": 0.0028,
      "step": 16000
    },
    {
      "epoch": 7.408607126330403,
      "grad_norm": 0.022588804364204407,
      "learning_rate": 0.0851827857473392,
      "loss": 0.0034,
      "step": 16010
    },
    {
      "epoch": 7.413234613604812,
      "grad_norm": 0.005192426033318043,
      "learning_rate": 0.08517353077279038,
      "loss": 0.0009,
      "step": 16020
    },
    {
      "epoch": 7.417862100879223,
      "grad_norm": 3.423762321472168,
      "learning_rate": 0.08516427579824155,
      "loss": 0.0016,
      "step": 16030
    },
    {
      "epoch": 7.422489588153633,
      "grad_norm": 0.9584567546844482,
      "learning_rate": 0.08515502082369275,
      "loss": 0.0094,
      "step": 16040
    },
    {
      "epoch": 7.4271170754280424,
      "grad_norm": 0.01286490261554718,
      "learning_rate": 0.08514576584914392,
      "loss": 0.0068,
      "step": 16050
    },
    {
      "epoch": 7.431744562702453,
      "grad_norm": 0.02808506228029728,
      "learning_rate": 0.0851365108745951,
      "loss": 0.0017,
      "step": 16060
    },
    {
      "epoch": 7.436372049976862,
      "grad_norm": 0.05893167480826378,
      "learning_rate": 0.08512725590004627,
      "loss": 0.0053,
      "step": 16070
    },
    {
      "epoch": 7.4409995372512725,
      "grad_norm": 0.002793866442516446,
      "learning_rate": 0.08511800092549746,
      "loss": 0.0022,
      "step": 16080
    },
    {
      "epoch": 7.445627024525683,
      "grad_norm": 0.00314032519236207,
      "learning_rate": 0.08510874595094864,
      "loss": 0.0016,
      "step": 16090
    },
    {
      "epoch": 7.450254511800092,
      "grad_norm": 0.010845919139683247,
      "learning_rate": 0.08509949097639982,
      "loss": 0.0069,
      "step": 16100
    },
    {
      "epoch": 7.4548819990745026,
      "grad_norm": 0.05984874442219734,
      "learning_rate": 0.085090236001851,
      "loss": 0.0024,
      "step": 16110
    },
    {
      "epoch": 7.459509486348913,
      "grad_norm": 0.007856263779103756,
      "learning_rate": 0.08508098102730217,
      "loss": 0.0006,
      "step": 16120
    },
    {
      "epoch": 7.464136973623322,
      "grad_norm": 0.359328955411911,
      "learning_rate": 0.08507172605275337,
      "loss": 0.0016,
      "step": 16130
    },
    {
      "epoch": 7.468764460897733,
      "grad_norm": 0.15153641998767853,
      "learning_rate": 0.08506247107820454,
      "loss": 0.0077,
      "step": 16140
    },
    {
      "epoch": 7.473391948172143,
      "grad_norm": 2.9491686820983887,
      "learning_rate": 0.08505321610365572,
      "loss": 0.0016,
      "step": 16150
    },
    {
      "epoch": 7.478019435446552,
      "grad_norm": 0.7350857257843018,
      "learning_rate": 0.0850439611291069,
      "loss": 0.0012,
      "step": 16160
    },
    {
      "epoch": 7.482646922720963,
      "grad_norm": 3.741821527481079,
      "learning_rate": 0.08503470615455808,
      "loss": 0.0025,
      "step": 16170
    },
    {
      "epoch": 7.487274409995372,
      "grad_norm": 0.005828088149428368,
      "learning_rate": 0.08502545118000926,
      "loss": 0.0072,
      "step": 16180
    },
    {
      "epoch": 7.491901897269782,
      "grad_norm": 0.6150103211402893,
      "learning_rate": 0.08501619620546044,
      "loss": 0.002,
      "step": 16190
    },
    {
      "epoch": 7.496529384544193,
      "grad_norm": 11.188875198364258,
      "learning_rate": 0.08500694123091163,
      "loss": 0.0047,
      "step": 16200
    },
    {
      "epoch": 7.501156871818602,
      "grad_norm": 0.0060814740136265755,
      "learning_rate": 0.0849976862563628,
      "loss": 0.0068,
      "step": 16210
    },
    {
      "epoch": 7.5057843590930124,
      "grad_norm": 7.621312618255615,
      "learning_rate": 0.08498843128181398,
      "loss": 0.0043,
      "step": 16220
    },
    {
      "epoch": 7.510411846367423,
      "grad_norm": 0.04749695211648941,
      "learning_rate": 0.08497917630726516,
      "loss": 0.0057,
      "step": 16230
    },
    {
      "epoch": 7.515039333641832,
      "grad_norm": 0.029449718073010445,
      "learning_rate": 0.08496992133271634,
      "loss": 0.0022,
      "step": 16240
    },
    {
      "epoch": 7.5196668209162425,
      "grad_norm": 0.3243824541568756,
      "learning_rate": 0.08496066635816751,
      "loss": 0.0059,
      "step": 16250
    },
    {
      "epoch": 7.524294308190653,
      "grad_norm": 0.04547794908285141,
      "learning_rate": 0.0849514113836187,
      "loss": 0.0004,
      "step": 16260
    },
    {
      "epoch": 7.528921795465062,
      "grad_norm": 2.5473122596740723,
      "learning_rate": 0.08494215640906988,
      "loss": 0.0022,
      "step": 16270
    },
    {
      "epoch": 7.5335492827394726,
      "grad_norm": 0.015109916217625141,
      "learning_rate": 0.08493290143452106,
      "loss": 0.0169,
      "step": 16280
    },
    {
      "epoch": 7.538176770013882,
      "grad_norm": 0.0021287724375724792,
      "learning_rate": 0.08492364645997225,
      "loss": 0.0011,
      "step": 16290
    },
    {
      "epoch": 7.542804257288292,
      "grad_norm": 0.004084049724042416,
      "learning_rate": 0.08491439148542342,
      "loss": 0.0003,
      "step": 16300
    },
    {
      "epoch": 7.547431744562703,
      "grad_norm": 0.04530559480190277,
      "learning_rate": 0.0849051365108746,
      "loss": 0.0087,
      "step": 16310
    },
    {
      "epoch": 7.552059231837113,
      "grad_norm": 0.020632637664675713,
      "learning_rate": 0.08489588153632578,
      "loss": 0.0016,
      "step": 16320
    },
    {
      "epoch": 7.556686719111522,
      "grad_norm": 3.2261645793914795,
      "learning_rate": 0.08488662656177696,
      "loss": 0.0026,
      "step": 16330
    },
    {
      "epoch": 7.561314206385933,
      "grad_norm": 0.004894089885056019,
      "learning_rate": 0.08487737158722813,
      "loss": 0.0026,
      "step": 16340
    },
    {
      "epoch": 7.565941693660342,
      "grad_norm": 0.07154728472232819,
      "learning_rate": 0.08486811661267932,
      "loss": 0.0069,
      "step": 16350
    },
    {
      "epoch": 7.570569180934752,
      "grad_norm": 8.826306343078613,
      "learning_rate": 0.0848588616381305,
      "loss": 0.0147,
      "step": 16360
    },
    {
      "epoch": 7.575196668209163,
      "grad_norm": 0.140349879860878,
      "learning_rate": 0.08484960666358168,
      "loss": 0.0032,
      "step": 16370
    },
    {
      "epoch": 7.579824155483572,
      "grad_norm": 3.229151725769043,
      "learning_rate": 0.08484035168903287,
      "loss": 0.0198,
      "step": 16380
    },
    {
      "epoch": 7.5844516427579824,
      "grad_norm": 0.20650345087051392,
      "learning_rate": 0.08483109671448404,
      "loss": 0.0035,
      "step": 16390
    },
    {
      "epoch": 7.589079130032393,
      "grad_norm": 0.0205560103058815,
      "learning_rate": 0.08482184173993522,
      "loss": 0.0031,
      "step": 16400
    },
    {
      "epoch": 7.593706617306802,
      "grad_norm": 0.5129316449165344,
      "learning_rate": 0.0848125867653864,
      "loss": 0.0016,
      "step": 16410
    },
    {
      "epoch": 7.5983341045812125,
      "grad_norm": 0.0054254126735031605,
      "learning_rate": 0.08480333179083759,
      "loss": 0.0127,
      "step": 16420
    },
    {
      "epoch": 7.602961591855623,
      "grad_norm": 0.018799038603901863,
      "learning_rate": 0.08479407681628875,
      "loss": 0.0041,
      "step": 16430
    },
    {
      "epoch": 7.607589079130032,
      "grad_norm": 0.010133935138583183,
      "learning_rate": 0.08478482184173994,
      "loss": 0.0092,
      "step": 16440
    },
    {
      "epoch": 7.6122165664044426,
      "grad_norm": 0.006086079869419336,
      "learning_rate": 0.08477556686719112,
      "loss": 0.012,
      "step": 16450
    },
    {
      "epoch": 7.616844053678852,
      "grad_norm": 0.030448798090219498,
      "learning_rate": 0.0847663118926423,
      "loss": 0.0022,
      "step": 16460
    },
    {
      "epoch": 7.621471540953262,
      "grad_norm": 0.12331540882587433,
      "learning_rate": 0.08475705691809349,
      "loss": 0.0008,
      "step": 16470
    },
    {
      "epoch": 7.626099028227673,
      "grad_norm": 1.2521635293960571,
      "learning_rate": 0.08474780194354466,
      "loss": 0.0042,
      "step": 16480
    },
    {
      "epoch": 7.630726515502082,
      "grad_norm": 9.360363006591797,
      "learning_rate": 0.08473854696899584,
      "loss": 0.0083,
      "step": 16490
    },
    {
      "epoch": 7.635354002776492,
      "grad_norm": 0.0896158441901207,
      "learning_rate": 0.08472929199444702,
      "loss": 0.0015,
      "step": 16500
    },
    {
      "epoch": 7.639981490050903,
      "grad_norm": 0.040539249777793884,
      "learning_rate": 0.0847200370198982,
      "loss": 0.0004,
      "step": 16510
    },
    {
      "epoch": 7.644608977325312,
      "grad_norm": 0.010017123073339462,
      "learning_rate": 0.08471078204534938,
      "loss": 0.0064,
      "step": 16520
    },
    {
      "epoch": 7.649236464599722,
      "grad_norm": 0.1421491950750351,
      "learning_rate": 0.08470152707080056,
      "loss": 0.0012,
      "step": 16530
    },
    {
      "epoch": 7.653863951874133,
      "grad_norm": 0.6447556018829346,
      "learning_rate": 0.08469227209625174,
      "loss": 0.0035,
      "step": 16540
    },
    {
      "epoch": 7.658491439148542,
      "grad_norm": 0.0018564903875812888,
      "learning_rate": 0.08468301712170292,
      "loss": 0.0224,
      "step": 16550
    },
    {
      "epoch": 7.6631189264229524,
      "grad_norm": 4.1137471199035645,
      "learning_rate": 0.08467376214715411,
      "loss": 0.0228,
      "step": 16560
    },
    {
      "epoch": 7.667746413697362,
      "grad_norm": 0.004466772545129061,
      "learning_rate": 0.08466450717260528,
      "loss": 0.0016,
      "step": 16570
    },
    {
      "epoch": 7.672373900971772,
      "grad_norm": 0.05232297256588936,
      "learning_rate": 0.08465525219805646,
      "loss": 0.0002,
      "step": 16580
    },
    {
      "epoch": 7.6770013882461825,
      "grad_norm": 0.025230512022972107,
      "learning_rate": 0.08464599722350764,
      "loss": 0.0006,
      "step": 16590
    },
    {
      "epoch": 7.681628875520593,
      "grad_norm": 0.009755488485097885,
      "learning_rate": 0.08463674224895883,
      "loss": 0.0056,
      "step": 16600
    },
    {
      "epoch": 7.686256362795002,
      "grad_norm": 0.009010324254631996,
      "learning_rate": 0.08462748727441,
      "loss": 0.0012,
      "step": 16610
    },
    {
      "epoch": 7.6908838500694126,
      "grad_norm": 2.0703701972961426,
      "learning_rate": 0.08461823229986118,
      "loss": 0.0086,
      "step": 16620
    },
    {
      "epoch": 7.695511337343822,
      "grad_norm": 0.002205155324190855,
      "learning_rate": 0.08460897732531236,
      "loss": 0.0037,
      "step": 16630
    },
    {
      "epoch": 7.700138824618232,
      "grad_norm": 0.36987417936325073,
      "learning_rate": 0.08459972235076355,
      "loss": 0.0026,
      "step": 16640
    },
    {
      "epoch": 7.704766311892643,
      "grad_norm": 0.22848565876483917,
      "learning_rate": 0.08459046737621473,
      "loss": 0.0003,
      "step": 16650
    },
    {
      "epoch": 7.709393799167052,
      "grad_norm": 0.08216369897127151,
      "learning_rate": 0.0845812124016659,
      "loss": 0.0029,
      "step": 16660
    },
    {
      "epoch": 7.714021286441462,
      "grad_norm": 0.00888446718454361,
      "learning_rate": 0.08457195742711708,
      "loss": 0.0003,
      "step": 16670
    },
    {
      "epoch": 7.718648773715873,
      "grad_norm": 0.048709530383348465,
      "learning_rate": 0.08456270245256825,
      "loss": 0.0019,
      "step": 16680
    },
    {
      "epoch": 7.723276260990282,
      "grad_norm": 0.7364208698272705,
      "learning_rate": 0.08455344747801945,
      "loss": 0.0038,
      "step": 16690
    },
    {
      "epoch": 7.727903748264692,
      "grad_norm": 0.0033863293938338757,
      "learning_rate": 0.08454419250347062,
      "loss": 0.0061,
      "step": 16700
    },
    {
      "epoch": 7.732531235539103,
      "grad_norm": 4.86577844619751,
      "learning_rate": 0.0845349375289218,
      "loss": 0.0086,
      "step": 16710
    },
    {
      "epoch": 7.737158722813512,
      "grad_norm": 0.1698947548866272,
      "learning_rate": 0.08452568255437298,
      "loss": 0.0007,
      "step": 16720
    },
    {
      "epoch": 7.7417862100879224,
      "grad_norm": 0.5306904315948486,
      "learning_rate": 0.08451642757982417,
      "loss": 0.0019,
      "step": 16730
    },
    {
      "epoch": 7.746413697362332,
      "grad_norm": 0.007135247346013784,
      "learning_rate": 0.08450717260527535,
      "loss": 0.0045,
      "step": 16740
    },
    {
      "epoch": 7.751041184636742,
      "grad_norm": 1.2878544330596924,
      "learning_rate": 0.08449791763072652,
      "loss": 0.0086,
      "step": 16750
    },
    {
      "epoch": 7.7556686719111525,
      "grad_norm": 0.054388377815485,
      "learning_rate": 0.0844886626561777,
      "loss": 0.0061,
      "step": 16760
    },
    {
      "epoch": 7.760296159185562,
      "grad_norm": 1.4889297485351562,
      "learning_rate": 0.08447940768162887,
      "loss": 0.0156,
      "step": 16770
    },
    {
      "epoch": 7.764923646459972,
      "grad_norm": 2.6695778369903564,
      "learning_rate": 0.08447015270708007,
      "loss": 0.0053,
      "step": 16780
    },
    {
      "epoch": 7.7695511337343826,
      "grad_norm": 0.7617689967155457,
      "learning_rate": 0.08446089773253124,
      "loss": 0.0052,
      "step": 16790
    },
    {
      "epoch": 7.774178621008792,
      "grad_norm": 0.04373033344745636,
      "learning_rate": 0.08445164275798242,
      "loss": 0.0016,
      "step": 16800
    },
    {
      "epoch": 7.778806108283202,
      "grad_norm": 0.01591411605477333,
      "learning_rate": 0.0844423877834336,
      "loss": 0.0045,
      "step": 16810
    },
    {
      "epoch": 7.783433595557613,
      "grad_norm": 0.12668804824352264,
      "learning_rate": 0.08443313280888479,
      "loss": 0.0118,
      "step": 16820
    },
    {
      "epoch": 7.788061082832022,
      "grad_norm": 0.010005512274801731,
      "learning_rate": 0.08442387783433597,
      "loss": 0.0131,
      "step": 16830
    },
    {
      "epoch": 7.792688570106432,
      "grad_norm": 1.6395010948181152,
      "learning_rate": 0.08441462285978714,
      "loss": 0.0069,
      "step": 16840
    },
    {
      "epoch": 7.797316057380842,
      "grad_norm": 1.0884274244308472,
      "learning_rate": 0.08440536788523832,
      "loss": 0.0032,
      "step": 16850
    },
    {
      "epoch": 7.801943544655252,
      "grad_norm": 0.004634227138012648,
      "learning_rate": 0.08439611291068949,
      "loss": 0.0002,
      "step": 16860
    },
    {
      "epoch": 7.806571031929662,
      "grad_norm": 0.0025913857389241457,
      "learning_rate": 0.08438685793614069,
      "loss": 0.0018,
      "step": 16870
    },
    {
      "epoch": 7.811198519204072,
      "grad_norm": 0.002878504805266857,
      "learning_rate": 0.08437760296159186,
      "loss": 0.0005,
      "step": 16880
    },
    {
      "epoch": 7.815826006478482,
      "grad_norm": 4.672496795654297,
      "learning_rate": 0.08436834798704304,
      "loss": 0.0221,
      "step": 16890
    },
    {
      "epoch": 7.8204534937528924,
      "grad_norm": 1.1355175971984863,
      "learning_rate": 0.08435909301249422,
      "loss": 0.0046,
      "step": 16900
    },
    {
      "epoch": 7.825080981027302,
      "grad_norm": 0.010433297604322433,
      "learning_rate": 0.0843498380379454,
      "loss": 0.0029,
      "step": 16910
    },
    {
      "epoch": 7.829708468301712,
      "grad_norm": 1.5418983697891235,
      "learning_rate": 0.08434058306339659,
      "loss": 0.0252,
      "step": 16920
    },
    {
      "epoch": 7.8343359555761225,
      "grad_norm": 0.004244658164680004,
      "learning_rate": 0.08433132808884776,
      "loss": 0.0012,
      "step": 16930
    },
    {
      "epoch": 7.838963442850532,
      "grad_norm": 0.6943039894104004,
      "learning_rate": 0.08432207311429894,
      "loss": 0.0008,
      "step": 16940
    },
    {
      "epoch": 7.843590930124942,
      "grad_norm": 0.11280135810375214,
      "learning_rate": 0.08431281813975011,
      "loss": 0.0005,
      "step": 16950
    },
    {
      "epoch": 7.848218417399352,
      "grad_norm": 0.5868281126022339,
      "learning_rate": 0.08430356316520131,
      "loss": 0.0012,
      "step": 16960
    },
    {
      "epoch": 7.852845904673762,
      "grad_norm": 0.002184077398851514,
      "learning_rate": 0.08429430819065248,
      "loss": 0.0011,
      "step": 16970
    },
    {
      "epoch": 7.857473391948172,
      "grad_norm": 0.03395078331232071,
      "learning_rate": 0.08428505321610366,
      "loss": 0.0075,
      "step": 16980
    },
    {
      "epoch": 7.862100879222583,
      "grad_norm": 0.0053215110674500465,
      "learning_rate": 0.08427579824155484,
      "loss": 0.0016,
      "step": 16990
    },
    {
      "epoch": 7.866728366496992,
      "grad_norm": 0.012906277552247047,
      "learning_rate": 0.08426654326700601,
      "loss": 0.0017,
      "step": 17000
    },
    {
      "epoch": 7.871355853771402,
      "grad_norm": 0.7167133688926697,
      "learning_rate": 0.08425728829245721,
      "loss": 0.0005,
      "step": 17010
    },
    {
      "epoch": 7.875983341045812,
      "grad_norm": 0.01473214291036129,
      "learning_rate": 0.08424803331790838,
      "loss": 0.0049,
      "step": 17020
    },
    {
      "epoch": 7.880610828320222,
      "grad_norm": 0.014318075962364674,
      "learning_rate": 0.08423877834335956,
      "loss": 0.0013,
      "step": 17030
    },
    {
      "epoch": 7.885238315594632,
      "grad_norm": 0.14192475378513336,
      "learning_rate": 0.08422952336881073,
      "loss": 0.0008,
      "step": 17040
    },
    {
      "epoch": 7.889865802869042,
      "grad_norm": 0.24727410078048706,
      "learning_rate": 0.08422026839426193,
      "loss": 0.0035,
      "step": 17050
    },
    {
      "epoch": 7.894493290143452,
      "grad_norm": 0.013416620902717113,
      "learning_rate": 0.0842110134197131,
      "loss": 0.0016,
      "step": 17060
    },
    {
      "epoch": 7.8991207774178624,
      "grad_norm": 0.009204809553921223,
      "learning_rate": 0.08420175844516428,
      "loss": 0.0044,
      "step": 17070
    },
    {
      "epoch": 7.903748264692272,
      "grad_norm": 3.6309776306152344,
      "learning_rate": 0.08419250347061547,
      "loss": 0.0025,
      "step": 17080
    },
    {
      "epoch": 7.908375751966682,
      "grad_norm": 2.729480504989624,
      "learning_rate": 0.08418324849606663,
      "loss": 0.0006,
      "step": 17090
    },
    {
      "epoch": 7.9130032392410925,
      "grad_norm": 0.016188036650419235,
      "learning_rate": 0.08417399352151783,
      "loss": 0.0057,
      "step": 17100
    },
    {
      "epoch": 7.917630726515502,
      "grad_norm": 0.009204888716340065,
      "learning_rate": 0.084164738546969,
      "loss": 0.0035,
      "step": 17110
    },
    {
      "epoch": 7.922258213789912,
      "grad_norm": 0.11360727995634079,
      "learning_rate": 0.08415548357242018,
      "loss": 0.0076,
      "step": 17120
    },
    {
      "epoch": 7.926885701064322,
      "grad_norm": 0.013046988286077976,
      "learning_rate": 0.08414622859787135,
      "loss": 0.0002,
      "step": 17130
    },
    {
      "epoch": 7.931513188338732,
      "grad_norm": 0.6371522545814514,
      "learning_rate": 0.08413697362332254,
      "loss": 0.0009,
      "step": 17140
    },
    {
      "epoch": 7.936140675613142,
      "grad_norm": 0.004221196286380291,
      "learning_rate": 0.08412771864877372,
      "loss": 0.0011,
      "step": 17150
    },
    {
      "epoch": 7.940768162887552,
      "grad_norm": 0.9756187200546265,
      "learning_rate": 0.0841184636742249,
      "loss": 0.0013,
      "step": 17160
    },
    {
      "epoch": 7.945395650161962,
      "grad_norm": 0.023057883605360985,
      "learning_rate": 0.08410920869967609,
      "loss": 0.007,
      "step": 17170
    },
    {
      "epoch": 7.950023137436372,
      "grad_norm": 0.060014862567186356,
      "learning_rate": 0.08409995372512725,
      "loss": 0.0829,
      "step": 17180
    },
    {
      "epoch": 7.954650624710782,
      "grad_norm": 0.04657117277383804,
      "learning_rate": 0.08409069875057845,
      "loss": 0.0006,
      "step": 17190
    },
    {
      "epoch": 7.959278111985192,
      "grad_norm": 0.015411969274282455,
      "learning_rate": 0.08408144377602962,
      "loss": 0.0012,
      "step": 17200
    },
    {
      "epoch": 7.963905599259602,
      "grad_norm": 0.020417574793100357,
      "learning_rate": 0.0840721888014808,
      "loss": 0.0103,
      "step": 17210
    },
    {
      "epoch": 7.968533086534012,
      "grad_norm": 0.0023915315978229046,
      "learning_rate": 0.08406293382693197,
      "loss": 0.0079,
      "step": 17220
    },
    {
      "epoch": 7.973160573808422,
      "grad_norm": 0.045351289212703705,
      "learning_rate": 0.08405367885238316,
      "loss": 0.0025,
      "step": 17230
    },
    {
      "epoch": 7.977788061082832,
      "grad_norm": 1.7711727619171143,
      "learning_rate": 0.08404442387783434,
      "loss": 0.0025,
      "step": 17240
    },
    {
      "epoch": 7.982415548357242,
      "grad_norm": 0.007978714071214199,
      "learning_rate": 0.08403516890328552,
      "loss": 0.0133,
      "step": 17250
    },
    {
      "epoch": 7.987043035631652,
      "grad_norm": 0.056471798568964005,
      "learning_rate": 0.0840259139287367,
      "loss": 0.0047,
      "step": 17260
    },
    {
      "epoch": 7.9916705229060625,
      "grad_norm": 0.0953853651881218,
      "learning_rate": 0.08401665895418788,
      "loss": 0.0016,
      "step": 17270
    },
    {
      "epoch": 7.996298010180472,
      "grad_norm": 0.8686148524284363,
      "learning_rate": 0.08400740397963907,
      "loss": 0.0208,
      "step": 17280
    },
    {
      "epoch": 8.0,
      "eval_accuracy_branch1": 0.9819750423663535,
      "eval_accuracy_branch2": 0.4980742566630719,
      "eval_f1_branch1": 0.9826281574068347,
      "eval_f1_branch2": 0.49797774389500615,
      "eval_loss": 0.03868070989847183,
      "eval_precision_branch1": 0.9831442468477207,
      "eval_precision_branch2": 0.4980727746422914,
      "eval_recall_branch1": 0.9826612473439398,
      "eval_recall_branch2": 0.4980742566630719,
      "eval_runtime": 29.1838,
      "eval_samples_per_second": 444.836,
      "eval_steps_per_second": 55.613,
      "step": 17288
    },
    {
      "epoch": 8.000925497454881,
      "grad_norm": 0.05346282199025154,
      "learning_rate": 0.08399814900509024,
      "loss": 0.0067,
      "step": 17290
    },
    {
      "epoch": 8.005552984729292,
      "grad_norm": 0.6846897006034851,
      "learning_rate": 0.08398889403054142,
      "loss": 0.0017,
      "step": 17300
    },
    {
      "epoch": 8.010180472003702,
      "grad_norm": 0.18051612377166748,
      "learning_rate": 0.0839796390559926,
      "loss": 0.0131,
      "step": 17310
    },
    {
      "epoch": 8.014807959278112,
      "grad_norm": 0.038909927010536194,
      "learning_rate": 0.08397038408144378,
      "loss": 0.0024,
      "step": 17320
    },
    {
      "epoch": 8.019435446552523,
      "grad_norm": 0.30934131145477295,
      "learning_rate": 0.08396112910689496,
      "loss": 0.0028,
      "step": 17330
    },
    {
      "epoch": 8.024062933826931,
      "grad_norm": 0.02460440807044506,
      "learning_rate": 0.08395187413234614,
      "loss": 0.0004,
      "step": 17340
    },
    {
      "epoch": 8.028690421101341,
      "grad_norm": 0.5045796632766724,
      "learning_rate": 0.08394261915779733,
      "loss": 0.0007,
      "step": 17350
    },
    {
      "epoch": 8.033317908375752,
      "grad_norm": 0.09708400070667267,
      "learning_rate": 0.0839333641832485,
      "loss": 0.0002,
      "step": 17360
    },
    {
      "epoch": 8.037945395650162,
      "grad_norm": 0.03961939737200737,
      "learning_rate": 0.08392410920869968,
      "loss": 0.0028,
      "step": 17370
    },
    {
      "epoch": 8.042572882924572,
      "grad_norm": 0.05462352931499481,
      "learning_rate": 0.08391485423415086,
      "loss": 0.0006,
      "step": 17380
    },
    {
      "epoch": 8.047200370198983,
      "grad_norm": 0.005416444968432188,
      "learning_rate": 0.08390559925960205,
      "loss": 0.0003,
      "step": 17390
    },
    {
      "epoch": 8.051827857473391,
      "grad_norm": 0.03547990322113037,
      "learning_rate": 0.08389634428505321,
      "loss": 0.0022,
      "step": 17400
    },
    {
      "epoch": 8.056455344747802,
      "grad_norm": 0.0132749043405056,
      "learning_rate": 0.0838870893105044,
      "loss": 0.0054,
      "step": 17410
    },
    {
      "epoch": 8.061082832022212,
      "grad_norm": 0.005730575881898403,
      "learning_rate": 0.08387783433595558,
      "loss": 0.0001,
      "step": 17420
    },
    {
      "epoch": 8.065710319296622,
      "grad_norm": 0.022086868062615395,
      "learning_rate": 0.08386857936140676,
      "loss": 0.0008,
      "step": 17430
    },
    {
      "epoch": 8.070337806571033,
      "grad_norm": 0.23419694602489471,
      "learning_rate": 0.08385932438685795,
      "loss": 0.0011,
      "step": 17440
    },
    {
      "epoch": 8.074965293845441,
      "grad_norm": 0.2179063856601715,
      "learning_rate": 0.08385006941230912,
      "loss": 0.0034,
      "step": 17450
    },
    {
      "epoch": 8.079592781119851,
      "grad_norm": 0.007391185034066439,
      "learning_rate": 0.0838408144377603,
      "loss": 0.0011,
      "step": 17460
    },
    {
      "epoch": 8.084220268394262,
      "grad_norm": 0.007552179973572493,
      "learning_rate": 0.08383155946321148,
      "loss": 0.0002,
      "step": 17470
    },
    {
      "epoch": 8.088847755668672,
      "grad_norm": 0.015785565599799156,
      "learning_rate": 0.08382230448866267,
      "loss": 0.0039,
      "step": 17480
    },
    {
      "epoch": 8.093475242943082,
      "grad_norm": 0.007390834856778383,
      "learning_rate": 0.08381304951411384,
      "loss": 0.0094,
      "step": 17490
    },
    {
      "epoch": 8.098102730217493,
      "grad_norm": 0.08650918304920197,
      "learning_rate": 0.08380379453956502,
      "loss": 0.0018,
      "step": 17500
    },
    {
      "epoch": 8.102730217491901,
      "grad_norm": 0.006832892075181007,
      "learning_rate": 0.0837945395650162,
      "loss": 0.0095,
      "step": 17510
    },
    {
      "epoch": 8.107357704766311,
      "grad_norm": 0.43252548575401306,
      "learning_rate": 0.08378528459046738,
      "loss": 0.0076,
      "step": 17520
    },
    {
      "epoch": 8.111985192040722,
      "grad_norm": 0.007840468548238277,
      "learning_rate": 0.08377602961591857,
      "loss": 0.013,
      "step": 17530
    },
    {
      "epoch": 8.116612679315132,
      "grad_norm": 0.020174501463770866,
      "learning_rate": 0.08376677464136974,
      "loss": 0.0006,
      "step": 17540
    },
    {
      "epoch": 8.121240166589542,
      "grad_norm": 0.0011400508228689432,
      "learning_rate": 0.08375751966682092,
      "loss": 0.0015,
      "step": 17550
    },
    {
      "epoch": 8.125867653863953,
      "grad_norm": 2.6202266216278076,
      "learning_rate": 0.0837482646922721,
      "loss": 0.0032,
      "step": 17560
    },
    {
      "epoch": 8.130495141138361,
      "grad_norm": 0.01400886382907629,
      "learning_rate": 0.08373900971772329,
      "loss": 0.0027,
      "step": 17570
    },
    {
      "epoch": 8.135122628412772,
      "grad_norm": 0.09224983304738998,
      "learning_rate": 0.08372975474317446,
      "loss": 0.0049,
      "step": 17580
    },
    {
      "epoch": 8.139750115687182,
      "grad_norm": 0.004831543192267418,
      "learning_rate": 0.08372049976862564,
      "loss": 0.0048,
      "step": 17590
    },
    {
      "epoch": 8.144377602961592,
      "grad_norm": 0.01657578907907009,
      "learning_rate": 0.08371124479407682,
      "loss": 0.0005,
      "step": 17600
    },
    {
      "epoch": 8.149005090236003,
      "grad_norm": 0.1394452154636383,
      "learning_rate": 0.083701989819528,
      "loss": 0.0011,
      "step": 17610
    },
    {
      "epoch": 8.153632577510411,
      "grad_norm": 0.3329937160015106,
      "learning_rate": 0.08369273484497919,
      "loss": 0.0039,
      "step": 17620
    },
    {
      "epoch": 8.158260064784821,
      "grad_norm": 0.013374288566410542,
      "learning_rate": 0.08368347987043036,
      "loss": 0.0003,
      "step": 17630
    },
    {
      "epoch": 8.162887552059232,
      "grad_norm": 0.21873630583286285,
      "learning_rate": 0.08367422489588154,
      "loss": 0.0098,
      "step": 17640
    },
    {
      "epoch": 8.167515039333642,
      "grad_norm": 0.0874624252319336,
      "learning_rate": 0.08366496992133272,
      "loss": 0.0013,
      "step": 17650
    },
    {
      "epoch": 8.172142526608052,
      "grad_norm": 0.02392677776515484,
      "learning_rate": 0.08365571494678391,
      "loss": 0.0006,
      "step": 17660
    },
    {
      "epoch": 8.176770013882463,
      "grad_norm": 0.015239119529724121,
      "learning_rate": 0.08364645997223508,
      "loss": 0.0068,
      "step": 17670
    },
    {
      "epoch": 8.181397501156871,
      "grad_norm": 0.35009852051734924,
      "learning_rate": 0.08363720499768626,
      "loss": 0.0052,
      "step": 17680
    },
    {
      "epoch": 8.186024988431281,
      "grad_norm": 3.9501421451568604,
      "learning_rate": 0.08362795002313744,
      "loss": 0.0096,
      "step": 17690
    },
    {
      "epoch": 8.190652475705692,
      "grad_norm": 0.2226182371377945,
      "learning_rate": 0.08361869504858863,
      "loss": 0.0002,
      "step": 17700
    },
    {
      "epoch": 8.195279962980102,
      "grad_norm": 0.14022955298423767,
      "learning_rate": 0.08360944007403981,
      "loss": 0.0002,
      "step": 17710
    },
    {
      "epoch": 8.199907450254512,
      "grad_norm": 0.1265917718410492,
      "learning_rate": 0.08360018509949098,
      "loss": 0.0002,
      "step": 17720
    },
    {
      "epoch": 8.204534937528921,
      "grad_norm": 0.0028010322712361813,
      "learning_rate": 0.08359093012494216,
      "loss": 0.0019,
      "step": 17730
    },
    {
      "epoch": 8.209162424803331,
      "grad_norm": 0.7323324084281921,
      "learning_rate": 0.08358167515039334,
      "loss": 0.0004,
      "step": 17740
    },
    {
      "epoch": 8.213789912077742,
      "grad_norm": 0.25786787271499634,
      "learning_rate": 0.08357242017584453,
      "loss": 0.0008,
      "step": 17750
    },
    {
      "epoch": 8.218417399352152,
      "grad_norm": 1.9025520086288452,
      "learning_rate": 0.0835631652012957,
      "loss": 0.0023,
      "step": 17760
    },
    {
      "epoch": 8.223044886626562,
      "grad_norm": 0.06401601433753967,
      "learning_rate": 0.08355391022674688,
      "loss": 0.0001,
      "step": 17770
    },
    {
      "epoch": 8.227672373900973,
      "grad_norm": 0.04055513069033623,
      "learning_rate": 0.08354465525219806,
      "loss": 0.011,
      "step": 17780
    },
    {
      "epoch": 8.232299861175381,
      "grad_norm": 0.0014623795868828893,
      "learning_rate": 0.08353540027764925,
      "loss": 0.0011,
      "step": 17790
    },
    {
      "epoch": 8.236927348449791,
      "grad_norm": 0.08146198093891144,
      "learning_rate": 0.08352614530310043,
      "loss": 0.0009,
      "step": 17800
    },
    {
      "epoch": 8.241554835724202,
      "grad_norm": 0.0027750313747674227,
      "learning_rate": 0.0835168903285516,
      "loss": 0.0068,
      "step": 17810
    },
    {
      "epoch": 8.246182322998612,
      "grad_norm": 0.03716188296675682,
      "learning_rate": 0.08350763535400278,
      "loss": 0.0006,
      "step": 17820
    },
    {
      "epoch": 8.250809810273022,
      "grad_norm": 0.020451730117201805,
      "learning_rate": 0.08349838037945395,
      "loss": 0.0001,
      "step": 17830
    },
    {
      "epoch": 8.25543729754743,
      "grad_norm": 0.03539538010954857,
      "learning_rate": 0.08348912540490515,
      "loss": 0.0042,
      "step": 17840
    },
    {
      "epoch": 8.260064784821841,
      "grad_norm": 0.0647193118929863,
      "learning_rate": 0.08347987043035632,
      "loss": 0.0003,
      "step": 17850
    },
    {
      "epoch": 8.264692272096251,
      "grad_norm": 0.3035922646522522,
      "learning_rate": 0.0834706154558075,
      "loss": 0.0009,
      "step": 17860
    },
    {
      "epoch": 8.269319759370662,
      "grad_norm": 0.024671975523233414,
      "learning_rate": 0.08346136048125868,
      "loss": 0.0117,
      "step": 17870
    },
    {
      "epoch": 8.273947246645072,
      "grad_norm": 0.0011616608826443553,
      "learning_rate": 0.08345210550670987,
      "loss": 0.0022,
      "step": 17880
    },
    {
      "epoch": 8.278574733919482,
      "grad_norm": 0.04078285023570061,
      "learning_rate": 0.08344285053216105,
      "loss": 0.0031,
      "step": 17890
    },
    {
      "epoch": 8.283202221193891,
      "grad_norm": 0.017173590138554573,
      "learning_rate": 0.08343359555761222,
      "loss": 0.0024,
      "step": 17900
    },
    {
      "epoch": 8.287829708468301,
      "grad_norm": 0.005793723743408918,
      "learning_rate": 0.0834243405830634,
      "loss": 0.0022,
      "step": 17910
    },
    {
      "epoch": 8.292457195742712,
      "grad_norm": 3.0855226516723633,
      "learning_rate": 0.08341508560851457,
      "loss": 0.0103,
      "step": 17920
    },
    {
      "epoch": 8.297084683017122,
      "grad_norm": 0.09557405859231949,
      "learning_rate": 0.08340583063396577,
      "loss": 0.0017,
      "step": 17930
    },
    {
      "epoch": 8.301712170291532,
      "grad_norm": 0.1176900640130043,
      "learning_rate": 0.08339657565941694,
      "loss": 0.0004,
      "step": 17940
    },
    {
      "epoch": 8.306339657565943,
      "grad_norm": 0.2054518610239029,
      "learning_rate": 0.08338732068486812,
      "loss": 0.007,
      "step": 17950
    },
    {
      "epoch": 8.310967144840351,
      "grad_norm": 0.21320761740207672,
      "learning_rate": 0.0833780657103193,
      "loss": 0.0045,
      "step": 17960
    },
    {
      "epoch": 8.315594632114761,
      "grad_norm": 0.0038477564230561256,
      "learning_rate": 0.08336881073577049,
      "loss": 0.0005,
      "step": 17970
    },
    {
      "epoch": 8.320222119389172,
      "grad_norm": 0.019057270139455795,
      "learning_rate": 0.08335955576122167,
      "loss": 0.0006,
      "step": 17980
    },
    {
      "epoch": 8.324849606663582,
      "grad_norm": 0.024701260030269623,
      "learning_rate": 0.08335030078667284,
      "loss": 0.0008,
      "step": 17990
    },
    {
      "epoch": 8.329477093937992,
      "grad_norm": 0.2702063322067261,
      "learning_rate": 0.08334104581212402,
      "loss": 0.0077,
      "step": 18000
    },
    {
      "epoch": 8.3341045812124,
      "grad_norm": 0.021672643721103668,
      "learning_rate": 0.08333179083757519,
      "loss": 0.0123,
      "step": 18010
    },
    {
      "epoch": 8.338732068486811,
      "grad_norm": 0.02457916922867298,
      "learning_rate": 0.08332253586302639,
      "loss": 0.0022,
      "step": 18020
    },
    {
      "epoch": 8.343359555761221,
      "grad_norm": 0.15968585014343262,
      "learning_rate": 0.08331328088847756,
      "loss": 0.0027,
      "step": 18030
    },
    {
      "epoch": 8.347987043035632,
      "grad_norm": 2.201761245727539,
      "learning_rate": 0.08330402591392874,
      "loss": 0.0039,
      "step": 18040
    },
    {
      "epoch": 8.352614530310042,
      "grad_norm": 4.347860336303711,
      "learning_rate": 0.08329477093937993,
      "loss": 0.0171,
      "step": 18050
    },
    {
      "epoch": 8.357242017584452,
      "grad_norm": 0.07361533492803574,
      "learning_rate": 0.0832855159648311,
      "loss": 0.0013,
      "step": 18060
    },
    {
      "epoch": 8.361869504858861,
      "grad_norm": 3.416471004486084,
      "learning_rate": 0.08327626099028229,
      "loss": 0.0037,
      "step": 18070
    },
    {
      "epoch": 8.366496992133271,
      "grad_norm": 0.5416802167892456,
      "learning_rate": 0.08326700601573346,
      "loss": 0.0045,
      "step": 18080
    },
    {
      "epoch": 8.371124479407682,
      "grad_norm": 6.828433990478516,
      "learning_rate": 0.08325775104118464,
      "loss": 0.0085,
      "step": 18090
    },
    {
      "epoch": 8.375751966682092,
      "grad_norm": 0.3441332280635834,
      "learning_rate": 0.08324849606663581,
      "loss": 0.0086,
      "step": 18100
    },
    {
      "epoch": 8.380379453956502,
      "grad_norm": 3.395188093185425,
      "learning_rate": 0.08323924109208701,
      "loss": 0.0191,
      "step": 18110
    },
    {
      "epoch": 8.385006941230913,
      "grad_norm": 0.13119488954544067,
      "learning_rate": 0.08322998611753818,
      "loss": 0.0022,
      "step": 18120
    },
    {
      "epoch": 8.389634428505321,
      "grad_norm": 0.2996274530887604,
      "learning_rate": 0.08322073114298936,
      "loss": 0.0027,
      "step": 18130
    },
    {
      "epoch": 8.394261915779731,
      "grad_norm": 0.0066368430852890015,
      "learning_rate": 0.08321147616844055,
      "loss": 0.0033,
      "step": 18140
    },
    {
      "epoch": 8.398889403054142,
      "grad_norm": 0.0025741562712937593,
      "learning_rate": 0.08320222119389172,
      "loss": 0.0065,
      "step": 18150
    },
    {
      "epoch": 8.403516890328552,
      "grad_norm": 2.480731725692749,
      "learning_rate": 0.08319296621934291,
      "loss": 0.0021,
      "step": 18160
    },
    {
      "epoch": 8.408144377602962,
      "grad_norm": 0.00273851933889091,
      "learning_rate": 0.08318371124479408,
      "loss": 0.0041,
      "step": 18170
    },
    {
      "epoch": 8.41277186487737,
      "grad_norm": 1.3622642755508423,
      "learning_rate": 0.08317445627024526,
      "loss": 0.011,
      "step": 18180
    },
    {
      "epoch": 8.417399352151781,
      "grad_norm": 0.050039660185575485,
      "learning_rate": 0.08316520129569643,
      "loss": 0.0022,
      "step": 18190
    },
    {
      "epoch": 8.422026839426191,
      "grad_norm": 2.8814592361450195,
      "learning_rate": 0.08315594632114762,
      "loss": 0.0034,
      "step": 18200
    },
    {
      "epoch": 8.426654326700602,
      "grad_norm": 0.058240581303834915,
      "learning_rate": 0.0831466913465988,
      "loss": 0.0031,
      "step": 18210
    },
    {
      "epoch": 8.431281813975012,
      "grad_norm": 0.009870858862996101,
      "learning_rate": 0.08313743637204998,
      "loss": 0.0153,
      "step": 18220
    },
    {
      "epoch": 8.435909301249422,
      "grad_norm": 0.011170064099133015,
      "learning_rate": 0.08312818139750117,
      "loss": 0.001,
      "step": 18230
    },
    {
      "epoch": 8.440536788523831,
      "grad_norm": 0.027892833575606346,
      "learning_rate": 0.08311892642295234,
      "loss": 0.0041,
      "step": 18240
    },
    {
      "epoch": 8.445164275798241,
      "grad_norm": 0.17410790920257568,
      "learning_rate": 0.08310967144840353,
      "loss": 0.0022,
      "step": 18250
    },
    {
      "epoch": 8.449791763072652,
      "grad_norm": 0.1370145082473755,
      "learning_rate": 0.0831004164738547,
      "loss": 0.0043,
      "step": 18260
    },
    {
      "epoch": 8.454419250347062,
      "grad_norm": 1.317419171333313,
      "learning_rate": 0.08309116149930589,
      "loss": 0.0015,
      "step": 18270
    },
    {
      "epoch": 8.459046737621472,
      "grad_norm": 0.016333548352122307,
      "learning_rate": 0.08308190652475705,
      "loss": 0.0005,
      "step": 18280
    },
    {
      "epoch": 8.46367422489588,
      "grad_norm": 0.6774040460586548,
      "learning_rate": 0.08307265155020824,
      "loss": 0.0076,
      "step": 18290
    },
    {
      "epoch": 8.468301712170291,
      "grad_norm": 10.88205337524414,
      "learning_rate": 0.08306339657565942,
      "loss": 0.0062,
      "step": 18300
    },
    {
      "epoch": 8.472929199444701,
      "grad_norm": 0.01400909572839737,
      "learning_rate": 0.0830541416011106,
      "loss": 0.0073,
      "step": 18310
    },
    {
      "epoch": 8.477556686719112,
      "grad_norm": 0.0034660794772207737,
      "learning_rate": 0.08304488662656179,
      "loss": 0.0053,
      "step": 18320
    },
    {
      "epoch": 8.482184173993522,
      "grad_norm": 0.093760184943676,
      "learning_rate": 0.08303563165201296,
      "loss": 0.0045,
      "step": 18330
    },
    {
      "epoch": 8.486811661267932,
      "grad_norm": 0.017924562096595764,
      "learning_rate": 0.08302637667746415,
      "loss": 0.0054,
      "step": 18340
    },
    {
      "epoch": 8.49143914854234,
      "grad_norm": 2.286431312561035,
      "learning_rate": 0.08301712170291532,
      "loss": 0.0043,
      "step": 18350
    },
    {
      "epoch": 8.496066635816751,
      "grad_norm": 0.6380449533462524,
      "learning_rate": 0.0830078667283665,
      "loss": 0.0005,
      "step": 18360
    },
    {
      "epoch": 8.500694123091161,
      "grad_norm": 0.004642189480364323,
      "learning_rate": 0.08299861175381767,
      "loss": 0.0004,
      "step": 18370
    },
    {
      "epoch": 8.505321610365572,
      "grad_norm": 1.243062973022461,
      "learning_rate": 0.08298935677926886,
      "loss": 0.0017,
      "step": 18380
    },
    {
      "epoch": 8.509949097639982,
      "grad_norm": 0.23927341401576996,
      "learning_rate": 0.08298010180472004,
      "loss": 0.0023,
      "step": 18390
    },
    {
      "epoch": 8.51457658491439,
      "grad_norm": 0.004587081260979176,
      "learning_rate": 0.08297084683017122,
      "loss": 0.0038,
      "step": 18400
    },
    {
      "epoch": 8.519204072188801,
      "grad_norm": 0.019626205787062645,
      "learning_rate": 0.08296159185562241,
      "loss": 0.0045,
      "step": 18410
    },
    {
      "epoch": 8.523831559463211,
      "grad_norm": 0.03767254576086998,
      "learning_rate": 0.08295233688107358,
      "loss": 0.0047,
      "step": 18420
    },
    {
      "epoch": 8.528459046737622,
      "grad_norm": 0.23205462098121643,
      "learning_rate": 0.08294308190652476,
      "loss": 0.0022,
      "step": 18430
    },
    {
      "epoch": 8.533086534012032,
      "grad_norm": 0.009506681002676487,
      "learning_rate": 0.08293382693197594,
      "loss": 0.0003,
      "step": 18440
    },
    {
      "epoch": 8.537714021286442,
      "grad_norm": 0.00944399181753397,
      "learning_rate": 0.08292457195742713,
      "loss": 0.0015,
      "step": 18450
    },
    {
      "epoch": 8.54234150856085,
      "grad_norm": 0.029061926528811455,
      "learning_rate": 0.0829153169828783,
      "loss": 0.0034,
      "step": 18460
    },
    {
      "epoch": 8.546968995835261,
      "grad_norm": 0.009950606152415276,
      "learning_rate": 0.08290606200832948,
      "loss": 0.0004,
      "step": 18470
    },
    {
      "epoch": 8.551596483109671,
      "grad_norm": 0.050052568316459656,
      "learning_rate": 0.08289680703378066,
      "loss": 0.0077,
      "step": 18480
    },
    {
      "epoch": 8.556223970384082,
      "grad_norm": 0.030286569148302078,
      "learning_rate": 0.08288755205923184,
      "loss": 0.0051,
      "step": 18490
    },
    {
      "epoch": 8.560851457658492,
      "grad_norm": 0.0029292358085513115,
      "learning_rate": 0.08287829708468303,
      "loss": 0.0027,
      "step": 18500
    },
    {
      "epoch": 8.565478944932902,
      "grad_norm": 0.12350740283727646,
      "learning_rate": 0.0828690421101342,
      "loss": 0.0059,
      "step": 18510
    },
    {
      "epoch": 8.57010643220731,
      "grad_norm": 0.006509784143418074,
      "learning_rate": 0.08285978713558538,
      "loss": 0.0014,
      "step": 18520
    },
    {
      "epoch": 8.574733919481721,
      "grad_norm": 3.1604673862457275,
      "learning_rate": 0.08285053216103656,
      "loss": 0.0017,
      "step": 18530
    },
    {
      "epoch": 8.579361406756131,
      "grad_norm": 0.7653020620346069,
      "learning_rate": 0.08284127718648775,
      "loss": 0.002,
      "step": 18540
    },
    {
      "epoch": 8.583988894030542,
      "grad_norm": 0.7538771033287048,
      "learning_rate": 0.08283202221193892,
      "loss": 0.0029,
      "step": 18550
    },
    {
      "epoch": 8.588616381304952,
      "grad_norm": 0.0689101293683052,
      "learning_rate": 0.0828227672373901,
      "loss": 0.0055,
      "step": 18560
    },
    {
      "epoch": 8.59324386857936,
      "grad_norm": 0.0011824581306427717,
      "learning_rate": 0.08281351226284128,
      "loss": 0.0002,
      "step": 18570
    },
    {
      "epoch": 8.597871355853771,
      "grad_norm": 0.0026978745590895414,
      "learning_rate": 0.08280425728829247,
      "loss": 0.0113,
      "step": 18580
    },
    {
      "epoch": 8.602498843128181,
      "grad_norm": 14.5280179977417,
      "learning_rate": 0.08279500231374365,
      "loss": 0.0125,
      "step": 18590
    },
    {
      "epoch": 8.607126330402592,
      "grad_norm": 0.013075006194412708,
      "learning_rate": 0.08278574733919482,
      "loss": 0.0018,
      "step": 18600
    },
    {
      "epoch": 8.611753817677002,
      "grad_norm": 0.006025061011314392,
      "learning_rate": 0.082776492364646,
      "loss": 0.0029,
      "step": 18610
    },
    {
      "epoch": 8.616381304951412,
      "grad_norm": 0.05655771121382713,
      "learning_rate": 0.08276723739009718,
      "loss": 0.0087,
      "step": 18620
    },
    {
      "epoch": 8.62100879222582,
      "grad_norm": 0.9635332226753235,
      "learning_rate": 0.08275798241554837,
      "loss": 0.0142,
      "step": 18630
    },
    {
      "epoch": 8.625636279500231,
      "grad_norm": 1.5885177850723267,
      "learning_rate": 0.08274872744099954,
      "loss": 0.0004,
      "step": 18640
    },
    {
      "epoch": 8.630263766774641,
      "grad_norm": 0.03088592179119587,
      "learning_rate": 0.08273947246645072,
      "loss": 0.0004,
      "step": 18650
    },
    {
      "epoch": 8.634891254049052,
      "grad_norm": 0.10596951842308044,
      "learning_rate": 0.0827302174919019,
      "loss": 0.0279,
      "step": 18660
    },
    {
      "epoch": 8.639518741323462,
      "grad_norm": 0.0034712557680904865,
      "learning_rate": 0.08272096251735309,
      "loss": 0.0008,
      "step": 18670
    },
    {
      "epoch": 8.644146228597872,
      "grad_norm": 0.006855657324194908,
      "learning_rate": 0.08271170754280427,
      "loss": 0.0045,
      "step": 18680
    },
    {
      "epoch": 8.64877371587228,
      "grad_norm": 0.5261572599411011,
      "learning_rate": 0.08270245256825544,
      "loss": 0.001,
      "step": 18690
    },
    {
      "epoch": 8.653401203146691,
      "grad_norm": 0.038862839341163635,
      "learning_rate": 0.08269319759370662,
      "loss": 0.004,
      "step": 18700
    },
    {
      "epoch": 8.658028690421101,
      "grad_norm": 0.23889444768428802,
      "learning_rate": 0.0826839426191578,
      "loss": 0.0024,
      "step": 18710
    },
    {
      "epoch": 8.662656177695512,
      "grad_norm": 0.3931433856487274,
      "learning_rate": 0.08267468764460899,
      "loss": 0.0033,
      "step": 18720
    },
    {
      "epoch": 8.667283664969922,
      "grad_norm": 0.16823476552963257,
      "learning_rate": 0.08266543267006016,
      "loss": 0.0011,
      "step": 18730
    },
    {
      "epoch": 8.67191115224433,
      "grad_norm": 0.20056602358818054,
      "learning_rate": 0.08265617769551134,
      "loss": 0.0024,
      "step": 18740
    },
    {
      "epoch": 8.676538639518741,
      "grad_norm": 9.54015827178955,
      "learning_rate": 0.08264692272096252,
      "loss": 0.0184,
      "step": 18750
    },
    {
      "epoch": 8.681166126793151,
      "grad_norm": 8.835222244262695,
      "learning_rate": 0.0826376677464137,
      "loss": 0.0069,
      "step": 18760
    },
    {
      "epoch": 8.685793614067562,
      "grad_norm": 0.011882002465426922,
      "learning_rate": 0.08262841277186489,
      "loss": 0.0002,
      "step": 18770
    },
    {
      "epoch": 8.690421101341972,
      "grad_norm": 3.8280751705169678,
      "learning_rate": 0.08261915779731606,
      "loss": 0.002,
      "step": 18780
    },
    {
      "epoch": 8.69504858861638,
      "grad_norm": 0.16807211935520172,
      "learning_rate": 0.08260990282276724,
      "loss": 0.0003,
      "step": 18790
    },
    {
      "epoch": 8.69967607589079,
      "grad_norm": 0.031196575611829758,
      "learning_rate": 0.08260064784821843,
      "loss": 0.0025,
      "step": 18800
    },
    {
      "epoch": 8.704303563165201,
      "grad_norm": 0.08439802378416061,
      "learning_rate": 0.08259139287366961,
      "loss": 0.0053,
      "step": 18810
    },
    {
      "epoch": 8.708931050439611,
      "grad_norm": 0.0024892704095691442,
      "learning_rate": 0.08258213789912078,
      "loss": 0.0004,
      "step": 18820
    },
    {
      "epoch": 8.713558537714022,
      "grad_norm": 0.0038486183620989323,
      "learning_rate": 0.08257288292457196,
      "loss": 0.0086,
      "step": 18830
    },
    {
      "epoch": 8.718186024988432,
      "grad_norm": 10.89154052734375,
      "learning_rate": 0.08256362795002314,
      "loss": 0.0106,
      "step": 18840
    },
    {
      "epoch": 8.72281351226284,
      "grad_norm": 1.4233916997909546,
      "learning_rate": 0.08255437297547433,
      "loss": 0.0024,
      "step": 18850
    },
    {
      "epoch": 8.72744099953725,
      "grad_norm": 0.03257794678211212,
      "learning_rate": 0.08254511800092551,
      "loss": 0.0103,
      "step": 18860
    },
    {
      "epoch": 8.732068486811661,
      "grad_norm": 0.008784813806414604,
      "learning_rate": 0.08253586302637668,
      "loss": 0.0036,
      "step": 18870
    },
    {
      "epoch": 8.736695974086071,
      "grad_norm": 0.014821058139204979,
      "learning_rate": 0.08252660805182786,
      "loss": 0.0007,
      "step": 18880
    },
    {
      "epoch": 8.741323461360482,
      "grad_norm": 0.5577259659767151,
      "learning_rate": 0.08251735307727903,
      "loss": 0.0008,
      "step": 18890
    },
    {
      "epoch": 8.745950948634892,
      "grad_norm": 0.4258460998535156,
      "learning_rate": 0.08250809810273023,
      "loss": 0.0013,
      "step": 18900
    },
    {
      "epoch": 8.7505784359093,
      "grad_norm": 0.05390565097332001,
      "learning_rate": 0.0824988431281814,
      "loss": 0.0007,
      "step": 18910
    },
    {
      "epoch": 8.755205923183711,
      "grad_norm": 0.07300110906362534,
      "learning_rate": 0.08248958815363258,
      "loss": 0.0006,
      "step": 18920
    },
    {
      "epoch": 8.759833410458121,
      "grad_norm": 2.642045259475708,
      "learning_rate": 0.08248033317908376,
      "loss": 0.0011,
      "step": 18930
    },
    {
      "epoch": 8.764460897732532,
      "grad_norm": 0.0007800397579558194,
      "learning_rate": 0.08247107820453495,
      "loss": 0.0038,
      "step": 18940
    },
    {
      "epoch": 8.769088385006942,
      "grad_norm": 0.046822406351566315,
      "learning_rate": 0.08246182322998613,
      "loss": 0.0004,
      "step": 18950
    },
    {
      "epoch": 8.77371587228135,
      "grad_norm": 1.0053082704544067,
      "learning_rate": 0.0824525682554373,
      "loss": 0.0291,
      "step": 18960
    },
    {
      "epoch": 8.77834335955576,
      "grad_norm": 1.1302305459976196,
      "learning_rate": 0.08244331328088848,
      "loss": 0.0035,
      "step": 18970
    },
    {
      "epoch": 8.782970846830171,
      "grad_norm": 0.008684742264449596,
      "learning_rate": 0.08243405830633965,
      "loss": 0.0022,
      "step": 18980
    },
    {
      "epoch": 8.787598334104581,
      "grad_norm": 0.5885068774223328,
      "learning_rate": 0.08242480333179085,
      "loss": 0.0028,
      "step": 18990
    },
    {
      "epoch": 8.792225821378992,
      "grad_norm": 0.199768528342247,
      "learning_rate": 0.08241554835724202,
      "loss": 0.0092,
      "step": 19000
    },
    {
      "epoch": 8.796853308653402,
      "grad_norm": 0.4279334545135498,
      "learning_rate": 0.0824062933826932,
      "loss": 0.0022,
      "step": 19010
    },
    {
      "epoch": 8.80148079592781,
      "grad_norm": 2.197464942932129,
      "learning_rate": 0.08239703840814439,
      "loss": 0.0029,
      "step": 19020
    },
    {
      "epoch": 8.80610828320222,
      "grad_norm": 0.11093857884407043,
      "learning_rate": 0.08238778343359557,
      "loss": 0.0016,
      "step": 19030
    },
    {
      "epoch": 8.810735770476631,
      "grad_norm": 0.04667534679174423,
      "learning_rate": 0.08237852845904675,
      "loss": 0.0002,
      "step": 19040
    },
    {
      "epoch": 8.815363257751041,
      "grad_norm": 0.0021873977966606617,
      "learning_rate": 0.08236927348449792,
      "loss": 0.0077,
      "step": 19050
    },
    {
      "epoch": 8.819990745025452,
      "grad_norm": 0.005669937003403902,
      "learning_rate": 0.0823600185099491,
      "loss": 0.0016,
      "step": 19060
    },
    {
      "epoch": 8.824618232299862,
      "grad_norm": 0.03952070698142052,
      "learning_rate": 0.08235076353540027,
      "loss": 0.0058,
      "step": 19070
    },
    {
      "epoch": 8.82924571957427,
      "grad_norm": 0.003319469280540943,
      "learning_rate": 0.08234150856085147,
      "loss": 0.005,
      "step": 19080
    },
    {
      "epoch": 8.833873206848681,
      "grad_norm": 0.001084221643395722,
      "learning_rate": 0.08233225358630264,
      "loss": 0.0052,
      "step": 19090
    },
    {
      "epoch": 8.838500694123091,
      "grad_norm": 2.9017202854156494,
      "learning_rate": 0.08232299861175382,
      "loss": 0.0016,
      "step": 19100
    },
    {
      "epoch": 8.843128181397502,
      "grad_norm": 0.10172766447067261,
      "learning_rate": 0.082313743637205,
      "loss": 0.0006,
      "step": 19110
    },
    {
      "epoch": 8.847755668671912,
      "grad_norm": 0.016283249482512474,
      "learning_rate": 0.08230448866265618,
      "loss": 0.0048,
      "step": 19120
    },
    {
      "epoch": 8.85238315594632,
      "grad_norm": 0.03717831149697304,
      "learning_rate": 0.08229523368810737,
      "loss": 0.0063,
      "step": 19130
    },
    {
      "epoch": 8.85701064322073,
      "grad_norm": 0.005096043460071087,
      "learning_rate": 0.08228597871355854,
      "loss": 0.0024,
      "step": 19140
    },
    {
      "epoch": 8.861638130495141,
      "grad_norm": 0.1696271002292633,
      "learning_rate": 0.08227672373900972,
      "loss": 0.0011,
      "step": 19150
    },
    {
      "epoch": 8.866265617769551,
      "grad_norm": 0.6713436841964722,
      "learning_rate": 0.0822674687644609,
      "loss": 0.0052,
      "step": 19160
    },
    {
      "epoch": 8.870893105043962,
      "grad_norm": 0.8848990201950073,
      "learning_rate": 0.08225821378991209,
      "loss": 0.0034,
      "step": 19170
    },
    {
      "epoch": 8.87552059231837,
      "grad_norm": 0.13635171949863434,
      "learning_rate": 0.08224895881536326,
      "loss": 0.013,
      "step": 19180
    },
    {
      "epoch": 8.88014807959278,
      "grad_norm": 0.05462685227394104,
      "learning_rate": 0.08223970384081444,
      "loss": 0.0008,
      "step": 19190
    },
    {
      "epoch": 8.88477556686719,
      "grad_norm": 0.14327004551887512,
      "learning_rate": 0.08223044886626563,
      "loss": 0.0004,
      "step": 19200
    },
    {
      "epoch": 8.889403054141601,
      "grad_norm": 0.013470211997628212,
      "learning_rate": 0.0822211938917168,
      "loss": 0.0001,
      "step": 19210
    },
    {
      "epoch": 8.894030541416011,
      "grad_norm": 0.06132253631949425,
      "learning_rate": 0.08221193891716799,
      "loss": 0.0002,
      "step": 19220
    },
    {
      "epoch": 8.898658028690422,
      "grad_norm": 0.016796374693512917,
      "learning_rate": 0.08220268394261916,
      "loss": 0.0005,
      "step": 19230
    },
    {
      "epoch": 8.90328551596483,
      "grad_norm": 0.145019069314003,
      "learning_rate": 0.08219342896807035,
      "loss": 0.002,
      "step": 19240
    },
    {
      "epoch": 8.90791300323924,
      "grad_norm": 0.003383817384019494,
      "learning_rate": 0.08218417399352151,
      "loss": 0.0008,
      "step": 19250
    },
    {
      "epoch": 8.912540490513651,
      "grad_norm": 1.3251670598983765,
      "learning_rate": 0.08217491901897271,
      "loss": 0.0033,
      "step": 19260
    },
    {
      "epoch": 8.917167977788061,
      "grad_norm": 0.07579595595598221,
      "learning_rate": 0.08216566404442388,
      "loss": 0.01,
      "step": 19270
    },
    {
      "epoch": 8.921795465062472,
      "grad_norm": 0.31974244117736816,
      "learning_rate": 0.08215640906987506,
      "loss": 0.0006,
      "step": 19280
    },
    {
      "epoch": 8.926422952336882,
      "grad_norm": 0.13682930171489716,
      "learning_rate": 0.08214715409532625,
      "loss": 0.0038,
      "step": 19290
    },
    {
      "epoch": 8.93105043961129,
      "grad_norm": 0.0034074359573423862,
      "learning_rate": 0.08213789912077742,
      "loss": 0.0116,
      "step": 19300
    },
    {
      "epoch": 8.9356779268857,
      "grad_norm": 0.014812659472227097,
      "learning_rate": 0.08212864414622861,
      "loss": 0.0002,
      "step": 19310
    },
    {
      "epoch": 8.940305414160111,
      "grad_norm": 0.08779162913560867,
      "learning_rate": 0.08211938917167978,
      "loss": 0.0002,
      "step": 19320
    },
    {
      "epoch": 8.944932901434521,
      "grad_norm": 0.007994843646883965,
      "learning_rate": 0.08211013419713097,
      "loss": 0.0005,
      "step": 19330
    },
    {
      "epoch": 8.949560388708932,
      "grad_norm": 0.005389764439314604,
      "learning_rate": 0.08210087922258213,
      "loss": 0.0009,
      "step": 19340
    },
    {
      "epoch": 8.95418787598334,
      "grad_norm": 0.6923669576644897,
      "learning_rate": 0.08209162424803332,
      "loss": 0.0009,
      "step": 19350
    },
    {
      "epoch": 8.95881536325775,
      "grad_norm": 0.0038046452682465315,
      "learning_rate": 0.0820823692734845,
      "loss": 0.002,
      "step": 19360
    },
    {
      "epoch": 8.96344285053216,
      "grad_norm": 0.004380997736006975,
      "learning_rate": 0.08207311429893568,
      "loss": 0.0002,
      "step": 19370
    },
    {
      "epoch": 8.968070337806571,
      "grad_norm": 0.029162421822547913,
      "learning_rate": 0.08206385932438687,
      "loss": 0.001,
      "step": 19380
    },
    {
      "epoch": 8.972697825080981,
      "grad_norm": 0.0013459345791488886,
      "learning_rate": 0.08205460434983804,
      "loss": 0.0003,
      "step": 19390
    },
    {
      "epoch": 8.977325312355392,
      "grad_norm": 0.07981850951910019,
      "learning_rate": 0.08204534937528923,
      "loss": 0.0024,
      "step": 19400
    },
    {
      "epoch": 8.9819527996298,
      "grad_norm": 0.007649685721844435,
      "learning_rate": 0.0820360944007404,
      "loss": 0.0005,
      "step": 19410
    },
    {
      "epoch": 8.98658028690421,
      "grad_norm": 0.0040990146808326244,
      "learning_rate": 0.08202683942619159,
      "loss": 0.0003,
      "step": 19420
    },
    {
      "epoch": 8.991207774178621,
      "grad_norm": 0.18287979066371918,
      "learning_rate": 0.08201758445164276,
      "loss": 0.0042,
      "step": 19430
    },
    {
      "epoch": 8.995835261453031,
      "grad_norm": 3.7376413345336914,
      "learning_rate": 0.08200832947709394,
      "loss": 0.0029,
      "step": 19440
    },
    {
      "epoch": 9.0,
      "eval_accuracy_branch1": 0.9853643506393468,
      "eval_accuracy_branch2": 0.49922970266522876,
      "eval_f1_branch1": 0.9848989330426051,
      "eval_f1_branch2": 0.4989848349914965,
      "eval_loss": 0.03553098812699318,
      "eval_precision_branch1": 0.9857655046314889,
      "eval_precision_branch2": 0.4992281938056126,
      "eval_recall_branch1": 0.9843436685586283,
      "eval_recall_branch2": 0.4992297026652288,
      "eval_runtime": 29.0178,
      "eval_samples_per_second": 447.381,
      "eval_steps_per_second": 55.931,
      "step": 19449
    },
    {
      "epoch": 9.000462748727442,
      "grad_norm": 0.09616875648498535,
      "learning_rate": 0.08199907450254512,
      "loss": 0.0423,
      "step": 19450
    },
    {
      "epoch": 9.005090236001852,
      "grad_norm": 0.006069712806493044,
      "learning_rate": 0.0819898195279963,
      "loss": 0.0004,
      "step": 19460
    },
    {
      "epoch": 9.00971772327626,
      "grad_norm": 0.20800688862800598,
      "learning_rate": 0.08198056455344749,
      "loss": 0.0022,
      "step": 19470
    },
    {
      "epoch": 9.01434521055067,
      "grad_norm": 0.029056614264845848,
      "learning_rate": 0.08197130957889866,
      "loss": 0.0051,
      "step": 19480
    },
    {
      "epoch": 9.018972697825081,
      "grad_norm": 0.012046326883137226,
      "learning_rate": 0.08196205460434985,
      "loss": 0.0055,
      "step": 19490
    },
    {
      "epoch": 9.023600185099491,
      "grad_norm": 0.002330205636098981,
      "learning_rate": 0.08195279962980102,
      "loss": 0.0004,
      "step": 19500
    },
    {
      "epoch": 9.028227672373902,
      "grad_norm": 0.003054163884371519,
      "learning_rate": 0.0819435446552522,
      "loss": 0.0004,
      "step": 19510
    },
    {
      "epoch": 9.03285515964831,
      "grad_norm": 0.001453039119951427,
      "learning_rate": 0.08193428968070338,
      "loss": 0.0044,
      "step": 19520
    },
    {
      "epoch": 9.03748264692272,
      "grad_norm": 0.014564011245965958,
      "learning_rate": 0.08192503470615456,
      "loss": 0.0003,
      "step": 19530
    },
    {
      "epoch": 9.04211013419713,
      "grad_norm": 0.18700142204761505,
      "learning_rate": 0.08191577973160574,
      "loss": 0.0014,
      "step": 19540
    },
    {
      "epoch": 9.046737621471541,
      "grad_norm": 0.0015368611784651875,
      "learning_rate": 0.08190652475705693,
      "loss": 0.003,
      "step": 19550
    },
    {
      "epoch": 9.051365108745951,
      "grad_norm": 0.0193448755890131,
      "learning_rate": 0.08189726978250811,
      "loss": 0.0005,
      "step": 19560
    },
    {
      "epoch": 9.055992596020362,
      "grad_norm": 0.0017890522722154856,
      "learning_rate": 0.08188801480795928,
      "loss": 0.001,
      "step": 19570
    },
    {
      "epoch": 9.06062008329477,
      "grad_norm": 0.48773202300071716,
      "learning_rate": 0.08187875983341046,
      "loss": 0.0003,
      "step": 19580
    },
    {
      "epoch": 9.06524757056918,
      "grad_norm": 0.010069596581161022,
      "learning_rate": 0.08186950485886164,
      "loss": 0.0072,
      "step": 19590
    },
    {
      "epoch": 9.069875057843591,
      "grad_norm": 0.42679476737976074,
      "learning_rate": 0.08186024988431283,
      "loss": 0.0019,
      "step": 19600
    },
    {
      "epoch": 9.074502545118001,
      "grad_norm": 0.6074008941650391,
      "learning_rate": 0.081850994909764,
      "loss": 0.008,
      "step": 19610
    },
    {
      "epoch": 9.079130032392412,
      "grad_norm": 0.01647527515888214,
      "learning_rate": 0.08184173993521518,
      "loss": 0.0014,
      "step": 19620
    },
    {
      "epoch": 9.08375751966682,
      "grad_norm": 0.08421681821346283,
      "learning_rate": 0.08183248496066636,
      "loss": 0.0011,
      "step": 19630
    },
    {
      "epoch": 9.08838500694123,
      "grad_norm": 0.004674702417105436,
      "learning_rate": 0.08182322998611755,
      "loss": 0.0007,
      "step": 19640
    },
    {
      "epoch": 9.09301249421564,
      "grad_norm": 0.006923607550561428,
      "learning_rate": 0.08181397501156873,
      "loss": 0.001,
      "step": 19650
    },
    {
      "epoch": 9.097639981490051,
      "grad_norm": 0.028752915561199188,
      "learning_rate": 0.0818047200370199,
      "loss": 0.0108,
      "step": 19660
    },
    {
      "epoch": 9.102267468764461,
      "grad_norm": 0.011442359536886215,
      "learning_rate": 0.08179546506247108,
      "loss": 0.0016,
      "step": 19670
    },
    {
      "epoch": 9.106894956038872,
      "grad_norm": 0.00508849648758769,
      "learning_rate": 0.08178621008792226,
      "loss": 0.0014,
      "step": 19680
    },
    {
      "epoch": 9.11152244331328,
      "grad_norm": 0.028825026005506516,
      "learning_rate": 0.08177695511337345,
      "loss": 0.0001,
      "step": 19690
    },
    {
      "epoch": 9.11614993058769,
      "grad_norm": 0.016618402674794197,
      "learning_rate": 0.08176770013882462,
      "loss": 0.0032,
      "step": 19700
    },
    {
      "epoch": 9.1207774178621,
      "grad_norm": 0.06562256813049316,
      "learning_rate": 0.0817584451642758,
      "loss": 0.0019,
      "step": 19710
    },
    {
      "epoch": 9.125404905136511,
      "grad_norm": 0.11325477063655853,
      "learning_rate": 0.08174919018972698,
      "loss": 0.0017,
      "step": 19720
    },
    {
      "epoch": 9.130032392410921,
      "grad_norm": 0.030362872406840324,
      "learning_rate": 0.08173993521517817,
      "loss": 0.0003,
      "step": 19730
    },
    {
      "epoch": 9.13465987968533,
      "grad_norm": 0.01534687727689743,
      "learning_rate": 0.08173068024062935,
      "loss": 0.0002,
      "step": 19740
    },
    {
      "epoch": 9.13928736695974,
      "grad_norm": 0.03321117162704468,
      "learning_rate": 0.08172142526608052,
      "loss": 0.0004,
      "step": 19750
    },
    {
      "epoch": 9.14391485423415,
      "grad_norm": 0.07041098922491074,
      "learning_rate": 0.0817121702915317,
      "loss": 0.005,
      "step": 19760
    },
    {
      "epoch": 9.148542341508561,
      "grad_norm": 0.27123841643333435,
      "learning_rate": 0.08170291531698289,
      "loss": 0.0023,
      "step": 19770
    },
    {
      "epoch": 9.153169828782971,
      "grad_norm": 0.015709783881902695,
      "learning_rate": 0.08169366034243407,
      "loss": 0.0028,
      "step": 19780
    },
    {
      "epoch": 9.157797316057382,
      "grad_norm": 0.023368066176772118,
      "learning_rate": 0.08168440536788524,
      "loss": 0.0011,
      "step": 19790
    },
    {
      "epoch": 9.16242480333179,
      "grad_norm": 0.07880688458681107,
      "learning_rate": 0.08167515039333642,
      "loss": 0.0008,
      "step": 19800
    },
    {
      "epoch": 9.1670522906062,
      "grad_norm": 0.3895060420036316,
      "learning_rate": 0.0816658954187876,
      "loss": 0.0017,
      "step": 19810
    },
    {
      "epoch": 9.17167977788061,
      "grad_norm": 0.04650069400668144,
      "learning_rate": 0.08165664044423879,
      "loss": 0.0126,
      "step": 19820
    },
    {
      "epoch": 9.176307265155021,
      "grad_norm": 0.09027592837810516,
      "learning_rate": 0.08164738546968997,
      "loss": 0.0006,
      "step": 19830
    },
    {
      "epoch": 9.180934752429431,
      "grad_norm": 0.053218238055706024,
      "learning_rate": 0.08163813049514114,
      "loss": 0.0008,
      "step": 19840
    },
    {
      "epoch": 9.185562239703842,
      "grad_norm": 0.00439313892275095,
      "learning_rate": 0.08162887552059232,
      "loss": 0.001,
      "step": 19850
    },
    {
      "epoch": 9.19018972697825,
      "grad_norm": 0.6081904172897339,
      "learning_rate": 0.0816196205460435,
      "loss": 0.0006,
      "step": 19860
    },
    {
      "epoch": 9.19481721425266,
      "grad_norm": 0.019156884402036667,
      "learning_rate": 0.08161036557149469,
      "loss": 0.0019,
      "step": 19870
    },
    {
      "epoch": 9.19944470152707,
      "grad_norm": 1.891975998878479,
      "learning_rate": 0.08160111059694586,
      "loss": 0.0006,
      "step": 19880
    },
    {
      "epoch": 9.204072188801481,
      "grad_norm": 3.040182113647461,
      "learning_rate": 0.08159185562239704,
      "loss": 0.0022,
      "step": 19890
    },
    {
      "epoch": 9.208699676075891,
      "grad_norm": 1.7044638395309448,
      "learning_rate": 0.08158260064784822,
      "loss": 0.0009,
      "step": 19900
    },
    {
      "epoch": 9.2133271633503,
      "grad_norm": 0.05003730207681656,
      "learning_rate": 0.08157334567329941,
      "loss": 0.0006,
      "step": 19910
    },
    {
      "epoch": 9.21795465062471,
      "grad_norm": 0.0006854088278487325,
      "learning_rate": 0.08156409069875059,
      "loss": 0.0002,
      "step": 19920
    },
    {
      "epoch": 9.22258213789912,
      "grad_norm": 0.08408914506435394,
      "learning_rate": 0.08155483572420176,
      "loss": 0.0003,
      "step": 19930
    },
    {
      "epoch": 9.227209625173531,
      "grad_norm": 6.447606086730957,
      "learning_rate": 0.08154558074965294,
      "loss": 0.0028,
      "step": 19940
    },
    {
      "epoch": 9.231837112447941,
      "grad_norm": 0.006326613947749138,
      "learning_rate": 0.08153632577510413,
      "loss": 0.0005,
      "step": 19950
    },
    {
      "epoch": 9.236464599722352,
      "grad_norm": 0.0031780777499079704,
      "learning_rate": 0.08152707080055531,
      "loss": 0.0002,
      "step": 19960
    },
    {
      "epoch": 9.24109208699676,
      "grad_norm": 0.0495259054005146,
      "learning_rate": 0.08151781582600648,
      "loss": 0.0002,
      "step": 19970
    },
    {
      "epoch": 9.24571957427117,
      "grad_norm": 0.12839394807815552,
      "learning_rate": 0.08150856085145766,
      "loss": 0.001,
      "step": 19980
    },
    {
      "epoch": 9.25034706154558,
      "grad_norm": 0.10079488903284073,
      "learning_rate": 0.08149930587690885,
      "loss": 0.0005,
      "step": 19990
    },
    {
      "epoch": 9.254974548819991,
      "grad_norm": 0.028115680441260338,
      "learning_rate": 0.08149005090236003,
      "loss": 0.0049,
      "step": 20000
    },
    {
      "epoch": 9.259602036094401,
      "grad_norm": 0.0019958994816988707,
      "learning_rate": 0.08148079592781121,
      "loss": 0.0018,
      "step": 20010
    },
    {
      "epoch": 9.26422952336881,
      "grad_norm": 0.006876958068460226,
      "learning_rate": 0.08147154095326238,
      "loss": 0.0009,
      "step": 20020
    },
    {
      "epoch": 9.26885701064322,
      "grad_norm": 0.0031082644127309322,
      "learning_rate": 0.08146228597871356,
      "loss": 0.0005,
      "step": 20030
    },
    {
      "epoch": 9.27348449791763,
      "grad_norm": 1.1183080673217773,
      "learning_rate": 0.08145303100416473,
      "loss": 0.0091,
      "step": 20040
    },
    {
      "epoch": 9.27811198519204,
      "grad_norm": 0.4794125258922577,
      "learning_rate": 0.08144377602961593,
      "loss": 0.0035,
      "step": 20050
    },
    {
      "epoch": 9.282739472466451,
      "grad_norm": 0.029327698051929474,
      "learning_rate": 0.0814345210550671,
      "loss": 0.0002,
      "step": 20060
    },
    {
      "epoch": 9.287366959740861,
      "grad_norm": 0.06471839547157288,
      "learning_rate": 0.08142526608051828,
      "loss": 0.0003,
      "step": 20070
    },
    {
      "epoch": 9.29199444701527,
      "grad_norm": 0.019364183768630028,
      "learning_rate": 0.08141601110596947,
      "loss": 0.0009,
      "step": 20080
    },
    {
      "epoch": 9.29662193428968,
      "grad_norm": 0.05427541956305504,
      "learning_rate": 0.08140675613142065,
      "loss": 0.0015,
      "step": 20090
    },
    {
      "epoch": 9.30124942156409,
      "grad_norm": 0.000942313636187464,
      "learning_rate": 0.08139750115687183,
      "loss": 0.003,
      "step": 20100
    },
    {
      "epoch": 9.305876908838501,
      "grad_norm": 0.2586749494075775,
      "learning_rate": 0.081388246182323,
      "loss": 0.0001,
      "step": 20110
    },
    {
      "epoch": 9.310504396112911,
      "grad_norm": 0.7143619060516357,
      "learning_rate": 0.08137899120777418,
      "loss": 0.0003,
      "step": 20120
    },
    {
      "epoch": 9.31513188338732,
      "grad_norm": 12.954694747924805,
      "learning_rate": 0.08136973623322535,
      "loss": 0.004,
      "step": 20130
    },
    {
      "epoch": 9.31975937066173,
      "grad_norm": 0.1487238109111786,
      "learning_rate": 0.08136048125867655,
      "loss": 0.0008,
      "step": 20140
    },
    {
      "epoch": 9.32438685793614,
      "grad_norm": 0.006353516597300768,
      "learning_rate": 0.08135122628412772,
      "loss": 0.001,
      "step": 20150
    },
    {
      "epoch": 9.32901434521055,
      "grad_norm": 0.06548784673213959,
      "learning_rate": 0.0813419713095789,
      "loss": 0.0064,
      "step": 20160
    },
    {
      "epoch": 9.333641832484961,
      "grad_norm": 4.771080493927002,
      "learning_rate": 0.08133271633503009,
      "loss": 0.0027,
      "step": 20170
    },
    {
      "epoch": 9.338269319759371,
      "grad_norm": 0.11668693274259567,
      "learning_rate": 0.08132346136048127,
      "loss": 0.0015,
      "step": 20180
    },
    {
      "epoch": 9.34289680703378,
      "grad_norm": 3.621521234512329,
      "learning_rate": 0.08131420638593245,
      "loss": 0.0013,
      "step": 20190
    },
    {
      "epoch": 9.34752429430819,
      "grad_norm": 0.04602691903710365,
      "learning_rate": 0.08130495141138362,
      "loss": 0.0001,
      "step": 20200
    },
    {
      "epoch": 9.3521517815826,
      "grad_norm": 0.0341297909617424,
      "learning_rate": 0.0812956964368348,
      "loss": 0.0001,
      "step": 20210
    },
    {
      "epoch": 9.35677926885701,
      "grad_norm": 0.040640782564878464,
      "learning_rate": 0.08128644146228597,
      "loss": 0.0027,
      "step": 20220
    },
    {
      "epoch": 9.361406756131421,
      "grad_norm": 8.740728662814945e-05,
      "learning_rate": 0.08127718648773717,
      "loss": 0.0034,
      "step": 20230
    },
    {
      "epoch": 9.366034243405831,
      "grad_norm": 0.03606230020523071,
      "learning_rate": 0.08126793151318834,
      "loss": 0.0007,
      "step": 20240
    },
    {
      "epoch": 9.37066173068024,
      "grad_norm": 0.8852952718734741,
      "learning_rate": 0.08125867653863952,
      "loss": 0.0006,
      "step": 20250
    },
    {
      "epoch": 9.37528921795465,
      "grad_norm": 0.4319747984409332,
      "learning_rate": 0.0812494215640907,
      "loss": 0.0007,
      "step": 20260
    },
    {
      "epoch": 9.37991670522906,
      "grad_norm": 0.006034043151885271,
      "learning_rate": 0.08124016658954188,
      "loss": 0.0001,
      "step": 20270
    },
    {
      "epoch": 9.384544192503471,
      "grad_norm": 0.6606208086013794,
      "learning_rate": 0.08123091161499307,
      "loss": 0.0042,
      "step": 20280
    },
    {
      "epoch": 9.389171679777881,
      "grad_norm": 0.015159642323851585,
      "learning_rate": 0.08122165664044424,
      "loss": 0.0058,
      "step": 20290
    },
    {
      "epoch": 9.39379916705229,
      "grad_norm": 2.0095152854919434,
      "learning_rate": 0.08121240166589543,
      "loss": 0.0013,
      "step": 20300
    },
    {
      "epoch": 9.3984266543267,
      "grad_norm": 0.001993149518966675,
      "learning_rate": 0.0812031466913466,
      "loss": 0.006,
      "step": 20310
    },
    {
      "epoch": 9.40305414160111,
      "grad_norm": 0.0007588294101879001,
      "learning_rate": 0.08119389171679779,
      "loss": 0.0004,
      "step": 20320
    },
    {
      "epoch": 9.40768162887552,
      "grad_norm": 0.004718748386949301,
      "learning_rate": 0.08118463674224896,
      "loss": 0.0022,
      "step": 20330
    },
    {
      "epoch": 9.412309116149931,
      "grad_norm": 0.019182132557034492,
      "learning_rate": 0.08117538176770014,
      "loss": 0.0088,
      "step": 20340
    },
    {
      "epoch": 9.416936603424341,
      "grad_norm": 0.10482165217399597,
      "learning_rate": 0.08116612679315133,
      "loss": 0.0006,
      "step": 20350
    },
    {
      "epoch": 9.42156409069875,
      "grad_norm": 0.014454337768256664,
      "learning_rate": 0.0811568718186025,
      "loss": 0.002,
      "step": 20360
    },
    {
      "epoch": 9.42619157797316,
      "grad_norm": 0.4755759835243225,
      "learning_rate": 0.0811476168440537,
      "loss": 0.0013,
      "step": 20370
    },
    {
      "epoch": 9.43081906524757,
      "grad_norm": 0.0027821173425763845,
      "learning_rate": 0.08113836186950486,
      "loss": 0.0006,
      "step": 20380
    },
    {
      "epoch": 9.43544655252198,
      "grad_norm": 0.019866157323122025,
      "learning_rate": 0.08112910689495605,
      "loss": 0.0018,
      "step": 20390
    },
    {
      "epoch": 9.440074039796391,
      "grad_norm": 0.0022477239836007357,
      "learning_rate": 0.08111985192040722,
      "loss": 0.0003,
      "step": 20400
    },
    {
      "epoch": 9.444701527070801,
      "grad_norm": 0.00866080354899168,
      "learning_rate": 0.08111059694585841,
      "loss": 0.0001,
      "step": 20410
    },
    {
      "epoch": 9.44932901434521,
      "grad_norm": 0.017791317775845528,
      "learning_rate": 0.08110134197130958,
      "loss": 0.0069,
      "step": 20420
    },
    {
      "epoch": 9.45395650161962,
      "grad_norm": 9.130044937133789,
      "learning_rate": 0.08109208699676076,
      "loss": 0.0148,
      "step": 20430
    },
    {
      "epoch": 9.45858398889403,
      "grad_norm": 2.2367279529571533,
      "learning_rate": 0.08108283202221195,
      "loss": 0.0005,
      "step": 20440
    },
    {
      "epoch": 9.463211476168441,
      "grad_norm": 0.19125264883041382,
      "learning_rate": 0.08107357704766312,
      "loss": 0.0003,
      "step": 20450
    },
    {
      "epoch": 9.467838963442851,
      "grad_norm": 0.0396219827234745,
      "learning_rate": 0.08106432207311431,
      "loss": 0.0002,
      "step": 20460
    },
    {
      "epoch": 9.47246645071726,
      "grad_norm": 0.010875625535845757,
      "learning_rate": 0.08105506709856548,
      "loss": 0.0003,
      "step": 20470
    },
    {
      "epoch": 9.47709393799167,
      "grad_norm": 0.2076364904642105,
      "learning_rate": 0.08104581212401667,
      "loss": 0.0004,
      "step": 20480
    },
    {
      "epoch": 9.48172142526608,
      "grad_norm": 0.0009783796267583966,
      "learning_rate": 0.08103655714946784,
      "loss": 0.0017,
      "step": 20490
    },
    {
      "epoch": 9.48634891254049,
      "grad_norm": 0.032113779336214066,
      "learning_rate": 0.08102730217491902,
      "loss": 0.0026,
      "step": 20500
    },
    {
      "epoch": 9.490976399814901,
      "grad_norm": 0.18424199521541595,
      "learning_rate": 0.0810180472003702,
      "loss": 0.0012,
      "step": 20510
    },
    {
      "epoch": 9.495603887089311,
      "grad_norm": 0.10712137818336487,
      "learning_rate": 0.08100879222582139,
      "loss": 0.0005,
      "step": 20520
    },
    {
      "epoch": 9.50023137436372,
      "grad_norm": 0.3179902732372284,
      "learning_rate": 0.08099953725127257,
      "loss": 0.0029,
      "step": 20530
    },
    {
      "epoch": 9.50485886163813,
      "grad_norm": 0.03266333043575287,
      "learning_rate": 0.08099028227672374,
      "loss": 0.0109,
      "step": 20540
    },
    {
      "epoch": 9.50948634891254,
      "grad_norm": 0.0847034603357315,
      "learning_rate": 0.08098102730217493,
      "loss": 0.0008,
      "step": 20550
    },
    {
      "epoch": 9.51411383618695,
      "grad_norm": 0.0009537790901958942,
      "learning_rate": 0.0809717723276261,
      "loss": 0.0036,
      "step": 20560
    },
    {
      "epoch": 9.518741323461361,
      "grad_norm": 0.047850921750068665,
      "learning_rate": 0.08096251735307729,
      "loss": 0.0086,
      "step": 20570
    },
    {
      "epoch": 9.52336881073577,
      "grad_norm": 0.008694103918969631,
      "learning_rate": 0.08095326237852846,
      "loss": 0.0052,
      "step": 20580
    },
    {
      "epoch": 9.52799629801018,
      "grad_norm": 5.143781661987305,
      "learning_rate": 0.08094400740397964,
      "loss": 0.0027,
      "step": 20590
    },
    {
      "epoch": 9.53262378528459,
      "grad_norm": 0.01897713541984558,
      "learning_rate": 0.08093475242943082,
      "loss": 0.0033,
      "step": 20600
    },
    {
      "epoch": 9.537251272559,
      "grad_norm": 0.2811080813407898,
      "learning_rate": 0.080925497454882,
      "loss": 0.0024,
      "step": 20610
    },
    {
      "epoch": 9.541878759833411,
      "grad_norm": 0.07491640746593475,
      "learning_rate": 0.08091624248033319,
      "loss": 0.0074,
      "step": 20620
    },
    {
      "epoch": 9.546506247107821,
      "grad_norm": 0.011094938963651657,
      "learning_rate": 0.08090698750578436,
      "loss": 0.0002,
      "step": 20630
    },
    {
      "epoch": 9.55113373438223,
      "grad_norm": 0.0037744997534900904,
      "learning_rate": 0.08089773253123556,
      "loss": 0.0019,
      "step": 20640
    },
    {
      "epoch": 9.55576122165664,
      "grad_norm": 0.051442522555589676,
      "learning_rate": 0.08088847755668672,
      "loss": 0.0049,
      "step": 20650
    },
    {
      "epoch": 9.56038870893105,
      "grad_norm": 0.017087528482079506,
      "learning_rate": 0.08087922258213791,
      "loss": 0.0032,
      "step": 20660
    },
    {
      "epoch": 9.56501619620546,
      "grad_norm": 0.015012924559414387,
      "learning_rate": 0.08086996760758908,
      "loss": 0.0007,
      "step": 20670
    },
    {
      "epoch": 9.569643683479871,
      "grad_norm": 0.19889675080776215,
      "learning_rate": 0.08086071263304026,
      "loss": 0.004,
      "step": 20680
    },
    {
      "epoch": 9.57427117075428,
      "grad_norm": 0.0025912297423928976,
      "learning_rate": 0.08085145765849144,
      "loss": 0.0003,
      "step": 20690
    },
    {
      "epoch": 9.57889865802869,
      "grad_norm": 0.20451296865940094,
      "learning_rate": 0.08084220268394263,
      "loss": 0.0038,
      "step": 20700
    },
    {
      "epoch": 9.5835261453031,
      "grad_norm": 0.0032784463837742805,
      "learning_rate": 0.08083294770939381,
      "loss": 0.0007,
      "step": 20710
    },
    {
      "epoch": 9.58815363257751,
      "grad_norm": 0.004333465360105038,
      "learning_rate": 0.08082369273484498,
      "loss": 0.0004,
      "step": 20720
    },
    {
      "epoch": 9.59278111985192,
      "grad_norm": 0.01847347989678383,
      "learning_rate": 0.08081443776029616,
      "loss": 0.0005,
      "step": 20730
    },
    {
      "epoch": 9.597408607126331,
      "grad_norm": 3.9571373462677,
      "learning_rate": 0.08080518278574735,
      "loss": 0.002,
      "step": 20740
    },
    {
      "epoch": 9.60203609440074,
      "grad_norm": 0.00817243754863739,
      "learning_rate": 0.08079592781119853,
      "loss": 0.0007,
      "step": 20750
    },
    {
      "epoch": 9.60666358167515,
      "grad_norm": 0.11605294793844223,
      "learning_rate": 0.0807866728366497,
      "loss": 0.0012,
      "step": 20760
    },
    {
      "epoch": 9.61129106894956,
      "grad_norm": 0.09016462415456772,
      "learning_rate": 0.08077741786210088,
      "loss": 0.0059,
      "step": 20770
    },
    {
      "epoch": 9.61591855622397,
      "grad_norm": 0.027828898280858994,
      "learning_rate": 0.08076816288755206,
      "loss": 0.0004,
      "step": 20780
    },
    {
      "epoch": 9.620546043498381,
      "grad_norm": 0.010011762380599976,
      "learning_rate": 0.08075890791300325,
      "loss": 0.0019,
      "step": 20790
    },
    {
      "epoch": 9.625173530772791,
      "grad_norm": 1.3138036727905273,
      "learning_rate": 0.08074965293845443,
      "loss": 0.0007,
      "step": 20800
    },
    {
      "epoch": 9.6298010180472,
      "grad_norm": 1.0579049587249756,
      "learning_rate": 0.0807403979639056,
      "loss": 0.0016,
      "step": 20810
    },
    {
      "epoch": 9.63442850532161,
      "grad_norm": 0.2689979374408722,
      "learning_rate": 0.08073114298935678,
      "loss": 0.0003,
      "step": 20820
    },
    {
      "epoch": 9.63905599259602,
      "grad_norm": 0.09524838626384735,
      "learning_rate": 0.08072188801480797,
      "loss": 0.0005,
      "step": 20830
    },
    {
      "epoch": 9.64368347987043,
      "grad_norm": 1.7681392431259155,
      "learning_rate": 0.08071263304025915,
      "loss": 0.0024,
      "step": 20840
    },
    {
      "epoch": 9.648310967144841,
      "grad_norm": 0.1100466325879097,
      "learning_rate": 0.08070337806571032,
      "loss": 0.0008,
      "step": 20850
    },
    {
      "epoch": 9.65293845441925,
      "grad_norm": 0.0024674066808074713,
      "learning_rate": 0.0806941230911615,
      "loss": 0.0045,
      "step": 20860
    },
    {
      "epoch": 9.65756594169366,
      "grad_norm": 0.286462664604187,
      "learning_rate": 0.08068486811661268,
      "loss": 0.0008,
      "step": 20870
    },
    {
      "epoch": 9.66219342896807,
      "grad_norm": 0.11140140146017075,
      "learning_rate": 0.08067561314206387,
      "loss": 0.0028,
      "step": 20880
    },
    {
      "epoch": 9.66682091624248,
      "grad_norm": 0.03969059884548187,
      "learning_rate": 0.08066635816751505,
      "loss": 0.0004,
      "step": 20890
    },
    {
      "epoch": 9.67144840351689,
      "grad_norm": 0.004673117306083441,
      "learning_rate": 0.08065710319296622,
      "loss": 0.0048,
      "step": 20900
    },
    {
      "epoch": 9.676075890791301,
      "grad_norm": 0.28312864899635315,
      "learning_rate": 0.0806478482184174,
      "loss": 0.0065,
      "step": 20910
    },
    {
      "epoch": 9.68070337806571,
      "grad_norm": 0.00128802505787462,
      "learning_rate": 0.08063859324386859,
      "loss": 0.0014,
      "step": 20920
    },
    {
      "epoch": 9.68533086534012,
      "grad_norm": 0.15690878033638,
      "learning_rate": 0.08062933826931977,
      "loss": 0.0007,
      "step": 20930
    },
    {
      "epoch": 9.68995835261453,
      "grad_norm": 2.067344903945923,
      "learning_rate": 0.08062008329477094,
      "loss": 0.0013,
      "step": 20940
    },
    {
      "epoch": 9.69458583988894,
      "grad_norm": 0.0023480060044676065,
      "learning_rate": 0.08061082832022212,
      "loss": 0.0023,
      "step": 20950
    },
    {
      "epoch": 9.699213327163351,
      "grad_norm": 0.00115106999874115,
      "learning_rate": 0.0806015733456733,
      "loss": 0.0122,
      "step": 20960
    },
    {
      "epoch": 9.70384081443776,
      "grad_norm": 0.21378687024116516,
      "learning_rate": 0.08059231837112449,
      "loss": 0.0017,
      "step": 20970
    },
    {
      "epoch": 9.70846830171217,
      "grad_norm": 0.010492407716810703,
      "learning_rate": 0.08058306339657567,
      "loss": 0.0073,
      "step": 20980
    },
    {
      "epoch": 9.71309578898658,
      "grad_norm": 0.001734636607579887,
      "learning_rate": 0.08057380842202684,
      "loss": 0.0112,
      "step": 20990
    },
    {
      "epoch": 9.71772327626099,
      "grad_norm": 0.874639630317688,
      "learning_rate": 0.08056455344747802,
      "loss": 0.0008,
      "step": 21000
    },
    {
      "epoch": 9.7223507635354,
      "grad_norm": 0.0008648813818581402,
      "learning_rate": 0.08055529847292921,
      "loss": 0.0017,
      "step": 21010
    },
    {
      "epoch": 9.726978250809811,
      "grad_norm": 0.021650012582540512,
      "learning_rate": 0.08054604349838039,
      "loss": 0.0021,
      "step": 21020
    },
    {
      "epoch": 9.73160573808422,
      "grad_norm": 0.11165350675582886,
      "learning_rate": 0.08053678852383156,
      "loss": 0.0007,
      "step": 21030
    },
    {
      "epoch": 9.73623322535863,
      "grad_norm": 6.821748733520508,
      "learning_rate": 0.08052753354928274,
      "loss": 0.0048,
      "step": 21040
    },
    {
      "epoch": 9.74086071263304,
      "grad_norm": 0.14323613047599792,
      "learning_rate": 0.08051827857473393,
      "loss": 0.0015,
      "step": 21050
    },
    {
      "epoch": 9.74548819990745,
      "grad_norm": 0.012243803590536118,
      "learning_rate": 0.08050902360018511,
      "loss": 0.0004,
      "step": 21060
    },
    {
      "epoch": 9.75011568718186,
      "grad_norm": 1.6724085807800293,
      "learning_rate": 0.08049976862563629,
      "loss": 0.007,
      "step": 21070
    },
    {
      "epoch": 9.75474317445627,
      "grad_norm": 0.08350905030965805,
      "learning_rate": 0.08049051365108746,
      "loss": 0.0022,
      "step": 21080
    },
    {
      "epoch": 9.75937066173068,
      "grad_norm": 0.03233575448393822,
      "learning_rate": 0.08048125867653864,
      "loss": 0.0017,
      "step": 21090
    },
    {
      "epoch": 9.76399814900509,
      "grad_norm": 0.00955159030854702,
      "learning_rate": 0.08047200370198983,
      "loss": 0.0029,
      "step": 21100
    },
    {
      "epoch": 9.7686256362795,
      "grad_norm": 0.2892037034034729,
      "learning_rate": 0.08046274872744101,
      "loss": 0.0003,
      "step": 21110
    },
    {
      "epoch": 9.77325312355391,
      "grad_norm": 0.0692330002784729,
      "learning_rate": 0.08045349375289218,
      "loss": 0.0008,
      "step": 21120
    },
    {
      "epoch": 9.777880610828321,
      "grad_norm": 0.5672599673271179,
      "learning_rate": 0.08044423877834336,
      "loss": 0.0012,
      "step": 21130
    },
    {
      "epoch": 9.78250809810273,
      "grad_norm": 0.002314167795702815,
      "learning_rate": 0.08043498380379455,
      "loss": 0.0018,
      "step": 21140
    },
    {
      "epoch": 9.78713558537714,
      "grad_norm": 0.0031835269182920456,
      "learning_rate": 0.08042572882924573,
      "loss": 0.0001,
      "step": 21150
    },
    {
      "epoch": 9.79176307265155,
      "grad_norm": 0.027584338560700417,
      "learning_rate": 0.08041647385469691,
      "loss": 0.0022,
      "step": 21160
    },
    {
      "epoch": 9.79639055992596,
      "grad_norm": 0.005348224192857742,
      "learning_rate": 0.08040721888014808,
      "loss": 0.0002,
      "step": 21170
    },
    {
      "epoch": 9.80101804720037,
      "grad_norm": 0.03187555819749832,
      "learning_rate": 0.08039796390559927,
      "loss": 0.0019,
      "step": 21180
    },
    {
      "epoch": 9.805645534474781,
      "grad_norm": 0.017595505341887474,
      "learning_rate": 0.08038870893105043,
      "loss": 0.0004,
      "step": 21190
    },
    {
      "epoch": 9.81027302174919,
      "grad_norm": 0.02274467423558235,
      "learning_rate": 0.08037945395650163,
      "loss": 0.0003,
      "step": 21200
    },
    {
      "epoch": 9.8149005090236,
      "grad_norm": 0.038994014263153076,
      "learning_rate": 0.0803701989819528,
      "loss": 0.0002,
      "step": 21210
    },
    {
      "epoch": 9.81952799629801,
      "grad_norm": 0.024348212406039238,
      "learning_rate": 0.08036094400740398,
      "loss": 0.0001,
      "step": 21220
    },
    {
      "epoch": 9.82415548357242,
      "grad_norm": 0.010954674333333969,
      "learning_rate": 0.08035168903285517,
      "loss": 0.0014,
      "step": 21230
    },
    {
      "epoch": 9.82878297084683,
      "grad_norm": 0.006333983037620783,
      "learning_rate": 0.08034243405830635,
      "loss": 0.0013,
      "step": 21240
    },
    {
      "epoch": 9.83341045812124,
      "grad_norm": 0.008308117277920246,
      "learning_rate": 0.08033317908375753,
      "loss": 0.0002,
      "step": 21250
    },
    {
      "epoch": 9.83803794539565,
      "grad_norm": 0.3681240975856781,
      "learning_rate": 0.0803239241092087,
      "loss": 0.0012,
      "step": 21260
    },
    {
      "epoch": 9.84266543267006,
      "grad_norm": 0.0016510034911334515,
      "learning_rate": 0.08031466913465989,
      "loss": 0.0004,
      "step": 21270
    },
    {
      "epoch": 9.84729291994447,
      "grad_norm": 0.27015745639801025,
      "learning_rate": 0.08030541416011105,
      "loss": 0.0004,
      "step": 21280
    },
    {
      "epoch": 9.85192040721888,
      "grad_norm": 0.0006540408940054476,
      "learning_rate": 0.08029615918556225,
      "loss": 0.0007,
      "step": 21290
    },
    {
      "epoch": 9.856547894493291,
      "grad_norm": 0.001125602750107646,
      "learning_rate": 0.08028690421101342,
      "loss": 0.0002,
      "step": 21300
    },
    {
      "epoch": 9.8611753817677,
      "grad_norm": 0.0038324007764458656,
      "learning_rate": 0.0802776492364646,
      "loss": 0.003,
      "step": 21310
    },
    {
      "epoch": 9.86580286904211,
      "grad_norm": 0.207280695438385,
      "learning_rate": 0.08026839426191579,
      "loss": 0.0006,
      "step": 21320
    },
    {
      "epoch": 9.87043035631652,
      "grad_norm": 0.009676830843091011,
      "learning_rate": 0.08025913928736697,
      "loss": 0.0018,
      "step": 21330
    },
    {
      "epoch": 9.87505784359093,
      "grad_norm": 1.4312376976013184,
      "learning_rate": 0.08024988431281815,
      "loss": 0.0024,
      "step": 21340
    },
    {
      "epoch": 9.87968533086534,
      "grad_norm": 0.005766875576227903,
      "learning_rate": 0.08024062933826932,
      "loss": 0.003,
      "step": 21350
    },
    {
      "epoch": 9.884312818139751,
      "grad_norm": 0.23770096898078918,
      "learning_rate": 0.0802313743637205,
      "loss": 0.008,
      "step": 21360
    },
    {
      "epoch": 9.88894030541416,
      "grad_norm": 0.00705069350078702,
      "learning_rate": 0.08022211938917168,
      "loss": 0.0007,
      "step": 21370
    },
    {
      "epoch": 9.89356779268857,
      "grad_norm": 0.6869858503341675,
      "learning_rate": 0.08021286441462287,
      "loss": 0.0014,
      "step": 21380
    },
    {
      "epoch": 9.89819527996298,
      "grad_norm": 1.2287670373916626,
      "learning_rate": 0.08020360944007404,
      "loss": 0.0019,
      "step": 21390
    },
    {
      "epoch": 9.90282276723739,
      "grad_norm": 0.011706405319273472,
      "learning_rate": 0.08019435446552522,
      "loss": 0.0004,
      "step": 21400
    },
    {
      "epoch": 9.9074502545118,
      "grad_norm": 1.6658562421798706,
      "learning_rate": 0.08018509949097641,
      "loss": 0.0014,
      "step": 21410
    },
    {
      "epoch": 9.91207774178621,
      "grad_norm": 0.5268867611885071,
      "learning_rate": 0.08017584451642758,
      "loss": 0.0035,
      "step": 21420
    },
    {
      "epoch": 9.91670522906062,
      "grad_norm": 0.002456085290759802,
      "learning_rate": 0.08016658954187877,
      "loss": 0.0026,
      "step": 21430
    },
    {
      "epoch": 9.92133271633503,
      "grad_norm": 0.05834183096885681,
      "learning_rate": 0.08015733456732994,
      "loss": 0.0009,
      "step": 21440
    },
    {
      "epoch": 9.92596020360944,
      "grad_norm": 0.004720394033938646,
      "learning_rate": 0.08014807959278113,
      "loss": 0.0006,
      "step": 21450
    },
    {
      "epoch": 9.93058769088385,
      "grad_norm": 0.013343128375709057,
      "learning_rate": 0.0801388246182323,
      "loss": 0.0006,
      "step": 21460
    },
    {
      "epoch": 9.93521517815826,
      "grad_norm": 4.900824069976807,
      "learning_rate": 0.0801295696436835,
      "loss": 0.0017,
      "step": 21470
    },
    {
      "epoch": 9.93984266543267,
      "grad_norm": 0.08097276091575623,
      "learning_rate": 0.08012031466913466,
      "loss": 0.0032,
      "step": 21480
    },
    {
      "epoch": 9.94447015270708,
      "grad_norm": 0.016832496970891953,
      "learning_rate": 0.08011105969458585,
      "loss": 0.0003,
      "step": 21490
    },
    {
      "epoch": 9.94909763998149,
      "grad_norm": 0.40193578600883484,
      "learning_rate": 0.08010180472003703,
      "loss": 0.0011,
      "step": 21500
    },
    {
      "epoch": 9.9537251272559,
      "grad_norm": 1.0450998544692993,
      "learning_rate": 0.0800925497454882,
      "loss": 0.0091,
      "step": 21510
    },
    {
      "epoch": 9.95835261453031,
      "grad_norm": 0.0019372620154172182,
      "learning_rate": 0.0800832947709394,
      "loss": 0.0012,
      "step": 21520
    },
    {
      "epoch": 9.96298010180472,
      "grad_norm": 0.0700039193034172,
      "learning_rate": 0.08007403979639056,
      "loss": 0.0004,
      "step": 21530
    },
    {
      "epoch": 9.96760758907913,
      "grad_norm": 0.32837435603141785,
      "learning_rate": 0.08006478482184175,
      "loss": 0.0023,
      "step": 21540
    },
    {
      "epoch": 9.97223507635354,
      "grad_norm": 0.002181772608309984,
      "learning_rate": 0.08005552984729292,
      "loss": 0.0056,
      "step": 21550
    },
    {
      "epoch": 9.97686256362795,
      "grad_norm": 0.03780953958630562,
      "learning_rate": 0.08004627487274411,
      "loss": 0.0003,
      "step": 21560
    },
    {
      "epoch": 9.98149005090236,
      "grad_norm": 0.0037029378581792116,
      "learning_rate": 0.08003701989819528,
      "loss": 0.001,
      "step": 21570
    },
    {
      "epoch": 9.98611753817677,
      "grad_norm": 0.6225430369377136,
      "learning_rate": 0.08002776492364647,
      "loss": 0.0046,
      "step": 21580
    },
    {
      "epoch": 9.99074502545118,
      "grad_norm": 0.011370493099093437,
      "learning_rate": 0.08001850994909765,
      "loss": 0.0014,
      "step": 21590
    },
    {
      "epoch": 9.99537251272559,
      "grad_norm": 0.1884516328573227,
      "learning_rate": 0.08000925497454882,
      "loss": 0.0028,
      "step": 21600
    },
    {
      "epoch": 10.0,
      "grad_norm": 121.98411560058594,
      "learning_rate": 0.08000000000000002,
      "loss": 0.4449,
      "step": 21610
    },
    {
      "epoch": 10.0,
      "eval_accuracy_branch1": 0.9837467262363272,
      "eval_accuracy_branch2": 0.4929902942535819,
      "eval_f1_branch1": 0.9843690837850414,
      "eval_f1_branch2": 0.4669753828437828,
      "eval_loss": 0.030241137370467186,
      "eval_precision_branch1": 0.9840403460880068,
      "eval_precision_branch2": 0.4912898582169803,
      "eval_recall_branch1": 0.9848496397613555,
      "eval_recall_branch2": 0.4929902942535819,
      "eval_runtime": 28.7462,
      "eval_samples_per_second": 451.607,
      "eval_steps_per_second": 56.46,
      "step": 21610
    },
    {
      "epoch": 10.00462748727441,
      "grad_norm": 0.01335853710770607,
      "learning_rate": 0.07999074502545118,
      "loss": 0.0015,
      "step": 21620
    },
    {
      "epoch": 10.00925497454882,
      "grad_norm": 0.0030161654576659203,
      "learning_rate": 0.07998149005090237,
      "loss": 0.0002,
      "step": 21630
    },
    {
      "epoch": 10.01388246182323,
      "grad_norm": 0.027215467765927315,
      "learning_rate": 0.07997223507635354,
      "loss": 0.0003,
      "step": 21640
    },
    {
      "epoch": 10.01850994909764,
      "grad_norm": 0.18400371074676514,
      "learning_rate": 0.07996298010180472,
      "loss": 0.0025,
      "step": 21650
    },
    {
      "epoch": 10.02313743637205,
      "grad_norm": 0.006157465744763613,
      "learning_rate": 0.0799537251272559,
      "loss": 0.001,
      "step": 21660
    },
    {
      "epoch": 10.02776492364646,
      "grad_norm": 0.05466605722904205,
      "learning_rate": 0.07994447015270709,
      "loss": 0.0005,
      "step": 21670
    },
    {
      "epoch": 10.03239241092087,
      "grad_norm": 3.275588035583496,
      "learning_rate": 0.07993521517815827,
      "loss": 0.0042,
      "step": 21680
    },
    {
      "epoch": 10.03701989819528,
      "grad_norm": 0.0040382095612585545,
      "learning_rate": 0.07992596020360944,
      "loss": 0.0001,
      "step": 21690
    },
    {
      "epoch": 10.04164738546969,
      "grad_norm": 0.011419439688324928,
      "learning_rate": 0.07991670522906064,
      "loss": 0.0006,
      "step": 21700
    },
    {
      "epoch": 10.0462748727441,
      "grad_norm": 0.014596911147236824,
      "learning_rate": 0.0799074502545118,
      "loss": 0.0003,
      "step": 21710
    },
    {
      "epoch": 10.05090236001851,
      "grad_norm": 0.9976112842559814,
      "learning_rate": 0.07989819527996299,
      "loss": 0.0153,
      "step": 21720
    },
    {
      "epoch": 10.05552984729292,
      "grad_norm": 0.006734840571880341,
      "learning_rate": 0.07988894030541416,
      "loss": 0.0112,
      "step": 21730
    },
    {
      "epoch": 10.06015733456733,
      "grad_norm": 0.9508712291717529,
      "learning_rate": 0.07987968533086534,
      "loss": 0.0007,
      "step": 21740
    },
    {
      "epoch": 10.064784821841739,
      "grad_norm": 0.026190120726823807,
      "learning_rate": 0.07987043035631652,
      "loss": 0.0005,
      "step": 21750
    },
    {
      "epoch": 10.06941230911615,
      "grad_norm": 1.2343662977218628,
      "learning_rate": 0.07986117538176771,
      "loss": 0.0045,
      "step": 21760
    },
    {
      "epoch": 10.07403979639056,
      "grad_norm": 0.21333155035972595,
      "learning_rate": 0.07985192040721889,
      "loss": 0.0006,
      "step": 21770
    },
    {
      "epoch": 10.07866728366497,
      "grad_norm": 0.004014021251350641,
      "learning_rate": 0.07984266543267006,
      "loss": 0.0002,
      "step": 21780
    },
    {
      "epoch": 10.08329477093938,
      "grad_norm": 0.002372983144596219,
      "learning_rate": 0.07983341045812126,
      "loss": 0.0004,
      "step": 21790
    },
    {
      "epoch": 10.08792225821379,
      "grad_norm": 5.986867427825928,
      "learning_rate": 0.07982415548357243,
      "loss": 0.0038,
      "step": 21800
    },
    {
      "epoch": 10.0925497454882,
      "grad_norm": 0.12987424433231354,
      "learning_rate": 0.07981490050902361,
      "loss": 0.0002,
      "step": 21810
    },
    {
      "epoch": 10.09717723276261,
      "grad_norm": 0.03613550588488579,
      "learning_rate": 0.07980564553447478,
      "loss": 0.0003,
      "step": 21820
    },
    {
      "epoch": 10.10180472003702,
      "grad_norm": 0.0017817113548517227,
      "learning_rate": 0.07979639055992596,
      "loss": 0.0004,
      "step": 21830
    },
    {
      "epoch": 10.10643220731143,
      "grad_norm": 0.1725984811782837,
      "learning_rate": 0.07978713558537714,
      "loss": 0.0004,
      "step": 21840
    },
    {
      "epoch": 10.11105969458584,
      "grad_norm": 6.31382417678833,
      "learning_rate": 0.07977788061082833,
      "loss": 0.0053,
      "step": 21850
    },
    {
      "epoch": 10.11568718186025,
      "grad_norm": 0.009699560701847076,
      "learning_rate": 0.07976862563627951,
      "loss": 0.016,
      "step": 21860
    },
    {
      "epoch": 10.12031466913466,
      "grad_norm": 1.2453868389129639,
      "learning_rate": 0.07975937066173068,
      "loss": 0.0029,
      "step": 21870
    },
    {
      "epoch": 10.12494215640907,
      "grad_norm": 0.004966548178344965,
      "learning_rate": 0.07975011568718186,
      "loss": 0.0002,
      "step": 21880
    },
    {
      "epoch": 10.12956964368348,
      "grad_norm": 0.031046748161315918,
      "learning_rate": 0.07974086071263305,
      "loss": 0.0021,
      "step": 21890
    },
    {
      "epoch": 10.13419713095789,
      "grad_norm": 3.1418352127075195,
      "learning_rate": 0.07973160573808423,
      "loss": 0.0011,
      "step": 21900
    },
    {
      "epoch": 10.1388246182323,
      "grad_norm": 0.010680857114493847,
      "learning_rate": 0.0797223507635354,
      "loss": 0.0014,
      "step": 21910
    },
    {
      "epoch": 10.143452105506709,
      "grad_norm": 0.44299450516700745,
      "learning_rate": 0.07971309578898658,
      "loss": 0.0045,
      "step": 21920
    },
    {
      "epoch": 10.14807959278112,
      "grad_norm": 0.006921839900314808,
      "learning_rate": 0.07970384081443777,
      "loss": 0.0016,
      "step": 21930
    },
    {
      "epoch": 10.15270708005553,
      "grad_norm": 0.030943144112825394,
      "learning_rate": 0.07969458583988895,
      "loss": 0.0044,
      "step": 21940
    },
    {
      "epoch": 10.15733456732994,
      "grad_norm": 0.06315548717975616,
      "learning_rate": 0.07968533086534013,
      "loss": 0.0004,
      "step": 21950
    },
    {
      "epoch": 10.16196205460435,
      "grad_norm": 0.05808228254318237,
      "learning_rate": 0.0796760758907913,
      "loss": 0.0033,
      "step": 21960
    },
    {
      "epoch": 10.16658954187876,
      "grad_norm": 0.08873689919710159,
      "learning_rate": 0.07966682091624248,
      "loss": 0.0024,
      "step": 21970
    },
    {
      "epoch": 10.17121702915317,
      "grad_norm": 0.21106214821338654,
      "learning_rate": 0.07965756594169367,
      "loss": 0.0056,
      "step": 21980
    },
    {
      "epoch": 10.17584451642758,
      "grad_norm": 0.06032373383641243,
      "learning_rate": 0.07964831096714485,
      "loss": 0.0002,
      "step": 21990
    },
    {
      "epoch": 10.18047200370199,
      "grad_norm": 0.03846386820077896,
      "learning_rate": 0.07963905599259602,
      "loss": 0.0003,
      "step": 22000
    },
    {
      "epoch": 10.1850994909764,
      "grad_norm": 0.009766001254320145,
      "learning_rate": 0.0796298010180472,
      "loss": 0.004,
      "step": 22010
    },
    {
      "epoch": 10.18972697825081,
      "grad_norm": 0.00240911147557199,
      "learning_rate": 0.07962054604349839,
      "loss": 0.0017,
      "step": 22020
    },
    {
      "epoch": 10.194354465525219,
      "grad_norm": 0.0016291887732222676,
      "learning_rate": 0.07961129106894957,
      "loss": 0.0068,
      "step": 22030
    },
    {
      "epoch": 10.19898195279963,
      "grad_norm": 0.30761227011680603,
      "learning_rate": 0.07960203609440075,
      "loss": 0.0002,
      "step": 22040
    },
    {
      "epoch": 10.20360944007404,
      "grad_norm": 0.08614476770162582,
      "learning_rate": 0.07959278111985192,
      "loss": 0.005,
      "step": 22050
    },
    {
      "epoch": 10.20823692734845,
      "grad_norm": 0.01065568346530199,
      "learning_rate": 0.0795835261453031,
      "loss": 0.0001,
      "step": 22060
    },
    {
      "epoch": 10.21286441462286,
      "grad_norm": 0.0024306823033839464,
      "learning_rate": 0.07957427117075429,
      "loss": 0.0001,
      "step": 22070
    },
    {
      "epoch": 10.21749190189727,
      "grad_norm": 0.013379170559346676,
      "learning_rate": 0.07956501619620547,
      "loss": 0.0015,
      "step": 22080
    },
    {
      "epoch": 10.222119389171679,
      "grad_norm": 0.010156492702662945,
      "learning_rate": 0.07955576122165664,
      "loss": 0.0002,
      "step": 22090
    },
    {
      "epoch": 10.22674687644609,
      "grad_norm": 0.003353227162733674,
      "learning_rate": 0.07954650624710782,
      "loss": 0.023,
      "step": 22100
    },
    {
      "epoch": 10.2313743637205,
      "grad_norm": 0.17629186809062958,
      "learning_rate": 0.079537251272559,
      "loss": 0.001,
      "step": 22110
    },
    {
      "epoch": 10.23600185099491,
      "grad_norm": 0.40132325887680054,
      "learning_rate": 0.07952799629801019,
      "loss": 0.0003,
      "step": 22120
    },
    {
      "epoch": 10.24062933826932,
      "grad_norm": 0.012173724360764027,
      "learning_rate": 0.07951874132346137,
      "loss": 0.0016,
      "step": 22130
    },
    {
      "epoch": 10.24525682554373,
      "grad_norm": 0.4817168712615967,
      "learning_rate": 0.07950948634891254,
      "loss": 0.0004,
      "step": 22140
    },
    {
      "epoch": 10.24988431281814,
      "grad_norm": 0.028859181329607964,
      "learning_rate": 0.07950023137436373,
      "loss": 0.0061,
      "step": 22150
    },
    {
      "epoch": 10.25451180009255,
      "grad_norm": 0.2434406578540802,
      "learning_rate": 0.07949097639981491,
      "loss": 0.0003,
      "step": 22160
    },
    {
      "epoch": 10.25913928736696,
      "grad_norm": 0.032292239367961884,
      "learning_rate": 0.07948172142526609,
      "loss": 0.0011,
      "step": 22170
    },
    {
      "epoch": 10.26376677464137,
      "grad_norm": 0.003625033888965845,
      "learning_rate": 0.07947246645071726,
      "loss": 0.0043,
      "step": 22180
    },
    {
      "epoch": 10.26839426191578,
      "grad_norm": 1.9235517978668213,
      "learning_rate": 0.07946321147616844,
      "loss": 0.0011,
      "step": 22190
    },
    {
      "epoch": 10.273021749190189,
      "grad_norm": 0.34376493096351624,
      "learning_rate": 0.07945395650161963,
      "loss": 0.0022,
      "step": 22200
    },
    {
      "epoch": 10.2776492364646,
      "grad_norm": 0.05878986045718193,
      "learning_rate": 0.07944470152707081,
      "loss": 0.0002,
      "step": 22210
    },
    {
      "epoch": 10.28227672373901,
      "grad_norm": 0.038518283516168594,
      "learning_rate": 0.079435446552522,
      "loss": 0.0002,
      "step": 22220
    },
    {
      "epoch": 10.28690421101342,
      "grad_norm": 0.0026666359044611454,
      "learning_rate": 0.07942619157797316,
      "loss": 0.0129,
      "step": 22230
    },
    {
      "epoch": 10.29153169828783,
      "grad_norm": 0.007022749166935682,
      "learning_rate": 0.07941693660342435,
      "loss": 0.0037,
      "step": 22240
    },
    {
      "epoch": 10.29615918556224,
      "grad_norm": 0.6447603106498718,
      "learning_rate": 0.07940768162887553,
      "loss": 0.0018,
      "step": 22250
    },
    {
      "epoch": 10.300786672836649,
      "grad_norm": 0.0016778538702055812,
      "learning_rate": 0.07939842665432671,
      "loss": 0.0011,
      "step": 22260
    },
    {
      "epoch": 10.30541416011106,
      "grad_norm": 0.00298993568867445,
      "learning_rate": 0.07938917167977788,
      "loss": 0.0009,
      "step": 22270
    },
    {
      "epoch": 10.31004164738547,
      "grad_norm": 0.0032284220214933157,
      "learning_rate": 0.07937991670522906,
      "loss": 0.0016,
      "step": 22280
    },
    {
      "epoch": 10.31466913465988,
      "grad_norm": 0.007983014918863773,
      "learning_rate": 0.07937066173068025,
      "loss": 0.0063,
      "step": 22290
    },
    {
      "epoch": 10.31929662193429,
      "grad_norm": 0.00450190668925643,
      "learning_rate": 0.07936140675613143,
      "loss": 0.0029,
      "step": 22300
    },
    {
      "epoch": 10.323924109208699,
      "grad_norm": 0.23839156329631805,
      "learning_rate": 0.07935215178158261,
      "loss": 0.0007,
      "step": 22310
    },
    {
      "epoch": 10.32855159648311,
      "grad_norm": 0.044505514204502106,
      "learning_rate": 0.07934289680703378,
      "loss": 0.0001,
      "step": 22320
    },
    {
      "epoch": 10.33317908375752,
      "grad_norm": 0.005099327303469181,
      "learning_rate": 0.07933364183248497,
      "loss": 0.0008,
      "step": 22330
    },
    {
      "epoch": 10.33780657103193,
      "grad_norm": 0.022550126537680626,
      "learning_rate": 0.07932438685793614,
      "loss": 0.0033,
      "step": 22340
    },
    {
      "epoch": 10.34243405830634,
      "grad_norm": 0.002479447051882744,
      "learning_rate": 0.07931513188338733,
      "loss": 0.0002,
      "step": 22350
    },
    {
      "epoch": 10.34706154558075,
      "grad_norm": 0.9078967571258545,
      "learning_rate": 0.0793058769088385,
      "loss": 0.0027,
      "step": 22360
    },
    {
      "epoch": 10.351689032855159,
      "grad_norm": 11.592653274536133,
      "learning_rate": 0.07929662193428968,
      "loss": 0.0038,
      "step": 22370
    },
    {
      "epoch": 10.35631652012957,
      "grad_norm": 0.14824390411376953,
      "learning_rate": 0.07928736695974087,
      "loss": 0.0018,
      "step": 22380
    },
    {
      "epoch": 10.36094400740398,
      "grad_norm": 0.3877401053905487,
      "learning_rate": 0.07927811198519205,
      "loss": 0.0069,
      "step": 22390
    },
    {
      "epoch": 10.36557149467839,
      "grad_norm": 0.004780826158821583,
      "learning_rate": 0.07926885701064323,
      "loss": 0.0004,
      "step": 22400
    },
    {
      "epoch": 10.3701989819528,
      "grad_norm": 0.062295712530612946,
      "learning_rate": 0.0792596020360944,
      "loss": 0.003,
      "step": 22410
    },
    {
      "epoch": 10.374826469227209,
      "grad_norm": 0.05389823019504547,
      "learning_rate": 0.07925034706154559,
      "loss": 0.0003,
      "step": 22420
    },
    {
      "epoch": 10.379453956501619,
      "grad_norm": 0.0895492434501648,
      "learning_rate": 0.07924109208699676,
      "loss": 0.0004,
      "step": 22430
    },
    {
      "epoch": 10.38408144377603,
      "grad_norm": 0.002233629347756505,
      "learning_rate": 0.07923183711244795,
      "loss": 0.0002,
      "step": 22440
    },
    {
      "epoch": 10.38870893105044,
      "grad_norm": 1.5146193504333496,
      "learning_rate": 0.07922258213789912,
      "loss": 0.0023,
      "step": 22450
    },
    {
      "epoch": 10.39333641832485,
      "grad_norm": 1.1231276988983154,
      "learning_rate": 0.0792133271633503,
      "loss": 0.0021,
      "step": 22460
    },
    {
      "epoch": 10.39796390559926,
      "grad_norm": 0.05932661518454552,
      "learning_rate": 0.07920407218880149,
      "loss": 0.0004,
      "step": 22470
    },
    {
      "epoch": 10.402591392873669,
      "grad_norm": 2.367685079574585,
      "learning_rate": 0.07919481721425267,
      "loss": 0.0014,
      "step": 22480
    },
    {
      "epoch": 10.40721888014808,
      "grad_norm": 0.004601913038641214,
      "learning_rate": 0.07918556223970385,
      "loss": 0.0003,
      "step": 22490
    },
    {
      "epoch": 10.41184636742249,
      "grad_norm": 0.003903683042153716,
      "learning_rate": 0.07917630726515502,
      "loss": 0.0001,
      "step": 22500
    },
    {
      "epoch": 10.4164738546969,
      "grad_norm": 0.0042386846616864204,
      "learning_rate": 0.07916705229060621,
      "loss": 0.0009,
      "step": 22510
    },
    {
      "epoch": 10.42110134197131,
      "grad_norm": 0.0022379031870514154,
      "learning_rate": 0.07915779731605738,
      "loss": 0.0,
      "step": 22520
    },
    {
      "epoch": 10.42572882924572,
      "grad_norm": 0.007001281250268221,
      "learning_rate": 0.07914854234150857,
      "loss": 0.0009,
      "step": 22530
    },
    {
      "epoch": 10.430356316520129,
      "grad_norm": 0.4177533984184265,
      "learning_rate": 0.07913928736695974,
      "loss": 0.0005,
      "step": 22540
    },
    {
      "epoch": 10.43498380379454,
      "grad_norm": 0.01963001862168312,
      "learning_rate": 0.07913003239241093,
      "loss": 0.0044,
      "step": 22550
    },
    {
      "epoch": 10.43961129106895,
      "grad_norm": 0.0032058358192443848,
      "learning_rate": 0.07912077741786211,
      "loss": 0.0004,
      "step": 22560
    },
    {
      "epoch": 10.44423877834336,
      "grad_norm": 0.00018212318536825478,
      "learning_rate": 0.07911152244331328,
      "loss": 0.0005,
      "step": 22570
    },
    {
      "epoch": 10.44886626561777,
      "grad_norm": 0.028574645519256592,
      "learning_rate": 0.07910226746876448,
      "loss": 0.0001,
      "step": 22580
    },
    {
      "epoch": 10.453493752892179,
      "grad_norm": 0.0014182121958583593,
      "learning_rate": 0.07909301249421564,
      "loss": 0.0001,
      "step": 22590
    },
    {
      "epoch": 10.458121240166589,
      "grad_norm": 0.002409721491858363,
      "learning_rate": 0.07908375751966683,
      "loss": 0.0046,
      "step": 22600
    },
    {
      "epoch": 10.462748727441,
      "grad_norm": 0.026478661224246025,
      "learning_rate": 0.079074502545118,
      "loss": 0.0004,
      "step": 22610
    },
    {
      "epoch": 10.46737621471541,
      "grad_norm": 0.008615450002253056,
      "learning_rate": 0.0790652475705692,
      "loss": 0.0023,
      "step": 22620
    },
    {
      "epoch": 10.47200370198982,
      "grad_norm": 0.734757661819458,
      "learning_rate": 0.07905599259602036,
      "loss": 0.0003,
      "step": 22630
    },
    {
      "epoch": 10.47663118926423,
      "grad_norm": 1.3927274942398071,
      "learning_rate": 0.07904673762147155,
      "loss": 0.0008,
      "step": 22640
    },
    {
      "epoch": 10.481258676538639,
      "grad_norm": 0.026218639686703682,
      "learning_rate": 0.07903748264692273,
      "loss": 0.0007,
      "step": 22650
    },
    {
      "epoch": 10.48588616381305,
      "grad_norm": 0.004509658552706242,
      "learning_rate": 0.0790282276723739,
      "loss": 0.0022,
      "step": 22660
    },
    {
      "epoch": 10.49051365108746,
      "grad_norm": 0.0010420851176604629,
      "learning_rate": 0.0790189726978251,
      "loss": 0.0014,
      "step": 22670
    },
    {
      "epoch": 10.49514113836187,
      "grad_norm": 0.23356766998767853,
      "learning_rate": 0.07900971772327627,
      "loss": 0.0002,
      "step": 22680
    },
    {
      "epoch": 10.49976862563628,
      "grad_norm": 0.018356366083025932,
      "learning_rate": 0.07900046274872745,
      "loss": 0.0001,
      "step": 22690
    },
    {
      "epoch": 10.50439611291069,
      "grad_norm": 0.004938923753798008,
      "learning_rate": 0.07899120777417862,
      "loss": 0.0077,
      "step": 22700
    },
    {
      "epoch": 10.509023600185099,
      "grad_norm": 0.9543502330780029,
      "learning_rate": 0.07898195279962981,
      "loss": 0.0003,
      "step": 22710
    },
    {
      "epoch": 10.51365108745951,
      "grad_norm": 0.010712768882513046,
      "learning_rate": 0.07897269782508098,
      "loss": 0.0003,
      "step": 22720
    },
    {
      "epoch": 10.51827857473392,
      "grad_norm": 0.005782227963209152,
      "learning_rate": 0.07896344285053217,
      "loss": 0.0133,
      "step": 22730
    },
    {
      "epoch": 10.52290606200833,
      "grad_norm": 0.003909889608621597,
      "learning_rate": 0.07895418787598335,
      "loss": 0.0008,
      "step": 22740
    },
    {
      "epoch": 10.52753354928274,
      "grad_norm": 0.14771351218223572,
      "learning_rate": 0.07894493290143452,
      "loss": 0.001,
      "step": 22750
    },
    {
      "epoch": 10.532161036557149,
      "grad_norm": 0.035765390843153,
      "learning_rate": 0.07893567792688572,
      "loss": 0.0001,
      "step": 22760
    },
    {
      "epoch": 10.536788523831559,
      "grad_norm": 0.02795669250190258,
      "learning_rate": 0.07892642295233689,
      "loss": 0.0002,
      "step": 22770
    },
    {
      "epoch": 10.54141601110597,
      "grad_norm": 0.0026734480634331703,
      "learning_rate": 0.07891716797778807,
      "loss": 0.0032,
      "step": 22780
    },
    {
      "epoch": 10.54604349838038,
      "grad_norm": 0.012440849095582962,
      "learning_rate": 0.07890791300323924,
      "loss": 0.003,
      "step": 22790
    },
    {
      "epoch": 10.55067098565479,
      "grad_norm": 0.054193753749132156,
      "learning_rate": 0.07889865802869042,
      "loss": 0.0051,
      "step": 22800
    },
    {
      "epoch": 10.555298472929199,
      "grad_norm": 0.005407280754297972,
      "learning_rate": 0.0788894030541416,
      "loss": 0.0092,
      "step": 22810
    },
    {
      "epoch": 10.559925960203609,
      "grad_norm": 0.00571455666795373,
      "learning_rate": 0.07888014807959279,
      "loss": 0.0003,
      "step": 22820
    },
    {
      "epoch": 10.56455344747802,
      "grad_norm": 0.024443484842777252,
      "learning_rate": 0.07887089310504397,
      "loss": 0.0003,
      "step": 22830
    },
    {
      "epoch": 10.56918093475243,
      "grad_norm": 0.03122754767537117,
      "learning_rate": 0.07886163813049514,
      "loss": 0.0207,
      "step": 22840
    },
    {
      "epoch": 10.57380842202684,
      "grad_norm": 8.661419868469238,
      "learning_rate": 0.07885238315594634,
      "loss": 0.0036,
      "step": 22850
    },
    {
      "epoch": 10.57843590930125,
      "grad_norm": 0.012929103337228298,
      "learning_rate": 0.0788431281813975,
      "loss": 0.0042,
      "step": 22860
    },
    {
      "epoch": 10.583063396575659,
      "grad_norm": 0.002488166792318225,
      "learning_rate": 0.07883387320684869,
      "loss": 0.0037,
      "step": 22870
    },
    {
      "epoch": 10.587690883850069,
      "grad_norm": 0.003506639041006565,
      "learning_rate": 0.07882461823229986,
      "loss": 0.0002,
      "step": 22880
    },
    {
      "epoch": 10.59231837112448,
      "grad_norm": 0.016043340787291527,
      "learning_rate": 0.07881536325775104,
      "loss": 0.0005,
      "step": 22890
    },
    {
      "epoch": 10.59694585839889,
      "grad_norm": 0.08762142807245255,
      "learning_rate": 0.07880610828320223,
      "loss": 0.0002,
      "step": 22900
    },
    {
      "epoch": 10.6015733456733,
      "grad_norm": 0.09638086706399918,
      "learning_rate": 0.07879685330865341,
      "loss": 0.0003,
      "step": 22910
    },
    {
      "epoch": 10.60620083294771,
      "grad_norm": 0.00659558130428195,
      "learning_rate": 0.07878759833410459,
      "loss": 0.0012,
      "step": 22920
    },
    {
      "epoch": 10.610828320222119,
      "grad_norm": 0.24061769247055054,
      "learning_rate": 0.07877834335955576,
      "loss": 0.0006,
      "step": 22930
    },
    {
      "epoch": 10.615455807496529,
      "grad_norm": 0.09999267756938934,
      "learning_rate": 0.07876908838500696,
      "loss": 0.0072,
      "step": 22940
    },
    {
      "epoch": 10.62008329477094,
      "grad_norm": 0.0046084122732281685,
      "learning_rate": 0.07875983341045813,
      "loss": 0.0002,
      "step": 22950
    },
    {
      "epoch": 10.62471078204535,
      "grad_norm": 0.05796726420521736,
      "learning_rate": 0.07875057843590931,
      "loss": 0.0009,
      "step": 22960
    },
    {
      "epoch": 10.62933826931976,
      "grad_norm": 0.0038420904893428087,
      "learning_rate": 0.07874132346136048,
      "loss": 0.0067,
      "step": 22970
    },
    {
      "epoch": 10.633965756594169,
      "grad_norm": 0.000650081317871809,
      "learning_rate": 0.07873206848681166,
      "loss": 0.0005,
      "step": 22980
    },
    {
      "epoch": 10.638593243868579,
      "grad_norm": 0.004013174679130316,
      "learning_rate": 0.07872281351226285,
      "loss": 0.0014,
      "step": 22990
    },
    {
      "epoch": 10.64322073114299,
      "grad_norm": 0.0399954728782177,
      "learning_rate": 0.07871355853771403,
      "loss": 0.0002,
      "step": 23000
    },
    {
      "epoch": 10.6478482184174,
      "grad_norm": 0.08807599544525146,
      "learning_rate": 0.07870430356316521,
      "loss": 0.0005,
      "step": 23010
    },
    {
      "epoch": 10.65247570569181,
      "grad_norm": 0.19170629978179932,
      "learning_rate": 0.07869504858861638,
      "loss": 0.0007,
      "step": 23020
    },
    {
      "epoch": 10.65710319296622,
      "grad_norm": 0.0040372274816036224,
      "learning_rate": 0.07868579361406756,
      "loss": 0.0001,
      "step": 23030
    },
    {
      "epoch": 10.661730680240629,
      "grad_norm": 0.00355763197876513,
      "learning_rate": 0.07867653863951875,
      "loss": 0.0005,
      "step": 23040
    },
    {
      "epoch": 10.666358167515039,
      "grad_norm": 0.009175215847790241,
      "learning_rate": 0.07866728366496993,
      "loss": 0.0017,
      "step": 23050
    },
    {
      "epoch": 10.67098565478945,
      "grad_norm": 6.473567485809326,
      "learning_rate": 0.0786580286904211,
      "loss": 0.0092,
      "step": 23060
    },
    {
      "epoch": 10.67561314206386,
      "grad_norm": 0.003207736648619175,
      "learning_rate": 0.07864877371587228,
      "loss": 0.0003,
      "step": 23070
    },
    {
      "epoch": 10.68024062933827,
      "grad_norm": 0.006676693446934223,
      "learning_rate": 0.07863951874132347,
      "loss": 0.0012,
      "step": 23080
    },
    {
      "epoch": 10.68486811661268,
      "grad_norm": 7.0098371505737305,
      "learning_rate": 0.07863026376677465,
      "loss": 0.004,
      "step": 23090
    },
    {
      "epoch": 10.689495603887089,
      "grad_norm": 0.0012856130488216877,
      "learning_rate": 0.07862100879222583,
      "loss": 0.0019,
      "step": 23100
    },
    {
      "epoch": 10.694123091161499,
      "grad_norm": 0.03798314556479454,
      "learning_rate": 0.078611753817677,
      "loss": 0.0029,
      "step": 23110
    },
    {
      "epoch": 10.69875057843591,
      "grad_norm": 0.0002241838228655979,
      "learning_rate": 0.07860249884312819,
      "loss": 0.0004,
      "step": 23120
    },
    {
      "epoch": 10.70337806571032,
      "grad_norm": 0.5076243877410889,
      "learning_rate": 0.07859324386857937,
      "loss": 0.0045,
      "step": 23130
    },
    {
      "epoch": 10.70800555298473,
      "grad_norm": 1.5279803276062012,
      "learning_rate": 0.07858398889403055,
      "loss": 0.0032,
      "step": 23140
    },
    {
      "epoch": 10.712633040259139,
      "grad_norm": 0.0027902929577976465,
      "learning_rate": 0.07857473391948172,
      "loss": 0.0039,
      "step": 23150
    },
    {
      "epoch": 10.717260527533549,
      "grad_norm": 4.619667053222656,
      "learning_rate": 0.0785654789449329,
      "loss": 0.0135,
      "step": 23160
    },
    {
      "epoch": 10.72188801480796,
      "grad_norm": 0.006540990434587002,
      "learning_rate": 0.07855622397038409,
      "loss": 0.0005,
      "step": 23170
    },
    {
      "epoch": 10.72651550208237,
      "grad_norm": 0.0014851763844490051,
      "learning_rate": 0.07854696899583527,
      "loss": 0.0018,
      "step": 23180
    },
    {
      "epoch": 10.73114298935678,
      "grad_norm": 0.0008924950379878283,
      "learning_rate": 0.07853771402128645,
      "loss": 0.0007,
      "step": 23190
    },
    {
      "epoch": 10.735770476631188,
      "grad_norm": 9.443310737609863,
      "learning_rate": 0.07852845904673762,
      "loss": 0.0032,
      "step": 23200
    },
    {
      "epoch": 10.740397963905599,
      "grad_norm": 0.11186037957668304,
      "learning_rate": 0.0785192040721888,
      "loss": 0.0004,
      "step": 23210
    },
    {
      "epoch": 10.745025451180009,
      "grad_norm": 0.0030260842759162188,
      "learning_rate": 0.07850994909763999,
      "loss": 0.0002,
      "step": 23220
    },
    {
      "epoch": 10.74965293845442,
      "grad_norm": 0.010425955057144165,
      "learning_rate": 0.07850069412309117,
      "loss": 0.0006,
      "step": 23230
    },
    {
      "epoch": 10.75428042572883,
      "grad_norm": 0.0012524136109277606,
      "learning_rate": 0.07849143914854234,
      "loss": 0.0016,
      "step": 23240
    },
    {
      "epoch": 10.75890791300324,
      "grad_norm": 0.012914796359837055,
      "learning_rate": 0.07848218417399352,
      "loss": 0.0002,
      "step": 23250
    },
    {
      "epoch": 10.763535400277648,
      "grad_norm": 0.025533221662044525,
      "learning_rate": 0.07847292919944471,
      "loss": 0.0003,
      "step": 23260
    },
    {
      "epoch": 10.768162887552059,
      "grad_norm": 0.05095488950610161,
      "learning_rate": 0.07846367422489589,
      "loss": 0.0066,
      "step": 23270
    },
    {
      "epoch": 10.772790374826469,
      "grad_norm": 0.0016215464565902948,
      "learning_rate": 0.07845441925034707,
      "loss": 0.0002,
      "step": 23280
    },
    {
      "epoch": 10.77741786210088,
      "grad_norm": 0.06904719769954681,
      "learning_rate": 0.07844516427579824,
      "loss": 0.0002,
      "step": 23290
    },
    {
      "epoch": 10.78204534937529,
      "grad_norm": 0.038671959191560745,
      "learning_rate": 0.07843590930124943,
      "loss": 0.0012,
      "step": 23300
    },
    {
      "epoch": 10.7866728366497,
      "grad_norm": 0.08277668058872223,
      "learning_rate": 0.07842665432670061,
      "loss": 0.0021,
      "step": 23310
    },
    {
      "epoch": 10.791300323924109,
      "grad_norm": 0.01876063644886017,
      "learning_rate": 0.07841739935215179,
      "loss": 0.0095,
      "step": 23320
    },
    {
      "epoch": 10.795927811198519,
      "grad_norm": 0.07324645668268204,
      "learning_rate": 0.07840814437760296,
      "loss": 0.0006,
      "step": 23330
    },
    {
      "epoch": 10.80055529847293,
      "grad_norm": 0.0008504117140546441,
      "learning_rate": 0.07839888940305414,
      "loss": 0.0007,
      "step": 23340
    },
    {
      "epoch": 10.80518278574734,
      "grad_norm": 0.015282953158020973,
      "learning_rate": 0.07838963442850533,
      "loss": 0.0001,
      "step": 23350
    },
    {
      "epoch": 10.80981027302175,
      "grad_norm": 0.019278017804026604,
      "learning_rate": 0.07838037945395651,
      "loss": 0.0004,
      "step": 23360
    },
    {
      "epoch": 10.814437760296158,
      "grad_norm": 0.020513467490673065,
      "learning_rate": 0.0783711244794077,
      "loss": 0.0026,
      "step": 23370
    },
    {
      "epoch": 10.819065247570569,
      "grad_norm": 0.007787060923874378,
      "learning_rate": 0.07836186950485886,
      "loss": 0.0009,
      "step": 23380
    },
    {
      "epoch": 10.823692734844979,
      "grad_norm": 0.08802787959575653,
      "learning_rate": 0.07835261453031005,
      "loss": 0.0011,
      "step": 23390
    },
    {
      "epoch": 10.82832022211939,
      "grad_norm": 0.0030392154585570097,
      "learning_rate": 0.07834335955576122,
      "loss": 0.0056,
      "step": 23400
    },
    {
      "epoch": 10.8329477093938,
      "grad_norm": 0.14879941940307617,
      "learning_rate": 0.07833410458121241,
      "loss": 0.003,
      "step": 23410
    },
    {
      "epoch": 10.83757519666821,
      "grad_norm": 0.02081853710114956,
      "learning_rate": 0.07832484960666358,
      "loss": 0.0002,
      "step": 23420
    },
    {
      "epoch": 10.842202683942618,
      "grad_norm": 0.0011044889688491821,
      "learning_rate": 0.07831559463211477,
      "loss": 0.0001,
      "step": 23430
    },
    {
      "epoch": 10.846830171217029,
      "grad_norm": 0.13875064253807068,
      "learning_rate": 0.07830633965756595,
      "loss": 0.0008,
      "step": 23440
    },
    {
      "epoch": 10.851457658491439,
      "grad_norm": 0.1159716472029686,
      "learning_rate": 0.07829708468301713,
      "loss": 0.0069,
      "step": 23450
    },
    {
      "epoch": 10.85608514576585,
      "grad_norm": 0.005953473504632711,
      "learning_rate": 0.07828782970846832,
      "loss": 0.0005,
      "step": 23460
    },
    {
      "epoch": 10.86071263304026,
      "grad_norm": 0.002388658467680216,
      "learning_rate": 0.07827857473391948,
      "loss": 0.0001,
      "step": 23470
    },
    {
      "epoch": 10.86534012031467,
      "grad_norm": 0.0015329712769016623,
      "learning_rate": 0.07826931975937067,
      "loss": 0.0001,
      "step": 23480
    },
    {
      "epoch": 10.869967607589079,
      "grad_norm": 0.022464806213974953,
      "learning_rate": 0.07826006478482184,
      "loss": 0.0012,
      "step": 23490
    },
    {
      "epoch": 10.874595094863489,
      "grad_norm": 0.0649266391992569,
      "learning_rate": 0.07825080981027303,
      "loss": 0.0008,
      "step": 23500
    },
    {
      "epoch": 10.8792225821379,
      "grad_norm": 0.013196879997849464,
      "learning_rate": 0.0782415548357242,
      "loss": 0.0002,
      "step": 23510
    },
    {
      "epoch": 10.88385006941231,
      "grad_norm": 1.09131920337677,
      "learning_rate": 0.07823229986117539,
      "loss": 0.0006,
      "step": 23520
    },
    {
      "epoch": 10.88847755668672,
      "grad_norm": 0.06166210025548935,
      "learning_rate": 0.07822304488662657,
      "loss": 0.0007,
      "step": 23530
    },
    {
      "epoch": 10.893105043961128,
      "grad_norm": 0.022104352712631226,
      "learning_rate": 0.07821378991207775,
      "loss": 0.0011,
      "step": 23540
    },
    {
      "epoch": 10.897732531235539,
      "grad_norm": 0.0017638037679716945,
      "learning_rate": 0.07820453493752894,
      "loss": 0.004,
      "step": 23550
    },
    {
      "epoch": 10.902360018509949,
      "grad_norm": 0.5347952246665955,
      "learning_rate": 0.0781952799629801,
      "loss": 0.001,
      "step": 23560
    },
    {
      "epoch": 10.90698750578436,
      "grad_norm": 0.4755040407180786,
      "learning_rate": 0.07818602498843129,
      "loss": 0.0005,
      "step": 23570
    },
    {
      "epoch": 10.91161499305877,
      "grad_norm": 0.002508612582460046,
      "learning_rate": 0.07817677001388246,
      "loss": 0.0002,
      "step": 23580
    },
    {
      "epoch": 10.91624248033318,
      "grad_norm": 0.023572832345962524,
      "learning_rate": 0.07816751503933365,
      "loss": 0.0009,
      "step": 23590
    },
    {
      "epoch": 10.920869967607588,
      "grad_norm": 0.0013321276055648923,
      "learning_rate": 0.07815826006478482,
      "loss": 0.0003,
      "step": 23600
    },
    {
      "epoch": 10.925497454881999,
      "grad_norm": 0.0013583162799477577,
      "learning_rate": 0.078149005090236,
      "loss": 0.0003,
      "step": 23610
    },
    {
      "epoch": 10.930124942156409,
      "grad_norm": 0.36786913871765137,
      "learning_rate": 0.07813975011568719,
      "loss": 0.0004,
      "step": 23620
    },
    {
      "epoch": 10.93475242943082,
      "grad_norm": 1.0304290056228638,
      "learning_rate": 0.07813049514113836,
      "loss": 0.0004,
      "step": 23630
    },
    {
      "epoch": 10.93937991670523,
      "grad_norm": 0.013922171667218208,
      "learning_rate": 0.07812124016658956,
      "loss": 0.0028,
      "step": 23640
    },
    {
      "epoch": 10.944007403979638,
      "grad_norm": 0.9233263731002808,
      "learning_rate": 0.07811198519204073,
      "loss": 0.0077,
      "step": 23650
    },
    {
      "epoch": 10.948634891254049,
      "grad_norm": 0.15871280431747437,
      "learning_rate": 0.07810273021749191,
      "loss": 0.003,
      "step": 23660
    },
    {
      "epoch": 10.953262378528459,
      "grad_norm": 0.03009091317653656,
      "learning_rate": 0.07809347524294308,
      "loss": 0.0065,
      "step": 23670
    },
    {
      "epoch": 10.95788986580287,
      "grad_norm": 0.0007507267873734236,
      "learning_rate": 0.07808422026839427,
      "loss": 0.0003,
      "step": 23680
    },
    {
      "epoch": 10.96251735307728,
      "grad_norm": 0.007825757376849651,
      "learning_rate": 0.07807496529384544,
      "loss": 0.0005,
      "step": 23690
    },
    {
      "epoch": 10.96714484035169,
      "grad_norm": 0.28666672110557556,
      "learning_rate": 0.07806571031929663,
      "loss": 0.0005,
      "step": 23700
    },
    {
      "epoch": 10.971772327626098,
      "grad_norm": 0.21672441065311432,
      "learning_rate": 0.07805645534474781,
      "loss": 0.0005,
      "step": 23710
    },
    {
      "epoch": 10.976399814900509,
      "grad_norm": 0.01616145856678486,
      "learning_rate": 0.07804720037019898,
      "loss": 0.009,
      "step": 23720
    },
    {
      "epoch": 10.981027302174919,
      "grad_norm": 0.9163095355033875,
      "learning_rate": 0.07803794539565018,
      "loss": 0.0006,
      "step": 23730
    },
    {
      "epoch": 10.98565478944933,
      "grad_norm": 0.00552898645401001,
      "learning_rate": 0.07802869042110135,
      "loss": 0.0003,
      "step": 23740
    },
    {
      "epoch": 10.99028227672374,
      "grad_norm": 0.021746542304754257,
      "learning_rate": 0.07801943544655253,
      "loss": 0.0002,
      "step": 23750
    },
    {
      "epoch": 10.994909763998148,
      "grad_norm": 0.0010662669083103538,
      "learning_rate": 0.0780101804720037,
      "loss": 0.0006,
      "step": 23760
    },
    {
      "epoch": 10.999537251272558,
      "grad_norm": 0.002816189546138048,
      "learning_rate": 0.0780009254974549,
      "loss": 0.0003,
      "step": 23770
    },
    {
      "epoch": 11.0,
      "eval_accuracy_branch1": 0.9898320751810199,
      "eval_accuracy_branch2": 0.4993067323987059,
      "eval_f1_branch1": 0.9896851790200681,
      "eval_f1_branch2": 0.4985509362264327,
      "eval_loss": 0.023808149620890617,
      "eval_precision_branch1": 0.990241356432643,
      "eval_precision_branch2": 0.4993025274083841,
      "eval_recall_branch1": 0.9892562457566858,
      "eval_recall_branch2": 0.49930673239870593,
      "eval_runtime": 30.1246,
      "eval_samples_per_second": 430.943,
      "eval_steps_per_second": 53.876,
      "step": 23771
    },
    {
      "epoch": 11.004164738546969,
      "grad_norm": 0.0009295015479438007,
      "learning_rate": 0.07799167052290606,
      "loss": 0.0007,
      "step": 23780
    },
    {
      "epoch": 11.008792225821379,
      "grad_norm": 0.008781446143984795,
      "learning_rate": 0.07798241554835725,
      "loss": 0.0009,
      "step": 23790
    },
    {
      "epoch": 11.01341971309579,
      "grad_norm": 0.053758904337882996,
      "learning_rate": 0.07797316057380843,
      "loss": 0.0003,
      "step": 23800
    },
    {
      "epoch": 11.0180472003702,
      "grad_norm": 0.00863800011575222,
      "learning_rate": 0.0779639055992596,
      "loss": 0.0002,
      "step": 23810
    },
    {
      "epoch": 11.022674687644608,
      "grad_norm": 0.005919660907238722,
      "learning_rate": 0.0779546506247108,
      "loss": 0.0001,
      "step": 23820
    },
    {
      "epoch": 11.027302174919019,
      "grad_norm": 0.03598663955926895,
      "learning_rate": 0.07794539565016197,
      "loss": 0.0001,
      "step": 23830
    },
    {
      "epoch": 11.031929662193429,
      "grad_norm": 0.023375438526272774,
      "learning_rate": 0.07793614067561315,
      "loss": 0.0007,
      "step": 23840
    },
    {
      "epoch": 11.03655714946784,
      "grad_norm": 0.007686545606702566,
      "learning_rate": 0.07792688570106432,
      "loss": 0.0015,
      "step": 23850
    },
    {
      "epoch": 11.04118463674225,
      "grad_norm": 0.41270866990089417,
      "learning_rate": 0.0779176307265155,
      "loss": 0.0054,
      "step": 23860
    },
    {
      "epoch": 11.04581212401666,
      "grad_norm": 0.0010340010048821568,
      "learning_rate": 0.07790837575196669,
      "loss": 0.0003,
      "step": 23870
    },
    {
      "epoch": 11.050439611291068,
      "grad_norm": 0.055625610053539276,
      "learning_rate": 0.07789912077741787,
      "loss": 0.0006,
      "step": 23880
    },
    {
      "epoch": 11.055067098565479,
      "grad_norm": 0.1441066414117813,
      "learning_rate": 0.07788986580286905,
      "loss": 0.0002,
      "step": 23890
    },
    {
      "epoch": 11.059694585839889,
      "grad_norm": 0.631314218044281,
      "learning_rate": 0.07788061082832022,
      "loss": 0.0007,
      "step": 23900
    },
    {
      "epoch": 11.0643220731143,
      "grad_norm": 0.0030960249714553356,
      "learning_rate": 0.07787135585377142,
      "loss": 0.0,
      "step": 23910
    },
    {
      "epoch": 11.06894956038871,
      "grad_norm": 0.01680895872414112,
      "learning_rate": 0.07786210087922259,
      "loss": 0.0001,
      "step": 23920
    },
    {
      "epoch": 11.073577047663118,
      "grad_norm": 0.05278925597667694,
      "learning_rate": 0.07785284590467377,
      "loss": 0.0001,
      "step": 23930
    },
    {
      "epoch": 11.078204534937528,
      "grad_norm": 6.770425319671631,
      "learning_rate": 0.07784359093012494,
      "loss": 0.0032,
      "step": 23940
    },
    {
      "epoch": 11.082832022211939,
      "grad_norm": 0.003560876939445734,
      "learning_rate": 0.07783433595557612,
      "loss": 0.0049,
      "step": 23950
    },
    {
      "epoch": 11.087459509486349,
      "grad_norm": 0.0017865055706351995,
      "learning_rate": 0.0778250809810273,
      "loss": 0.0002,
      "step": 23960
    },
    {
      "epoch": 11.09208699676076,
      "grad_norm": 0.052150242030620575,
      "learning_rate": 0.07781582600647849,
      "loss": 0.0001,
      "step": 23970
    },
    {
      "epoch": 11.09671448403517,
      "grad_norm": 0.10576895624399185,
      "learning_rate": 0.07780657103192967,
      "loss": 0.0001,
      "step": 23980
    },
    {
      "epoch": 11.101341971309578,
      "grad_norm": 4.009208679199219,
      "learning_rate": 0.07779731605738084,
      "loss": 0.001,
      "step": 23990
    },
    {
      "epoch": 11.105969458583989,
      "grad_norm": 0.19531796872615814,
      "learning_rate": 0.07778806108283204,
      "loss": 0.0004,
      "step": 24000
    },
    {
      "epoch": 11.110596945858399,
      "grad_norm": 0.0032116370275616646,
      "learning_rate": 0.07777880610828321,
      "loss": 0.0001,
      "step": 24010
    },
    {
      "epoch": 11.11522443313281,
      "grad_norm": 0.0005086000310257077,
      "learning_rate": 0.07776955113373439,
      "loss": 0.0001,
      "step": 24020
    },
    {
      "epoch": 11.11985192040722,
      "grad_norm": 0.03589858487248421,
      "learning_rate": 0.07776029615918556,
      "loss": 0.0007,
      "step": 24030
    },
    {
      "epoch": 11.124479407681628,
      "grad_norm": 0.8416681885719299,
      "learning_rate": 0.07775104118463674,
      "loss": 0.0016,
      "step": 24040
    },
    {
      "epoch": 11.129106894956038,
      "grad_norm": 0.7834193706512451,
      "learning_rate": 0.07774178621008793,
      "loss": 0.0036,
      "step": 24050
    },
    {
      "epoch": 11.133734382230449,
      "grad_norm": 0.2191469669342041,
      "learning_rate": 0.07773253123553911,
      "loss": 0.001,
      "step": 24060
    },
    {
      "epoch": 11.138361869504859,
      "grad_norm": 0.006162421777844429,
      "learning_rate": 0.07772327626099029,
      "loss": 0.0092,
      "step": 24070
    },
    {
      "epoch": 11.14298935677927,
      "grad_norm": 0.02134808897972107,
      "learning_rate": 0.07771402128644146,
      "loss": 0.0001,
      "step": 24080
    },
    {
      "epoch": 11.14761684405368,
      "grad_norm": 0.008029092103242874,
      "learning_rate": 0.07770476631189265,
      "loss": 0.0004,
      "step": 24090
    },
    {
      "epoch": 11.152244331328088,
      "grad_norm": 0.0532844215631485,
      "learning_rate": 0.07769551133734383,
      "loss": 0.0002,
      "step": 24100
    },
    {
      "epoch": 11.156871818602498,
      "grad_norm": 0.06920596957206726,
      "learning_rate": 0.07768625636279501,
      "loss": 0.0005,
      "step": 24110
    },
    {
      "epoch": 11.161499305876909,
      "grad_norm": 0.028671834617853165,
      "learning_rate": 0.07767700138824618,
      "loss": 0.0003,
      "step": 24120
    },
    {
      "epoch": 11.166126793151319,
      "grad_norm": 0.001292603905312717,
      "learning_rate": 0.07766774641369736,
      "loss": 0.0007,
      "step": 24130
    },
    {
      "epoch": 11.17075428042573,
      "grad_norm": 0.046115901321172714,
      "learning_rate": 0.07765849143914855,
      "loss": 0.0001,
      "step": 24140
    },
    {
      "epoch": 11.17538176770014,
      "grad_norm": 0.009807253256440163,
      "learning_rate": 0.07764923646459973,
      "loss": 0.0002,
      "step": 24150
    },
    {
      "epoch": 11.180009254974548,
      "grad_norm": 1.0080885887145996,
      "learning_rate": 0.07763998149005091,
      "loss": 0.0002,
      "step": 24160
    },
    {
      "epoch": 11.184636742248959,
      "grad_norm": 0.006732344627380371,
      "learning_rate": 0.07763072651550208,
      "loss": 0.0009,
      "step": 24170
    },
    {
      "epoch": 11.189264229523369,
      "grad_norm": 0.002920221770182252,
      "learning_rate": 0.07762147154095327,
      "loss": 0.004,
      "step": 24180
    },
    {
      "epoch": 11.19389171679778,
      "grad_norm": 0.0006562101189047098,
      "learning_rate": 0.07761221656640445,
      "loss": 0.002,
      "step": 24190
    },
    {
      "epoch": 11.19851920407219,
      "grad_norm": 0.0006484141340479255,
      "learning_rate": 0.07760296159185563,
      "loss": 0.0004,
      "step": 24200
    },
    {
      "epoch": 11.203146691346598,
      "grad_norm": 0.27023845911026,
      "learning_rate": 0.0775937066173068,
      "loss": 0.0002,
      "step": 24210
    },
    {
      "epoch": 11.207774178621008,
      "grad_norm": 0.0003232257440686226,
      "learning_rate": 0.07758445164275798,
      "loss": 0.0004,
      "step": 24220
    },
    {
      "epoch": 11.212401665895419,
      "grad_norm": 0.06917775422334671,
      "learning_rate": 0.07757519666820917,
      "loss": 0.0003,
      "step": 24230
    },
    {
      "epoch": 11.217029153169829,
      "grad_norm": 0.0012914540711790323,
      "learning_rate": 0.07756594169366035,
      "loss": 0.0217,
      "step": 24240
    },
    {
      "epoch": 11.22165664044424,
      "grad_norm": 0.015477988868951797,
      "learning_rate": 0.07755668671911153,
      "loss": 0.0002,
      "step": 24250
    },
    {
      "epoch": 11.22628412771865,
      "grad_norm": 4.094212532043457,
      "learning_rate": 0.0775474317445627,
      "loss": 0.0016,
      "step": 24260
    },
    {
      "epoch": 11.230911614993058,
      "grad_norm": 1.9801710844039917,
      "learning_rate": 0.07753817677001389,
      "loss": 0.0029,
      "step": 24270
    },
    {
      "epoch": 11.235539102267468,
      "grad_norm": 0.0030350147280842066,
      "learning_rate": 0.07752892179546507,
      "loss": 0.0007,
      "step": 24280
    },
    {
      "epoch": 11.240166589541879,
      "grad_norm": 0.000424691301304847,
      "learning_rate": 0.07751966682091625,
      "loss": 0.0004,
      "step": 24290
    },
    {
      "epoch": 11.244794076816289,
      "grad_norm": 0.30472972989082336,
      "learning_rate": 0.07751041184636742,
      "loss": 0.0031,
      "step": 24300
    },
    {
      "epoch": 11.2494215640907,
      "grad_norm": 0.01656877063214779,
      "learning_rate": 0.0775011568718186,
      "loss": 0.0001,
      "step": 24310
    },
    {
      "epoch": 11.254049051365108,
      "grad_norm": 0.043529167771339417,
      "learning_rate": 0.07749190189726979,
      "loss": 0.0011,
      "step": 24320
    },
    {
      "epoch": 11.258676538639518,
      "grad_norm": 0.00039861869299784303,
      "learning_rate": 0.07748264692272097,
      "loss": 0.0001,
      "step": 24330
    },
    {
      "epoch": 11.263304025913929,
      "grad_norm": 5.10860538482666,
      "learning_rate": 0.07747339194817215,
      "loss": 0.0018,
      "step": 24340
    },
    {
      "epoch": 11.267931513188339,
      "grad_norm": 0.008989681489765644,
      "learning_rate": 0.07746413697362332,
      "loss": 0.0001,
      "step": 24350
    },
    {
      "epoch": 11.27255900046275,
      "grad_norm": 0.007839083671569824,
      "learning_rate": 0.0774548819990745,
      "loss": 0.0003,
      "step": 24360
    },
    {
      "epoch": 11.27718648773716,
      "grad_norm": 0.0032969603780657053,
      "learning_rate": 0.07744562702452569,
      "loss": 0.0018,
      "step": 24370
    },
    {
      "epoch": 11.281813975011568,
      "grad_norm": 0.002190896077081561,
      "learning_rate": 0.07743637204997687,
      "loss": 0.0012,
      "step": 24380
    },
    {
      "epoch": 11.286441462285978,
      "grad_norm": 0.0009719006484374404,
      "learning_rate": 0.07742711707542804,
      "loss": 0.012,
      "step": 24390
    },
    {
      "epoch": 11.291068949560389,
      "grad_norm": 0.02123691514134407,
      "learning_rate": 0.07741786210087923,
      "loss": 0.0013,
      "step": 24400
    },
    {
      "epoch": 11.295696436834799,
      "grad_norm": 0.0022638048976659775,
      "learning_rate": 0.07740860712633041,
      "loss": 0.0001,
      "step": 24410
    },
    {
      "epoch": 11.30032392410921,
      "grad_norm": 0.005991003476083279,
      "learning_rate": 0.07739935215178159,
      "loss": 0.0059,
      "step": 24420
    },
    {
      "epoch": 11.30495141138362,
      "grad_norm": 0.0006922601023688912,
      "learning_rate": 0.07739009717723278,
      "loss": 0.0048,
      "step": 24430
    },
    {
      "epoch": 11.309578898658028,
      "grad_norm": 2.328339099884033,
      "learning_rate": 0.07738084220268394,
      "loss": 0.0024,
      "step": 24440
    },
    {
      "epoch": 11.314206385932438,
      "grad_norm": 0.034190062433481216,
      "learning_rate": 0.07737158722813513,
      "loss": 0.0005,
      "step": 24450
    },
    {
      "epoch": 11.318833873206849,
      "grad_norm": 0.0013430435210466385,
      "learning_rate": 0.07736233225358631,
      "loss": 0.0001,
      "step": 24460
    },
    {
      "epoch": 11.323461360481259,
      "grad_norm": 0.0037341781426221132,
      "learning_rate": 0.0773530772790375,
      "loss": 0.0076,
      "step": 24470
    },
    {
      "epoch": 11.32808884775567,
      "grad_norm": 0.0017292893026024103,
      "learning_rate": 0.07734382230448866,
      "loss": 0.0008,
      "step": 24480
    },
    {
      "epoch": 11.332716335030078,
      "grad_norm": 0.002332238480448723,
      "learning_rate": 0.07733456732993985,
      "loss": 0.0003,
      "step": 24490
    },
    {
      "epoch": 11.337343822304488,
      "grad_norm": 0.002465362660586834,
      "learning_rate": 0.07732531235539103,
      "loss": 0.0001,
      "step": 24500
    },
    {
      "epoch": 11.341971309578899,
      "grad_norm": 0.007021562196314335,
      "learning_rate": 0.07731605738084221,
      "loss": 0.0004,
      "step": 24510
    },
    {
      "epoch": 11.346598796853309,
      "grad_norm": 2.9439189434051514,
      "learning_rate": 0.0773068024062934,
      "loss": 0.0013,
      "step": 24520
    },
    {
      "epoch": 11.35122628412772,
      "grad_norm": 0.028505083173513412,
      "learning_rate": 0.07729754743174456,
      "loss": 0.0004,
      "step": 24530
    },
    {
      "epoch": 11.35585377140213,
      "grad_norm": 0.005588136147707701,
      "learning_rate": 0.07728829245719575,
      "loss": 0.0003,
      "step": 24540
    },
    {
      "epoch": 11.360481258676538,
      "grad_norm": 0.002255135914310813,
      "learning_rate": 0.07727903748264692,
      "loss": 0.0006,
      "step": 24550
    },
    {
      "epoch": 11.365108745950948,
      "grad_norm": 0.012049190700054169,
      "learning_rate": 0.07726978250809811,
      "loss": 0.0004,
      "step": 24560
    },
    {
      "epoch": 11.369736233225359,
      "grad_norm": 0.004474971443414688,
      "learning_rate": 0.07726052753354928,
      "loss": 0.0001,
      "step": 24570
    },
    {
      "epoch": 11.374363720499769,
      "grad_norm": 2.4629323482513428,
      "learning_rate": 0.07725127255900047,
      "loss": 0.0005,
      "step": 24580
    },
    {
      "epoch": 11.37899120777418,
      "grad_norm": 0.0323486365377903,
      "learning_rate": 0.07724201758445165,
      "loss": 0.0016,
      "step": 24590
    },
    {
      "epoch": 11.383618695048588,
      "grad_norm": 0.012675236910581589,
      "learning_rate": 0.07723276260990283,
      "loss": 0.0004,
      "step": 24600
    },
    {
      "epoch": 11.388246182322998,
      "grad_norm": 0.004254332277923822,
      "learning_rate": 0.07722350763535402,
      "loss": 0.0001,
      "step": 24610
    },
    {
      "epoch": 11.392873669597408,
      "grad_norm": 0.12002136558294296,
      "learning_rate": 0.07721425266080519,
      "loss": 0.0058,
      "step": 24620
    },
    {
      "epoch": 11.397501156871819,
      "grad_norm": 0.09030963480472565,
      "learning_rate": 0.07720499768625637,
      "loss": 0.0002,
      "step": 24630
    },
    {
      "epoch": 11.402128644146229,
      "grad_norm": 0.9088791012763977,
      "learning_rate": 0.07719574271170754,
      "loss": 0.0005,
      "step": 24640
    },
    {
      "epoch": 11.40675613142064,
      "grad_norm": 6.122878074645996,
      "learning_rate": 0.07718648773715873,
      "loss": 0.0031,
      "step": 24650
    },
    {
      "epoch": 11.411383618695048,
      "grad_norm": 2.13169527053833,
      "learning_rate": 0.0771772327626099,
      "loss": 0.0007,
      "step": 24660
    },
    {
      "epoch": 11.416011105969458,
      "grad_norm": 0.0015361174009740353,
      "learning_rate": 0.07716797778806109,
      "loss": 0.0032,
      "step": 24670
    },
    {
      "epoch": 11.420638593243869,
      "grad_norm": 0.028253184631466866,
      "learning_rate": 0.07715872281351227,
      "loss": 0.0007,
      "step": 24680
    },
    {
      "epoch": 11.425266080518279,
      "grad_norm": 0.0029704279731959105,
      "learning_rate": 0.07714946783896345,
      "loss": 0.0,
      "step": 24690
    },
    {
      "epoch": 11.42989356779269,
      "grad_norm": 0.00861792080104351,
      "learning_rate": 0.07714021286441464,
      "loss": 0.0014,
      "step": 24700
    },
    {
      "epoch": 11.434521055067098,
      "grad_norm": 0.01414167694747448,
      "learning_rate": 0.0771309578898658,
      "loss": 0.0008,
      "step": 24710
    },
    {
      "epoch": 11.439148542341508,
      "grad_norm": 0.3648584187030792,
      "learning_rate": 0.07712170291531699,
      "loss": 0.0046,
      "step": 24720
    },
    {
      "epoch": 11.443776029615918,
      "grad_norm": 0.0017204148462042212,
      "learning_rate": 0.07711244794076816,
      "loss": 0.0005,
      "step": 24730
    },
    {
      "epoch": 11.448403516890329,
      "grad_norm": 0.014656447805464268,
      "learning_rate": 0.07710319296621936,
      "loss": 0.0057,
      "step": 24740
    },
    {
      "epoch": 11.453031004164739,
      "grad_norm": 0.003776783822104335,
      "learning_rate": 0.07709393799167052,
      "loss": 0.0001,
      "step": 24750
    },
    {
      "epoch": 11.45765849143915,
      "grad_norm": 0.007806267123669386,
      "learning_rate": 0.07708468301712171,
      "loss": 0.0083,
      "step": 24760
    },
    {
      "epoch": 11.462285978713558,
      "grad_norm": 0.007730068638920784,
      "learning_rate": 0.07707542804257289,
      "loss": 0.0004,
      "step": 24770
    },
    {
      "epoch": 11.466913465987968,
      "grad_norm": 0.5959315896034241,
      "learning_rate": 0.07706617306802406,
      "loss": 0.0004,
      "step": 24780
    },
    {
      "epoch": 11.471540953262378,
      "grad_norm": 0.032383840531110764,
      "learning_rate": 0.07705691809347526,
      "loss": 0.0001,
      "step": 24790
    },
    {
      "epoch": 11.476168440536789,
      "grad_norm": 0.05896087735891342,
      "learning_rate": 0.07704766311892643,
      "loss": 0.0003,
      "step": 24800
    },
    {
      "epoch": 11.480795927811199,
      "grad_norm": 0.007970916107296944,
      "learning_rate": 0.07703840814437761,
      "loss": 0.0001,
      "step": 24810
    },
    {
      "epoch": 11.48542341508561,
      "grad_norm": 0.012446200475096703,
      "learning_rate": 0.07702915316982878,
      "loss": 0.0013,
      "step": 24820
    },
    {
      "epoch": 11.490050902360018,
      "grad_norm": 0.0010223505087196827,
      "learning_rate": 0.07701989819527998,
      "loss": 0.0003,
      "step": 24830
    },
    {
      "epoch": 11.494678389634428,
      "grad_norm": 0.0038804872892796993,
      "learning_rate": 0.07701064322073115,
      "loss": 0.0054,
      "step": 24840
    },
    {
      "epoch": 11.499305876908839,
      "grad_norm": 0.04290078207850456,
      "learning_rate": 0.07700138824618233,
      "loss": 0.0002,
      "step": 24850
    },
    {
      "epoch": 11.503933364183249,
      "grad_norm": 0.01844259537756443,
      "learning_rate": 0.07699213327163351,
      "loss": 0.0001,
      "step": 24860
    },
    {
      "epoch": 11.50856085145766,
      "grad_norm": 0.060022301971912384,
      "learning_rate": 0.07698287829708468,
      "loss": 0.0039,
      "step": 24870
    },
    {
      "epoch": 11.513188338732068,
      "grad_norm": 0.00226225215010345,
      "learning_rate": 0.07697362332253588,
      "loss": 0.0003,
      "step": 24880
    },
    {
      "epoch": 11.517815826006478,
      "grad_norm": 0.07327134907245636,
      "learning_rate": 0.07696436834798705,
      "loss": 0.0001,
      "step": 24890
    },
    {
      "epoch": 11.522443313280888,
      "grad_norm": 0.03149097040295601,
      "learning_rate": 0.07695511337343823,
      "loss": 0.0001,
      "step": 24900
    },
    {
      "epoch": 11.527070800555299,
      "grad_norm": 0.015799054875969887,
      "learning_rate": 0.0769458583988894,
      "loss": 0.0051,
      "step": 24910
    },
    {
      "epoch": 11.531698287829709,
      "grad_norm": 0.004488724283874035,
      "learning_rate": 0.0769366034243406,
      "loss": 0.001,
      "step": 24920
    },
    {
      "epoch": 11.53632577510412,
      "grad_norm": 0.13392460346221924,
      "learning_rate": 0.07692734844979177,
      "loss": 0.0038,
      "step": 24930
    },
    {
      "epoch": 11.540953262378528,
      "grad_norm": 0.008199326694011688,
      "learning_rate": 0.07691809347524295,
      "loss": 0.0008,
      "step": 24940
    },
    {
      "epoch": 11.545580749652938,
      "grad_norm": 0.000776244152802974,
      "learning_rate": 0.07690883850069413,
      "loss": 0.0032,
      "step": 24950
    },
    {
      "epoch": 11.550208236927348,
      "grad_norm": 0.012825886718928814,
      "learning_rate": 0.0768995835261453,
      "loss": 0.0001,
      "step": 24960
    },
    {
      "epoch": 11.554835724201759,
      "grad_norm": 0.022468335926532745,
      "learning_rate": 0.0768903285515965,
      "loss": 0.0004,
      "step": 24970
    },
    {
      "epoch": 11.559463211476169,
      "grad_norm": 0.7578822374343872,
      "learning_rate": 0.07688107357704767,
      "loss": 0.0005,
      "step": 24980
    },
    {
      "epoch": 11.56409069875058,
      "grad_norm": 0.012434184551239014,
      "learning_rate": 0.07687181860249885,
      "loss": 0.0035,
      "step": 24990
    },
    {
      "epoch": 11.568718186024988,
      "grad_norm": 8.65259075164795,
      "learning_rate": 0.07686256362795002,
      "loss": 0.0035,
      "step": 25000
    },
    {
      "epoch": 11.573345673299398,
      "grad_norm": 0.09355442225933075,
      "learning_rate": 0.0768533086534012,
      "loss": 0.0011,
      "step": 25010
    },
    {
      "epoch": 11.577973160573809,
      "grad_norm": 0.14755110442638397,
      "learning_rate": 0.07684405367885239,
      "loss": 0.0039,
      "step": 25020
    },
    {
      "epoch": 11.582600647848219,
      "grad_norm": 0.0011099525727331638,
      "learning_rate": 0.07683479870430357,
      "loss": 0.0027,
      "step": 25030
    },
    {
      "epoch": 11.58722813512263,
      "grad_norm": 0.0014113407814875245,
      "learning_rate": 0.07682554372975475,
      "loss": 0.0001,
      "step": 25040
    },
    {
      "epoch": 11.591855622397038,
      "grad_norm": 0.0014881342649459839,
      "learning_rate": 0.07681628875520592,
      "loss": 0.0009,
      "step": 25050
    },
    {
      "epoch": 11.596483109671448,
      "grad_norm": 9.45868968963623,
      "learning_rate": 0.07680703378065712,
      "loss": 0.0039,
      "step": 25060
    },
    {
      "epoch": 11.601110596945858,
      "grad_norm": 0.017709840089082718,
      "learning_rate": 0.07679777880610829,
      "loss": 0.0013,
      "step": 25070
    },
    {
      "epoch": 11.605738084220269,
      "grad_norm": 0.21852664649486542,
      "learning_rate": 0.07678852383155947,
      "loss": 0.0005,
      "step": 25080
    },
    {
      "epoch": 11.610365571494679,
      "grad_norm": 0.3590586483478546,
      "learning_rate": 0.07677926885701064,
      "loss": 0.0004,
      "step": 25090
    },
    {
      "epoch": 11.614993058769087,
      "grad_norm": 0.20568223297595978,
      "learning_rate": 0.07677001388246182,
      "loss": 0.0015,
      "step": 25100
    },
    {
      "epoch": 11.619620546043498,
      "grad_norm": 0.34068387746810913,
      "learning_rate": 0.07676075890791301,
      "loss": 0.0004,
      "step": 25110
    },
    {
      "epoch": 11.624248033317908,
      "grad_norm": 0.06856366246938705,
      "learning_rate": 0.07675150393336419,
      "loss": 0.0044,
      "step": 25120
    },
    {
      "epoch": 11.628875520592318,
      "grad_norm": 0.004239187575876713,
      "learning_rate": 0.07674224895881537,
      "loss": 0.0003,
      "step": 25130
    },
    {
      "epoch": 11.633503007866729,
      "grad_norm": 0.004064093343913555,
      "learning_rate": 0.07673299398426654,
      "loss": 0.0003,
      "step": 25140
    },
    {
      "epoch": 11.638130495141139,
      "grad_norm": 0.004977804142981768,
      "learning_rate": 0.07672373900971774,
      "loss": 0.0005,
      "step": 25150
    },
    {
      "epoch": 11.642757982415548,
      "grad_norm": 0.07693234831094742,
      "learning_rate": 0.07671448403516891,
      "loss": 0.0016,
      "step": 25160
    },
    {
      "epoch": 11.647385469689958,
      "grad_norm": 0.016174806281924248,
      "learning_rate": 0.07670522906062009,
      "loss": 0.0008,
      "step": 25170
    },
    {
      "epoch": 11.652012956964368,
      "grad_norm": 0.027679115533828735,
      "learning_rate": 0.07669597408607126,
      "loss": 0.0013,
      "step": 25180
    },
    {
      "epoch": 11.656640444238779,
      "grad_norm": 0.06073547899723053,
      "learning_rate": 0.07668671911152244,
      "loss": 0.0048,
      "step": 25190
    },
    {
      "epoch": 11.661267931513189,
      "grad_norm": 2.268357038497925,
      "learning_rate": 0.07667746413697363,
      "loss": 0.0008,
      "step": 25200
    },
    {
      "epoch": 11.6658954187876,
      "grad_norm": 0.0012236407492309809,
      "learning_rate": 0.07666820916242481,
      "loss": 0.0003,
      "step": 25210
    },
    {
      "epoch": 11.670522906062008,
      "grad_norm": 0.002628159709274769,
      "learning_rate": 0.076658954187876,
      "loss": 0.0006,
      "step": 25220
    },
    {
      "epoch": 11.675150393336418,
      "grad_norm": 0.00025157100753858685,
      "learning_rate": 0.07664969921332716,
      "loss": 0.0002,
      "step": 25230
    },
    {
      "epoch": 11.679777880610828,
      "grad_norm": 0.021088801324367523,
      "learning_rate": 0.07664044423877835,
      "loss": 0.0003,
      "step": 25240
    },
    {
      "epoch": 11.684405367885239,
      "grad_norm": 0.011081693693995476,
      "learning_rate": 0.07663118926422953,
      "loss": 0.0001,
      "step": 25250
    },
    {
      "epoch": 11.689032855159649,
      "grad_norm": 0.009084563702344894,
      "learning_rate": 0.07662193428968071,
      "loss": 0.0006,
      "step": 25260
    },
    {
      "epoch": 11.693660342434057,
      "grad_norm": 0.04636407271027565,
      "learning_rate": 0.07661267931513188,
      "loss": 0.0006,
      "step": 25270
    },
    {
      "epoch": 11.698287829708468,
      "grad_norm": 0.3162980079650879,
      "learning_rate": 0.07660342434058307,
      "loss": 0.0005,
      "step": 25280
    },
    {
      "epoch": 11.702915316982878,
      "grad_norm": 0.0018738266080617905,
      "learning_rate": 0.07659416936603425,
      "loss": 0.0004,
      "step": 25290
    },
    {
      "epoch": 11.707542804257288,
      "grad_norm": 0.00035733357071876526,
      "learning_rate": 0.07658491439148543,
      "loss": 0.0003,
      "step": 25300
    },
    {
      "epoch": 11.712170291531699,
      "grad_norm": 0.006031413562595844,
      "learning_rate": 0.07657565941693661,
      "loss": 0.0002,
      "step": 25310
    },
    {
      "epoch": 11.716797778806109,
      "grad_norm": 0.06608562916517258,
      "learning_rate": 0.07656640444238778,
      "loss": 0.0002,
      "step": 25320
    },
    {
      "epoch": 11.721425266080518,
      "grad_norm": 0.0029731597751379013,
      "learning_rate": 0.07655714946783897,
      "loss": 0.0002,
      "step": 25330
    },
    {
      "epoch": 11.726052753354928,
      "grad_norm": 0.1356624811887741,
      "learning_rate": 0.07654789449329015,
      "loss": 0.0005,
      "step": 25340
    },
    {
      "epoch": 11.730680240629338,
      "grad_norm": 3.7961671352386475,
      "learning_rate": 0.07653863951874133,
      "loss": 0.0012,
      "step": 25350
    },
    {
      "epoch": 11.735307727903749,
      "grad_norm": 0.2146289199590683,
      "learning_rate": 0.0765293845441925,
      "loss": 0.0009,
      "step": 25360
    },
    {
      "epoch": 11.739935215178159,
      "grad_norm": 0.7367137670516968,
      "learning_rate": 0.07652012956964369,
      "loss": 0.0007,
      "step": 25370
    },
    {
      "epoch": 11.74456270245257,
      "grad_norm": 0.005372110288590193,
      "learning_rate": 0.07651087459509487,
      "loss": 0.0025,
      "step": 25380
    },
    {
      "epoch": 11.749190189726978,
      "grad_norm": 0.058070357888936996,
      "learning_rate": 0.07650161962054605,
      "loss": 0.0002,
      "step": 25390
    },
    {
      "epoch": 11.753817677001388,
      "grad_norm": 0.018764367327094078,
      "learning_rate": 0.07649236464599724,
      "loss": 0.0006,
      "step": 25400
    },
    {
      "epoch": 11.758445164275798,
      "grad_norm": 0.03604166954755783,
      "learning_rate": 0.0764831096714484,
      "loss": 0.0002,
      "step": 25410
    },
    {
      "epoch": 11.763072651550209,
      "grad_norm": 0.06867493689060211,
      "learning_rate": 0.07647385469689959,
      "loss": 0.0012,
      "step": 25420
    },
    {
      "epoch": 11.767700138824619,
      "grad_norm": 0.03922247514128685,
      "learning_rate": 0.07646459972235077,
      "loss": 0.0029,
      "step": 25430
    },
    {
      "epoch": 11.772327626099027,
      "grad_norm": 0.020288582891225815,
      "learning_rate": 0.07645534474780195,
      "loss": 0.0001,
      "step": 25440
    },
    {
      "epoch": 11.776955113373438,
      "grad_norm": 0.024524010717868805,
      "learning_rate": 0.07644608977325312,
      "loss": 0.0019,
      "step": 25450
    },
    {
      "epoch": 11.781582600647848,
      "grad_norm": 0.09460584819316864,
      "learning_rate": 0.0764368347987043,
      "loss": 0.0001,
      "step": 25460
    },
    {
      "epoch": 11.786210087922258,
      "grad_norm": 0.0027946087066084146,
      "learning_rate": 0.07642757982415549,
      "loss": 0.0001,
      "step": 25470
    },
    {
      "epoch": 11.790837575196669,
      "grad_norm": 0.006567866075783968,
      "learning_rate": 0.07641832484960667,
      "loss": 0.0001,
      "step": 25480
    },
    {
      "epoch": 11.795465062471077,
      "grad_norm": 0.01045437902212143,
      "learning_rate": 0.07640906987505786,
      "loss": 0.0022,
      "step": 25490
    },
    {
      "epoch": 11.800092549745488,
      "grad_norm": 6.245118618011475,
      "learning_rate": 0.07639981490050902,
      "loss": 0.0036,
      "step": 25500
    },
    {
      "epoch": 11.804720037019898,
      "grad_norm": 0.007348841987550259,
      "learning_rate": 0.07639055992596021,
      "loss": 0.0006,
      "step": 25510
    },
    {
      "epoch": 11.809347524294308,
      "grad_norm": 0.008773505687713623,
      "learning_rate": 0.07638130495141139,
      "loss": 0.0016,
      "step": 25520
    },
    {
      "epoch": 11.813975011568719,
      "grad_norm": 0.006288779899477959,
      "learning_rate": 0.07637204997686257,
      "loss": 0.0003,
      "step": 25530
    },
    {
      "epoch": 11.818602498843129,
      "grad_norm": 0.01612059213221073,
      "learning_rate": 0.07636279500231374,
      "loss": 0.0001,
      "step": 25540
    },
    {
      "epoch": 11.823229986117537,
      "grad_norm": 0.13715291023254395,
      "learning_rate": 0.07635354002776493,
      "loss": 0.0015,
      "step": 25550
    },
    {
      "epoch": 11.827857473391948,
      "grad_norm": 0.003398038214072585,
      "learning_rate": 0.07634428505321611,
      "loss": 0.0001,
      "step": 25560
    },
    {
      "epoch": 11.832484960666358,
      "grad_norm": 0.08703350275754929,
      "learning_rate": 0.0763350300786673,
      "loss": 0.0007,
      "step": 25570
    },
    {
      "epoch": 11.837112447940768,
      "grad_norm": 0.016477230936288834,
      "learning_rate": 0.07632577510411848,
      "loss": 0.0015,
      "step": 25580
    },
    {
      "epoch": 11.841739935215179,
      "grad_norm": 0.26099660992622375,
      "learning_rate": 0.07631652012956965,
      "loss": 0.0006,
      "step": 25590
    },
    {
      "epoch": 11.846367422489589,
      "grad_norm": 0.028109127655625343,
      "learning_rate": 0.07630726515502083,
      "loss": 0.0001,
      "step": 25600
    },
    {
      "epoch": 11.850994909763997,
      "grad_norm": 0.022753041237592697,
      "learning_rate": 0.07629801018047201,
      "loss": 0.001,
      "step": 25610
    },
    {
      "epoch": 11.855622397038408,
      "grad_norm": 0.3162533938884735,
      "learning_rate": 0.0762887552059232,
      "loss": 0.0019,
      "step": 25620
    },
    {
      "epoch": 11.860249884312818,
      "grad_norm": 0.02926912158727646,
      "learning_rate": 0.07627950023137436,
      "loss": 0.0021,
      "step": 25630
    },
    {
      "epoch": 11.864877371587228,
      "grad_norm": 0.00930239912122488,
      "learning_rate": 0.07627024525682555,
      "loss": 0.0001,
      "step": 25640
    },
    {
      "epoch": 11.869504858861639,
      "grad_norm": 0.0007459931075572968,
      "learning_rate": 0.07626099028227673,
      "loss": 0.0001,
      "step": 25650
    },
    {
      "epoch": 11.874132346136047,
      "grad_norm": 0.0041115619242191315,
      "learning_rate": 0.07625173530772791,
      "loss": 0.0003,
      "step": 25660
    },
    {
      "epoch": 11.878759833410458,
      "grad_norm": 0.01872916705906391,
      "learning_rate": 0.0762424803331791,
      "loss": 0.0022,
      "step": 25670
    },
    {
      "epoch": 11.883387320684868,
      "grad_norm": 0.0006148124812170863,
      "learning_rate": 0.07623322535863027,
      "loss": 0.0001,
      "step": 25680
    },
    {
      "epoch": 11.888014807959278,
      "grad_norm": 0.03907093033194542,
      "learning_rate": 0.07622397038408145,
      "loss": 0.0001,
      "step": 25690
    },
    {
      "epoch": 11.892642295233689,
      "grad_norm": 0.01571367122232914,
      "learning_rate": 0.07621471540953262,
      "loss": 0.0002,
      "step": 25700
    },
    {
      "epoch": 11.897269782508099,
      "grad_norm": 0.0022367697674781084,
      "learning_rate": 0.07620546043498382,
      "loss": 0.0001,
      "step": 25710
    },
    {
      "epoch": 11.901897269782507,
      "grad_norm": 0.0055947378277778625,
      "learning_rate": 0.07619620546043498,
      "loss": 0.0003,
      "step": 25720
    },
    {
      "epoch": 11.906524757056918,
      "grad_norm": 0.0021271277219057083,
      "learning_rate": 0.07618695048588617,
      "loss": 0.0016,
      "step": 25730
    },
    {
      "epoch": 11.911152244331328,
      "grad_norm": 0.0020271767862141132,
      "learning_rate": 0.07617769551133735,
      "loss": 0.0001,
      "step": 25740
    },
    {
      "epoch": 11.915779731605738,
      "grad_norm": 0.0038456562906503677,
      "learning_rate": 0.07616844053678853,
      "loss": 0.0002,
      "step": 25750
    },
    {
      "epoch": 11.920407218880149,
      "grad_norm": 0.0015633347211405635,
      "learning_rate": 0.07615918556223972,
      "loss": 0.0024,
      "step": 25760
    },
    {
      "epoch": 11.925034706154559,
      "grad_norm": 0.005204823799431324,
      "learning_rate": 0.07614993058769089,
      "loss": 0.0006,
      "step": 25770
    },
    {
      "epoch": 11.929662193428967,
      "grad_norm": 0.00970220286399126,
      "learning_rate": 0.07614067561314207,
      "loss": 0.0001,
      "step": 25780
    },
    {
      "epoch": 11.934289680703378,
      "grad_norm": 0.0409761480987072,
      "learning_rate": 0.07613142063859324,
      "loss": 0.0001,
      "step": 25790
    },
    {
      "epoch": 11.938917167977788,
      "grad_norm": 0.030637402087450027,
      "learning_rate": 0.07612216566404444,
      "loss": 0.0004,
      "step": 25800
    },
    {
      "epoch": 11.943544655252198,
      "grad_norm": 2.285283088684082,
      "learning_rate": 0.0761129106894956,
      "loss": 0.0011,
      "step": 25810
    },
    {
      "epoch": 11.948172142526609,
      "grad_norm": 0.005739652086049318,
      "learning_rate": 0.07610365571494679,
      "loss": 0.0004,
      "step": 25820
    },
    {
      "epoch": 11.952799629801017,
      "grad_norm": 0.03715667128562927,
      "learning_rate": 0.07609440074039797,
      "loss": 0.0001,
      "step": 25830
    },
    {
      "epoch": 11.957427117075428,
      "grad_norm": 0.002334508579224348,
      "learning_rate": 0.07608514576584915,
      "loss": 0.0001,
      "step": 25840
    },
    {
      "epoch": 11.962054604349838,
      "grad_norm": 0.011999079957604408,
      "learning_rate": 0.07607589079130034,
      "loss": 0.0003,
      "step": 25850
    },
    {
      "epoch": 11.966682091624248,
      "grad_norm": 1.9952033758163452,
      "learning_rate": 0.07606663581675151,
      "loss": 0.0006,
      "step": 25860
    },
    {
      "epoch": 11.971309578898659,
      "grad_norm": 0.0021761972457170486,
      "learning_rate": 0.07605738084220269,
      "loss": 0.0028,
      "step": 25870
    },
    {
      "epoch": 11.975937066173067,
      "grad_norm": 0.0005877177463844419,
      "learning_rate": 0.07604812586765386,
      "loss": 0.0004,
      "step": 25880
    },
    {
      "epoch": 11.980564553447477,
      "grad_norm": 0.31008994579315186,
      "learning_rate": 0.07603887089310506,
      "loss": 0.0008,
      "step": 25890
    },
    {
      "epoch": 11.985192040721888,
      "grad_norm": 0.07200247049331665,
      "learning_rate": 0.07602961591855623,
      "loss": 0.0005,
      "step": 25900
    },
    {
      "epoch": 11.989819527996298,
      "grad_norm": 0.046590983867645264,
      "learning_rate": 0.07602036094400741,
      "loss": 0.0027,
      "step": 25910
    },
    {
      "epoch": 11.994447015270708,
      "grad_norm": 0.005380931776016951,
      "learning_rate": 0.07601110596945859,
      "loss": 0.0006,
      "step": 25920
    },
    {
      "epoch": 11.999074502545119,
      "grad_norm": 16.9246883392334,
      "learning_rate": 0.07600185099490976,
      "loss": 0.0077,
      "step": 25930
    },
    {
      "epoch": 12.0,
      "eval_accuracy_branch1": 0.983592666769373,
      "eval_accuracy_branch2": 0.49946079186566017,
      "eval_f1_branch1": 0.9835613807775054,
      "eval_f1_branch2": 0.49907985645960284,
      "eval_loss": 0.034729067236185074,
      "eval_precision_branch1": 0.9834122719914323,
      "eval_precision_branch2": 0.49945914665180746,
      "eval_recall_branch1": 0.9841829416041591,
      "eval_recall_branch2": 0.49946079186566017,
      "eval_runtime": 28.8808,
      "eval_samples_per_second": 449.503,
      "eval_steps_per_second": 56.197,
      "step": 25932
    },
    {
      "epoch": 12.003701989819527,
      "grad_norm": 0.07652562856674194,
      "learning_rate": 0.07599259602036096,
      "loss": 0.0001,
      "step": 25940
    },
    {
      "epoch": 12.008329477093937,
      "grad_norm": 0.01767447404563427,
      "learning_rate": 0.07598334104581213,
      "loss": 0.0006,
      "step": 25950
    },
    {
      "epoch": 12.012956964368348,
      "grad_norm": 0.004640007391571999,
      "learning_rate": 0.07597408607126331,
      "loss": 0.0005,
      "step": 25960
    },
    {
      "epoch": 12.017584451642758,
      "grad_norm": 0.04659520089626312,
      "learning_rate": 0.07596483109671448,
      "loss": 0.0021,
      "step": 25970
    },
    {
      "epoch": 12.022211938917168,
      "grad_norm": 0.0008142891456373036,
      "learning_rate": 0.07595557612216568,
      "loss": 0.0015,
      "step": 25980
    },
    {
      "epoch": 12.026839426191579,
      "grad_norm": 0.07148801535367966,
      "learning_rate": 0.07594632114761685,
      "loss": 0.0003,
      "step": 25990
    },
    {
      "epoch": 12.031466913465987,
      "grad_norm": 0.004610586445778608,
      "learning_rate": 0.07593706617306803,
      "loss": 0.0015,
      "step": 26000
    },
    {
      "epoch": 12.036094400740398,
      "grad_norm": 4.139122009277344,
      "learning_rate": 0.07592781119851921,
      "loss": 0.0023,
      "step": 26010
    },
    {
      "epoch": 12.040721888014808,
      "grad_norm": 0.00925980694591999,
      "learning_rate": 0.07591855622397038,
      "loss": 0.0024,
      "step": 26020
    },
    {
      "epoch": 12.045349375289218,
      "grad_norm": 0.003928408958017826,
      "learning_rate": 0.07590930124942158,
      "loss": 0.0091,
      "step": 26030
    },
    {
      "epoch": 12.049976862563629,
      "grad_norm": 0.0019240101100876927,
      "learning_rate": 0.07590004627487275,
      "loss": 0.0002,
      "step": 26040
    },
    {
      "epoch": 12.054604349838037,
      "grad_norm": 0.396180123090744,
      "learning_rate": 0.07589079130032393,
      "loss": 0.0071,
      "step": 26050
    },
    {
      "epoch": 12.059231837112447,
      "grad_norm": 4.807583808898926,
      "learning_rate": 0.0758815363257751,
      "loss": 0.0008,
      "step": 26060
    },
    {
      "epoch": 12.063859324386858,
      "grad_norm": 0.10205517709255219,
      "learning_rate": 0.0758722813512263,
      "loss": 0.0003,
      "step": 26070
    },
    {
      "epoch": 12.068486811661268,
      "grad_norm": 0.013915685936808586,
      "learning_rate": 0.07586302637667747,
      "loss": 0.0002,
      "step": 26080
    },
    {
      "epoch": 12.073114298935678,
      "grad_norm": 0.029390579089522362,
      "learning_rate": 0.07585377140212865,
      "loss": 0.0004,
      "step": 26090
    },
    {
      "epoch": 12.077741786210089,
      "grad_norm": 0.41183096170425415,
      "learning_rate": 0.07584451642757983,
      "loss": 0.0022,
      "step": 26100
    },
    {
      "epoch": 12.082369273484497,
      "grad_norm": 0.05340367183089256,
      "learning_rate": 0.075835261453031,
      "loss": 0.0009,
      "step": 26110
    },
    {
      "epoch": 12.086996760758907,
      "grad_norm": 0.8897286057472229,
      "learning_rate": 0.0758260064784822,
      "loss": 0.0005,
      "step": 26120
    },
    {
      "epoch": 12.091624248033318,
      "grad_norm": 0.000874904275406152,
      "learning_rate": 0.07581675150393337,
      "loss": 0.0007,
      "step": 26130
    },
    {
      "epoch": 12.096251735307728,
      "grad_norm": 0.0379592590034008,
      "learning_rate": 0.07580749652938455,
      "loss": 0.0001,
      "step": 26140
    },
    {
      "epoch": 12.100879222582138,
      "grad_norm": 0.15723733603954315,
      "learning_rate": 0.07579824155483572,
      "loss": 0.0024,
      "step": 26150
    },
    {
      "epoch": 12.105506709856549,
      "grad_norm": 1.7501188516616821,
      "learning_rate": 0.0757889865802869,
      "loss": 0.0015,
      "step": 26160
    },
    {
      "epoch": 12.110134197130957,
      "grad_norm": 0.0020237506832927465,
      "learning_rate": 0.07577973160573809,
      "loss": 0.0001,
      "step": 26170
    },
    {
      "epoch": 12.114761684405368,
      "grad_norm": 0.0003291238972451538,
      "learning_rate": 0.07577047663118927,
      "loss": 0.0009,
      "step": 26180
    },
    {
      "epoch": 12.119389171679778,
      "grad_norm": 0.00510323466733098,
      "learning_rate": 0.07576122165664045,
      "loss": 0.0021,
      "step": 26190
    },
    {
      "epoch": 12.124016658954188,
      "grad_norm": 1.9815094470977783,
      "learning_rate": 0.07575196668209162,
      "loss": 0.0048,
      "step": 26200
    },
    {
      "epoch": 12.128644146228599,
      "grad_norm": 0.017600810155272484,
      "learning_rate": 0.07574271170754282,
      "loss": 0.0007,
      "step": 26210
    },
    {
      "epoch": 12.133271633503007,
      "grad_norm": 0.037899199873209,
      "learning_rate": 0.07573345673299399,
      "loss": 0.0005,
      "step": 26220
    },
    {
      "epoch": 12.137899120777417,
      "grad_norm": 2.0503737926483154,
      "learning_rate": 0.07572420175844517,
      "loss": 0.0007,
      "step": 26230
    },
    {
      "epoch": 12.142526608051828,
      "grad_norm": 0.006839328911155462,
      "learning_rate": 0.07571494678389634,
      "loss": 0.0001,
      "step": 26240
    },
    {
      "epoch": 12.147154095326238,
      "grad_norm": 0.028063524514436722,
      "learning_rate": 0.07570569180934753,
      "loss": 0.0001,
      "step": 26250
    },
    {
      "epoch": 12.151781582600648,
      "grad_norm": 0.00130935397464782,
      "learning_rate": 0.07569643683479871,
      "loss": 0.0072,
      "step": 26260
    },
    {
      "epoch": 12.156409069875059,
      "grad_norm": 0.010500806383788586,
      "learning_rate": 0.07568718186024989,
      "loss": 0.0001,
      "step": 26270
    },
    {
      "epoch": 12.161036557149467,
      "grad_norm": 0.0011559438426047564,
      "learning_rate": 0.07567792688570107,
      "loss": 0.0003,
      "step": 26280
    },
    {
      "epoch": 12.165664044423877,
      "grad_norm": 0.052414558827877045,
      "learning_rate": 0.07566867191115224,
      "loss": 0.0034,
      "step": 26290
    },
    {
      "epoch": 12.170291531698288,
      "grad_norm": 0.004649563226848841,
      "learning_rate": 0.07565941693660344,
      "loss": 0.0001,
      "step": 26300
    },
    {
      "epoch": 12.174919018972698,
      "grad_norm": 0.04892468824982643,
      "learning_rate": 0.07565016196205461,
      "loss": 0.0003,
      "step": 26310
    },
    {
      "epoch": 12.179546506247108,
      "grad_norm": 0.26736143231391907,
      "learning_rate": 0.0756409069875058,
      "loss": 0.0011,
      "step": 26320
    },
    {
      "epoch": 12.184173993521517,
      "grad_norm": 0.3678765594959259,
      "learning_rate": 0.07563165201295696,
      "loss": 0.0005,
      "step": 26330
    },
    {
      "epoch": 12.188801480795927,
      "grad_norm": 0.5113741159439087,
      "learning_rate": 0.07562239703840815,
      "loss": 0.0002,
      "step": 26340
    },
    {
      "epoch": 12.193428968070338,
      "grad_norm": 0.30758562684059143,
      "learning_rate": 0.07561314206385933,
      "loss": 0.0008,
      "step": 26350
    },
    {
      "epoch": 12.198056455344748,
      "grad_norm": 3.117286205291748,
      "learning_rate": 0.07560388708931051,
      "loss": 0.0007,
      "step": 26360
    },
    {
      "epoch": 12.202683942619158,
      "grad_norm": 0.11337759345769882,
      "learning_rate": 0.0755946321147617,
      "loss": 0.0003,
      "step": 26370
    },
    {
      "epoch": 12.207311429893569,
      "grad_norm": 0.0012979120947420597,
      "learning_rate": 0.07558537714021286,
      "loss": 0.0003,
      "step": 26380
    },
    {
      "epoch": 12.211938917167977,
      "grad_norm": 0.01552416943013668,
      "learning_rate": 0.07557612216566405,
      "loss": 0.0002,
      "step": 26390
    },
    {
      "epoch": 12.216566404442387,
      "grad_norm": 0.1160445287823677,
      "learning_rate": 0.07556686719111523,
      "loss": 0.002,
      "step": 26400
    },
    {
      "epoch": 12.221193891716798,
      "grad_norm": 0.59360671043396,
      "learning_rate": 0.07555761221656641,
      "loss": 0.0023,
      "step": 26410
    },
    {
      "epoch": 12.225821378991208,
      "grad_norm": 0.041527681052684784,
      "learning_rate": 0.07554835724201758,
      "loss": 0.0028,
      "step": 26420
    },
    {
      "epoch": 12.230448866265618,
      "grad_norm": 0.049828629940748215,
      "learning_rate": 0.07553910226746877,
      "loss": 0.0008,
      "step": 26430
    },
    {
      "epoch": 12.235076353540027,
      "grad_norm": 0.01175912469625473,
      "learning_rate": 0.07552984729291995,
      "loss": 0.0081,
      "step": 26440
    },
    {
      "epoch": 12.239703840814437,
      "grad_norm": 0.01958559826016426,
      "learning_rate": 0.07552059231837113,
      "loss": 0.0001,
      "step": 26450
    },
    {
      "epoch": 12.244331328088847,
      "grad_norm": 0.006707205902785063,
      "learning_rate": 0.07551133734382232,
      "loss": 0.0007,
      "step": 26460
    },
    {
      "epoch": 12.248958815363258,
      "grad_norm": 0.05156870186328888,
      "learning_rate": 0.07550208236927348,
      "loss": 0.0002,
      "step": 26470
    },
    {
      "epoch": 12.253586302637668,
      "grad_norm": 0.003666855860501528,
      "learning_rate": 0.07549282739472467,
      "loss": 0.0006,
      "step": 26480
    },
    {
      "epoch": 12.258213789912078,
      "grad_norm": 0.0006848713965155184,
      "learning_rate": 0.07548357242017585,
      "loss": 0.0002,
      "step": 26490
    },
    {
      "epoch": 12.262841277186487,
      "grad_norm": 0.05316818878054619,
      "learning_rate": 0.07547431744562703,
      "loss": 0.0003,
      "step": 26500
    },
    {
      "epoch": 12.267468764460897,
      "grad_norm": 0.002695816569030285,
      "learning_rate": 0.0754650624710782,
      "loss": 0.0001,
      "step": 26510
    },
    {
      "epoch": 12.272096251735308,
      "grad_norm": 1.2740038633346558,
      "learning_rate": 0.07545580749652939,
      "loss": 0.0004,
      "step": 26520
    },
    {
      "epoch": 12.276723739009718,
      "grad_norm": 0.01306938286870718,
      "learning_rate": 0.07544655252198057,
      "loss": 0.0002,
      "step": 26530
    },
    {
      "epoch": 12.281351226284128,
      "grad_norm": 0.00804950576275587,
      "learning_rate": 0.07543729754743175,
      "loss": 0.0,
      "step": 26540
    },
    {
      "epoch": 12.285978713558539,
      "grad_norm": 0.01120466273277998,
      "learning_rate": 0.07542804257288294,
      "loss": 0.0004,
      "step": 26550
    },
    {
      "epoch": 12.290606200832947,
      "grad_norm": 0.03072364069521427,
      "learning_rate": 0.0754187875983341,
      "loss": 0.0002,
      "step": 26560
    },
    {
      "epoch": 12.295233688107357,
      "grad_norm": 0.009749790653586388,
      "learning_rate": 0.07540953262378529,
      "loss": 0.0001,
      "step": 26570
    },
    {
      "epoch": 12.299861175381768,
      "grad_norm": 0.005641297437250614,
      "learning_rate": 0.07540027764923647,
      "loss": 0.0003,
      "step": 26580
    },
    {
      "epoch": 12.304488662656178,
      "grad_norm": 0.5048208236694336,
      "learning_rate": 0.07539102267468765,
      "loss": 0.0007,
      "step": 26590
    },
    {
      "epoch": 12.309116149930588,
      "grad_norm": 0.001833094283938408,
      "learning_rate": 0.07538176770013882,
      "loss": 0.0003,
      "step": 26600
    },
    {
      "epoch": 12.313743637204997,
      "grad_norm": 0.0171654112637043,
      "learning_rate": 0.07537251272559001,
      "loss": 0.0009,
      "step": 26610
    },
    {
      "epoch": 12.318371124479407,
      "grad_norm": 0.002610601484775543,
      "learning_rate": 0.07536325775104119,
      "loss": 0.0068,
      "step": 26620
    },
    {
      "epoch": 12.322998611753817,
      "grad_norm": 0.00046804541489109397,
      "learning_rate": 0.07535400277649237,
      "loss": 0.0004,
      "step": 26630
    },
    {
      "epoch": 12.327626099028228,
      "grad_norm": 0.573724091053009,
      "learning_rate": 0.07534474780194356,
      "loss": 0.0004,
      "step": 26640
    },
    {
      "epoch": 12.332253586302638,
      "grad_norm": 0.0014644424663856626,
      "learning_rate": 0.07533549282739473,
      "loss": 0.0007,
      "step": 26650
    },
    {
      "epoch": 12.336881073577048,
      "grad_norm": 0.12404563277959824,
      "learning_rate": 0.07532623785284591,
      "loss": 0.001,
      "step": 26660
    },
    {
      "epoch": 12.341508560851457,
      "grad_norm": 0.0021431492641568184,
      "learning_rate": 0.07531698287829709,
      "loss": 0.0001,
      "step": 26670
    },
    {
      "epoch": 12.346136048125867,
      "grad_norm": 0.022228579968214035,
      "learning_rate": 0.07530772790374828,
      "loss": 0.0001,
      "step": 26680
    },
    {
      "epoch": 12.350763535400278,
      "grad_norm": 0.00036657991586253047,
      "learning_rate": 0.07529847292919944,
      "loss": 0.0006,
      "step": 26690
    },
    {
      "epoch": 12.355391022674688,
      "grad_norm": 11.326581001281738,
      "learning_rate": 0.07528921795465063,
      "loss": 0.0093,
      "step": 26700
    },
    {
      "epoch": 12.360018509949098,
      "grad_norm": 0.02320355921983719,
      "learning_rate": 0.07527996298010181,
      "loss": 0.0059,
      "step": 26710
    },
    {
      "epoch": 12.364645997223509,
      "grad_norm": 0.00490647554397583,
      "learning_rate": 0.075270708005553,
      "loss": 0.0007,
      "step": 26720
    },
    {
      "epoch": 12.369273484497917,
      "grad_norm": 0.19336998462677002,
      "learning_rate": 0.07526145303100418,
      "loss": 0.0004,
      "step": 26730
    },
    {
      "epoch": 12.373900971772327,
      "grad_norm": 0.0034126925747841597,
      "learning_rate": 0.07525219805645535,
      "loss": 0.0016,
      "step": 26740
    },
    {
      "epoch": 12.378528459046738,
      "grad_norm": 0.0485217459499836,
      "learning_rate": 0.07524294308190653,
      "loss": 0.0001,
      "step": 26750
    },
    {
      "epoch": 12.383155946321148,
      "grad_norm": 0.01186278648674488,
      "learning_rate": 0.07523368810735771,
      "loss": 0.0001,
      "step": 26760
    },
    {
      "epoch": 12.387783433595558,
      "grad_norm": 0.009022693149745464,
      "learning_rate": 0.0752244331328089,
      "loss": 0.0034,
      "step": 26770
    },
    {
      "epoch": 12.392410920869967,
      "grad_norm": 0.029152870178222656,
      "learning_rate": 0.07521517815826007,
      "loss": 0.0001,
      "step": 26780
    },
    {
      "epoch": 12.397038408144377,
      "grad_norm": 0.05370934680104256,
      "learning_rate": 0.07520592318371125,
      "loss": 0.0004,
      "step": 26790
    },
    {
      "epoch": 12.401665895418787,
      "grad_norm": 0.011843562126159668,
      "learning_rate": 0.07519666820916243,
      "loss": 0.0001,
      "step": 26800
    },
    {
      "epoch": 12.406293382693198,
      "grad_norm": 0.00819134246557951,
      "learning_rate": 0.07518741323461361,
      "loss": 0.0003,
      "step": 26810
    },
    {
      "epoch": 12.410920869967608,
      "grad_norm": 0.006065100431442261,
      "learning_rate": 0.0751781582600648,
      "loss": 0.0,
      "step": 26820
    },
    {
      "epoch": 12.415548357242018,
      "grad_norm": 0.14439339935779572,
      "learning_rate": 0.07516890328551597,
      "loss": 0.0001,
      "step": 26830
    },
    {
      "epoch": 12.420175844516427,
      "grad_norm": 0.3735574781894684,
      "learning_rate": 0.07515964831096715,
      "loss": 0.0008,
      "step": 26840
    },
    {
      "epoch": 12.424803331790837,
      "grad_norm": 0.005582353565841913,
      "learning_rate": 0.07515039333641832,
      "loss": 0.0001,
      "step": 26850
    },
    {
      "epoch": 12.429430819065248,
      "grad_norm": 0.009939481504261494,
      "learning_rate": 0.07514113836186952,
      "loss": 0.0,
      "step": 26860
    },
    {
      "epoch": 12.434058306339658,
      "grad_norm": 0.10516560077667236,
      "learning_rate": 0.07513188338732069,
      "loss": 0.0001,
      "step": 26870
    },
    {
      "epoch": 12.438685793614068,
      "grad_norm": 0.001279969234019518,
      "learning_rate": 0.07512262841277187,
      "loss": 0.0001,
      "step": 26880
    },
    {
      "epoch": 12.443313280888477,
      "grad_norm": 0.0053840684704482555,
      "learning_rate": 0.07511337343822305,
      "loss": 0.0005,
      "step": 26890
    },
    {
      "epoch": 12.447940768162887,
      "grad_norm": 0.00139409932307899,
      "learning_rate": 0.07510411846367424,
      "loss": 0.0015,
      "step": 26900
    },
    {
      "epoch": 12.452568255437297,
      "grad_norm": 0.03167624771595001,
      "learning_rate": 0.07509486348912542,
      "loss": 0.0032,
      "step": 26910
    },
    {
      "epoch": 12.457195742711708,
      "grad_norm": 0.08447067439556122,
      "learning_rate": 0.07508560851457659,
      "loss": 0.0008,
      "step": 26920
    },
    {
      "epoch": 12.461823229986118,
      "grad_norm": 2.6375439167022705,
      "learning_rate": 0.07507635354002777,
      "loss": 0.0011,
      "step": 26930
    },
    {
      "epoch": 12.466450717260528,
      "grad_norm": 0.010401534847915173,
      "learning_rate": 0.07506709856547894,
      "loss": 0.0001,
      "step": 26940
    },
    {
      "epoch": 12.471078204534937,
      "grad_norm": 0.003993187565356493,
      "learning_rate": 0.07505784359093014,
      "loss": 0.0001,
      "step": 26950
    },
    {
      "epoch": 12.475705691809347,
      "grad_norm": 0.0032682272139936686,
      "learning_rate": 0.0750485886163813,
      "loss": 0.001,
      "step": 26960
    },
    {
      "epoch": 12.480333179083757,
      "grad_norm": 0.00232774019241333,
      "learning_rate": 0.07503933364183249,
      "loss": 0.0082,
      "step": 26970
    },
    {
      "epoch": 12.484960666358168,
      "grad_norm": 1.984826922416687,
      "learning_rate": 0.07503007866728367,
      "loss": 0.0017,
      "step": 26980
    },
    {
      "epoch": 12.489588153632578,
      "grad_norm": 0.0018219996709376574,
      "learning_rate": 0.07502082369273486,
      "loss": 0.0035,
      "step": 26990
    },
    {
      "epoch": 12.494215640906987,
      "grad_norm": 0.029613513499498367,
      "learning_rate": 0.07501156871818604,
      "loss": 0.0005,
      "step": 27000
    },
    {
      "epoch": 12.498843128181397,
      "grad_norm": 0.0139455022290349,
      "learning_rate": 0.07500231374363721,
      "loss": 0.0045,
      "step": 27010
    },
    {
      "epoch": 12.503470615455807,
      "grad_norm": 0.01047162339091301,
      "learning_rate": 0.07499305876908839,
      "loss": 0.001,
      "step": 27020
    },
    {
      "epoch": 12.508098102730218,
      "grad_norm": 3.595496416091919,
      "learning_rate": 0.07498380379453956,
      "loss": 0.0036,
      "step": 27030
    },
    {
      "epoch": 12.512725590004628,
      "grad_norm": 0.004334260709583759,
      "learning_rate": 0.07497454881999076,
      "loss": 0.0003,
      "step": 27040
    },
    {
      "epoch": 12.517353077279038,
      "grad_norm": 0.013311275281012058,
      "learning_rate": 0.07496529384544193,
      "loss": 0.0002,
      "step": 27050
    },
    {
      "epoch": 12.521980564553447,
      "grad_norm": 1.939039707183838,
      "learning_rate": 0.07495603887089311,
      "loss": 0.001,
      "step": 27060
    },
    {
      "epoch": 12.526608051827857,
      "grad_norm": 0.051968105137348175,
      "learning_rate": 0.07494678389634428,
      "loss": 0.0003,
      "step": 27070
    },
    {
      "epoch": 12.531235539102267,
      "grad_norm": 0.07404603064060211,
      "learning_rate": 0.07493752892179546,
      "loss": 0.0,
      "step": 27080
    },
    {
      "epoch": 12.535863026376678,
      "grad_norm": 0.003113485872745514,
      "learning_rate": 0.07492827394724665,
      "loss": 0.0004,
      "step": 27090
    },
    {
      "epoch": 12.540490513651088,
      "grad_norm": 0.0017192699015140533,
      "learning_rate": 0.07491901897269783,
      "loss": 0.0002,
      "step": 27100
    },
    {
      "epoch": 12.545118000925498,
      "grad_norm": 3.5067222118377686,
      "learning_rate": 0.07490976399814901,
      "loss": 0.001,
      "step": 27110
    },
    {
      "epoch": 12.549745488199907,
      "grad_norm": 0.0429854653775692,
      "learning_rate": 0.07490050902360018,
      "loss": 0.0001,
      "step": 27120
    },
    {
      "epoch": 12.554372975474317,
      "grad_norm": 0.023333080112934113,
      "learning_rate": 0.07489125404905138,
      "loss": 0.0008,
      "step": 27130
    },
    {
      "epoch": 12.559000462748728,
      "grad_norm": 0.01209182757884264,
      "learning_rate": 0.07488199907450255,
      "loss": 0.005,
      "step": 27140
    },
    {
      "epoch": 12.563627950023138,
      "grad_norm": 0.0009247971465811133,
      "learning_rate": 0.07487274409995373,
      "loss": 0.0002,
      "step": 27150
    },
    {
      "epoch": 12.568255437297548,
      "grad_norm": 0.3657504916191101,
      "learning_rate": 0.0748634891254049,
      "loss": 0.0003,
      "step": 27160
    },
    {
      "epoch": 12.572882924571957,
      "grad_norm": 0.012183954007923603,
      "learning_rate": 0.07485423415085608,
      "loss": 0.0002,
      "step": 27170
    },
    {
      "epoch": 12.577510411846367,
      "grad_norm": 1.9945987462997437,
      "learning_rate": 0.07484497917630727,
      "loss": 0.0017,
      "step": 27180
    },
    {
      "epoch": 12.582137899120777,
      "grad_norm": 0.004826193209737539,
      "learning_rate": 0.07483572420175845,
      "loss": 0.0002,
      "step": 27190
    },
    {
      "epoch": 12.586765386395188,
      "grad_norm": 0.006742830388247967,
      "learning_rate": 0.07482646922720963,
      "loss": 0.0004,
      "step": 27200
    },
    {
      "epoch": 12.591392873669598,
      "grad_norm": 0.04960212856531143,
      "learning_rate": 0.0748172142526608,
      "loss": 0.0028,
      "step": 27210
    },
    {
      "epoch": 12.596020360944008,
      "grad_norm": 0.0021413620561361313,
      "learning_rate": 0.074807959278112,
      "loss": 0.0005,
      "step": 27220
    },
    {
      "epoch": 12.600647848218417,
      "grad_norm": 1.5988045930862427,
      "learning_rate": 0.07479870430356317,
      "loss": 0.0018,
      "step": 27230
    },
    {
      "epoch": 12.605275335492827,
      "grad_norm": 10.94823932647705,
      "learning_rate": 0.07478944932901435,
      "loss": 0.0127,
      "step": 27240
    },
    {
      "epoch": 12.609902822767237,
      "grad_norm": 0.004403652157634497,
      "learning_rate": 0.07478019435446552,
      "loss": 0.0002,
      "step": 27250
    },
    {
      "epoch": 12.614530310041648,
      "grad_norm": 0.004316193051636219,
      "learning_rate": 0.0747709393799167,
      "loss": 0.0005,
      "step": 27260
    },
    {
      "epoch": 12.619157797316058,
      "grad_norm": 0.41084328293800354,
      "learning_rate": 0.07476168440536789,
      "loss": 0.0007,
      "step": 27270
    },
    {
      "epoch": 12.623785284590467,
      "grad_norm": 0.0727141872048378,
      "learning_rate": 0.07475242943081907,
      "loss": 0.0002,
      "step": 27280
    },
    {
      "epoch": 12.628412771864877,
      "grad_norm": 0.10770463943481445,
      "learning_rate": 0.07474317445627025,
      "loss": 0.0001,
      "step": 27290
    },
    {
      "epoch": 12.633040259139287,
      "grad_norm": 6.503815650939941,
      "learning_rate": 0.07473391948172142,
      "loss": 0.0055,
      "step": 27300
    },
    {
      "epoch": 12.637667746413698,
      "grad_norm": 0.12191729247570038,
      "learning_rate": 0.0747246645071726,
      "loss": 0.0002,
      "step": 27310
    },
    {
      "epoch": 12.642295233688108,
      "grad_norm": 0.029174620285630226,
      "learning_rate": 0.07471540953262379,
      "loss": 0.0008,
      "step": 27320
    },
    {
      "epoch": 12.646922720962518,
      "grad_norm": 0.0018013613298535347,
      "learning_rate": 0.07470615455807497,
      "loss": 0.0001,
      "step": 27330
    },
    {
      "epoch": 12.651550208236927,
      "grad_norm": 0.002194109372794628,
      "learning_rate": 0.07469689958352614,
      "loss": 0.0023,
      "step": 27340
    },
    {
      "epoch": 12.656177695511337,
      "grad_norm": 0.0971592590212822,
      "learning_rate": 0.07468764460897732,
      "loss": 0.0001,
      "step": 27350
    },
    {
      "epoch": 12.660805182785747,
      "grad_norm": 0.006317011546343565,
      "learning_rate": 0.07467838963442851,
      "loss": 0.0002,
      "step": 27360
    },
    {
      "epoch": 12.665432670060158,
      "grad_norm": 0.12590284645557404,
      "learning_rate": 0.07466913465987969,
      "loss": 0.0002,
      "step": 27370
    },
    {
      "epoch": 12.670060157334568,
      "grad_norm": 0.09890034794807434,
      "learning_rate": 0.07465987968533087,
      "loss": 0.0004,
      "step": 27380
    },
    {
      "epoch": 12.674687644608976,
      "grad_norm": 0.016792474314570427,
      "learning_rate": 0.07465062471078204,
      "loss": 0.0006,
      "step": 27390
    },
    {
      "epoch": 12.679315131883387,
      "grad_norm": 0.08543047308921814,
      "learning_rate": 0.07464136973623323,
      "loss": 0.0005,
      "step": 27400
    },
    {
      "epoch": 12.683942619157797,
      "grad_norm": 0.04316537454724312,
      "learning_rate": 0.07463211476168441,
      "loss": 0.0005,
      "step": 27410
    },
    {
      "epoch": 12.688570106432207,
      "grad_norm": 0.047227442264556885,
      "learning_rate": 0.07462285978713559,
      "loss": 0.0018,
      "step": 27420
    },
    {
      "epoch": 12.693197593706618,
      "grad_norm": 0.005655368324369192,
      "learning_rate": 0.07461360481258676,
      "loss": 0.0004,
      "step": 27430
    },
    {
      "epoch": 12.697825080981028,
      "grad_norm": 0.02933485060930252,
      "learning_rate": 0.07460434983803794,
      "loss": 0.0008,
      "step": 27440
    },
    {
      "epoch": 12.702452568255437,
      "grad_norm": 0.00963125005364418,
      "learning_rate": 0.07459509486348913,
      "loss": 0.0002,
      "step": 27450
    },
    {
      "epoch": 12.707080055529847,
      "grad_norm": 0.017829295247793198,
      "learning_rate": 0.07458583988894031,
      "loss": 0.0023,
      "step": 27460
    },
    {
      "epoch": 12.711707542804257,
      "grad_norm": 0.01706848479807377,
      "learning_rate": 0.0745765849143915,
      "loss": 0.0002,
      "step": 27470
    },
    {
      "epoch": 12.716335030078668,
      "grad_norm": 0.00459654163569212,
      "learning_rate": 0.07456732993984266,
      "loss": 0.0002,
      "step": 27480
    },
    {
      "epoch": 12.720962517353078,
      "grad_norm": 0.037497539073228836,
      "learning_rate": 0.07455807496529385,
      "loss": 0.0001,
      "step": 27490
    },
    {
      "epoch": 12.725590004627488,
      "grad_norm": 0.0038524032570421696,
      "learning_rate": 0.07454881999074503,
      "loss": 0.0001,
      "step": 27500
    },
    {
      "epoch": 12.730217491901897,
      "grad_norm": 4.643564224243164,
      "learning_rate": 0.07453956501619621,
      "loss": 0.0031,
      "step": 27510
    },
    {
      "epoch": 12.734844979176307,
      "grad_norm": 0.010996241122484207,
      "learning_rate": 0.07453031004164738,
      "loss": 0.0004,
      "step": 27520
    },
    {
      "epoch": 12.739472466450717,
      "grad_norm": 0.0035518906079232693,
      "learning_rate": 0.07452105506709857,
      "loss": 0.0001,
      "step": 27530
    },
    {
      "epoch": 12.744099953725128,
      "grad_norm": 0.7340753674507141,
      "learning_rate": 0.07451180009254975,
      "loss": 0.0009,
      "step": 27540
    },
    {
      "epoch": 12.748727440999538,
      "grad_norm": 1.1860098838806152,
      "learning_rate": 0.07450254511800093,
      "loss": 0.0007,
      "step": 27550
    },
    {
      "epoch": 12.753354928273946,
      "grad_norm": 0.04181082546710968,
      "learning_rate": 0.07449329014345211,
      "loss": 0.0001,
      "step": 27560
    },
    {
      "epoch": 12.757982415548357,
      "grad_norm": 0.014459045603871346,
      "learning_rate": 0.07448403516890328,
      "loss": 0.0004,
      "step": 27570
    },
    {
      "epoch": 12.762609902822767,
      "grad_norm": 0.00922314915806055,
      "learning_rate": 0.07447478019435447,
      "loss": 0.0003,
      "step": 27580
    },
    {
      "epoch": 12.767237390097177,
      "grad_norm": 0.011975112371146679,
      "learning_rate": 0.07446552521980565,
      "loss": 0.0002,
      "step": 27590
    },
    {
      "epoch": 12.771864877371588,
      "grad_norm": 0.004592002369463444,
      "learning_rate": 0.07445627024525683,
      "loss": 0.0035,
      "step": 27600
    },
    {
      "epoch": 12.776492364645998,
      "grad_norm": 0.04343056678771973,
      "learning_rate": 0.074447015270708,
      "loss": 0.0001,
      "step": 27610
    },
    {
      "epoch": 12.781119851920407,
      "grad_norm": 3.027763843536377,
      "learning_rate": 0.07443776029615919,
      "loss": 0.0013,
      "step": 27620
    },
    {
      "epoch": 12.785747339194817,
      "grad_norm": 0.0701015293598175,
      "learning_rate": 0.07442850532161037,
      "loss": 0.0013,
      "step": 27630
    },
    {
      "epoch": 12.790374826469227,
      "grad_norm": 0.008593352511525154,
      "learning_rate": 0.07441925034706155,
      "loss": 0.0002,
      "step": 27640
    },
    {
      "epoch": 12.795002313743638,
      "grad_norm": 3.486757755279541,
      "learning_rate": 0.07440999537251274,
      "loss": 0.006,
      "step": 27650
    },
    {
      "epoch": 12.799629801018048,
      "grad_norm": 0.0034653539769351482,
      "learning_rate": 0.0744007403979639,
      "loss": 0.0007,
      "step": 27660
    },
    {
      "epoch": 12.804257288292458,
      "grad_norm": 5.116995334625244,
      "learning_rate": 0.07439148542341509,
      "loss": 0.0018,
      "step": 27670
    },
    {
      "epoch": 12.808884775566867,
      "grad_norm": 0.9697750210762024,
      "learning_rate": 0.07438223044886627,
      "loss": 0.0005,
      "step": 27680
    },
    {
      "epoch": 12.813512262841277,
      "grad_norm": 0.011801129207015038,
      "learning_rate": 0.07437297547431745,
      "loss": 0.0028,
      "step": 27690
    },
    {
      "epoch": 12.818139750115687,
      "grad_norm": 0.006643501576036215,
      "learning_rate": 0.07436372049976862,
      "loss": 0.0004,
      "step": 27700
    },
    {
      "epoch": 12.822767237390098,
      "grad_norm": 0.0012148492969572544,
      "learning_rate": 0.0743544655252198,
      "loss": 0.0,
      "step": 27710
    },
    {
      "epoch": 12.827394724664508,
      "grad_norm": 0.012055542320013046,
      "learning_rate": 0.07434521055067099,
      "loss": 0.0003,
      "step": 27720
    },
    {
      "epoch": 12.832022211938916,
      "grad_norm": 0.02570533938705921,
      "learning_rate": 0.07433595557612217,
      "loss": 0.0,
      "step": 27730
    },
    {
      "epoch": 12.836649699213327,
      "grad_norm": 10.578978538513184,
      "learning_rate": 0.07432670060157336,
      "loss": 0.0042,
      "step": 27740
    },
    {
      "epoch": 12.841277186487737,
      "grad_norm": 0.02685488760471344,
      "learning_rate": 0.07431744562702453,
      "loss": 0.0003,
      "step": 27750
    },
    {
      "epoch": 12.845904673762147,
      "grad_norm": 0.0015466049080714583,
      "learning_rate": 0.07430819065247571,
      "loss": 0.0008,
      "step": 27760
    },
    {
      "epoch": 12.850532161036558,
      "grad_norm": 0.0023826053366065025,
      "learning_rate": 0.07429893567792688,
      "loss": 0.0001,
      "step": 27770
    },
    {
      "epoch": 12.855159648310966,
      "grad_norm": 0.007761843036860228,
      "learning_rate": 0.07428968070337807,
      "loss": 0.0001,
      "step": 27780
    },
    {
      "epoch": 12.859787135585377,
      "grad_norm": 0.058606285601854324,
      "learning_rate": 0.07428042572882924,
      "loss": 0.0001,
      "step": 27790
    },
    {
      "epoch": 12.864414622859787,
      "grad_norm": 0.05237727612257004,
      "learning_rate": 0.07427117075428043,
      "loss": 0.0004,
      "step": 27800
    },
    {
      "epoch": 12.869042110134197,
      "grad_norm": 0.22182898223400116,
      "learning_rate": 0.07426191577973161,
      "loss": 0.0002,
      "step": 27810
    },
    {
      "epoch": 12.873669597408608,
      "grad_norm": 0.0007184315472841263,
      "learning_rate": 0.0742526608051828,
      "loss": 0.0009,
      "step": 27820
    },
    {
      "epoch": 12.878297084683018,
      "grad_norm": 0.0025528031401336193,
      "learning_rate": 0.07424340583063398,
      "loss": 0.0002,
      "step": 27830
    },
    {
      "epoch": 12.882924571957426,
      "grad_norm": 0.029767172411084175,
      "learning_rate": 0.07423415085608515,
      "loss": 0.0001,
      "step": 27840
    },
    {
      "epoch": 12.887552059231837,
      "grad_norm": 0.02699652686715126,
      "learning_rate": 0.07422489588153633,
      "loss": 0.001,
      "step": 27850
    },
    {
      "epoch": 12.892179546506247,
      "grad_norm": 0.011270332150161266,
      "learning_rate": 0.0742156409069875,
      "loss": 0.0013,
      "step": 27860
    },
    {
      "epoch": 12.896807033780657,
      "grad_norm": 0.021269220858812332,
      "learning_rate": 0.0742063859324387,
      "loss": 0.0001,
      "step": 27870
    },
    {
      "epoch": 12.901434521055068,
      "grad_norm": 2.5262162685394287,
      "learning_rate": 0.07419713095788986,
      "loss": 0.0007,
      "step": 27880
    },
    {
      "epoch": 12.906062008329478,
      "grad_norm": 0.00228324206545949,
      "learning_rate": 0.07418787598334105,
      "loss": 0.0002,
      "step": 27890
    },
    {
      "epoch": 12.910689495603886,
      "grad_norm": 0.016940593719482422,
      "learning_rate": 0.07417862100879223,
      "loss": 0.0006,
      "step": 27900
    },
    {
      "epoch": 12.915316982878297,
      "grad_norm": 0.023997608572244644,
      "learning_rate": 0.07416936603424341,
      "loss": 0.0006,
      "step": 27910
    },
    {
      "epoch": 12.919944470152707,
      "grad_norm": 0.04249519109725952,
      "learning_rate": 0.0741601110596946,
      "loss": 0.0055,
      "step": 27920
    },
    {
      "epoch": 12.924571957427117,
      "grad_norm": 0.031196270138025284,
      "learning_rate": 0.07415085608514577,
      "loss": 0.0001,
      "step": 27930
    },
    {
      "epoch": 12.929199444701528,
      "grad_norm": 0.030080726370215416,
      "learning_rate": 0.07414160111059695,
      "loss": 0.0004,
      "step": 27940
    },
    {
      "epoch": 12.933826931975936,
      "grad_norm": 0.01322992891073227,
      "learning_rate": 0.07413234613604812,
      "loss": 0.0007,
      "step": 27950
    },
    {
      "epoch": 12.938454419250347,
      "grad_norm": 0.023526810109615326,
      "learning_rate": 0.07412309116149932,
      "loss": 0.0021,
      "step": 27960
    },
    {
      "epoch": 12.943081906524757,
      "grad_norm": 0.013624368235468864,
      "learning_rate": 0.07411383618695049,
      "loss": 0.0009,
      "step": 27970
    },
    {
      "epoch": 12.947709393799167,
      "grad_norm": 0.003420391818508506,
      "learning_rate": 0.07410458121240167,
      "loss": 0.0006,
      "step": 27980
    },
    {
      "epoch": 12.952336881073578,
      "grad_norm": 0.008506439626216888,
      "learning_rate": 0.07409532623785285,
      "loss": 0.0008,
      "step": 27990
    },
    {
      "epoch": 12.956964368347988,
      "grad_norm": 0.05191224440932274,
      "learning_rate": 0.07408607126330402,
      "loss": 0.0009,
      "step": 28000
    },
    {
      "epoch": 12.961591855622396,
      "grad_norm": 0.011683708056807518,
      "learning_rate": 0.07407681628875522,
      "loss": 0.0004,
      "step": 28010
    },
    {
      "epoch": 12.966219342896807,
      "grad_norm": 2.5307350158691406,
      "learning_rate": 0.07406756131420639,
      "loss": 0.0006,
      "step": 28020
    },
    {
      "epoch": 12.970846830171217,
      "grad_norm": 0.0020369088742882013,
      "learning_rate": 0.07405830633965757,
      "loss": 0.0003,
      "step": 28030
    },
    {
      "epoch": 12.975474317445627,
      "grad_norm": 0.008321794681251049,
      "learning_rate": 0.07404905136510874,
      "loss": 0.0021,
      "step": 28040
    },
    {
      "epoch": 12.980101804720038,
      "grad_norm": 0.004272689111530781,
      "learning_rate": 0.07403979639055994,
      "loss": 0.0004,
      "step": 28050
    },
    {
      "epoch": 12.984729291994448,
      "grad_norm": 0.41060930490493774,
      "learning_rate": 0.0740305414160111,
      "loss": 0.0002,
      "step": 28060
    },
    {
      "epoch": 12.989356779268856,
      "grad_norm": 0.044171467423439026,
      "learning_rate": 0.07402128644146229,
      "loss": 0.0004,
      "step": 28070
    },
    {
      "epoch": 12.993984266543267,
      "grad_norm": 0.8291472792625427,
      "learning_rate": 0.07401203146691347,
      "loss": 0.0012,
      "step": 28080
    },
    {
      "epoch": 12.998611753817677,
      "grad_norm": 0.0005691245896741748,
      "learning_rate": 0.07400277649236464,
      "loss": 0.0037,
      "step": 28090
    },
    {
      "epoch": 13.0,
      "eval_accuracy_branch1": 0.9860576182406409,
      "eval_accuracy_branch2": 0.49922970266522876,
      "eval_f1_branch1": 0.9868341829256042,
      "eval_f1_branch2": 0.4989779583914985,
      "eval_loss": 0.03547501936554909,
      "eval_precision_branch1": 0.9870812532978774,
      "eval_precision_branch2": 0.499228151368423,
      "eval_recall_branch1": 0.98680826490254,
      "eval_recall_branch2": 0.4992297026652288,
      "eval_runtime": 28.9845,
      "eval_samples_per_second": 447.895,
      "eval_steps_per_second": 55.996,
      "step": 28093
    },
    {
      "epoch": 13.003239241092087,
      "grad_norm": 0.02475171722471714,
      "learning_rate": 0.07399352151781584,
      "loss": 0.003,
      "step": 28100
    },
    {
      "epoch": 13.007866728366498,
      "grad_norm": 0.04518703743815422,
      "learning_rate": 0.07398426654326701,
      "loss": 0.002,
      "step": 28110
    },
    {
      "epoch": 13.012494215640906,
      "grad_norm": 0.034727297723293304,
      "learning_rate": 0.07397501156871819,
      "loss": 0.0009,
      "step": 28120
    },
    {
      "epoch": 13.017121702915317,
      "grad_norm": 0.008117033168673515,
      "learning_rate": 0.07396575659416936,
      "loss": 0.0002,
      "step": 28130
    },
    {
      "epoch": 13.021749190189727,
      "grad_norm": 0.008360279724001884,
      "learning_rate": 0.07395650161962056,
      "loss": 0.0005,
      "step": 28140
    },
    {
      "epoch": 13.026376677464137,
      "grad_norm": 0.04834964498877525,
      "learning_rate": 0.07394724664507173,
      "loss": 0.0024,
      "step": 28150
    },
    {
      "epoch": 13.031004164738548,
      "grad_norm": 0.001093146507628262,
      "learning_rate": 0.07393799167052291,
      "loss": 0.0005,
      "step": 28160
    },
    {
      "epoch": 13.035631652012958,
      "grad_norm": 0.012097273021936417,
      "learning_rate": 0.07392873669597409,
      "loss": 0.0005,
      "step": 28170
    },
    {
      "epoch": 13.040259139287366,
      "grad_norm": 0.006553033832460642,
      "learning_rate": 0.07391948172142526,
      "loss": 0.0002,
      "step": 28180
    },
    {
      "epoch": 13.044886626561777,
      "grad_norm": 0.47002315521240234,
      "learning_rate": 0.07391022674687646,
      "loss": 0.0003,
      "step": 28190
    },
    {
      "epoch": 13.049514113836187,
      "grad_norm": 0.0015734027838334441,
      "learning_rate": 0.07390097177232763,
      "loss": 0.0005,
      "step": 28200
    },
    {
      "epoch": 13.054141601110597,
      "grad_norm": 0.0012386705493554473,
      "learning_rate": 0.07389171679777881,
      "loss": 0.0015,
      "step": 28210
    },
    {
      "epoch": 13.058769088385008,
      "grad_norm": 0.004933077841997147,
      "learning_rate": 0.07388246182322998,
      "loss": 0.0003,
      "step": 28220
    },
    {
      "epoch": 13.063396575659416,
      "grad_norm": 0.01129495445638895,
      "learning_rate": 0.07387320684868116,
      "loss": 0.0004,
      "step": 28230
    },
    {
      "epoch": 13.068024062933826,
      "grad_norm": 0.004725296050310135,
      "learning_rate": 0.07386395187413235,
      "loss": 0.0006,
      "step": 28240
    },
    {
      "epoch": 13.072651550208237,
      "grad_norm": 1.232873797416687,
      "learning_rate": 0.07385469689958353,
      "loss": 0.0013,
      "step": 28250
    },
    {
      "epoch": 13.077279037482647,
      "grad_norm": 0.027236388996243477,
      "learning_rate": 0.07384544192503471,
      "loss": 0.0001,
      "step": 28260
    },
    {
      "epoch": 13.081906524757057,
      "grad_norm": 0.01893877610564232,
      "learning_rate": 0.07383618695048588,
      "loss": 0.0025,
      "step": 28270
    },
    {
      "epoch": 13.086534012031468,
      "grad_norm": 0.03943408653140068,
      "learning_rate": 0.07382693197593708,
      "loss": 0.0001,
      "step": 28280
    },
    {
      "epoch": 13.091161499305876,
      "grad_norm": 0.25729086995124817,
      "learning_rate": 0.07381767700138825,
      "loss": 0.0002,
      "step": 28290
    },
    {
      "epoch": 13.095788986580287,
      "grad_norm": 0.15803766250610352,
      "learning_rate": 0.07380842202683943,
      "loss": 0.0013,
      "step": 28300
    },
    {
      "epoch": 13.100416473854697,
      "grad_norm": 0.0007215825608000159,
      "learning_rate": 0.0737991670522906,
      "loss": 0.0001,
      "step": 28310
    },
    {
      "epoch": 13.105043961129107,
      "grad_norm": 0.016188252717256546,
      "learning_rate": 0.07378991207774178,
      "loss": 0.0001,
      "step": 28320
    },
    {
      "epoch": 13.109671448403518,
      "grad_norm": 0.005973208229988813,
      "learning_rate": 0.07378065710319297,
      "loss": 0.0,
      "step": 28330
    },
    {
      "epoch": 13.114298935677926,
      "grad_norm": 0.47002673149108887,
      "learning_rate": 0.07377140212864415,
      "loss": 0.0047,
      "step": 28340
    },
    {
      "epoch": 13.118926422952336,
      "grad_norm": 0.01238955743610859,
      "learning_rate": 0.07376214715409533,
      "loss": 0.0006,
      "step": 28350
    },
    {
      "epoch": 13.123553910226747,
      "grad_norm": 0.014009758830070496,
      "learning_rate": 0.0737528921795465,
      "loss": 0.0001,
      "step": 28360
    },
    {
      "epoch": 13.128181397501157,
      "grad_norm": 0.09646868705749512,
      "learning_rate": 0.07374363720499769,
      "loss": 0.0003,
      "step": 28370
    },
    {
      "epoch": 13.132808884775567,
      "grad_norm": 0.002079431898891926,
      "learning_rate": 0.07373438223044887,
      "loss": 0.0006,
      "step": 28380
    },
    {
      "epoch": 13.137436372049978,
      "grad_norm": 0.0020018017385154963,
      "learning_rate": 0.07372512725590005,
      "loss": 0.0001,
      "step": 28390
    },
    {
      "epoch": 13.142063859324386,
      "grad_norm": 0.0021450964268296957,
      "learning_rate": 0.07371587228135122,
      "loss": 0.0002,
      "step": 28400
    },
    {
      "epoch": 13.146691346598796,
      "grad_norm": 0.02840668521821499,
      "learning_rate": 0.0737066173068024,
      "loss": 0.0002,
      "step": 28410
    },
    {
      "epoch": 13.151318833873207,
      "grad_norm": 0.021746084094047546,
      "learning_rate": 0.07369736233225359,
      "loss": 0.0001,
      "step": 28420
    },
    {
      "epoch": 13.155946321147617,
      "grad_norm": 0.007649681996554136,
      "learning_rate": 0.07368810735770477,
      "loss": 0.0002,
      "step": 28430
    },
    {
      "epoch": 13.160573808422027,
      "grad_norm": 0.0026599594857543707,
      "learning_rate": 0.07367885238315595,
      "loss": 0.0007,
      "step": 28440
    },
    {
      "epoch": 13.165201295696438,
      "grad_norm": 0.005463656038045883,
      "learning_rate": 0.07366959740860712,
      "loss": 0.0002,
      "step": 28450
    },
    {
      "epoch": 13.169828782970846,
      "grad_norm": 0.01435594167560339,
      "learning_rate": 0.0736603424340583,
      "loss": 0.0015,
      "step": 28460
    },
    {
      "epoch": 13.174456270245257,
      "grad_norm": 0.03528694435954094,
      "learning_rate": 0.07365108745950949,
      "loss": 0.0004,
      "step": 28470
    },
    {
      "epoch": 13.179083757519667,
      "grad_norm": 0.003210837719962001,
      "learning_rate": 0.07364183248496067,
      "loss": 0.0,
      "step": 28480
    },
    {
      "epoch": 13.183711244794077,
      "grad_norm": 0.009456107392907143,
      "learning_rate": 0.07363257751041184,
      "loss": 0.0005,
      "step": 28490
    },
    {
      "epoch": 13.188338732068488,
      "grad_norm": 0.023156458511948586,
      "learning_rate": 0.07362332253586303,
      "loss": 0.0001,
      "step": 28500
    },
    {
      "epoch": 13.192966219342896,
      "grad_norm": 0.0015029736096039414,
      "learning_rate": 0.07361406756131421,
      "loss": 0.0001,
      "step": 28510
    },
    {
      "epoch": 13.197593706617306,
      "grad_norm": 0.02543008327484131,
      "learning_rate": 0.07360481258676539,
      "loss": 0.0017,
      "step": 28520
    },
    {
      "epoch": 13.202221193891717,
      "grad_norm": 0.0018777430523186922,
      "learning_rate": 0.07359555761221657,
      "loss": 0.0008,
      "step": 28530
    },
    {
      "epoch": 13.206848681166127,
      "grad_norm": 0.01020442321896553,
      "learning_rate": 0.07358630263766774,
      "loss": 0.0002,
      "step": 28540
    },
    {
      "epoch": 13.211476168440537,
      "grad_norm": 0.07035816460847855,
      "learning_rate": 0.07357704766311893,
      "loss": 0.0002,
      "step": 28550
    },
    {
      "epoch": 13.216103655714948,
      "grad_norm": 0.02873201109468937,
      "learning_rate": 0.07356779268857011,
      "loss": 0.0003,
      "step": 28560
    },
    {
      "epoch": 13.220731142989356,
      "grad_norm": 0.036609139293432236,
      "learning_rate": 0.0735585377140213,
      "loss": 0.0015,
      "step": 28570
    },
    {
      "epoch": 13.225358630263766,
      "grad_norm": 0.01302056573331356,
      "learning_rate": 0.07354928273947246,
      "loss": 0.0001,
      "step": 28580
    },
    {
      "epoch": 13.229986117538177,
      "grad_norm": 0.8512212634086609,
      "learning_rate": 0.07354002776492365,
      "loss": 0.0004,
      "step": 28590
    },
    {
      "epoch": 13.234613604812587,
      "grad_norm": 0.0025430917739868164,
      "learning_rate": 0.07353077279037483,
      "loss": 0.0001,
      "step": 28600
    },
    {
      "epoch": 13.239241092086997,
      "grad_norm": 0.06453188508749008,
      "learning_rate": 0.07352151781582601,
      "loss": 0.0001,
      "step": 28610
    },
    {
      "epoch": 13.243868579361406,
      "grad_norm": 0.015916844829916954,
      "learning_rate": 0.0735122628412772,
      "loss": 0.0002,
      "step": 28620
    },
    {
      "epoch": 13.248496066635816,
      "grad_norm": 0.008837905712425709,
      "learning_rate": 0.07350300786672836,
      "loss": 0.0,
      "step": 28630
    },
    {
      "epoch": 13.253123553910227,
      "grad_norm": 0.0033298118505626917,
      "learning_rate": 0.07349375289217955,
      "loss": 0.0001,
      "step": 28640
    },
    {
      "epoch": 13.257751041184637,
      "grad_norm": 0.0017286036163568497,
      "learning_rate": 0.07348449791763073,
      "loss": 0.0004,
      "step": 28650
    },
    {
      "epoch": 13.262378528459047,
      "grad_norm": 0.0026774832513183355,
      "learning_rate": 0.07347524294308191,
      "loss": 0.0001,
      "step": 28660
    },
    {
      "epoch": 13.267006015733458,
      "grad_norm": 0.0010779154254123569,
      "learning_rate": 0.07346598796853308,
      "loss": 0.0072,
      "step": 28670
    },
    {
      "epoch": 13.271633503007866,
      "grad_norm": 0.013517409563064575,
      "learning_rate": 0.07345673299398427,
      "loss": 0.0001,
      "step": 28680
    },
    {
      "epoch": 13.276260990282276,
      "grad_norm": 0.0434938408434391,
      "learning_rate": 0.07344747801943545,
      "loss": 0.0001,
      "step": 28690
    },
    {
      "epoch": 13.280888477556687,
      "grad_norm": 0.8577343821525574,
      "learning_rate": 0.07343822304488663,
      "loss": 0.0003,
      "step": 28700
    },
    {
      "epoch": 13.285515964831097,
      "grad_norm": 0.03243110701441765,
      "learning_rate": 0.07342896807033782,
      "loss": 0.0003,
      "step": 28710
    },
    {
      "epoch": 13.290143452105507,
      "grad_norm": 0.1991424709558487,
      "learning_rate": 0.07341971309578899,
      "loss": 0.001,
      "step": 28720
    },
    {
      "epoch": 13.294770939379916,
      "grad_norm": 0.0001527988788438961,
      "learning_rate": 0.07341045812124017,
      "loss": 0.0003,
      "step": 28730
    },
    {
      "epoch": 13.299398426654326,
      "grad_norm": 0.17963466048240662,
      "learning_rate": 0.07340120314669135,
      "loss": 0.0043,
      "step": 28740
    },
    {
      "epoch": 13.304025913928736,
      "grad_norm": 0.0019083223305642605,
      "learning_rate": 0.07339194817214253,
      "loss": 0.0013,
      "step": 28750
    },
    {
      "epoch": 13.308653401203147,
      "grad_norm": 0.03636859357357025,
      "learning_rate": 0.0733826931975937,
      "loss": 0.0006,
      "step": 28760
    },
    {
      "epoch": 13.313280888477557,
      "grad_norm": 0.0058568306267261505,
      "learning_rate": 0.07337343822304489,
      "loss": 0.0003,
      "step": 28770
    },
    {
      "epoch": 13.317908375751967,
      "grad_norm": 0.03141556680202484,
      "learning_rate": 0.07336418324849607,
      "loss": 0.0,
      "step": 28780
    },
    {
      "epoch": 13.322535863026376,
      "grad_norm": 0.0026261191815137863,
      "learning_rate": 0.07335492827394725,
      "loss": 0.0001,
      "step": 28790
    },
    {
      "epoch": 13.327163350300786,
      "grad_norm": 0.872816264629364,
      "learning_rate": 0.07334567329939844,
      "loss": 0.0003,
      "step": 28800
    },
    {
      "epoch": 13.331790837575197,
      "grad_norm": 0.034105151891708374,
      "learning_rate": 0.0733364183248496,
      "loss": 0.0002,
      "step": 28810
    },
    {
      "epoch": 13.336418324849607,
      "grad_norm": 0.003066056175157428,
      "learning_rate": 0.07332716335030079,
      "loss": 0.0016,
      "step": 28820
    },
    {
      "epoch": 13.341045812124017,
      "grad_norm": 0.7209982872009277,
      "learning_rate": 0.07331790837575196,
      "loss": 0.0007,
      "step": 28830
    },
    {
      "epoch": 13.345673299398428,
      "grad_norm": 0.021041264757514,
      "learning_rate": 0.07330865340120316,
      "loss": 0.0002,
      "step": 28840
    },
    {
      "epoch": 13.350300786672836,
      "grad_norm": 0.022116338834166527,
      "learning_rate": 0.07329939842665432,
      "loss": 0.0002,
      "step": 28850
    },
    {
      "epoch": 13.354928273947246,
      "grad_norm": 0.0016941165085881948,
      "learning_rate": 0.07329014345210551,
      "loss": 0.0002,
      "step": 28860
    },
    {
      "epoch": 13.359555761221657,
      "grad_norm": 0.02017504908144474,
      "learning_rate": 0.07328088847755669,
      "loss": 0.0001,
      "step": 28870
    },
    {
      "epoch": 13.364183248496067,
      "grad_norm": 0.0018483391031622887,
      "learning_rate": 0.07327163350300787,
      "loss": 0.0092,
      "step": 28880
    },
    {
      "epoch": 13.368810735770477,
      "grad_norm": 0.015027659945189953,
      "learning_rate": 0.07326237852845906,
      "loss": 0.0001,
      "step": 28890
    },
    {
      "epoch": 13.373438223044886,
      "grad_norm": 0.003303639590740204,
      "learning_rate": 0.07325312355391023,
      "loss": 0.0008,
      "step": 28900
    },
    {
      "epoch": 13.378065710319296,
      "grad_norm": 0.008627140894532204,
      "learning_rate": 0.07324386857936141,
      "loss": 0.0001,
      "step": 28910
    },
    {
      "epoch": 13.382693197593706,
      "grad_norm": 0.018430475145578384,
      "learning_rate": 0.07323461360481258,
      "loss": 0.0007,
      "step": 28920
    },
    {
      "epoch": 13.387320684868117,
      "grad_norm": 0.4858455955982208,
      "learning_rate": 0.07322535863026378,
      "loss": 0.0025,
      "step": 28930
    },
    {
      "epoch": 13.391948172142527,
      "grad_norm": 0.002885570051148534,
      "learning_rate": 0.07321610365571495,
      "loss": 0.0011,
      "step": 28940
    },
    {
      "epoch": 13.396575659416937,
      "grad_norm": 0.002547095064073801,
      "learning_rate": 0.07320684868116613,
      "loss": 0.0002,
      "step": 28950
    },
    {
      "epoch": 13.401203146691346,
      "grad_norm": 0.06674026697874069,
      "learning_rate": 0.07319759370661731,
      "loss": 0.0001,
      "step": 28960
    },
    {
      "epoch": 13.405830633965756,
      "grad_norm": 0.003072985215112567,
      "learning_rate": 0.0731883387320685,
      "loss": 0.0038,
      "step": 28970
    },
    {
      "epoch": 13.410458121240167,
      "grad_norm": 0.0675196498632431,
      "learning_rate": 0.07317908375751968,
      "loss": 0.0035,
      "step": 28980
    },
    {
      "epoch": 13.415085608514577,
      "grad_norm": 0.048975225538015366,
      "learning_rate": 0.07316982878297085,
      "loss": 0.0001,
      "step": 28990
    },
    {
      "epoch": 13.419713095788987,
      "grad_norm": 0.017199920490384102,
      "learning_rate": 0.07316057380842203,
      "loss": 0.0001,
      "step": 29000
    },
    {
      "epoch": 13.424340583063396,
      "grad_norm": 0.02050582878291607,
      "learning_rate": 0.0731513188338732,
      "loss": 0.0003,
      "step": 29010
    },
    {
      "epoch": 13.428968070337806,
      "grad_norm": 0.005186412949115038,
      "learning_rate": 0.0731420638593244,
      "loss": 0.0001,
      "step": 29020
    },
    {
      "epoch": 13.433595557612216,
      "grad_norm": 0.04462748393416405,
      "learning_rate": 0.07313280888477557,
      "loss": 0.0001,
      "step": 29030
    },
    {
      "epoch": 13.438223044886627,
      "grad_norm": 0.05202154815196991,
      "learning_rate": 0.07312355391022675,
      "loss": 0.0002,
      "step": 29040
    },
    {
      "epoch": 13.442850532161037,
      "grad_norm": 0.00034970048000104725,
      "learning_rate": 0.07311429893567793,
      "loss": 0.0,
      "step": 29050
    },
    {
      "epoch": 13.447478019435447,
      "grad_norm": 0.011484487913548946,
      "learning_rate": 0.0731050439611291,
      "loss": 0.0,
      "step": 29060
    },
    {
      "epoch": 13.452105506709856,
      "grad_norm": 0.0005271296831779182,
      "learning_rate": 0.0730957889865803,
      "loss": 0.0001,
      "step": 29070
    },
    {
      "epoch": 13.456732993984266,
      "grad_norm": 0.026205778121948242,
      "learning_rate": 0.07308653401203147,
      "loss": 0.0001,
      "step": 29080
    },
    {
      "epoch": 13.461360481258676,
      "grad_norm": 0.017770832404494286,
      "learning_rate": 0.07307727903748265,
      "loss": 0.0001,
      "step": 29090
    },
    {
      "epoch": 13.465987968533087,
      "grad_norm": 0.009936301968991756,
      "learning_rate": 0.07306802406293382,
      "loss": 0.0033,
      "step": 29100
    },
    {
      "epoch": 13.470615455807497,
      "grad_norm": 1.350947380065918,
      "learning_rate": 0.07305876908838502,
      "loss": 0.0049,
      "step": 29110
    },
    {
      "epoch": 13.475242943081906,
      "grad_norm": 0.008664999157190323,
      "learning_rate": 0.07304951411383619,
      "loss": 0.0001,
      "step": 29120
    },
    {
      "epoch": 13.479870430356316,
      "grad_norm": 0.0018726704875007272,
      "learning_rate": 0.07304025913928737,
      "loss": 0.0002,
      "step": 29130
    },
    {
      "epoch": 13.484497917630726,
      "grad_norm": 0.10211706906557083,
      "learning_rate": 0.07303100416473855,
      "loss": 0.0001,
      "step": 29140
    },
    {
      "epoch": 13.489125404905137,
      "grad_norm": 0.0011465969728305936,
      "learning_rate": 0.07302174919018972,
      "loss": 0.0002,
      "step": 29150
    },
    {
      "epoch": 13.493752892179547,
      "grad_norm": 0.004984952509403229,
      "learning_rate": 0.07301249421564092,
      "loss": 0.0011,
      "step": 29160
    },
    {
      "epoch": 13.498380379453957,
      "grad_norm": 0.016422785818576813,
      "learning_rate": 0.07300323924109209,
      "loss": 0.0002,
      "step": 29170
    },
    {
      "epoch": 13.503007866728366,
      "grad_norm": 0.002999394666403532,
      "learning_rate": 0.07299398426654327,
      "loss": 0.0056,
      "step": 29180
    },
    {
      "epoch": 13.507635354002776,
      "grad_norm": 0.023042330518364906,
      "learning_rate": 0.07298472929199444,
      "loss": 0.0,
      "step": 29190
    },
    {
      "epoch": 13.512262841277186,
      "grad_norm": 0.0027097840793430805,
      "learning_rate": 0.07297547431744564,
      "loss": 0.002,
      "step": 29200
    },
    {
      "epoch": 13.516890328551597,
      "grad_norm": 2.783170461654663,
      "learning_rate": 0.07296621934289681,
      "loss": 0.0019,
      "step": 29210
    },
    {
      "epoch": 13.521517815826007,
      "grad_norm": 0.0015114545822143555,
      "learning_rate": 0.07295696436834799,
      "loss": 0.0002,
      "step": 29220
    },
    {
      "epoch": 13.526145303100417,
      "grad_norm": 0.1628992110490799,
      "learning_rate": 0.07294770939379917,
      "loss": 0.0003,
      "step": 29230
    },
    {
      "epoch": 13.530772790374826,
      "grad_norm": 0.12338603287935257,
      "learning_rate": 0.07293845441925034,
      "loss": 0.0006,
      "step": 29240
    },
    {
      "epoch": 13.535400277649236,
      "grad_norm": 0.02184300124645233,
      "learning_rate": 0.07292919944470154,
      "loss": 0.0001,
      "step": 29250
    },
    {
      "epoch": 13.540027764923646,
      "grad_norm": 0.004865644033998251,
      "learning_rate": 0.07291994447015271,
      "loss": 0.0008,
      "step": 29260
    },
    {
      "epoch": 13.544655252198057,
      "grad_norm": 0.08049260079860687,
      "learning_rate": 0.07291068949560389,
      "loss": 0.0003,
      "step": 29270
    },
    {
      "epoch": 13.549282739472467,
      "grad_norm": 0.004251006990671158,
      "learning_rate": 0.07290143452105506,
      "loss": 0.0062,
      "step": 29280
    },
    {
      "epoch": 13.553910226746876,
      "grad_norm": 0.0021272171288728714,
      "learning_rate": 0.07289217954650624,
      "loss": 0.0009,
      "step": 29290
    },
    {
      "epoch": 13.558537714021286,
      "grad_norm": 0.0024423389695584774,
      "learning_rate": 0.07288292457195743,
      "loss": 0.0002,
      "step": 29300
    },
    {
      "epoch": 13.563165201295696,
      "grad_norm": 0.0053029535338282585,
      "learning_rate": 0.07287366959740861,
      "loss": 0.0001,
      "step": 29310
    },
    {
      "epoch": 13.567792688570107,
      "grad_norm": 0.005053826607763767,
      "learning_rate": 0.0728644146228598,
      "loss": 0.0001,
      "step": 29320
    },
    {
      "epoch": 13.572420175844517,
      "grad_norm": 0.0035496733617037535,
      "learning_rate": 0.07285515964831096,
      "loss": 0.0035,
      "step": 29330
    },
    {
      "epoch": 13.577047663118927,
      "grad_norm": 7.032960891723633,
      "learning_rate": 0.07284590467376216,
      "loss": 0.0015,
      "step": 29340
    },
    {
      "epoch": 13.581675150393336,
      "grad_norm": 0.06906769424676895,
      "learning_rate": 0.07283664969921333,
      "loss": 0.0002,
      "step": 29350
    },
    {
      "epoch": 13.586302637667746,
      "grad_norm": 0.0071262214332818985,
      "learning_rate": 0.07282739472466451,
      "loss": 0.0066,
      "step": 29360
    },
    {
      "epoch": 13.590930124942156,
      "grad_norm": 0.0342392697930336,
      "learning_rate": 0.07281813975011568,
      "loss": 0.0002,
      "step": 29370
    },
    {
      "epoch": 13.595557612216567,
      "grad_norm": 0.059771325439214706,
      "learning_rate": 0.07280888477556686,
      "loss": 0.0001,
      "step": 29380
    },
    {
      "epoch": 13.600185099490977,
      "grad_norm": 0.027312135323882103,
      "learning_rate": 0.07279962980101805,
      "loss": 0.0001,
      "step": 29390
    },
    {
      "epoch": 13.604812586765387,
      "grad_norm": 0.8066527843475342,
      "learning_rate": 0.07279037482646923,
      "loss": 0.0003,
      "step": 29400
    },
    {
      "epoch": 13.609440074039796,
      "grad_norm": 0.005692888516932726,
      "learning_rate": 0.07278111985192041,
      "loss": 0.0001,
      "step": 29410
    },
    {
      "epoch": 13.614067561314206,
      "grad_norm": 0.004638089332729578,
      "learning_rate": 0.07277186487737158,
      "loss": 0.0019,
      "step": 29420
    },
    {
      "epoch": 13.618695048588616,
      "grad_norm": 0.09862471371889114,
      "learning_rate": 0.07276260990282278,
      "loss": 0.0016,
      "step": 29430
    },
    {
      "epoch": 13.623322535863027,
      "grad_norm": 0.1652492731809616,
      "learning_rate": 0.07275335492827395,
      "loss": 0.0002,
      "step": 29440
    },
    {
      "epoch": 13.627950023137437,
      "grad_norm": 0.02238091453909874,
      "learning_rate": 0.07274409995372513,
      "loss": 0.0022,
      "step": 29450
    },
    {
      "epoch": 13.632577510411846,
      "grad_norm": 0.04261769354343414,
      "learning_rate": 0.0727348449791763,
      "loss": 0.0002,
      "step": 29460
    },
    {
      "epoch": 13.637204997686256,
      "grad_norm": 0.2065800130367279,
      "learning_rate": 0.07272559000462749,
      "loss": 0.0008,
      "step": 29470
    },
    {
      "epoch": 13.641832484960666,
      "grad_norm": 0.029236026108264923,
      "learning_rate": 0.07271633503007867,
      "loss": 0.0027,
      "step": 29480
    },
    {
      "epoch": 13.646459972235077,
      "grad_norm": 0.007442541886121035,
      "learning_rate": 0.07270708005552985,
      "loss": 0.0016,
      "step": 29490
    },
    {
      "epoch": 13.651087459509487,
      "grad_norm": 0.00047272967640310526,
      "learning_rate": 0.07269782508098104,
      "loss": 0.0033,
      "step": 29500
    },
    {
      "epoch": 13.655714946783895,
      "grad_norm": 0.00034017363213934004,
      "learning_rate": 0.0726885701064322,
      "loss": 0.0003,
      "step": 29510
    },
    {
      "epoch": 13.660342434058306,
      "grad_norm": 0.06360658258199692,
      "learning_rate": 0.07267931513188339,
      "loss": 0.0001,
      "step": 29520
    },
    {
      "epoch": 13.664969921332716,
      "grad_norm": 0.008401497267186642,
      "learning_rate": 0.07267006015733457,
      "loss": 0.0002,
      "step": 29530
    },
    {
      "epoch": 13.669597408607126,
      "grad_norm": 0.0075845555402338505,
      "learning_rate": 0.07266080518278575,
      "loss": 0.0002,
      "step": 29540
    },
    {
      "epoch": 13.674224895881537,
      "grad_norm": 0.011653061956167221,
      "learning_rate": 0.07265155020823692,
      "loss": 0.0001,
      "step": 29550
    },
    {
      "epoch": 13.678852383155947,
      "grad_norm": 0.005150178447365761,
      "learning_rate": 0.0726422952336881,
      "loss": 0.0023,
      "step": 29560
    },
    {
      "epoch": 13.683479870430356,
      "grad_norm": 1.0428696870803833,
      "learning_rate": 0.07263304025913929,
      "loss": 0.0008,
      "step": 29570
    },
    {
      "epoch": 13.688107357704766,
      "grad_norm": 0.0709560438990593,
      "learning_rate": 0.07262378528459047,
      "loss": 0.0085,
      "step": 29580
    },
    {
      "epoch": 13.692734844979176,
      "grad_norm": 0.02924428880214691,
      "learning_rate": 0.07261453031004166,
      "loss": 0.0003,
      "step": 29590
    },
    {
      "epoch": 13.697362332253586,
      "grad_norm": 0.1918119341135025,
      "learning_rate": 0.07260527533549282,
      "loss": 0.0004,
      "step": 29600
    },
    {
      "epoch": 13.701989819527997,
      "grad_norm": 0.017725251615047455,
      "learning_rate": 0.07259602036094401,
      "loss": 0.0004,
      "step": 29610
    },
    {
      "epoch": 13.706617306802407,
      "grad_norm": 0.05018647387623787,
      "learning_rate": 0.07258676538639519,
      "loss": 0.0006,
      "step": 29620
    },
    {
      "epoch": 13.711244794076816,
      "grad_norm": 0.06490300595760345,
      "learning_rate": 0.07257751041184637,
      "loss": 0.0007,
      "step": 29630
    },
    {
      "epoch": 13.715872281351226,
      "grad_norm": 0.05017650127410889,
      "learning_rate": 0.07256825543729754,
      "loss": 0.0056,
      "step": 29640
    },
    {
      "epoch": 13.720499768625636,
      "grad_norm": 9.430607795715332,
      "learning_rate": 0.07255900046274873,
      "loss": 0.0068,
      "step": 29650
    },
    {
      "epoch": 13.725127255900047,
      "grad_norm": 0.018554363399744034,
      "learning_rate": 0.07254974548819991,
      "loss": 0.0001,
      "step": 29660
    },
    {
      "epoch": 13.729754743174457,
      "grad_norm": 0.0030024144798517227,
      "learning_rate": 0.07254049051365109,
      "loss": 0.0001,
      "step": 29670
    },
    {
      "epoch": 13.734382230448865,
      "grad_norm": 0.0037586097605526447,
      "learning_rate": 0.07253123553910228,
      "loss": 0.0062,
      "step": 29680
    },
    {
      "epoch": 13.739009717723276,
      "grad_norm": 0.027729181572794914,
      "learning_rate": 0.07252198056455345,
      "loss": 0.0069,
      "step": 29690
    },
    {
      "epoch": 13.743637204997686,
      "grad_norm": 0.0009794262005016208,
      "learning_rate": 0.07251272559000463,
      "loss": 0.0005,
      "step": 29700
    },
    {
      "epoch": 13.748264692272096,
      "grad_norm": 0.08502081036567688,
      "learning_rate": 0.07250347061545581,
      "loss": 0.0005,
      "step": 29710
    },
    {
      "epoch": 13.752892179546507,
      "grad_norm": 0.019372833892703056,
      "learning_rate": 0.072494215640907,
      "loss": 0.0005,
      "step": 29720
    },
    {
      "epoch": 13.757519666820917,
      "grad_norm": 0.028294377028942108,
      "learning_rate": 0.07248496066635816,
      "loss": 0.0001,
      "step": 29730
    },
    {
      "epoch": 13.762147154095326,
      "grad_norm": 0.03851349651813507,
      "learning_rate": 0.07247570569180935,
      "loss": 0.0047,
      "step": 29740
    },
    {
      "epoch": 13.766774641369736,
      "grad_norm": 0.09295246750116348,
      "learning_rate": 0.07246645071726053,
      "loss": 0.0015,
      "step": 29750
    },
    {
      "epoch": 13.771402128644146,
      "grad_norm": 0.0302057433873415,
      "learning_rate": 0.07245719574271171,
      "loss": 0.0022,
      "step": 29760
    },
    {
      "epoch": 13.776029615918556,
      "grad_norm": 0.09024843573570251,
      "learning_rate": 0.0724479407681629,
      "loss": 0.0004,
      "step": 29770
    },
    {
      "epoch": 13.780657103192967,
      "grad_norm": 0.01138230785727501,
      "learning_rate": 0.07243868579361407,
      "loss": 0.0008,
      "step": 29780
    },
    {
      "epoch": 13.785284590467377,
      "grad_norm": 0.03543343394994736,
      "learning_rate": 0.07242943081906525,
      "loss": 0.0005,
      "step": 29790
    },
    {
      "epoch": 13.789912077741786,
      "grad_norm": 0.13031019270420074,
      "learning_rate": 0.07242017584451643,
      "loss": 0.0002,
      "step": 29800
    },
    {
      "epoch": 13.794539565016196,
      "grad_norm": 0.5284977555274963,
      "learning_rate": 0.07241092086996762,
      "loss": 0.0005,
      "step": 29810
    },
    {
      "epoch": 13.799167052290606,
      "grad_norm": 0.031058629974722862,
      "learning_rate": 0.07240166589541878,
      "loss": 0.0003,
      "step": 29820
    },
    {
      "epoch": 13.803794539565017,
      "grad_norm": 0.09362498670816422,
      "learning_rate": 0.07239241092086997,
      "loss": 0.0008,
      "step": 29830
    },
    {
      "epoch": 13.808422026839427,
      "grad_norm": 0.19350464642047882,
      "learning_rate": 0.07238315594632115,
      "loss": 0.0001,
      "step": 29840
    },
    {
      "epoch": 13.813049514113835,
      "grad_norm": 0.05205896124243736,
      "learning_rate": 0.07237390097177233,
      "loss": 0.0001,
      "step": 29850
    },
    {
      "epoch": 13.817677001388246,
      "grad_norm": 0.04226134344935417,
      "learning_rate": 0.07236464599722352,
      "loss": 0.0001,
      "step": 29860
    },
    {
      "epoch": 13.822304488662656,
      "grad_norm": 0.027569126337766647,
      "learning_rate": 0.07235539102267469,
      "loss": 0.0002,
      "step": 29870
    },
    {
      "epoch": 13.826931975937066,
      "grad_norm": 0.3802274167537689,
      "learning_rate": 0.07234613604812587,
      "loss": 0.0002,
      "step": 29880
    },
    {
      "epoch": 13.831559463211477,
      "grad_norm": 0.0025714144576340914,
      "learning_rate": 0.07233688107357705,
      "loss": 0.0001,
      "step": 29890
    },
    {
      "epoch": 13.836186950485887,
      "grad_norm": 0.004262302536517382,
      "learning_rate": 0.07232762609902824,
      "loss": 0.0043,
      "step": 29900
    },
    {
      "epoch": 13.840814437760296,
      "grad_norm": 0.006974983960390091,
      "learning_rate": 0.0723183711244794,
      "loss": 0.0004,
      "step": 29910
    },
    {
      "epoch": 13.845441925034706,
      "grad_norm": 0.05773567408323288,
      "learning_rate": 0.07230911614993059,
      "loss": 0.0082,
      "step": 29920
    },
    {
      "epoch": 13.850069412309116,
      "grad_norm": 0.004833472426980734,
      "learning_rate": 0.07229986117538177,
      "loss": 0.0038,
      "step": 29930
    },
    {
      "epoch": 13.854696899583526,
      "grad_norm": 0.01885456033051014,
      "learning_rate": 0.07229060620083295,
      "loss": 0.0008,
      "step": 29940
    },
    {
      "epoch": 13.859324386857937,
      "grad_norm": 0.25192245841026306,
      "learning_rate": 0.07228135122628414,
      "loss": 0.0028,
      "step": 29950
    },
    {
      "epoch": 13.863951874132345,
      "grad_norm": 0.0508849211037159,
      "learning_rate": 0.07227209625173531,
      "loss": 0.0034,
      "step": 29960
    },
    {
      "epoch": 13.868579361406756,
      "grad_norm": 0.03254441171884537,
      "learning_rate": 0.07226284127718649,
      "loss": 0.0086,
      "step": 29970
    },
    {
      "epoch": 13.873206848681166,
      "grad_norm": 0.003871832974255085,
      "learning_rate": 0.07225358630263766,
      "loss": 0.0008,
      "step": 29980
    },
    {
      "epoch": 13.877834335955576,
      "grad_norm": 0.005820084363222122,
      "learning_rate": 0.07224433132808886,
      "loss": 0.0009,
      "step": 29990
    },
    {
      "epoch": 13.882461823229987,
      "grad_norm": 0.03452540561556816,
      "learning_rate": 0.07223507635354003,
      "loss": 0.0002,
      "step": 30000
    },
    {
      "epoch": 13.887089310504397,
      "grad_norm": 0.8548208475112915,
      "learning_rate": 0.07222582137899121,
      "loss": 0.0005,
      "step": 30010
    },
    {
      "epoch": 13.891716797778805,
      "grad_norm": 0.671286404132843,
      "learning_rate": 0.07221656640444239,
      "loss": 0.0014,
      "step": 30020
    },
    {
      "epoch": 13.896344285053216,
      "grad_norm": 0.05884505808353424,
      "learning_rate": 0.07220731142989358,
      "loss": 0.0013,
      "step": 30030
    },
    {
      "epoch": 13.900971772327626,
      "grad_norm": 0.22725751996040344,
      "learning_rate": 0.07219805645534476,
      "loss": 0.0013,
      "step": 30040
    },
    {
      "epoch": 13.905599259602036,
      "grad_norm": 0.12064078450202942,
      "learning_rate": 0.07218880148079593,
      "loss": 0.0001,
      "step": 30050
    },
    {
      "epoch": 13.910226746876447,
      "grad_norm": 0.31671756505966187,
      "learning_rate": 0.07217954650624711,
      "loss": 0.0039,
      "step": 30060
    },
    {
      "epoch": 13.914854234150855,
      "grad_norm": 0.011231550946831703,
      "learning_rate": 0.07217029153169828,
      "loss": 0.0002,
      "step": 30070
    },
    {
      "epoch": 13.919481721425266,
      "grad_norm": 1.2505695819854736,
      "learning_rate": 0.07216103655714948,
      "loss": 0.0005,
      "step": 30080
    },
    {
      "epoch": 13.924109208699676,
      "grad_norm": 0.05256236344575882,
      "learning_rate": 0.07215178158260065,
      "loss": 0.0004,
      "step": 30090
    },
    {
      "epoch": 13.928736695974086,
      "grad_norm": 0.5229363441467285,
      "learning_rate": 0.07214252660805183,
      "loss": 0.0002,
      "step": 30100
    },
    {
      "epoch": 13.933364183248496,
      "grad_norm": 0.36035868525505066,
      "learning_rate": 0.07213327163350301,
      "loss": 0.0002,
      "step": 30110
    },
    {
      "epoch": 13.937991670522907,
      "grad_norm": 0.14011560380458832,
      "learning_rate": 0.0721240166589542,
      "loss": 0.0004,
      "step": 30120
    },
    {
      "epoch": 13.942619157797315,
      "grad_norm": 0.002362610772252083,
      "learning_rate": 0.07211476168440538,
      "loss": 0.0001,
      "step": 30130
    },
    {
      "epoch": 13.947246645071726,
      "grad_norm": 0.012781745754182339,
      "learning_rate": 0.07210550670985655,
      "loss": 0.0003,
      "step": 30140
    },
    {
      "epoch": 13.951874132346136,
      "grad_norm": 0.03631236404180527,
      "learning_rate": 0.07209625173530773,
      "loss": 0.0003,
      "step": 30150
    },
    {
      "epoch": 13.956501619620546,
      "grad_norm": 0.07621260732412338,
      "learning_rate": 0.0720869967607589,
      "loss": 0.0005,
      "step": 30160
    },
    {
      "epoch": 13.961129106894957,
      "grad_norm": 0.001405342947691679,
      "learning_rate": 0.0720777417862101,
      "loss": 0.0001,
      "step": 30170
    },
    {
      "epoch": 13.965756594169367,
      "grad_norm": 12.516144752502441,
      "learning_rate": 0.07206848681166127,
      "loss": 0.0049,
      "step": 30180
    },
    {
      "epoch": 13.970384081443775,
      "grad_norm": 0.010665510781109333,
      "learning_rate": 0.07205923183711245,
      "loss": 0.0004,
      "step": 30190
    },
    {
      "epoch": 13.975011568718186,
      "grad_norm": 0.008477664552628994,
      "learning_rate": 0.07204997686256363,
      "loss": 0.0003,
      "step": 30200
    },
    {
      "epoch": 13.979639055992596,
      "grad_norm": 0.010513911955058575,
      "learning_rate": 0.0720407218880148,
      "loss": 0.0003,
      "step": 30210
    },
    {
      "epoch": 13.984266543267006,
      "grad_norm": 0.005609109997749329,
      "learning_rate": 0.072031466913466,
      "loss": 0.0002,
      "step": 30220
    },
    {
      "epoch": 13.988894030541417,
      "grad_norm": 0.10572510957717896,
      "learning_rate": 0.07202221193891717,
      "loss": 0.0024,
      "step": 30230
    },
    {
      "epoch": 13.993521517815825,
      "grad_norm": 1.62306547164917,
      "learning_rate": 0.07201295696436835,
      "loss": 0.0004,
      "step": 30240
    },
    {
      "epoch": 13.998149005090236,
      "grad_norm": 0.19647349417209625,
      "learning_rate": 0.07200370198981952,
      "loss": 0.0003,
      "step": 30250
    },
    {
      "epoch": 14.0,
      "eval_accuracy_branch1": 0.9867508858419349,
      "eval_accuracy_branch2": 0.49915267293175164,
      "eval_f1_branch1": 0.987975906263971,
      "eval_f1_branch2": 0.4991483815903649,
      "eval_loss": 0.02480684407055378,
      "eval_precision_branch1": 0.9882666230480835,
      "eval_precision_branch2": 0.49915264389086045,
      "eval_recall_branch1": 0.9878835135798983,
      "eval_recall_branch2": 0.4991526729317517,
      "eval_runtime": 28.9336,
      "eval_samples_per_second": 448.683,
      "eval_steps_per_second": 56.094,
      "step": 30254
    },
    {
      "epoch": 14.002776492364646,
      "grad_norm": 0.5429806709289551,
      "learning_rate": 0.07199444701527072,
      "loss": 0.006,
      "step": 30260
    },
    {
      "epoch": 14.007403979639056,
      "grad_norm": 0.1480785757303238,
      "learning_rate": 0.07198519204072189,
      "loss": 0.0002,
      "step": 30270
    },
    {
      "epoch": 14.012031466913466,
      "grad_norm": 0.005469101946800947,
      "learning_rate": 0.07197593706617307,
      "loss": 0.0002,
      "step": 30280
    },
    {
      "epoch": 14.016658954187877,
      "grad_norm": 0.00840224139392376,
      "learning_rate": 0.07196668209162425,
      "loss": 0.0002,
      "step": 30290
    },
    {
      "epoch": 14.021286441462285,
      "grad_norm": 0.11753710359334946,
      "learning_rate": 0.07195742711707542,
      "loss": 0.0001,
      "step": 30300
    },
    {
      "epoch": 14.025913928736696,
      "grad_norm": 0.02297276258468628,
      "learning_rate": 0.07194817214252662,
      "loss": 0.0001,
      "step": 30310
    },
    {
      "epoch": 14.030541416011106,
      "grad_norm": 0.01422705128788948,
      "learning_rate": 0.07193891716797779,
      "loss": 0.0006,
      "step": 30320
    },
    {
      "epoch": 14.035168903285516,
      "grad_norm": 0.008120762184262276,
      "learning_rate": 0.07192966219342897,
      "loss": 0.0004,
      "step": 30330
    },
    {
      "epoch": 14.039796390559927,
      "grad_norm": 0.11242847144603729,
      "learning_rate": 0.07192040721888014,
      "loss": 0.0001,
      "step": 30340
    },
    {
      "epoch": 14.044423877834335,
      "grad_norm": 0.0014385849935933948,
      "learning_rate": 0.07191115224433134,
      "loss": 0.0013,
      "step": 30350
    },
    {
      "epoch": 14.049051365108745,
      "grad_norm": 0.005635877139866352,
      "learning_rate": 0.07190189726978251,
      "loss": 0.0182,
      "step": 30360
    },
    {
      "epoch": 14.053678852383156,
      "grad_norm": 0.015795893967151642,
      "learning_rate": 0.07189264229523369,
      "loss": 0.0003,
      "step": 30370
    },
    {
      "epoch": 14.058306339657566,
      "grad_norm": 0.0016141398809850216,
      "learning_rate": 0.07188338732068487,
      "loss": 0.0004,
      "step": 30380
    },
    {
      "epoch": 14.062933826931976,
      "grad_norm": 0.008187522180378437,
      "learning_rate": 0.07187413234613604,
      "loss": 0.0001,
      "step": 30390
    },
    {
      "epoch": 14.067561314206387,
      "grad_norm": 0.5168082118034363,
      "learning_rate": 0.07186487737158724,
      "loss": 0.0019,
      "step": 30400
    },
    {
      "epoch": 14.072188801480795,
      "grad_norm": 0.09002969413995743,
      "learning_rate": 0.07185562239703841,
      "loss": 0.0002,
      "step": 30410
    },
    {
      "epoch": 14.076816288755206,
      "grad_norm": 0.04363182187080383,
      "learning_rate": 0.0718463674224896,
      "loss": 0.0007,
      "step": 30420
    },
    {
      "epoch": 14.081443776029616,
      "grad_norm": 0.00805986113846302,
      "learning_rate": 0.07183711244794076,
      "loss": 0.0006,
      "step": 30430
    },
    {
      "epoch": 14.086071263304026,
      "grad_norm": 0.3339265286922455,
      "learning_rate": 0.07182785747339195,
      "loss": 0.0003,
      "step": 30440
    },
    {
      "epoch": 14.090698750578436,
      "grad_norm": 0.0017676681745797396,
      "learning_rate": 0.07181860249884313,
      "loss": 0.0008,
      "step": 30450
    },
    {
      "epoch": 14.095326237852847,
      "grad_norm": 0.09136863797903061,
      "learning_rate": 0.07180934752429431,
      "loss": 0.0003,
      "step": 30460
    },
    {
      "epoch": 14.099953725127255,
      "grad_norm": 0.3885354697704315,
      "learning_rate": 0.0718000925497455,
      "loss": 0.0006,
      "step": 30470
    },
    {
      "epoch": 14.104581212401666,
      "grad_norm": 0.008872997015714645,
      "learning_rate": 0.07179083757519666,
      "loss": 0.0003,
      "step": 30480
    },
    {
      "epoch": 14.109208699676076,
      "grad_norm": 2.3615000247955322,
      "learning_rate": 0.07178158260064786,
      "loss": 0.0005,
      "step": 30490
    },
    {
      "epoch": 14.113836186950486,
      "grad_norm": 0.01410853210836649,
      "learning_rate": 0.07177232762609903,
      "loss": 0.0014,
      "step": 30500
    },
    {
      "epoch": 14.118463674224897,
      "grad_norm": 0.06338142603635788,
      "learning_rate": 0.07176307265155021,
      "loss": 0.0004,
      "step": 30510
    },
    {
      "epoch": 14.123091161499305,
      "grad_norm": 0.4122106432914734,
      "learning_rate": 0.07175381767700138,
      "loss": 0.0003,
      "step": 30520
    },
    {
      "epoch": 14.127718648773715,
      "grad_norm": 0.0006013567908667028,
      "learning_rate": 0.07174456270245257,
      "loss": 0.0004,
      "step": 30530
    },
    {
      "epoch": 14.132346136048126,
      "grad_norm": 14.862019538879395,
      "learning_rate": 0.07173530772790375,
      "loss": 0.0125,
      "step": 30540
    },
    {
      "epoch": 14.136973623322536,
      "grad_norm": 0.07666894048452377,
      "learning_rate": 0.07172605275335493,
      "loss": 0.0048,
      "step": 30550
    },
    {
      "epoch": 14.141601110596946,
      "grad_norm": 0.006141989957541227,
      "learning_rate": 0.07171679777880612,
      "loss": 0.0004,
      "step": 30560
    },
    {
      "epoch": 14.146228597871357,
      "grad_norm": 0.008095579221844673,
      "learning_rate": 0.07170754280425728,
      "loss": 0.0001,
      "step": 30570
    },
    {
      "epoch": 14.150856085145765,
      "grad_norm": 0.011152864433825016,
      "learning_rate": 0.07169828782970848,
      "loss": 0.0009,
      "step": 30580
    },
    {
      "epoch": 14.155483572420176,
      "grad_norm": 0.12063924223184586,
      "learning_rate": 0.07168903285515965,
      "loss": 0.0001,
      "step": 30590
    },
    {
      "epoch": 14.160111059694586,
      "grad_norm": 0.0011685778154060245,
      "learning_rate": 0.07167977788061083,
      "loss": 0.0001,
      "step": 30600
    },
    {
      "epoch": 14.164738546968996,
      "grad_norm": 0.033578235656023026,
      "learning_rate": 0.071670522906062,
      "loss": 0.0015,
      "step": 30610
    },
    {
      "epoch": 14.169366034243406,
      "grad_norm": 0.002324342494830489,
      "learning_rate": 0.07166126793151319,
      "loss": 0.0001,
      "step": 30620
    },
    {
      "epoch": 14.173993521517815,
      "grad_norm": 0.1337449848651886,
      "learning_rate": 0.07165201295696437,
      "loss": 0.0008,
      "step": 30630
    },
    {
      "epoch": 14.178621008792225,
      "grad_norm": 0.0023607956245541573,
      "learning_rate": 0.07164275798241555,
      "loss": 0.0003,
      "step": 30640
    },
    {
      "epoch": 14.183248496066636,
      "grad_norm": 0.003068142104893923,
      "learning_rate": 0.07163350300786674,
      "loss": 0.0052,
      "step": 30650
    },
    {
      "epoch": 14.187875983341046,
      "grad_norm": 0.003714979626238346,
      "learning_rate": 0.0716242480333179,
      "loss": 0.0042,
      "step": 30660
    },
    {
      "epoch": 14.192503470615456,
      "grad_norm": 0.0053290738724172115,
      "learning_rate": 0.07161499305876909,
      "loss": 0.0002,
      "step": 30670
    },
    {
      "epoch": 14.197130957889867,
      "grad_norm": 0.002165879588574171,
      "learning_rate": 0.07160573808422027,
      "loss": 0.001,
      "step": 30680
    },
    {
      "epoch": 14.201758445164275,
      "grad_norm": 0.10088621824979782,
      "learning_rate": 0.07159648310967145,
      "loss": 0.0001,
      "step": 30690
    },
    {
      "epoch": 14.206385932438685,
      "grad_norm": 2.13144588470459,
      "learning_rate": 0.07158722813512262,
      "loss": 0.0008,
      "step": 30700
    },
    {
      "epoch": 14.211013419713096,
      "grad_norm": 0.0017766241217032075,
      "learning_rate": 0.07157797316057381,
      "loss": 0.0007,
      "step": 30710
    },
    {
      "epoch": 14.215640906987506,
      "grad_norm": 0.011114581488072872,
      "learning_rate": 0.07156871818602499,
      "loss": 0.0001,
      "step": 30720
    },
    {
      "epoch": 14.220268394261916,
      "grad_norm": 0.013696788810193539,
      "learning_rate": 0.07155946321147617,
      "loss": 0.0002,
      "step": 30730
    },
    {
      "epoch": 14.224895881536327,
      "grad_norm": 0.34624722599983215,
      "learning_rate": 0.07155020823692736,
      "loss": 0.0002,
      "step": 30740
    },
    {
      "epoch": 14.229523368810735,
      "grad_norm": 0.10354460775852203,
      "learning_rate": 0.07154095326237853,
      "loss": 0.001,
      "step": 30750
    },
    {
      "epoch": 14.234150856085146,
      "grad_norm": 0.006139432545751333,
      "learning_rate": 0.07153169828782971,
      "loss": 0.0001,
      "step": 30760
    },
    {
      "epoch": 14.238778343359556,
      "grad_norm": 0.009186618030071259,
      "learning_rate": 0.07152244331328089,
      "loss": 0.0042,
      "step": 30770
    },
    {
      "epoch": 14.243405830633966,
      "grad_norm": 0.004347573965787888,
      "learning_rate": 0.07151318833873208,
      "loss": 0.0003,
      "step": 30780
    },
    {
      "epoch": 14.248033317908376,
      "grad_norm": 0.014075512066483498,
      "learning_rate": 0.07150393336418324,
      "loss": 0.0002,
      "step": 30790
    },
    {
      "epoch": 14.252660805182785,
      "grad_norm": 0.013876568526029587,
      "learning_rate": 0.07149467838963443,
      "loss": 0.0009,
      "step": 30800
    },
    {
      "epoch": 14.257288292457195,
      "grad_norm": 0.009347386658191681,
      "learning_rate": 0.07148542341508561,
      "loss": 0.0002,
      "step": 30810
    },
    {
      "epoch": 14.261915779731606,
      "grad_norm": 0.003401788417249918,
      "learning_rate": 0.0714761684405368,
      "loss": 0.0015,
      "step": 30820
    },
    {
      "epoch": 14.266543267006016,
      "grad_norm": 0.004650371614843607,
      "learning_rate": 0.07146691346598798,
      "loss": 0.0002,
      "step": 30830
    },
    {
      "epoch": 14.271170754280426,
      "grad_norm": 0.0233121607452631,
      "learning_rate": 0.07145765849143915,
      "loss": 0.0004,
      "step": 30840
    },
    {
      "epoch": 14.275798241554837,
      "grad_norm": 0.33489879965782166,
      "learning_rate": 0.07144840351689033,
      "loss": 0.0002,
      "step": 30850
    },
    {
      "epoch": 14.280425728829245,
      "grad_norm": 0.04348711296916008,
      "learning_rate": 0.07143914854234151,
      "loss": 0.0002,
      "step": 30860
    },
    {
      "epoch": 14.285053216103655,
      "grad_norm": 0.0006756057264283299,
      "learning_rate": 0.0714298935677927,
      "loss": 0.0019,
      "step": 30870
    },
    {
      "epoch": 14.289680703378066,
      "grad_norm": 0.0010058649349957705,
      "learning_rate": 0.07142063859324387,
      "loss": 0.001,
      "step": 30880
    },
    {
      "epoch": 14.294308190652476,
      "grad_norm": 0.008192881010472775,
      "learning_rate": 0.07141138361869505,
      "loss": 0.0002,
      "step": 30890
    },
    {
      "epoch": 14.298935677926886,
      "grad_norm": 0.021596841514110565,
      "learning_rate": 0.07140212864414623,
      "loss": 0.0015,
      "step": 30900
    },
    {
      "epoch": 14.303563165201295,
      "grad_norm": 0.0053459880873560905,
      "learning_rate": 0.07139287366959741,
      "loss": 0.0013,
      "step": 30910
    },
    {
      "epoch": 14.308190652475705,
      "grad_norm": 0.023823104798793793,
      "learning_rate": 0.0713836186950486,
      "loss": 0.0012,
      "step": 30920
    },
    {
      "epoch": 14.312818139750116,
      "grad_norm": 0.048137519508600235,
      "learning_rate": 0.07137436372049977,
      "loss": 0.0047,
      "step": 30930
    },
    {
      "epoch": 14.317445627024526,
      "grad_norm": 0.004600062500685453,
      "learning_rate": 0.07136510874595095,
      "loss": 0.0001,
      "step": 30940
    },
    {
      "epoch": 14.322073114298936,
      "grad_norm": 0.011072972789406776,
      "learning_rate": 0.07135585377140213,
      "loss": 0.0002,
      "step": 30950
    },
    {
      "epoch": 14.326700601573346,
      "grad_norm": 0.07711079716682434,
      "learning_rate": 0.07134659879685332,
      "loss": 0.0,
      "step": 30960
    },
    {
      "epoch": 14.331328088847755,
      "grad_norm": 1.3548485040664673,
      "learning_rate": 0.07133734382230449,
      "loss": 0.0033,
      "step": 30970
    },
    {
      "epoch": 14.335955576122165,
      "grad_norm": 0.07404562830924988,
      "learning_rate": 0.07132808884775567,
      "loss": 0.0007,
      "step": 30980
    },
    {
      "epoch": 14.340583063396576,
      "grad_norm": 0.1839129477739334,
      "learning_rate": 0.07131883387320685,
      "loss": 0.0002,
      "step": 30990
    },
    {
      "epoch": 14.345210550670986,
      "grad_norm": 0.675919234752655,
      "learning_rate": 0.07130957889865804,
      "loss": 0.0011,
      "step": 31000
    },
    {
      "epoch": 14.349838037945396,
      "grad_norm": 0.52068030834198,
      "learning_rate": 0.07130032392410922,
      "loss": 0.0014,
      "step": 31010
    },
    {
      "epoch": 14.354465525219805,
      "grad_norm": 0.037191059440374374,
      "learning_rate": 0.07129106894956039,
      "loss": 0.0004,
      "step": 31020
    },
    {
      "epoch": 14.359093012494215,
      "grad_norm": 0.018482303246855736,
      "learning_rate": 0.07128181397501157,
      "loss": 0.0001,
      "step": 31030
    },
    {
      "epoch": 14.363720499768625,
      "grad_norm": 0.03393247723579407,
      "learning_rate": 0.07127255900046275,
      "loss": 0.0008,
      "step": 31040
    },
    {
      "epoch": 14.368347987043036,
      "grad_norm": 0.0053327870555222034,
      "learning_rate": 0.07126330402591394,
      "loss": 0.0002,
      "step": 31050
    },
    {
      "epoch": 14.372975474317446,
      "grad_norm": 0.011537343263626099,
      "learning_rate": 0.0712540490513651,
      "loss": 0.0001,
      "step": 31060
    },
    {
      "epoch": 14.377602961591856,
      "grad_norm": 0.4601881504058838,
      "learning_rate": 0.07124479407681629,
      "loss": 0.0002,
      "step": 31070
    },
    {
      "epoch": 14.382230448866265,
      "grad_norm": 0.0005510601331479847,
      "learning_rate": 0.07123553910226747,
      "loss": 0.0,
      "step": 31080
    },
    {
      "epoch": 14.386857936140675,
      "grad_norm": 0.001221023965626955,
      "learning_rate": 0.07122628412771866,
      "loss": 0.0001,
      "step": 31090
    },
    {
      "epoch": 14.391485423415086,
      "grad_norm": 0.008019424974918365,
      "learning_rate": 0.07121702915316984,
      "loss": 0.0005,
      "step": 31100
    },
    {
      "epoch": 14.396112910689496,
      "grad_norm": 0.5078443288803101,
      "learning_rate": 0.07120777417862101,
      "loss": 0.0002,
      "step": 31110
    },
    {
      "epoch": 14.400740397963906,
      "grad_norm": 0.13478603959083557,
      "learning_rate": 0.07119851920407219,
      "loss": 0.0008,
      "step": 31120
    },
    {
      "epoch": 14.405367885238316,
      "grad_norm": 0.009074507281184196,
      "learning_rate": 0.07118926422952336,
      "loss": 0.0001,
      "step": 31130
    },
    {
      "epoch": 14.409995372512725,
      "grad_norm": 0.07090335339307785,
      "learning_rate": 0.07118000925497456,
      "loss": 0.0002,
      "step": 31140
    },
    {
      "epoch": 14.414622859787135,
      "grad_norm": 0.04049674794077873,
      "learning_rate": 0.07117075428042573,
      "loss": 0.0002,
      "step": 31150
    },
    {
      "epoch": 14.419250347061546,
      "grad_norm": 0.006055534817278385,
      "learning_rate": 0.07116149930587691,
      "loss": 0.0005,
      "step": 31160
    },
    {
      "epoch": 14.423877834335956,
      "grad_norm": 0.036791831254959106,
      "learning_rate": 0.0711522443313281,
      "loss": 0.0012,
      "step": 31170
    },
    {
      "epoch": 14.428505321610366,
      "grad_norm": 0.01719510369002819,
      "learning_rate": 0.07114298935677928,
      "loss": 0.001,
      "step": 31180
    },
    {
      "epoch": 14.433132808884775,
      "grad_norm": 0.01810295507311821,
      "learning_rate": 0.07113373438223046,
      "loss": 0.0001,
      "step": 31190
    },
    {
      "epoch": 14.437760296159185,
      "grad_norm": 0.006502177100628614,
      "learning_rate": 0.07112447940768163,
      "loss": 0.0065,
      "step": 31200
    },
    {
      "epoch": 14.442387783433595,
      "grad_norm": 0.1702018827199936,
      "learning_rate": 0.07111522443313281,
      "loss": 0.0015,
      "step": 31210
    },
    {
      "epoch": 14.447015270708006,
      "grad_norm": 0.0021849798504263163,
      "learning_rate": 0.07110596945858398,
      "loss": 0.0001,
      "step": 31220
    },
    {
      "epoch": 14.451642757982416,
      "grad_norm": 0.0758337527513504,
      "learning_rate": 0.07109671448403518,
      "loss": 0.0002,
      "step": 31230
    },
    {
      "epoch": 14.456270245256826,
      "grad_norm": 0.008838743902742863,
      "learning_rate": 0.07108745950948635,
      "loss": 0.0001,
      "step": 31240
    },
    {
      "epoch": 14.460897732531235,
      "grad_norm": 0.03331116586923599,
      "learning_rate": 0.07107820453493753,
      "loss": 0.0001,
      "step": 31250
    },
    {
      "epoch": 14.465525219805645,
      "grad_norm": 0.04526963457465172,
      "learning_rate": 0.07106894956038871,
      "loss": 0.0005,
      "step": 31260
    },
    {
      "epoch": 14.470152707080056,
      "grad_norm": 0.001619143527932465,
      "learning_rate": 0.0710596945858399,
      "loss": 0.0002,
      "step": 31270
    },
    {
      "epoch": 14.474780194354466,
      "grad_norm": 0.0024804577697068453,
      "learning_rate": 0.07105043961129108,
      "loss": 0.0001,
      "step": 31280
    },
    {
      "epoch": 14.479407681628876,
      "grad_norm": 0.06665942072868347,
      "learning_rate": 0.07104118463674225,
      "loss": 0.0019,
      "step": 31290
    },
    {
      "epoch": 14.484035168903285,
      "grad_norm": 0.006158741191029549,
      "learning_rate": 0.07103192966219343,
      "loss": 0.0008,
      "step": 31300
    },
    {
      "epoch": 14.488662656177695,
      "grad_norm": 0.008449073880910873,
      "learning_rate": 0.0710226746876446,
      "loss": 0.0001,
      "step": 31310
    },
    {
      "epoch": 14.493290143452105,
      "grad_norm": 0.28939560055732727,
      "learning_rate": 0.0710134197130958,
      "loss": 0.0006,
      "step": 31320
    },
    {
      "epoch": 14.497917630726516,
      "grad_norm": 0.00470602884888649,
      "learning_rate": 0.07100416473854697,
      "loss": 0.0003,
      "step": 31330
    },
    {
      "epoch": 14.502545118000926,
      "grad_norm": 0.03856189176440239,
      "learning_rate": 0.07099490976399815,
      "loss": 0.0001,
      "step": 31340
    },
    {
      "epoch": 14.507172605275336,
      "grad_norm": 4.247815132141113,
      "learning_rate": 0.07098565478944933,
      "loss": 0.0006,
      "step": 31350
    },
    {
      "epoch": 14.511800092549745,
      "grad_norm": 0.003771847579628229,
      "learning_rate": 0.0709763998149005,
      "loss": 0.0005,
      "step": 31360
    },
    {
      "epoch": 14.516427579824155,
      "grad_norm": 0.0629115104675293,
      "learning_rate": 0.0709671448403517,
      "loss": 0.0008,
      "step": 31370
    },
    {
      "epoch": 14.521055067098565,
      "grad_norm": 0.0026985921431332827,
      "learning_rate": 0.07095788986580287,
      "loss": 0.0029,
      "step": 31380
    },
    {
      "epoch": 14.525682554372976,
      "grad_norm": 0.0007926467224024236,
      "learning_rate": 0.07094863489125405,
      "loss": 0.0005,
      "step": 31390
    },
    {
      "epoch": 14.530310041647386,
      "grad_norm": 0.06100649759173393,
      "learning_rate": 0.07093937991670522,
      "loss": 0.0001,
      "step": 31400
    },
    {
      "epoch": 14.534937528921795,
      "grad_norm": 0.042818114161491394,
      "learning_rate": 0.07093012494215642,
      "loss": 0.005,
      "step": 31410
    },
    {
      "epoch": 14.539565016196205,
      "grad_norm": 5.6680192947387695,
      "learning_rate": 0.07092086996760759,
      "loss": 0.0108,
      "step": 31420
    },
    {
      "epoch": 14.544192503470615,
      "grad_norm": 0.0018663534428924322,
      "learning_rate": 0.07091161499305877,
      "loss": 0.0002,
      "step": 31430
    },
    {
      "epoch": 14.548819990745026,
      "grad_norm": 0.08852740377187729,
      "learning_rate": 0.07090236001850996,
      "loss": 0.0001,
      "step": 31440
    },
    {
      "epoch": 14.553447478019436,
      "grad_norm": 0.2067161500453949,
      "learning_rate": 0.07089310504396112,
      "loss": 0.0097,
      "step": 31450
    },
    {
      "epoch": 14.558074965293846,
      "grad_norm": 0.04736487939953804,
      "learning_rate": 0.07088385006941232,
      "loss": 0.0003,
      "step": 31460
    },
    {
      "epoch": 14.562702452568255,
      "grad_norm": 0.027002617716789246,
      "learning_rate": 0.07087459509486349,
      "loss": 0.0001,
      "step": 31470
    },
    {
      "epoch": 14.567329939842665,
      "grad_norm": 0.048384975641965866,
      "learning_rate": 0.07086534012031467,
      "loss": 0.0,
      "step": 31480
    },
    {
      "epoch": 14.571957427117075,
      "grad_norm": 0.0011264930944889784,
      "learning_rate": 0.07085608514576584,
      "loss": 0.0002,
      "step": 31490
    },
    {
      "epoch": 14.576584914391486,
      "grad_norm": 0.00907834805548191,
      "learning_rate": 0.07084683017121704,
      "loss": 0.0002,
      "step": 31500
    },
    {
      "epoch": 14.581212401665896,
      "grad_norm": 0.001408707699738443,
      "learning_rate": 0.07083757519666821,
      "loss": 0.0,
      "step": 31510
    },
    {
      "epoch": 14.585839888940306,
      "grad_norm": 0.013034912757575512,
      "learning_rate": 0.07082832022211939,
      "loss": 0.0035,
      "step": 31520
    },
    {
      "epoch": 14.590467376214715,
      "grad_norm": 0.0047446140088140965,
      "learning_rate": 0.07081906524757058,
      "loss": 0.0001,
      "step": 31530
    },
    {
      "epoch": 14.595094863489125,
      "grad_norm": 0.016755566000938416,
      "learning_rate": 0.07080981027302174,
      "loss": 0.0,
      "step": 31540
    },
    {
      "epoch": 14.599722350763535,
      "grad_norm": 0.006408160552382469,
      "learning_rate": 0.07080055529847294,
      "loss": 0.0001,
      "step": 31550
    },
    {
      "epoch": 14.604349838037946,
      "grad_norm": 0.056195370852947235,
      "learning_rate": 0.07079130032392411,
      "loss": 0.0018,
      "step": 31560
    },
    {
      "epoch": 14.608977325312356,
      "grad_norm": 0.007324615493416786,
      "learning_rate": 0.0707820453493753,
      "loss": 0.0001,
      "step": 31570
    },
    {
      "epoch": 14.613604812586765,
      "grad_norm": 0.004227905068546534,
      "learning_rate": 0.07077279037482646,
      "loss": 0.0004,
      "step": 31580
    },
    {
      "epoch": 14.618232299861175,
      "grad_norm": 0.001200879574753344,
      "learning_rate": 0.07076353540027765,
      "loss": 0.0012,
      "step": 31590
    },
    {
      "epoch": 14.622859787135585,
      "grad_norm": 0.0499490350484848,
      "learning_rate": 0.07075428042572883,
      "loss": 0.0002,
      "step": 31600
    },
    {
      "epoch": 14.627487274409996,
      "grad_norm": 0.004223085008561611,
      "learning_rate": 0.07074502545118001,
      "loss": 0.0017,
      "step": 31610
    },
    {
      "epoch": 14.632114761684406,
      "grad_norm": 0.20155437290668488,
      "learning_rate": 0.0707357704766312,
      "loss": 0.0001,
      "step": 31620
    },
    {
      "epoch": 14.636742248958816,
      "grad_norm": 0.00657466659322381,
      "learning_rate": 0.07072651550208237,
      "loss": 0.0006,
      "step": 31630
    },
    {
      "epoch": 14.641369736233225,
      "grad_norm": 0.004811923485249281,
      "learning_rate": 0.07071726052753356,
      "loss": 0.0002,
      "step": 31640
    },
    {
      "epoch": 14.645997223507635,
      "grad_norm": 0.07269946485757828,
      "learning_rate": 0.07070800555298473,
      "loss": 0.0001,
      "step": 31650
    },
    {
      "epoch": 14.650624710782045,
      "grad_norm": 0.01632070168852806,
      "learning_rate": 0.07069875057843591,
      "loss": 0.0001,
      "step": 31660
    },
    {
      "epoch": 14.655252198056456,
      "grad_norm": 0.003318197326734662,
      "learning_rate": 0.07068949560388708,
      "loss": 0.0211,
      "step": 31670
    },
    {
      "epoch": 14.659879685330866,
      "grad_norm": 1.1334152221679688,
      "learning_rate": 0.07068024062933827,
      "loss": 0.0004,
      "step": 31680
    },
    {
      "epoch": 14.664507172605276,
      "grad_norm": 0.18547523021697998,
      "learning_rate": 0.07067098565478945,
      "loss": 0.0001,
      "step": 31690
    },
    {
      "epoch": 14.669134659879685,
      "grad_norm": 0.041686106473207474,
      "learning_rate": 0.07066173068024063,
      "loss": 0.0011,
      "step": 31700
    },
    {
      "epoch": 14.673762147154095,
      "grad_norm": 0.015722600743174553,
      "learning_rate": 0.07065247570569182,
      "loss": 0.0001,
      "step": 31710
    },
    {
      "epoch": 14.678389634428505,
      "grad_norm": 0.006485623773187399,
      "learning_rate": 0.07064322073114299,
      "loss": 0.0003,
      "step": 31720
    },
    {
      "epoch": 14.683017121702916,
      "grad_norm": 0.0019478979520499706,
      "learning_rate": 0.07063396575659418,
      "loss": 0.0,
      "step": 31730
    },
    {
      "epoch": 14.687644608977326,
      "grad_norm": 0.005593540612608194,
      "learning_rate": 0.07062471078204535,
      "loss": 0.0001,
      "step": 31740
    },
    {
      "epoch": 14.692272096251735,
      "grad_norm": 0.007487508002668619,
      "learning_rate": 0.07061545580749654,
      "loss": 0.0001,
      "step": 31750
    },
    {
      "epoch": 14.696899583526145,
      "grad_norm": 0.8620026111602783,
      "learning_rate": 0.0706062008329477,
      "loss": 0.0011,
      "step": 31760
    },
    {
      "epoch": 14.701527070800555,
      "grad_norm": 0.0005873433547094464,
      "learning_rate": 0.07059694585839889,
      "loss": 0.0012,
      "step": 31770
    },
    {
      "epoch": 14.706154558074966,
      "grad_norm": 0.03906900808215141,
      "learning_rate": 0.07058769088385007,
      "loss": 0.0001,
      "step": 31780
    },
    {
      "epoch": 14.710782045349376,
      "grad_norm": 1.9457728862762451,
      "learning_rate": 0.07057843590930125,
      "loss": 0.0034,
      "step": 31790
    },
    {
      "epoch": 14.715409532623784,
      "grad_norm": 0.04206281900405884,
      "learning_rate": 0.07056918093475244,
      "loss": 0.0002,
      "step": 31800
    },
    {
      "epoch": 14.720037019898195,
      "grad_norm": 0.0029097951482981443,
      "learning_rate": 0.0705599259602036,
      "loss": 0.0002,
      "step": 31810
    },
    {
      "epoch": 14.724664507172605,
      "grad_norm": 0.015636684373021126,
      "learning_rate": 0.07055067098565479,
      "loss": 0.0001,
      "step": 31820
    },
    {
      "epoch": 14.729291994447015,
      "grad_norm": 0.005386679898947477,
      "learning_rate": 0.07054141601110597,
      "loss": 0.0002,
      "step": 31830
    },
    {
      "epoch": 14.733919481721426,
      "grad_norm": 0.3599061369895935,
      "learning_rate": 0.07053216103655716,
      "loss": 0.0005,
      "step": 31840
    },
    {
      "epoch": 14.738546968995836,
      "grad_norm": 0.010662542656064034,
      "learning_rate": 0.07052290606200833,
      "loss": 0.001,
      "step": 31850
    },
    {
      "epoch": 14.743174456270244,
      "grad_norm": 0.01756569929420948,
      "learning_rate": 0.07051365108745951,
      "loss": 0.0003,
      "step": 31860
    },
    {
      "epoch": 14.747801943544655,
      "grad_norm": 0.03737106919288635,
      "learning_rate": 0.07050439611291069,
      "loss": 0.0001,
      "step": 31870
    },
    {
      "epoch": 14.752429430819065,
      "grad_norm": 0.0028223406989127398,
      "learning_rate": 0.07049514113836187,
      "loss": 0.0006,
      "step": 31880
    },
    {
      "epoch": 14.757056918093475,
      "grad_norm": 0.0025245524011552334,
      "learning_rate": 0.07048588616381306,
      "loss": 0.0002,
      "step": 31890
    },
    {
      "epoch": 14.761684405367886,
      "grad_norm": 0.08449435979127884,
      "learning_rate": 0.07047663118926423,
      "loss": 0.0002,
      "step": 31900
    },
    {
      "epoch": 14.766311892642296,
      "grad_norm": 0.03387993946671486,
      "learning_rate": 0.07046737621471541,
      "loss": 0.0003,
      "step": 31910
    },
    {
      "epoch": 14.770939379916705,
      "grad_norm": 0.01360099297016859,
      "learning_rate": 0.0704581212401666,
      "loss": 0.0003,
      "step": 31920
    },
    {
      "epoch": 14.775566867191115,
      "grad_norm": 0.700653612613678,
      "learning_rate": 0.07044886626561778,
      "loss": 0.0004,
      "step": 31930
    },
    {
      "epoch": 14.780194354465525,
      "grad_norm": 0.0017529408214613795,
      "learning_rate": 0.07043961129106895,
      "loss": 0.0001,
      "step": 31940
    },
    {
      "epoch": 14.784821841739936,
      "grad_norm": 0.0010463307844474912,
      "learning_rate": 0.07043035631652013,
      "loss": 0.0001,
      "step": 31950
    },
    {
      "epoch": 14.789449329014346,
      "grad_norm": 0.02776746265590191,
      "learning_rate": 0.07042110134197131,
      "loss": 0.001,
      "step": 31960
    },
    {
      "epoch": 14.794076816288754,
      "grad_norm": 0.038974978029727936,
      "learning_rate": 0.0704118463674225,
      "loss": 0.006,
      "step": 31970
    },
    {
      "epoch": 14.798704303563165,
      "grad_norm": 0.0040287659503519535,
      "learning_rate": 0.07040259139287368,
      "loss": 0.0001,
      "step": 31980
    },
    {
      "epoch": 14.803331790837575,
      "grad_norm": 0.00048493841313757,
      "learning_rate": 0.07039333641832485,
      "loss": 0.0005,
      "step": 31990
    },
    {
      "epoch": 14.807959278111985,
      "grad_norm": 0.0070878081023693085,
      "learning_rate": 0.07038408144377603,
      "loss": 0.0002,
      "step": 32000
    },
    {
      "epoch": 14.812586765386396,
      "grad_norm": 0.0006404687301255763,
      "learning_rate": 0.07037482646922721,
      "loss": 0.0001,
      "step": 32010
    },
    {
      "epoch": 14.817214252660806,
      "grad_norm": 0.010901572182774544,
      "learning_rate": 0.0703655714946784,
      "loss": 0.0001,
      "step": 32020
    },
    {
      "epoch": 14.821841739935214,
      "grad_norm": 0.08277009427547455,
      "learning_rate": 0.07035631652012957,
      "loss": 0.0005,
      "step": 32030
    },
    {
      "epoch": 14.826469227209625,
      "grad_norm": 0.004563712980598211,
      "learning_rate": 0.07034706154558075,
      "loss": 0.0,
      "step": 32040
    },
    {
      "epoch": 14.831096714484035,
      "grad_norm": 0.044948775321245193,
      "learning_rate": 0.07033780657103193,
      "loss": 0.0027,
      "step": 32050
    },
    {
      "epoch": 14.835724201758445,
      "grad_norm": 0.0016888414975255728,
      "learning_rate": 0.07032855159648312,
      "loss": 0.0001,
      "step": 32060
    },
    {
      "epoch": 14.840351689032856,
      "grad_norm": 0.001469725975766778,
      "learning_rate": 0.0703192966219343,
      "loss": 0.0016,
      "step": 32070
    },
    {
      "epoch": 14.844979176307266,
      "grad_norm": 0.010325869545340538,
      "learning_rate": 0.07031004164738547,
      "loss": 0.001,
      "step": 32080
    },
    {
      "epoch": 14.849606663581675,
      "grad_norm": 0.014198212884366512,
      "learning_rate": 0.07030078667283665,
      "loss": 0.0004,
      "step": 32090
    },
    {
      "epoch": 14.854234150856085,
      "grad_norm": 0.005350902676582336,
      "learning_rate": 0.07029153169828783,
      "loss": 0.0004,
      "step": 32100
    },
    {
      "epoch": 14.858861638130495,
      "grad_norm": 0.0031609453726559877,
      "learning_rate": 0.07028227672373902,
      "loss": 0.0001,
      "step": 32110
    },
    {
      "epoch": 14.863489125404906,
      "grad_norm": 0.004592928569763899,
      "learning_rate": 0.07027302174919019,
      "loss": 0.0,
      "step": 32120
    },
    {
      "epoch": 14.868116612679316,
      "grad_norm": 0.016888601705431938,
      "learning_rate": 0.07026376677464137,
      "loss": 0.0002,
      "step": 32130
    },
    {
      "epoch": 14.872744099953724,
      "grad_norm": 0.0004912322037853301,
      "learning_rate": 0.07025451180009255,
      "loss": 0.0004,
      "step": 32140
    },
    {
      "epoch": 14.877371587228135,
      "grad_norm": 0.18149635195732117,
      "learning_rate": 0.07024525682554374,
      "loss": 0.0023,
      "step": 32150
    },
    {
      "epoch": 14.881999074502545,
      "grad_norm": 0.004462265409529209,
      "learning_rate": 0.07023600185099492,
      "loss": 0.0001,
      "step": 32160
    },
    {
      "epoch": 14.886626561776955,
      "grad_norm": 0.021197430789470673,
      "learning_rate": 0.07022674687644609,
      "loss": 0.0109,
      "step": 32170
    },
    {
      "epoch": 14.891254049051366,
      "grad_norm": 0.022037718445062637,
      "learning_rate": 0.07021749190189727,
      "loss": 0.0005,
      "step": 32180
    },
    {
      "epoch": 14.895881536325774,
      "grad_norm": 0.3768925666809082,
      "learning_rate": 0.07020823692734846,
      "loss": 0.0013,
      "step": 32190
    },
    {
      "epoch": 14.900509023600184,
      "grad_norm": 0.0036332011222839355,
      "learning_rate": 0.07019898195279964,
      "loss": 0.0001,
      "step": 32200
    },
    {
      "epoch": 14.905136510874595,
      "grad_norm": 5.840809345245361,
      "learning_rate": 0.07018972697825081,
      "loss": 0.0013,
      "step": 32210
    },
    {
      "epoch": 14.909763998149005,
      "grad_norm": 0.006153414491564035,
      "learning_rate": 0.07018047200370199,
      "loss": 0.0,
      "step": 32220
    },
    {
      "epoch": 14.914391485423415,
      "grad_norm": 0.048927854746580124,
      "learning_rate": 0.07017121702915317,
      "loss": 0.0003,
      "step": 32230
    },
    {
      "epoch": 14.919018972697826,
      "grad_norm": 0.024617604911327362,
      "learning_rate": 0.07016196205460436,
      "loss": 0.0001,
      "step": 32240
    },
    {
      "epoch": 14.923646459972234,
      "grad_norm": 0.002124546328559518,
      "learning_rate": 0.07015270708005554,
      "loss": 0.0023,
      "step": 32250
    },
    {
      "epoch": 14.928273947246645,
      "grad_norm": 0.026515841484069824,
      "learning_rate": 0.07014345210550671,
      "loss": 0.0001,
      "step": 32260
    },
    {
      "epoch": 14.932901434521055,
      "grad_norm": 0.08418575674295425,
      "learning_rate": 0.07013419713095789,
      "loss": 0.0003,
      "step": 32270
    },
    {
      "epoch": 14.937528921795465,
      "grad_norm": 11.949780464172363,
      "learning_rate": 0.07012494215640906,
      "loss": 0.0032,
      "step": 32280
    },
    {
      "epoch": 14.942156409069876,
      "grad_norm": 0.005456339102238417,
      "learning_rate": 0.07011568718186026,
      "loss": 0.0007,
      "step": 32290
    },
    {
      "epoch": 14.946783896344286,
      "grad_norm": 0.011648409999907017,
      "learning_rate": 0.07010643220731143,
      "loss": 0.0014,
      "step": 32300
    },
    {
      "epoch": 14.951411383618694,
      "grad_norm": 0.029172904789447784,
      "learning_rate": 0.07009717723276261,
      "loss": 0.0002,
      "step": 32310
    },
    {
      "epoch": 14.956038870893105,
      "grad_norm": 0.42004987597465515,
      "learning_rate": 0.0700879222582138,
      "loss": 0.0012,
      "step": 32320
    },
    {
      "epoch": 14.960666358167515,
      "grad_norm": 0.00548666762188077,
      "learning_rate": 0.07007866728366498,
      "loss": 0.0006,
      "step": 32330
    },
    {
      "epoch": 14.965293845441925,
      "grad_norm": 0.0008228577207773924,
      "learning_rate": 0.07006941230911616,
      "loss": 0.0001,
      "step": 32340
    },
    {
      "epoch": 14.969921332716336,
      "grad_norm": 0.0031764903105795383,
      "learning_rate": 0.07006015733456733,
      "loss": 0.0007,
      "step": 32350
    },
    {
      "epoch": 14.974548819990744,
      "grad_norm": 0.04574374109506607,
      "learning_rate": 0.07005090236001851,
      "loss": 0.0012,
      "step": 32360
    },
    {
      "epoch": 14.979176307265154,
      "grad_norm": 0.043763063848018646,
      "learning_rate": 0.07004164738546968,
      "loss": 0.0004,
      "step": 32370
    },
    {
      "epoch": 14.983803794539565,
      "grad_norm": 0.008058122359216213,
      "learning_rate": 0.07003239241092088,
      "loss": 0.0009,
      "step": 32380
    },
    {
      "epoch": 14.988431281813975,
      "grad_norm": 0.10211534798145294,
      "learning_rate": 0.07002313743637205,
      "loss": 0.0002,
      "step": 32390
    },
    {
      "epoch": 14.993058769088385,
      "grad_norm": 0.11345242708921432,
      "learning_rate": 0.07001388246182323,
      "loss": 0.0001,
      "step": 32400
    },
    {
      "epoch": 14.997686256362796,
      "grad_norm": 0.0070924535393714905,
      "learning_rate": 0.07000462748727442,
      "loss": 0.0002,
      "step": 32410
    },
    {
      "epoch": 15.0,
      "eval_accuracy_branch1": 0.9902942535818826,
      "eval_accuracy_branch2": 0.4996148513326144,
      "eval_f1_branch1": 0.9899081289228193,
      "eval_f1_branch2": 0.4994262321063328,
      "eval_loss": 0.020086180418729782,
      "eval_precision_branch1": 0.9907105988616273,
      "eval_precision_branch2": 0.4996142699509434,
      "eval_recall_branch1": 0.989239078365922,
      "eval_recall_branch2": 0.4996148513326144,
      "eval_runtime": 28.8375,
      "eval_samples_per_second": 450.177,
      "eval_steps_per_second": 56.281,
      "step": 32415
    },
    {
      "epoch": 15.002313743637204,
      "grad_norm": 0.020929239690303802,
      "learning_rate": 0.0699953725127256,
      "loss": 0.0009,
      "step": 32420
    },
    {
      "epoch": 15.006941230911615,
      "grad_norm": 0.0021439504344016314,
      "learning_rate": 0.06998611753817678,
      "loss": 0.0006,
      "step": 32430
    },
    {
      "epoch": 15.011568718186025,
      "grad_norm": 0.004659893456846476,
      "learning_rate": 0.06997686256362795,
      "loss": 0.0001,
      "step": 32440
    },
    {
      "epoch": 15.016196205460435,
      "grad_norm": 0.0023096788208931684,
      "learning_rate": 0.06996760758907913,
      "loss": 0.0012,
      "step": 32450
    },
    {
      "epoch": 15.020823692734846,
      "grad_norm": 0.0011598709970712662,
      "learning_rate": 0.0699583526145303,
      "loss": 0.0027,
      "step": 32460
    },
    {
      "epoch": 15.025451180009256,
      "grad_norm": 0.05204036459326744,
      "learning_rate": 0.0699490976399815,
      "loss": 0.0002,
      "step": 32470
    },
    {
      "epoch": 15.030078667283664,
      "grad_norm": 0.0038065407425165176,
      "learning_rate": 0.06993984266543267,
      "loss": 0.0002,
      "step": 32480
    },
    {
      "epoch": 15.034706154558075,
      "grad_norm": 0.004799322225153446,
      "learning_rate": 0.06993058769088385,
      "loss": 0.0003,
      "step": 32490
    },
    {
      "epoch": 15.039333641832485,
      "grad_norm": 0.016621502116322517,
      "learning_rate": 0.06992133271633504,
      "loss": 0.0013,
      "step": 32500
    },
    {
      "epoch": 15.043961129106895,
      "grad_norm": 0.09242066740989685,
      "learning_rate": 0.0699120777417862,
      "loss": 0.0001,
      "step": 32510
    },
    {
      "epoch": 15.048588616381306,
      "grad_norm": 8.227058410644531,
      "learning_rate": 0.0699028227672374,
      "loss": 0.0106,
      "step": 32520
    },
    {
      "epoch": 15.053216103655714,
      "grad_norm": 0.00627460703253746,
      "learning_rate": 0.06989356779268857,
      "loss": 0.0004,
      "step": 32530
    },
    {
      "epoch": 15.057843590930124,
      "grad_norm": 0.520328164100647,
      "learning_rate": 0.06988431281813975,
      "loss": 0.0002,
      "step": 32540
    },
    {
      "epoch": 15.062471078204535,
      "grad_norm": 0.000800479028839618,
      "learning_rate": 0.06987505784359092,
      "loss": 0.0001,
      "step": 32550
    },
    {
      "epoch": 15.067098565478945,
      "grad_norm": 0.044878315180540085,
      "learning_rate": 0.06986580286904212,
      "loss": 0.0001,
      "step": 32560
    },
    {
      "epoch": 15.071726052753355,
      "grad_norm": 0.0046149734407663345,
      "learning_rate": 0.06985654789449329,
      "loss": 0.0007,
      "step": 32570
    },
    {
      "epoch": 15.076353540027766,
      "grad_norm": 0.2631194591522217,
      "learning_rate": 0.06984729291994447,
      "loss": 0.0003,
      "step": 32580
    },
    {
      "epoch": 15.080981027302174,
      "grad_norm": 0.0006213593878783286,
      "learning_rate": 0.06983803794539566,
      "loss": 0.0002,
      "step": 32590
    },
    {
      "epoch": 15.085608514576585,
      "grad_norm": 0.0003882063028868288,
      "learning_rate": 0.06982878297084683,
      "loss": 0.0041,
      "step": 32600
    },
    {
      "epoch": 15.090236001850995,
      "grad_norm": 0.007993373088538647,
      "learning_rate": 0.06981952799629802,
      "loss": 0.0005,
      "step": 32610
    },
    {
      "epoch": 15.094863489125405,
      "grad_norm": 0.016035014763474464,
      "learning_rate": 0.06981027302174919,
      "loss": 0.0001,
      "step": 32620
    },
    {
      "epoch": 15.099490976399816,
      "grad_norm": 0.033913504332304,
      "learning_rate": 0.06980101804720037,
      "loss": 0.0002,
      "step": 32630
    },
    {
      "epoch": 15.104118463674224,
      "grad_norm": 3.252532482147217,
      "learning_rate": 0.06979176307265154,
      "loss": 0.0007,
      "step": 32640
    },
    {
      "epoch": 15.108745950948634,
      "grad_norm": 0.003697444684803486,
      "learning_rate": 0.06978250809810274,
      "loss": 0.0001,
      "step": 32650
    },
    {
      "epoch": 15.113373438223045,
      "grad_norm": 0.18530027568340302,
      "learning_rate": 0.06977325312355391,
      "loss": 0.0078,
      "step": 32660
    },
    {
      "epoch": 15.118000925497455,
      "grad_norm": 0.0012121285544708371,
      "learning_rate": 0.0697639981490051,
      "loss": 0.0002,
      "step": 32670
    },
    {
      "epoch": 15.122628412771865,
      "grad_norm": 0.23900067806243896,
      "learning_rate": 0.06975474317445628,
      "loss": 0.0024,
      "step": 32680
    },
    {
      "epoch": 15.127255900046276,
      "grad_norm": 0.002653408795595169,
      "learning_rate": 0.06974548819990745,
      "loss": 0.0001,
      "step": 32690
    },
    {
      "epoch": 15.131883387320684,
      "grad_norm": 0.0010689328191801906,
      "learning_rate": 0.06973623322535864,
      "loss": 0.0026,
      "step": 32700
    },
    {
      "epoch": 15.136510874595094,
      "grad_norm": 0.0054213181138038635,
      "learning_rate": 0.06972697825080981,
      "loss": 0.0032,
      "step": 32710
    },
    {
      "epoch": 15.141138361869505,
      "grad_norm": 0.0464656800031662,
      "learning_rate": 0.069717723276261,
      "loss": 0.0001,
      "step": 32720
    },
    {
      "epoch": 15.145765849143915,
      "grad_norm": 0.02246890962123871,
      "learning_rate": 0.06970846830171216,
      "loss": 0.0001,
      "step": 32730
    },
    {
      "epoch": 15.150393336418325,
      "grad_norm": 0.026058359071612358,
      "learning_rate": 0.06969921332716335,
      "loss": 0.0004,
      "step": 32740
    },
    {
      "epoch": 15.155020823692734,
      "grad_norm": 0.4388390779495239,
      "learning_rate": 0.06968995835261453,
      "loss": 0.0049,
      "step": 32750
    },
    {
      "epoch": 15.159648310967144,
      "grad_norm": 0.009534331038594246,
      "learning_rate": 0.06968070337806571,
      "loss": 0.0004,
      "step": 32760
    },
    {
      "epoch": 15.164275798241555,
      "grad_norm": 0.02309446781873703,
      "learning_rate": 0.0696714484035169,
      "loss": 0.0001,
      "step": 32770
    },
    {
      "epoch": 15.168903285515965,
      "grad_norm": 0.026141053065657616,
      "learning_rate": 0.06966219342896807,
      "loss": 0.0001,
      "step": 32780
    },
    {
      "epoch": 15.173530772790375,
      "grad_norm": 0.0008573622908443213,
      "learning_rate": 0.06965293845441926,
      "loss": 0.0001,
      "step": 32790
    },
    {
      "epoch": 15.178158260064786,
      "grad_norm": 0.004192615393549204,
      "learning_rate": 0.06964368347987043,
      "loss": 0.0002,
      "step": 32800
    },
    {
      "epoch": 15.182785747339194,
      "grad_norm": 0.0174286849796772,
      "learning_rate": 0.06963442850532162,
      "loss": 0.0003,
      "step": 32810
    },
    {
      "epoch": 15.187413234613604,
      "grad_norm": 0.11681443452835083,
      "learning_rate": 0.06962517353077279,
      "loss": 0.0003,
      "step": 32820
    },
    {
      "epoch": 15.192040721888015,
      "grad_norm": 0.001514102448709309,
      "learning_rate": 0.06961591855622397,
      "loss": 0.0002,
      "step": 32830
    },
    {
      "epoch": 15.196668209162425,
      "grad_norm": 0.09141924232244492,
      "learning_rate": 0.06960666358167515,
      "loss": 0.0001,
      "step": 32840
    },
    {
      "epoch": 15.201295696436835,
      "grad_norm": 0.8821149468421936,
      "learning_rate": 0.06959740860712633,
      "loss": 0.0026,
      "step": 32850
    },
    {
      "epoch": 15.205923183711246,
      "grad_norm": 0.5714524388313293,
      "learning_rate": 0.06958815363257752,
      "loss": 0.0003,
      "step": 32860
    },
    {
      "epoch": 15.210550670985654,
      "grad_norm": 0.0018008975312113762,
      "learning_rate": 0.06957889865802869,
      "loss": 0.0,
      "step": 32870
    },
    {
      "epoch": 15.215178158260064,
      "grad_norm": 0.007512789685279131,
      "learning_rate": 0.06956964368347988,
      "loss": 0.0002,
      "step": 32880
    },
    {
      "epoch": 15.219805645534475,
      "grad_norm": 0.0011635933769866824,
      "learning_rate": 0.06956038870893105,
      "loss": 0.0001,
      "step": 32890
    },
    {
      "epoch": 15.224433132808885,
      "grad_norm": 0.002001967513933778,
      "learning_rate": 0.06955113373438224,
      "loss": 0.0001,
      "step": 32900
    },
    {
      "epoch": 15.229060620083295,
      "grad_norm": 0.002104381797835231,
      "learning_rate": 0.0695418787598334,
      "loss": 0.0002,
      "step": 32910
    },
    {
      "epoch": 15.233688107357704,
      "grad_norm": 0.0011527067981660366,
      "learning_rate": 0.06953262378528459,
      "loss": 0.0001,
      "step": 32920
    },
    {
      "epoch": 15.238315594632114,
      "grad_norm": 0.0013161123497411609,
      "learning_rate": 0.06952336881073577,
      "loss": 0.0001,
      "step": 32930
    },
    {
      "epoch": 15.242943081906525,
      "grad_norm": 0.0026165831368416548,
      "learning_rate": 0.06951411383618696,
      "loss": 0.0002,
      "step": 32940
    },
    {
      "epoch": 15.247570569180935,
      "grad_norm": 0.0010483900550752878,
      "learning_rate": 0.06950485886163814,
      "loss": 0.0001,
      "step": 32950
    },
    {
      "epoch": 15.252198056455345,
      "grad_norm": 0.17136186361312866,
      "learning_rate": 0.06949560388708931,
      "loss": 0.0003,
      "step": 32960
    },
    {
      "epoch": 15.256825543729756,
      "grad_norm": 0.005002390127629042,
      "learning_rate": 0.06948634891254049,
      "loss": 0.0006,
      "step": 32970
    },
    {
      "epoch": 15.261453031004164,
      "grad_norm": 0.009600907564163208,
      "learning_rate": 0.06947709393799167,
      "loss": 0.0001,
      "step": 32980
    },
    {
      "epoch": 15.266080518278574,
      "grad_norm": 0.005393465515226126,
      "learning_rate": 0.06946783896344286,
      "loss": 0.0001,
      "step": 32990
    },
    {
      "epoch": 15.270708005552985,
      "grad_norm": 0.04863661527633667,
      "learning_rate": 0.06945858398889403,
      "loss": 0.0001,
      "step": 33000
    },
    {
      "epoch": 15.275335492827395,
      "grad_norm": 0.0075609409250319,
      "learning_rate": 0.06944932901434521,
      "loss": 0.0001,
      "step": 33010
    },
    {
      "epoch": 15.279962980101805,
      "grad_norm": 0.024694673717021942,
      "learning_rate": 0.06944007403979639,
      "loss": 0.0003,
      "step": 33020
    },
    {
      "epoch": 15.284590467376216,
      "grad_norm": 0.01818828284740448,
      "learning_rate": 0.06943081906524758,
      "loss": 0.0001,
      "step": 33030
    },
    {
      "epoch": 15.289217954650624,
      "grad_norm": 0.006348945666104555,
      "learning_rate": 0.06942156409069876,
      "loss": 0.0051,
      "step": 33040
    },
    {
      "epoch": 15.293845441925034,
      "grad_norm": 0.015655551105737686,
      "learning_rate": 0.06941230911614993,
      "loss": 0.0008,
      "step": 33050
    },
    {
      "epoch": 15.298472929199445,
      "grad_norm": 0.008114445023238659,
      "learning_rate": 0.06940305414160111,
      "loss": 0.0022,
      "step": 33060
    },
    {
      "epoch": 15.303100416473855,
      "grad_norm": 0.004303701221942902,
      "learning_rate": 0.0693937991670523,
      "loss": 0.0001,
      "step": 33070
    },
    {
      "epoch": 15.307727903748265,
      "grad_norm": 0.056564394384622574,
      "learning_rate": 0.06938454419250348,
      "loss": 0.0001,
      "step": 33080
    },
    {
      "epoch": 15.312355391022674,
      "grad_norm": 0.0017197205452248454,
      "learning_rate": 0.06937528921795465,
      "loss": 0.0002,
      "step": 33090
    },
    {
      "epoch": 15.316982878297084,
      "grad_norm": 0.17554596066474915,
      "learning_rate": 0.06936603424340583,
      "loss": 0.0002,
      "step": 33100
    },
    {
      "epoch": 15.321610365571495,
      "grad_norm": 0.09241805970668793,
      "learning_rate": 0.06935677926885701,
      "loss": 0.0002,
      "step": 33110
    },
    {
      "epoch": 15.326237852845905,
      "grad_norm": 0.01218741200864315,
      "learning_rate": 0.0693475242943082,
      "loss": 0.0001,
      "step": 33120
    },
    {
      "epoch": 15.330865340120315,
      "grad_norm": 0.003405529074370861,
      "learning_rate": 0.06933826931975938,
      "loss": 0.0002,
      "step": 33130
    },
    {
      "epoch": 15.335492827394726,
      "grad_norm": 1.4475045204162598,
      "learning_rate": 0.06932901434521055,
      "loss": 0.0008,
      "step": 33140
    },
    {
      "epoch": 15.340120314669134,
      "grad_norm": 0.004027914721518755,
      "learning_rate": 0.06931975937066173,
      "loss": 0.0034,
      "step": 33150
    },
    {
      "epoch": 15.344747801943544,
      "grad_norm": 0.006033134646713734,
      "learning_rate": 0.06931050439611292,
      "loss": 0.0001,
      "step": 33160
    },
    {
      "epoch": 15.349375289217955,
      "grad_norm": 0.0023067162837833166,
      "learning_rate": 0.0693012494215641,
      "loss": 0.0057,
      "step": 33170
    },
    {
      "epoch": 15.354002776492365,
      "grad_norm": 0.001001139753498137,
      "learning_rate": 0.06929199444701527,
      "loss": 0.0001,
      "step": 33180
    },
    {
      "epoch": 15.358630263766775,
      "grad_norm": 1.20619797706604,
      "learning_rate": 0.06928273947246645,
      "loss": 0.0028,
      "step": 33190
    },
    {
      "epoch": 15.363257751041184,
      "grad_norm": 0.0018226426327601075,
      "learning_rate": 0.06927348449791763,
      "loss": 0.0,
      "step": 33200
    },
    {
      "epoch": 15.367885238315594,
      "grad_norm": 0.0008894935017451644,
      "learning_rate": 0.06926422952336882,
      "loss": 0.0006,
      "step": 33210
    },
    {
      "epoch": 15.372512725590004,
      "grad_norm": 5.5395660400390625,
      "learning_rate": 0.06925497454882,
      "loss": 0.0014,
      "step": 33220
    },
    {
      "epoch": 15.377140212864415,
      "grad_norm": 0.010596334002912045,
      "learning_rate": 0.06924571957427117,
      "loss": 0.0003,
      "step": 33230
    },
    {
      "epoch": 15.381767700138825,
      "grad_norm": 0.5515245199203491,
      "learning_rate": 0.06923646459972235,
      "loss": 0.0042,
      "step": 33240
    },
    {
      "epoch": 15.386395187413235,
      "grad_norm": 0.04656381532549858,
      "learning_rate": 0.06922720962517354,
      "loss": 0.0002,
      "step": 33250
    },
    {
      "epoch": 15.391022674687644,
      "grad_norm": 0.014019272290170193,
      "learning_rate": 0.06921795465062472,
      "loss": 0.0001,
      "step": 33260
    },
    {
      "epoch": 15.395650161962054,
      "grad_norm": 0.0018407149473205209,
      "learning_rate": 0.06920869967607589,
      "loss": 0.0004,
      "step": 33270
    },
    {
      "epoch": 15.400277649236465,
      "grad_norm": 0.012336318381130695,
      "learning_rate": 0.06919944470152707,
      "loss": 0.0001,
      "step": 33280
    },
    {
      "epoch": 15.404905136510875,
      "grad_norm": 0.01930483803153038,
      "learning_rate": 0.06919018972697825,
      "loss": 0.0002,
      "step": 33290
    },
    {
      "epoch": 15.409532623785285,
      "grad_norm": 0.0040853735990822315,
      "learning_rate": 0.06918093475242944,
      "loss": 0.0018,
      "step": 33300
    },
    {
      "epoch": 15.414160111059694,
      "grad_norm": 0.04990532994270325,
      "learning_rate": 0.06917167977788062,
      "loss": 0.0001,
      "step": 33310
    },
    {
      "epoch": 15.418787598334104,
      "grad_norm": 0.005886294413357973,
      "learning_rate": 0.06916242480333179,
      "loss": 0.0004,
      "step": 33320
    },
    {
      "epoch": 15.423415085608514,
      "grad_norm": 0.03935741260647774,
      "learning_rate": 0.06915316982878297,
      "loss": 0.0002,
      "step": 33330
    },
    {
      "epoch": 15.428042572882925,
      "grad_norm": 0.0132000632584095,
      "learning_rate": 0.06914391485423416,
      "loss": 0.0001,
      "step": 33340
    },
    {
      "epoch": 15.432670060157335,
      "grad_norm": 1.9608750343322754,
      "learning_rate": 0.06913465987968534,
      "loss": 0.0013,
      "step": 33350
    },
    {
      "epoch": 15.437297547431745,
      "grad_norm": 0.017148084938526154,
      "learning_rate": 0.06912540490513651,
      "loss": 0.0088,
      "step": 33360
    },
    {
      "epoch": 15.441925034706154,
      "grad_norm": 0.22765973210334778,
      "learning_rate": 0.06911614993058769,
      "loss": 0.0003,
      "step": 33370
    },
    {
      "epoch": 15.446552521980564,
      "grad_norm": 5.202823162078857,
      "learning_rate": 0.06910689495603888,
      "loss": 0.003,
      "step": 33380
    },
    {
      "epoch": 15.451180009254974,
      "grad_norm": 0.7760140299797058,
      "learning_rate": 0.06909763998149006,
      "loss": 0.0004,
      "step": 33390
    },
    {
      "epoch": 15.455807496529385,
      "grad_norm": 0.008394676260650158,
      "learning_rate": 0.06908838500694124,
      "loss": 0.0001,
      "step": 33400
    },
    {
      "epoch": 15.460434983803795,
      "grad_norm": 0.0009670300642028451,
      "learning_rate": 0.06907913003239241,
      "loss": 0.0002,
      "step": 33410
    },
    {
      "epoch": 15.465062471078205,
      "grad_norm": 0.022769073024392128,
      "learning_rate": 0.0690698750578436,
      "loss": 0.0001,
      "step": 33420
    },
    {
      "epoch": 15.469689958352614,
      "grad_norm": 0.001729306997731328,
      "learning_rate": 0.06906062008329476,
      "loss": 0.0001,
      "step": 33430
    },
    {
      "epoch": 15.474317445627024,
      "grad_norm": 0.003852754132822156,
      "learning_rate": 0.06905136510874596,
      "loss": 0.0003,
      "step": 33440
    },
    {
      "epoch": 15.478944932901435,
      "grad_norm": 0.02294524572789669,
      "learning_rate": 0.06904211013419713,
      "loss": 0.0007,
      "step": 33450
    },
    {
      "epoch": 15.483572420175845,
      "grad_norm": 0.14685916900634766,
      "learning_rate": 0.06903285515964831,
      "loss": 0.0006,
      "step": 33460
    },
    {
      "epoch": 15.488199907450255,
      "grad_norm": 0.0023760979529470205,
      "learning_rate": 0.0690236001850995,
      "loss": 0.0003,
      "step": 33470
    },
    {
      "epoch": 15.492827394724664,
      "grad_norm": 0.0003811015922110528,
      "learning_rate": 0.06901434521055068,
      "loss": 0.0006,
      "step": 33480
    },
    {
      "epoch": 15.497454881999074,
      "grad_norm": 0.0050741820596158504,
      "learning_rate": 0.06900509023600186,
      "loss": 0.0001,
      "step": 33490
    },
    {
      "epoch": 15.502082369273484,
      "grad_norm": 0.0039599123410880566,
      "learning_rate": 0.06899583526145303,
      "loss": 0.0019,
      "step": 33500
    },
    {
      "epoch": 15.506709856547895,
      "grad_norm": 0.011261243373155594,
      "learning_rate": 0.06898658028690421,
      "loss": 0.0005,
      "step": 33510
    },
    {
      "epoch": 15.511337343822305,
      "grad_norm": 0.001462652813643217,
      "learning_rate": 0.06897732531235538,
      "loss": 0.0002,
      "step": 33520
    },
    {
      "epoch": 15.515964831096715,
      "grad_norm": 0.004709211178123951,
      "learning_rate": 0.06896807033780658,
      "loss": 0.0003,
      "step": 33530
    },
    {
      "epoch": 15.520592318371124,
      "grad_norm": 0.023401809856295586,
      "learning_rate": 0.06895881536325775,
      "loss": 0.0001,
      "step": 33540
    },
    {
      "epoch": 15.525219805645534,
      "grad_norm": 0.0012634142767637968,
      "learning_rate": 0.06894956038870893,
      "loss": 0.001,
      "step": 33550
    },
    {
      "epoch": 15.529847292919944,
      "grad_norm": 0.007230529561638832,
      "learning_rate": 0.06894030541416012,
      "loss": 0.0002,
      "step": 33560
    },
    {
      "epoch": 15.534474780194355,
      "grad_norm": 0.001047945232130587,
      "learning_rate": 0.06893105043961129,
      "loss": 0.0003,
      "step": 33570
    },
    {
      "epoch": 15.539102267468765,
      "grad_norm": 0.028590524569153786,
      "learning_rate": 0.06892179546506248,
      "loss": 0.0017,
      "step": 33580
    },
    {
      "epoch": 15.543729754743174,
      "grad_norm": 0.0791935846209526,
      "learning_rate": 0.06891254049051365,
      "loss": 0.0001,
      "step": 33590
    },
    {
      "epoch": 15.548357242017584,
      "grad_norm": 0.06762051582336426,
      "learning_rate": 0.06890328551596483,
      "loss": 0.0001,
      "step": 33600
    },
    {
      "epoch": 15.552984729291994,
      "grad_norm": 0.015231356956064701,
      "learning_rate": 0.068894030541416,
      "loss": 0.0002,
      "step": 33610
    },
    {
      "epoch": 15.557612216566405,
      "grad_norm": 0.00701487623155117,
      "learning_rate": 0.0688847755668672,
      "loss": 0.0002,
      "step": 33620
    },
    {
      "epoch": 15.562239703840815,
      "grad_norm": 0.005856657866388559,
      "learning_rate": 0.06887552059231837,
      "loss": 0.0002,
      "step": 33630
    },
    {
      "epoch": 15.566867191115225,
      "grad_norm": 0.0006604714435525239,
      "learning_rate": 0.06886626561776955,
      "loss": 0.0001,
      "step": 33640
    },
    {
      "epoch": 15.571494678389634,
      "grad_norm": 0.19014102220535278,
      "learning_rate": 0.06885701064322074,
      "loss": 0.0001,
      "step": 33650
    },
    {
      "epoch": 15.576122165664044,
      "grad_norm": 0.003156342776492238,
      "learning_rate": 0.0688477556686719,
      "loss": 0.0011,
      "step": 33660
    },
    {
      "epoch": 15.580749652938454,
      "grad_norm": 0.026467110961675644,
      "learning_rate": 0.0688385006941231,
      "loss": 0.0001,
      "step": 33670
    },
    {
      "epoch": 15.585377140212865,
      "grad_norm": 0.019779998809099197,
      "learning_rate": 0.06882924571957427,
      "loss": 0.0001,
      "step": 33680
    },
    {
      "epoch": 15.590004627487275,
      "grad_norm": 0.0005970943020656705,
      "learning_rate": 0.06881999074502546,
      "loss": 0.0001,
      "step": 33690
    },
    {
      "epoch": 15.594632114761684,
      "grad_norm": 0.009753627702593803,
      "learning_rate": 0.06881073577047662,
      "loss": 0.0015,
      "step": 33700
    },
    {
      "epoch": 15.599259602036094,
      "grad_norm": 0.0001889976701932028,
      "learning_rate": 0.06880148079592782,
      "loss": 0.0,
      "step": 33710
    },
    {
      "epoch": 15.603887089310504,
      "grad_norm": 0.0007376174326054752,
      "learning_rate": 0.06879222582137899,
      "loss": 0.0001,
      "step": 33720
    },
    {
      "epoch": 15.608514576584914,
      "grad_norm": 0.0048148552887141705,
      "learning_rate": 0.06878297084683017,
      "loss": 0.0002,
      "step": 33730
    },
    {
      "epoch": 15.613142063859325,
      "grad_norm": 0.023212330415844917,
      "learning_rate": 0.06877371587228136,
      "loss": 0.0,
      "step": 33740
    },
    {
      "epoch": 15.617769551133735,
      "grad_norm": 0.02625279128551483,
      "learning_rate": 0.06876446089773253,
      "loss": 0.0001,
      "step": 33750
    },
    {
      "epoch": 15.622397038408144,
      "grad_norm": 0.016484249383211136,
      "learning_rate": 0.06875520592318372,
      "loss": 0.0001,
      "step": 33760
    },
    {
      "epoch": 15.627024525682554,
      "grad_norm": 0.0014628252247348428,
      "learning_rate": 0.06874595094863489,
      "loss": 0.0001,
      "step": 33770
    },
    {
      "epoch": 15.631652012956964,
      "grad_norm": 0.5577430129051208,
      "learning_rate": 0.06873669597408608,
      "loss": 0.0002,
      "step": 33780
    },
    {
      "epoch": 15.636279500231375,
      "grad_norm": 0.17686301469802856,
      "learning_rate": 0.06872744099953725,
      "loss": 0.0001,
      "step": 33790
    },
    {
      "epoch": 15.640906987505785,
      "grad_norm": 0.5884138941764832,
      "learning_rate": 0.06871818602498843,
      "loss": 0.0007,
      "step": 33800
    },
    {
      "epoch": 15.645534474780195,
      "grad_norm": 0.0008594905375503004,
      "learning_rate": 0.06870893105043961,
      "loss": 0.0019,
      "step": 33810
    },
    {
      "epoch": 15.650161962054604,
      "grad_norm": 0.0029872239101678133,
      "learning_rate": 0.0686996760758908,
      "loss": 0.0105,
      "step": 33820
    },
    {
      "epoch": 15.654789449329014,
      "grad_norm": 0.002330385847017169,
      "learning_rate": 0.06869042110134198,
      "loss": 0.0021,
      "step": 33830
    },
    {
      "epoch": 15.659416936603424,
      "grad_norm": 0.05896654352545738,
      "learning_rate": 0.06868116612679315,
      "loss": 0.0002,
      "step": 33840
    },
    {
      "epoch": 15.664044423877835,
      "grad_norm": 0.0017260691383853555,
      "learning_rate": 0.06867191115224434,
      "loss": 0.0001,
      "step": 33850
    },
    {
      "epoch": 15.668671911152245,
      "grad_norm": 0.0057187797501683235,
      "learning_rate": 0.06866265617769551,
      "loss": 0.0,
      "step": 33860
    },
    {
      "epoch": 15.673299398426654,
      "grad_norm": 0.008220753632485867,
      "learning_rate": 0.0686534012031467,
      "loss": 0.0003,
      "step": 33870
    },
    {
      "epoch": 15.677926885701064,
      "grad_norm": 0.5942864418029785,
      "learning_rate": 0.06864414622859787,
      "loss": 0.0004,
      "step": 33880
    },
    {
      "epoch": 15.682554372975474,
      "grad_norm": 0.07240033149719238,
      "learning_rate": 0.06863489125404905,
      "loss": 0.001,
      "step": 33890
    },
    {
      "epoch": 15.687181860249884,
      "grad_norm": 0.18682436645030975,
      "learning_rate": 0.06862563627950023,
      "loss": 0.0003,
      "step": 33900
    },
    {
      "epoch": 15.691809347524295,
      "grad_norm": 0.002241148380562663,
      "learning_rate": 0.06861638130495142,
      "loss": 0.0024,
      "step": 33910
    },
    {
      "epoch": 15.696436834798705,
      "grad_norm": 0.031084982678294182,
      "learning_rate": 0.0686071263304026,
      "loss": 0.0005,
      "step": 33920
    },
    {
      "epoch": 15.701064322073114,
      "grad_norm": 3.5519516468048096,
      "learning_rate": 0.06859787135585377,
      "loss": 0.0014,
      "step": 33930
    },
    {
      "epoch": 15.705691809347524,
      "grad_norm": 0.05698854476213455,
      "learning_rate": 0.06858861638130496,
      "loss": 0.0004,
      "step": 33940
    },
    {
      "epoch": 15.710319296621934,
      "grad_norm": 0.0019597881473600864,
      "learning_rate": 0.06857936140675613,
      "loss": 0.0005,
      "step": 33950
    },
    {
      "epoch": 15.714946783896345,
      "grad_norm": 0.011854877695441246,
      "learning_rate": 0.06857010643220732,
      "loss": 0.003,
      "step": 33960
    },
    {
      "epoch": 15.719574271170755,
      "grad_norm": 0.0021194329019635916,
      "learning_rate": 0.06856085145765849,
      "loss": 0.0008,
      "step": 33970
    },
    {
      "epoch": 15.724201758445165,
      "grad_norm": 0.012990167364478111,
      "learning_rate": 0.06855159648310967,
      "loss": 0.0001,
      "step": 33980
    },
    {
      "epoch": 15.728829245719574,
      "grad_norm": 0.00976520124822855,
      "learning_rate": 0.06854234150856085,
      "loss": 0.0007,
      "step": 33990
    },
    {
      "epoch": 15.733456732993984,
      "grad_norm": 0.0037759877741336823,
      "learning_rate": 0.06853308653401204,
      "loss": 0.0001,
      "step": 34000
    },
    {
      "epoch": 15.738084220268394,
      "grad_norm": 0.0014651319943368435,
      "learning_rate": 0.06852383155946322,
      "loss": 0.0001,
      "step": 34010
    },
    {
      "epoch": 15.742711707542805,
      "grad_norm": 0.004048450384289026,
      "learning_rate": 0.06851457658491439,
      "loss": 0.0019,
      "step": 34020
    },
    {
      "epoch": 15.747339194817215,
      "grad_norm": 0.02069605141878128,
      "learning_rate": 0.06850532161036557,
      "loss": 0.0003,
      "step": 34030
    },
    {
      "epoch": 15.751966682091624,
      "grad_norm": 0.007703700102865696,
      "learning_rate": 0.06849606663581675,
      "loss": 0.0081,
      "step": 34040
    },
    {
      "epoch": 15.756594169366034,
      "grad_norm": 0.01431269757449627,
      "learning_rate": 0.06848681166126794,
      "loss": 0.0006,
      "step": 34050
    },
    {
      "epoch": 15.761221656640444,
      "grad_norm": 0.01383037306368351,
      "learning_rate": 0.06847755668671911,
      "loss": 0.0004,
      "step": 34060
    },
    {
      "epoch": 15.765849143914854,
      "grad_norm": 0.025066496804356575,
      "learning_rate": 0.06846830171217029,
      "loss": 0.0004,
      "step": 34070
    },
    {
      "epoch": 15.770476631189265,
      "grad_norm": 0.009342954494059086,
      "learning_rate": 0.06845904673762147,
      "loss": 0.0003,
      "step": 34080
    },
    {
      "epoch": 15.775104118463673,
      "grad_norm": 0.00048367425915785134,
      "learning_rate": 0.06844979176307266,
      "loss": 0.0002,
      "step": 34090
    },
    {
      "epoch": 15.779731605738084,
      "grad_norm": 0.0020364064257591963,
      "learning_rate": 0.06844053678852384,
      "loss": 0.0002,
      "step": 34100
    },
    {
      "epoch": 15.784359093012494,
      "grad_norm": 0.0043258462101221085,
      "learning_rate": 0.06843128181397501,
      "loss": 0.0001,
      "step": 34110
    },
    {
      "epoch": 15.788986580286904,
      "grad_norm": 0.006345395930111408,
      "learning_rate": 0.06842202683942619,
      "loss": 0.0003,
      "step": 34120
    },
    {
      "epoch": 15.793614067561315,
      "grad_norm": 0.0009162654168903828,
      "learning_rate": 0.06841277186487738,
      "loss": 0.0001,
      "step": 34130
    },
    {
      "epoch": 15.798241554835725,
      "grad_norm": 0.12250527739524841,
      "learning_rate": 0.06840351689032856,
      "loss": 0.0002,
      "step": 34140
    },
    {
      "epoch": 15.802869042110133,
      "grad_norm": 0.0083156144246459,
      "learning_rate": 0.06839426191577973,
      "loss": 0.0008,
      "step": 34150
    },
    {
      "epoch": 15.807496529384544,
      "grad_norm": 0.0042938413098454475,
      "learning_rate": 0.06838500694123091,
      "loss": 0.0001,
      "step": 34160
    },
    {
      "epoch": 15.812124016658954,
      "grad_norm": 0.0019728210754692554,
      "learning_rate": 0.0683757519666821,
      "loss": 0.0006,
      "step": 34170
    },
    {
      "epoch": 15.816751503933364,
      "grad_norm": 0.24583090841770172,
      "learning_rate": 0.06836649699213328,
      "loss": 0.0006,
      "step": 34180
    },
    {
      "epoch": 15.821378991207775,
      "grad_norm": 0.44308194518089294,
      "learning_rate": 0.06835724201758446,
      "loss": 0.0015,
      "step": 34190
    },
    {
      "epoch": 15.826006478482185,
      "grad_norm": 0.009260856546461582,
      "learning_rate": 0.06834798704303563,
      "loss": 0.0012,
      "step": 34200
    },
    {
      "epoch": 15.830633965756594,
      "grad_norm": 0.041549403220415115,
      "learning_rate": 0.06833873206848681,
      "loss": 0.0001,
      "step": 34210
    },
    {
      "epoch": 15.835261453031004,
      "grad_norm": 0.006144050974398851,
      "learning_rate": 0.068329477093938,
      "loss": 0.0001,
      "step": 34220
    },
    {
      "epoch": 15.839888940305414,
      "grad_norm": 0.007195701356977224,
      "learning_rate": 0.06832022211938918,
      "loss": 0.0005,
      "step": 34230
    },
    {
      "epoch": 15.844516427579824,
      "grad_norm": 0.1057465448975563,
      "learning_rate": 0.06831096714484035,
      "loss": 0.0006,
      "step": 34240
    },
    {
      "epoch": 15.849143914854235,
      "grad_norm": 0.01927892304956913,
      "learning_rate": 0.06830171217029153,
      "loss": 0.0001,
      "step": 34250
    },
    {
      "epoch": 15.853771402128643,
      "grad_norm": 0.02601032890379429,
      "learning_rate": 0.06829245719574271,
      "loss": 0.0002,
      "step": 34260
    },
    {
      "epoch": 15.858398889403054,
      "grad_norm": 0.050782568752765656,
      "learning_rate": 0.0682832022211939,
      "loss": 0.0006,
      "step": 34270
    },
    {
      "epoch": 15.863026376677464,
      "grad_norm": 0.0005530646303668618,
      "learning_rate": 0.06827394724664508,
      "loss": 0.0001,
      "step": 34280
    },
    {
      "epoch": 15.867653863951874,
      "grad_norm": 0.0017905407585203648,
      "learning_rate": 0.06826469227209625,
      "loss": 0.0001,
      "step": 34290
    },
    {
      "epoch": 15.872281351226285,
      "grad_norm": 0.0015684935497120023,
      "learning_rate": 0.06825543729754743,
      "loss": 0.0006,
      "step": 34300
    },
    {
      "epoch": 15.876908838500695,
      "grad_norm": 0.05943430960178375,
      "learning_rate": 0.06824618232299862,
      "loss": 0.0003,
      "step": 34310
    },
    {
      "epoch": 15.881536325775103,
      "grad_norm": 0.001949095749296248,
      "learning_rate": 0.0682369273484498,
      "loss": 0.0006,
      "step": 34320
    },
    {
      "epoch": 15.886163813049514,
      "grad_norm": 0.006649557966738939,
      "learning_rate": 0.06822767237390097,
      "loss": 0.001,
      "step": 34330
    },
    {
      "epoch": 15.890791300323924,
      "grad_norm": 0.005134883802384138,
      "learning_rate": 0.06821841739935215,
      "loss": 0.0001,
      "step": 34340
    },
    {
      "epoch": 15.895418787598334,
      "grad_norm": 0.004998992662876844,
      "learning_rate": 0.06820916242480334,
      "loss": 0.0008,
      "step": 34350
    },
    {
      "epoch": 15.900046274872745,
      "grad_norm": 0.003957964945584536,
      "learning_rate": 0.06819990745025452,
      "loss": 0.0001,
      "step": 34360
    },
    {
      "epoch": 15.904673762147155,
      "grad_norm": 0.11147771775722504,
      "learning_rate": 0.0681906524757057,
      "loss": 0.0011,
      "step": 34370
    },
    {
      "epoch": 15.909301249421564,
      "grad_norm": 0.23216161131858826,
      "learning_rate": 0.06818139750115687,
      "loss": 0.0002,
      "step": 34380
    },
    {
      "epoch": 15.913928736695974,
      "grad_norm": 0.04652508720755577,
      "learning_rate": 0.06817214252660805,
      "loss": 0.0006,
      "step": 34390
    },
    {
      "epoch": 15.918556223970384,
      "grad_norm": 0.0007966207922436297,
      "learning_rate": 0.06816288755205924,
      "loss": 0.0006,
      "step": 34400
    },
    {
      "epoch": 15.923183711244794,
      "grad_norm": 0.010224034078419209,
      "learning_rate": 0.06815363257751042,
      "loss": 0.0002,
      "step": 34410
    },
    {
      "epoch": 15.927811198519205,
      "grad_norm": 0.015245450660586357,
      "learning_rate": 0.06814437760296159,
      "loss": 0.0001,
      "step": 34420
    },
    {
      "epoch": 15.932438685793613,
      "grad_norm": 0.016935719177126884,
      "learning_rate": 0.06813512262841277,
      "loss": 0.0001,
      "step": 34430
    },
    {
      "epoch": 15.937066173068024,
      "grad_norm": 0.0028894245624542236,
      "learning_rate": 0.06812586765386396,
      "loss": 0.0001,
      "step": 34440
    },
    {
      "epoch": 15.941693660342434,
      "grad_norm": 0.06099848076701164,
      "learning_rate": 0.06811661267931514,
      "loss": 0.0004,
      "step": 34450
    },
    {
      "epoch": 15.946321147616844,
      "grad_norm": 0.002741640666499734,
      "learning_rate": 0.06810735770476632,
      "loss": 0.0004,
      "step": 34460
    },
    {
      "epoch": 15.950948634891255,
      "grad_norm": 6.620332717895508,
      "learning_rate": 0.06809810273021749,
      "loss": 0.0079,
      "step": 34470
    },
    {
      "epoch": 15.955576122165663,
      "grad_norm": 0.034709297120571136,
      "learning_rate": 0.06808884775566867,
      "loss": 0.0002,
      "step": 34480
    },
    {
      "epoch": 15.960203609440073,
      "grad_norm": 0.00782504677772522,
      "learning_rate": 0.06807959278111984,
      "loss": 0.0001,
      "step": 34490
    },
    {
      "epoch": 15.964831096714484,
      "grad_norm": 0.005520820152014494,
      "learning_rate": 0.06807033780657104,
      "loss": 0.0031,
      "step": 34500
    },
    {
      "epoch": 15.969458583988894,
      "grad_norm": 0.2603209912776947,
      "learning_rate": 0.06806108283202221,
      "loss": 0.0003,
      "step": 34510
    },
    {
      "epoch": 15.974086071263304,
      "grad_norm": 0.009541439823806286,
      "learning_rate": 0.0680518278574734,
      "loss": 0.0001,
      "step": 34520
    },
    {
      "epoch": 15.978713558537715,
      "grad_norm": 0.15380924940109253,
      "learning_rate": 0.06804257288292458,
      "loss": 0.0016,
      "step": 34530
    },
    {
      "epoch": 15.983341045812123,
      "grad_norm": 0.00562285864725709,
      "learning_rate": 0.06803331790837576,
      "loss": 0.0003,
      "step": 34540
    },
    {
      "epoch": 15.987968533086534,
      "grad_norm": 0.0012319280067458749,
      "learning_rate": 0.06802406293382694,
      "loss": 0.0002,
      "step": 34550
    },
    {
      "epoch": 15.992596020360944,
      "grad_norm": 0.009642377495765686,
      "learning_rate": 0.06801480795927811,
      "loss": 0.0001,
      "step": 34560
    },
    {
      "epoch": 15.997223507635354,
      "grad_norm": 0.005602800287306309,
      "learning_rate": 0.0680055529847293,
      "loss": 0.0,
      "step": 34570
    },
    {
      "epoch": 16.0,
      "eval_accuracy_branch1": 0.9897550454475428,
      "eval_accuracy_branch2": 0.500616237867817,
      "eval_f1_branch1": 0.9908190837023596,
      "eval_f1_branch2": 0.5005938089036237,
      "eval_loss": 0.023814797401428223,
      "eval_precision_branch1": 0.9907278629753266,
      "eval_precision_branch2": 0.5006163485917987,
      "eval_recall_branch1": 0.9910700864321998,
      "eval_recall_branch2": 0.500616237867817,
      "eval_runtime": 29.0203,
      "eval_samples_per_second": 447.343,
      "eval_steps_per_second": 55.926,
      "step": 34576
    },
    {
      "epoch": 16.001850994909763,
      "grad_norm": 0.0025432733818888664,
      "learning_rate": 0.06799629801018046,
      "loss": 0.0036,
      "step": 34580
    },
    {
      "epoch": 16.006478482184175,
      "grad_norm": 0.002361998427659273,
      "learning_rate": 0.06798704303563166,
      "loss": 0.0001,
      "step": 34590
    },
    {
      "epoch": 16.011105969458583,
      "grad_norm": 0.015555746853351593,
      "learning_rate": 0.06797778806108283,
      "loss": 0.0005,
      "step": 34600
    },
    {
      "epoch": 16.015733456732995,
      "grad_norm": 0.0025829696096479893,
      "learning_rate": 0.06796853308653401,
      "loss": 0.0003,
      "step": 34610
    },
    {
      "epoch": 16.020360944007404,
      "grad_norm": 0.016492119058966637,
      "learning_rate": 0.0679592781119852,
      "loss": 0.0016,
      "step": 34620
    },
    {
      "epoch": 16.024988431281812,
      "grad_norm": 0.0017627611523494124,
      "learning_rate": 0.06795002313743638,
      "loss": 0.0004,
      "step": 34630
    },
    {
      "epoch": 16.029615918556225,
      "grad_norm": 0.0015615248121321201,
      "learning_rate": 0.06794076816288756,
      "loss": 0.001,
      "step": 34640
    },
    {
      "epoch": 16.034243405830633,
      "grad_norm": 0.0015280020888894796,
      "learning_rate": 0.06793151318833873,
      "loss": 0.0019,
      "step": 34650
    },
    {
      "epoch": 16.038870893105045,
      "grad_norm": 0.06981569528579712,
      "learning_rate": 0.06792225821378992,
      "loss": 0.0002,
      "step": 34660
    },
    {
      "epoch": 16.043498380379454,
      "grad_norm": 0.006789741106331348,
      "learning_rate": 0.06791300323924108,
      "loss": 0.0,
      "step": 34670
    },
    {
      "epoch": 16.048125867653862,
      "grad_norm": 0.0009320157114416361,
      "learning_rate": 0.06790374826469228,
      "loss": 0.0001,
      "step": 34680
    },
    {
      "epoch": 16.052753354928274,
      "grad_norm": 0.0018941299058496952,
      "learning_rate": 0.06789449329014345,
      "loss": 0.0001,
      "step": 34690
    },
    {
      "epoch": 16.057380842202683,
      "grad_norm": 0.014084198512136936,
      "learning_rate": 0.06788523831559463,
      "loss": 0.002,
      "step": 34700
    },
    {
      "epoch": 16.062008329477095,
      "grad_norm": 0.0008370119612663984,
      "learning_rate": 0.06787598334104582,
      "loss": 0.0001,
      "step": 34710
    },
    {
      "epoch": 16.066635816751504,
      "grad_norm": 0.0021346970461308956,
      "learning_rate": 0.06786672836649699,
      "loss": 0.0001,
      "step": 34720
    },
    {
      "epoch": 16.071263304025916,
      "grad_norm": 0.04460611939430237,
      "learning_rate": 0.06785747339194818,
      "loss": 0.0003,
      "step": 34730
    },
    {
      "epoch": 16.075890791300324,
      "grad_norm": 0.22371716797351837,
      "learning_rate": 0.06784821841739935,
      "loss": 0.0001,
      "step": 34740
    },
    {
      "epoch": 16.080518278574733,
      "grad_norm": 1.5311816930770874,
      "learning_rate": 0.06783896344285054,
      "loss": 0.0005,
      "step": 34750
    },
    {
      "epoch": 16.085145765849145,
      "grad_norm": 0.0026992708444595337,
      "learning_rate": 0.0678297084683017,
      "loss": 0.0001,
      "step": 34760
    },
    {
      "epoch": 16.089773253123553,
      "grad_norm": 0.004614531062543392,
      "learning_rate": 0.0678204534937529,
      "loss": 0.0001,
      "step": 34770
    },
    {
      "epoch": 16.094400740397965,
      "grad_norm": 0.004671925213187933,
      "learning_rate": 0.06781119851920407,
      "loss": 0.0,
      "step": 34780
    },
    {
      "epoch": 16.099028227672374,
      "grad_norm": 0.044063325971364975,
      "learning_rate": 0.06780194354465525,
      "loss": 0.0005,
      "step": 34790
    },
    {
      "epoch": 16.103655714946782,
      "grad_norm": 0.24964742362499237,
      "learning_rate": 0.06779268857010644,
      "loss": 0.0002,
      "step": 34800
    },
    {
      "epoch": 16.108283202221195,
      "grad_norm": 0.0028067061211913824,
      "learning_rate": 0.06778343359555761,
      "loss": 0.0,
      "step": 34810
    },
    {
      "epoch": 16.112910689495603,
      "grad_norm": 0.0041495938785374165,
      "learning_rate": 0.0677741786210088,
      "loss": 0.0004,
      "step": 34820
    },
    {
      "epoch": 16.117538176770015,
      "grad_norm": 0.007246834691613913,
      "learning_rate": 0.06776492364645997,
      "loss": 0.0001,
      "step": 34830
    },
    {
      "epoch": 16.122165664044424,
      "grad_norm": 0.005412817467004061,
      "learning_rate": 0.06775566867191116,
      "loss": 0.0002,
      "step": 34840
    },
    {
      "epoch": 16.126793151318832,
      "grad_norm": 0.014246257953345776,
      "learning_rate": 0.06774641369736233,
      "loss": 0.0001,
      "step": 34850
    },
    {
      "epoch": 16.131420638593244,
      "grad_norm": 0.8484742641448975,
      "learning_rate": 0.06773715872281352,
      "loss": 0.0006,
      "step": 34860
    },
    {
      "epoch": 16.136048125867653,
      "grad_norm": 0.0037220229860395193,
      "learning_rate": 0.06772790374826469,
      "loss": 0.0001,
      "step": 34870
    },
    {
      "epoch": 16.140675613142065,
      "grad_norm": 0.30881255865097046,
      "learning_rate": 0.06771864877371588,
      "loss": 0.0001,
      "step": 34880
    },
    {
      "epoch": 16.145303100416474,
      "grad_norm": 0.002166550839319825,
      "learning_rate": 0.06770939379916706,
      "loss": 0.0004,
      "step": 34890
    },
    {
      "epoch": 16.149930587690882,
      "grad_norm": 0.03479950502514839,
      "learning_rate": 0.06770013882461823,
      "loss": 0.007,
      "step": 34900
    },
    {
      "epoch": 16.154558074965294,
      "grad_norm": 0.0417681448161602,
      "learning_rate": 0.06769088385006942,
      "loss": 0.0027,
      "step": 34910
    },
    {
      "epoch": 16.159185562239703,
      "grad_norm": 0.020449919626116753,
      "learning_rate": 0.0676816288755206,
      "loss": 0.0001,
      "step": 34920
    },
    {
      "epoch": 16.163813049514115,
      "grad_norm": 0.027180660516023636,
      "learning_rate": 0.06767237390097178,
      "loss": 0.0004,
      "step": 34930
    },
    {
      "epoch": 16.168440536788523,
      "grad_norm": 0.0010356539860367775,
      "learning_rate": 0.06766311892642295,
      "loss": 0.0001,
      "step": 34940
    },
    {
      "epoch": 16.173068024062935,
      "grad_norm": 0.0008174291579052806,
      "learning_rate": 0.06765386395187413,
      "loss": 0.0003,
      "step": 34950
    },
    {
      "epoch": 16.177695511337344,
      "grad_norm": 0.01024558488279581,
      "learning_rate": 0.06764460897732531,
      "loss": 0.0001,
      "step": 34960
    },
    {
      "epoch": 16.182322998611752,
      "grad_norm": 0.0007465481758117676,
      "learning_rate": 0.0676353540027765,
      "loss": 0.0001,
      "step": 34970
    },
    {
      "epoch": 16.186950485886165,
      "grad_norm": 0.0008662918116897345,
      "learning_rate": 0.06762609902822768,
      "loss": 0.0002,
      "step": 34980
    },
    {
      "epoch": 16.191577973160573,
      "grad_norm": 0.06800618022680283,
      "learning_rate": 0.06761684405367885,
      "loss": 0.0003,
      "step": 34990
    },
    {
      "epoch": 16.196205460434985,
      "grad_norm": 0.0020831625442951918,
      "learning_rate": 0.06760758907913005,
      "loss": 0.0001,
      "step": 35000
    },
    {
      "epoch": 16.200832947709394,
      "grad_norm": 0.014668972231447697,
      "learning_rate": 0.06759833410458121,
      "loss": 0.0001,
      "step": 35010
    },
    {
      "epoch": 16.205460434983802,
      "grad_norm": 0.0005734986043535173,
      "learning_rate": 0.0675890791300324,
      "loss": 0.002,
      "step": 35020
    },
    {
      "epoch": 16.210087922258214,
      "grad_norm": 0.04945571720600128,
      "learning_rate": 0.06757982415548357,
      "loss": 0.0001,
      "step": 35030
    },
    {
      "epoch": 16.214715409532623,
      "grad_norm": 0.004931682255119085,
      "learning_rate": 0.06757056918093475,
      "loss": 0.0002,
      "step": 35040
    },
    {
      "epoch": 16.219342896807035,
      "grad_norm": 0.006780847441405058,
      "learning_rate": 0.06756131420638593,
      "loss": 0.0002,
      "step": 35050
    },
    {
      "epoch": 16.223970384081444,
      "grad_norm": 0.0005190190277062356,
      "learning_rate": 0.06755205923183712,
      "loss": 0.0003,
      "step": 35060
    },
    {
      "epoch": 16.228597871355852,
      "grad_norm": 0.08180694282054901,
      "learning_rate": 0.0675428042572883,
      "loss": 0.0016,
      "step": 35070
    },
    {
      "epoch": 16.233225358630264,
      "grad_norm": 0.0009532708209007978,
      "learning_rate": 0.06753354928273947,
      "loss": 0.0001,
      "step": 35080
    },
    {
      "epoch": 16.237852845904673,
      "grad_norm": 0.029469894245266914,
      "learning_rate": 0.06752429430819067,
      "loss": 0.0003,
      "step": 35090
    },
    {
      "epoch": 16.242480333179085,
      "grad_norm": 0.0004220107803121209,
      "learning_rate": 0.06751503933364184,
      "loss": 0.0001,
      "step": 35100
    },
    {
      "epoch": 16.247107820453493,
      "grad_norm": 0.20730635523796082,
      "learning_rate": 0.06750578435909302,
      "loss": 0.0003,
      "step": 35110
    },
    {
      "epoch": 16.251735307727905,
      "grad_norm": 0.000908771064132452,
      "learning_rate": 0.06749652938454419,
      "loss": 0.0001,
      "step": 35120
    },
    {
      "epoch": 16.256362795002314,
      "grad_norm": 0.02579602412879467,
      "learning_rate": 0.06748727440999537,
      "loss": 0.0001,
      "step": 35130
    },
    {
      "epoch": 16.260990282276722,
      "grad_norm": 0.0043165734969079494,
      "learning_rate": 0.06747801943544655,
      "loss": 0.0,
      "step": 35140
    },
    {
      "epoch": 16.265617769551135,
      "grad_norm": 0.002632379764690995,
      "learning_rate": 0.06746876446089774,
      "loss": 0.0001,
      "step": 35150
    },
    {
      "epoch": 16.270245256825543,
      "grad_norm": 0.0012504536425694823,
      "learning_rate": 0.06745950948634892,
      "loss": 0.0011,
      "step": 35160
    },
    {
      "epoch": 16.274872744099955,
      "grad_norm": 0.008353892713785172,
      "learning_rate": 0.06745025451180009,
      "loss": 0.0003,
      "step": 35170
    },
    {
      "epoch": 16.279500231374364,
      "grad_norm": 0.03896874561905861,
      "learning_rate": 0.06744099953725127,
      "loss": 0.0001,
      "step": 35180
    },
    {
      "epoch": 16.284127718648772,
      "grad_norm": 0.004045302513986826,
      "learning_rate": 0.06743174456270246,
      "loss": 0.0001,
      "step": 35190
    },
    {
      "epoch": 16.288755205923184,
      "grad_norm": 0.001678782282397151,
      "learning_rate": 0.06742248958815364,
      "loss": 0.0078,
      "step": 35200
    },
    {
      "epoch": 16.293382693197593,
      "grad_norm": 0.0016772723756730556,
      "learning_rate": 0.06741323461360481,
      "loss": 0.0003,
      "step": 35210
    },
    {
      "epoch": 16.298010180472005,
      "grad_norm": 0.006940938998013735,
      "learning_rate": 0.06740397963905599,
      "loss": 0.0002,
      "step": 35220
    },
    {
      "epoch": 16.302637667746414,
      "grad_norm": 0.001358550158329308,
      "learning_rate": 0.06739472466450717,
      "loss": 0.0002,
      "step": 35230
    },
    {
      "epoch": 16.307265155020822,
      "grad_norm": 0.0017638683784753084,
      "learning_rate": 0.06738546968995836,
      "loss": 0.0001,
      "step": 35240
    },
    {
      "epoch": 16.311892642295234,
      "grad_norm": 0.001957532251253724,
      "learning_rate": 0.06737621471540954,
      "loss": 0.0,
      "step": 35250
    },
    {
      "epoch": 16.316520129569643,
      "grad_norm": 0.0033425556030124426,
      "learning_rate": 0.06736695974086071,
      "loss": 0.0001,
      "step": 35260
    },
    {
      "epoch": 16.321147616844055,
      "grad_norm": 0.0009441362344659865,
      "learning_rate": 0.0673577047663119,
      "loss": 0.0002,
      "step": 35270
    },
    {
      "epoch": 16.325775104118463,
      "grad_norm": 0.002437818795442581,
      "learning_rate": 0.06734844979176308,
      "loss": 0.0001,
      "step": 35280
    },
    {
      "epoch": 16.330402591392875,
      "grad_norm": 0.009241954423487186,
      "learning_rate": 0.06733919481721426,
      "loss": 0.0033,
      "step": 35290
    },
    {
      "epoch": 16.335030078667284,
      "grad_norm": 0.13883142173290253,
      "learning_rate": 0.06732993984266543,
      "loss": 0.0001,
      "step": 35300
    },
    {
      "epoch": 16.339657565941692,
      "grad_norm": 0.12104482203722,
      "learning_rate": 0.06732068486811661,
      "loss": 0.0004,
      "step": 35310
    },
    {
      "epoch": 16.344285053216105,
      "grad_norm": 0.03796348720788956,
      "learning_rate": 0.0673114298935678,
      "loss": 0.003,
      "step": 35320
    },
    {
      "epoch": 16.348912540490513,
      "grad_norm": 0.003340556751936674,
      "learning_rate": 0.06730217491901898,
      "loss": 0.0004,
      "step": 35330
    },
    {
      "epoch": 16.353540027764925,
      "grad_norm": 0.002365505089983344,
      "learning_rate": 0.06729291994447016,
      "loss": 0.0001,
      "step": 35340
    },
    {
      "epoch": 16.358167515039334,
      "grad_norm": 0.007368223741650581,
      "learning_rate": 0.06728366496992133,
      "loss": 0.0001,
      "step": 35350
    },
    {
      "epoch": 16.362795002313742,
      "grad_norm": 3.7791314125061035,
      "learning_rate": 0.06727440999537251,
      "loss": 0.0011,
      "step": 35360
    },
    {
      "epoch": 16.367422489588154,
      "grad_norm": 0.02162100188434124,
      "learning_rate": 0.0672651550208237,
      "loss": 0.0023,
      "step": 35370
    },
    {
      "epoch": 16.372049976862563,
      "grad_norm": 0.01442405954003334,
      "learning_rate": 0.06725590004627488,
      "loss": 0.0002,
      "step": 35380
    },
    {
      "epoch": 16.376677464136975,
      "grad_norm": 0.053254567086696625,
      "learning_rate": 0.06724664507172605,
      "loss": 0.0001,
      "step": 35390
    },
    {
      "epoch": 16.381304951411384,
      "grad_norm": 0.0009705924312584102,
      "learning_rate": 0.06723739009717723,
      "loss": 0.0024,
      "step": 35400
    },
    {
      "epoch": 16.385932438685792,
      "grad_norm": 0.00521331001073122,
      "learning_rate": 0.06722813512262842,
      "loss": 0.0,
      "step": 35410
    },
    {
      "epoch": 16.390559925960204,
      "grad_norm": 0.012295222841203213,
      "learning_rate": 0.0672188801480796,
      "loss": 0.0018,
      "step": 35420
    },
    {
      "epoch": 16.395187413234613,
      "grad_norm": 0.011960173957049847,
      "learning_rate": 0.06720962517353078,
      "loss": 0.0029,
      "step": 35430
    },
    {
      "epoch": 16.399814900509025,
      "grad_norm": 0.0012362777488306165,
      "learning_rate": 0.06720037019898195,
      "loss": 0.0,
      "step": 35440
    },
    {
      "epoch": 16.404442387783433,
      "grad_norm": 0.0036109762731939554,
      "learning_rate": 0.06719111522443313,
      "loss": 0.0001,
      "step": 35450
    },
    {
      "epoch": 16.409069875057842,
      "grad_norm": 0.0057826656848192215,
      "learning_rate": 0.06718186024988432,
      "loss": 0.0002,
      "step": 35460
    },
    {
      "epoch": 16.413697362332254,
      "grad_norm": 0.003036967245861888,
      "learning_rate": 0.0671726052753355,
      "loss": 0.0003,
      "step": 35470
    },
    {
      "epoch": 16.418324849606662,
      "grad_norm": 1.630439281463623,
      "learning_rate": 0.06716335030078667,
      "loss": 0.0025,
      "step": 35480
    },
    {
      "epoch": 16.422952336881075,
      "grad_norm": 0.031112922355532646,
      "learning_rate": 0.06715409532623785,
      "loss": 0.0001,
      "step": 35490
    },
    {
      "epoch": 16.427579824155483,
      "grad_norm": 0.00454422552138567,
      "learning_rate": 0.06714484035168904,
      "loss": 0.0001,
      "step": 35500
    },
    {
      "epoch": 16.432207311429895,
      "grad_norm": 0.009528154507279396,
      "learning_rate": 0.06713558537714022,
      "loss": 0.0001,
      "step": 35510
    },
    {
      "epoch": 16.436834798704304,
      "grad_norm": 0.06830073148012161,
      "learning_rate": 0.0671263304025914,
      "loss": 0.0022,
      "step": 35520
    },
    {
      "epoch": 16.441462285978712,
      "grad_norm": 0.0521756187081337,
      "learning_rate": 0.06711707542804257,
      "loss": 0.0001,
      "step": 35530
    },
    {
      "epoch": 16.446089773253124,
      "grad_norm": 0.002156081609427929,
      "learning_rate": 0.06710782045349376,
      "loss": 0.0007,
      "step": 35540
    },
    {
      "epoch": 16.450717260527533,
      "grad_norm": 0.010144548490643501,
      "learning_rate": 0.06709856547894494,
      "loss": 0.0012,
      "step": 35550
    },
    {
      "epoch": 16.455344747801945,
      "grad_norm": 0.0030448923353105783,
      "learning_rate": 0.06708931050439612,
      "loss": 0.0002,
      "step": 35560
    },
    {
      "epoch": 16.459972235076354,
      "grad_norm": 0.03984622657299042,
      "learning_rate": 0.06708005552984729,
      "loss": 0.0001,
      "step": 35570
    },
    {
      "epoch": 16.464599722350762,
      "grad_norm": 0.2061179131269455,
      "learning_rate": 0.06707080055529847,
      "loss": 0.0022,
      "step": 35580
    },
    {
      "epoch": 16.469227209625174,
      "grad_norm": 0.15312424302101135,
      "learning_rate": 0.06706154558074966,
      "loss": 0.0003,
      "step": 35590
    },
    {
      "epoch": 16.473854696899583,
      "grad_norm": 0.019920583814382553,
      "learning_rate": 0.06705229060620084,
      "loss": 0.0002,
      "step": 35600
    },
    {
      "epoch": 16.478482184173995,
      "grad_norm": 0.00740458769723773,
      "learning_rate": 0.06704303563165202,
      "loss": 0.0002,
      "step": 35610
    },
    {
      "epoch": 16.483109671448403,
      "grad_norm": 0.007642925251275301,
      "learning_rate": 0.06703378065710319,
      "loss": 0.0002,
      "step": 35620
    },
    {
      "epoch": 16.487737158722812,
      "grad_norm": 0.03339261934161186,
      "learning_rate": 0.06702452568255438,
      "loss": 0.0003,
      "step": 35630
    },
    {
      "epoch": 16.492364645997224,
      "grad_norm": 0.001326613943092525,
      "learning_rate": 0.06701527070800554,
      "loss": 0.0002,
      "step": 35640
    },
    {
      "epoch": 16.496992133271632,
      "grad_norm": 0.0072630238719284534,
      "learning_rate": 0.06700601573345674,
      "loss": 0.0003,
      "step": 35650
    },
    {
      "epoch": 16.501619620546045,
      "grad_norm": 0.0017142180586233735,
      "learning_rate": 0.06699676075890791,
      "loss": 0.0074,
      "step": 35660
    },
    {
      "epoch": 16.506247107820453,
      "grad_norm": 0.004559036809951067,
      "learning_rate": 0.0669875057843591,
      "loss": 0.0006,
      "step": 35670
    },
    {
      "epoch": 16.51087459509486,
      "grad_norm": 0.1398930549621582,
      "learning_rate": 0.06697825080981028,
      "loss": 0.0003,
      "step": 35680
    },
    {
      "epoch": 16.515502082369274,
      "grad_norm": 0.005455721635371447,
      "learning_rate": 0.06696899583526146,
      "loss": 0.0003,
      "step": 35690
    },
    {
      "epoch": 16.520129569643682,
      "grad_norm": 0.028781073167920113,
      "learning_rate": 0.06695974086071264,
      "loss": 0.0001,
      "step": 35700
    },
    {
      "epoch": 16.524757056918094,
      "grad_norm": 0.24920065701007843,
      "learning_rate": 0.06695048588616381,
      "loss": 0.0001,
      "step": 35710
    },
    {
      "epoch": 16.529384544192503,
      "grad_norm": 3.557121515274048,
      "learning_rate": 0.066941230911615,
      "loss": 0.0011,
      "step": 35720
    },
    {
      "epoch": 16.534012031466915,
      "grad_norm": 0.0037313911598175764,
      "learning_rate": 0.06693197593706617,
      "loss": 0.0013,
      "step": 35730
    },
    {
      "epoch": 16.538639518741324,
      "grad_norm": 2.632668972015381,
      "learning_rate": 0.06692272096251736,
      "loss": 0.0013,
      "step": 35740
    },
    {
      "epoch": 16.543267006015732,
      "grad_norm": 0.43349236249923706,
      "learning_rate": 0.06691346598796853,
      "loss": 0.0075,
      "step": 35750
    },
    {
      "epoch": 16.547894493290144,
      "grad_norm": 0.01657818630337715,
      "learning_rate": 0.06690421101341971,
      "loss": 0.0005,
      "step": 35760
    },
    {
      "epoch": 16.552521980564553,
      "grad_norm": 0.002412733156234026,
      "learning_rate": 0.0668949560388709,
      "loss": 0.0022,
      "step": 35770
    },
    {
      "epoch": 16.557149467838965,
      "grad_norm": 0.015166168101131916,
      "learning_rate": 0.06688570106432208,
      "loss": 0.0002,
      "step": 35780
    },
    {
      "epoch": 16.561776955113373,
      "grad_norm": 0.015055110678076744,
      "learning_rate": 0.06687644608977326,
      "loss": 0.0009,
      "step": 35790
    },
    {
      "epoch": 16.566404442387782,
      "grad_norm": 0.0037490215618163347,
      "learning_rate": 0.06686719111522443,
      "loss": 0.001,
      "step": 35800
    },
    {
      "epoch": 16.571031929662194,
      "grad_norm": 0.8390722870826721,
      "learning_rate": 0.06685793614067562,
      "loss": 0.0002,
      "step": 35810
    },
    {
      "epoch": 16.575659416936602,
      "grad_norm": 0.013644666410982609,
      "learning_rate": 0.06684868116612679,
      "loss": 0.001,
      "step": 35820
    },
    {
      "epoch": 16.580286904211015,
      "grad_norm": 0.017345145344734192,
      "learning_rate": 0.06683942619157798,
      "loss": 0.0003,
      "step": 35830
    },
    {
      "epoch": 16.584914391485423,
      "grad_norm": 0.15726974606513977,
      "learning_rate": 0.06683017121702915,
      "loss": 0.0002,
      "step": 35840
    },
    {
      "epoch": 16.58954187875983,
      "grad_norm": 0.03426070883870125,
      "learning_rate": 0.06682091624248034,
      "loss": 0.0002,
      "step": 35850
    },
    {
      "epoch": 16.594169366034244,
      "grad_norm": 0.0010325117036700249,
      "learning_rate": 0.06681166126793152,
      "loss": 0.0002,
      "step": 35860
    },
    {
      "epoch": 16.598796853308652,
      "grad_norm": 0.016174176707863808,
      "learning_rate": 0.06680240629338269,
      "loss": 0.0003,
      "step": 35870
    },
    {
      "epoch": 16.603424340583064,
      "grad_norm": 2.545478582382202,
      "learning_rate": 0.06679315131883388,
      "loss": 0.0019,
      "step": 35880
    },
    {
      "epoch": 16.608051827857473,
      "grad_norm": 0.0028332022484391928,
      "learning_rate": 0.06678389634428505,
      "loss": 0.0001,
      "step": 35890
    },
    {
      "epoch": 16.612679315131885,
      "grad_norm": 0.0054247514344751835,
      "learning_rate": 0.06677464136973624,
      "loss": 0.0002,
      "step": 35900
    },
    {
      "epoch": 16.617306802406294,
      "grad_norm": 0.002466457197442651,
      "learning_rate": 0.0667653863951874,
      "loss": 0.0002,
      "step": 35910
    },
    {
      "epoch": 16.621934289680702,
      "grad_norm": 1.3547539710998535,
      "learning_rate": 0.0667561314206386,
      "loss": 0.0004,
      "step": 35920
    },
    {
      "epoch": 16.626561776955114,
      "grad_norm": 0.083623968064785,
      "learning_rate": 0.06674687644608977,
      "loss": 0.0035,
      "step": 35930
    },
    {
      "epoch": 16.631189264229523,
      "grad_norm": 0.012776367366313934,
      "learning_rate": 0.06673762147154096,
      "loss": 0.0008,
      "step": 35940
    },
    {
      "epoch": 16.635816751503935,
      "grad_norm": 0.0023381796199828386,
      "learning_rate": 0.06672836649699214,
      "loss": 0.0009,
      "step": 35950
    },
    {
      "epoch": 16.640444238778343,
      "grad_norm": 0.003327843965962529,
      "learning_rate": 0.06671911152244331,
      "loss": 0.0003,
      "step": 35960
    },
    {
      "epoch": 16.645071726052752,
      "grad_norm": 0.07717371731996536,
      "learning_rate": 0.0667098565478945,
      "loss": 0.0003,
      "step": 35970
    },
    {
      "epoch": 16.649699213327164,
      "grad_norm": 0.05398174747824669,
      "learning_rate": 0.06670060157334567,
      "loss": 0.0012,
      "step": 35980
    },
    {
      "epoch": 16.654326700601572,
      "grad_norm": 0.010150727815926075,
      "learning_rate": 0.06669134659879686,
      "loss": 0.0001,
      "step": 35990
    },
    {
      "epoch": 16.658954187875985,
      "grad_norm": 0.01407773606479168,
      "learning_rate": 0.06668209162424803,
      "loss": 0.0002,
      "step": 36000
    },
    {
      "epoch": 16.663581675150393,
      "grad_norm": 0.009708386845886707,
      "learning_rate": 0.06667283664969922,
      "loss": 0.0001,
      "step": 36010
    },
    {
      "epoch": 16.6682091624248,
      "grad_norm": 0.004928296431899071,
      "learning_rate": 0.0666635816751504,
      "loss": 0.0001,
      "step": 36020
    },
    {
      "epoch": 16.672836649699214,
      "grad_norm": 0.014581573195755482,
      "learning_rate": 0.06665432670060158,
      "loss": 0.0,
      "step": 36030
    },
    {
      "epoch": 16.677464136973622,
      "grad_norm": 0.010991938412189484,
      "learning_rate": 0.06664507172605276,
      "loss": 0.0009,
      "step": 36040
    },
    {
      "epoch": 16.682091624248034,
      "grad_norm": 0.005974722560495138,
      "learning_rate": 0.06663581675150393,
      "loss": 0.0001,
      "step": 36050
    },
    {
      "epoch": 16.686719111522443,
      "grad_norm": 0.038903314620256424,
      "learning_rate": 0.06662656177695513,
      "loss": 0.0052,
      "step": 36060
    },
    {
      "epoch": 16.691346598796855,
      "grad_norm": 0.04246809706091881,
      "learning_rate": 0.0666173068024063,
      "loss": 0.0001,
      "step": 36070
    },
    {
      "epoch": 16.695974086071264,
      "grad_norm": 0.02178207039833069,
      "learning_rate": 0.06660805182785748,
      "loss": 0.0008,
      "step": 36080
    },
    {
      "epoch": 16.700601573345672,
      "grad_norm": 0.011066857725381851,
      "learning_rate": 0.06659879685330865,
      "loss": 0.0011,
      "step": 36090
    },
    {
      "epoch": 16.705229060620084,
      "grad_norm": 0.027419699355959892,
      "learning_rate": 0.06658954187875983,
      "loss": 0.0036,
      "step": 36100
    },
    {
      "epoch": 16.709856547894493,
      "grad_norm": 0.005132668651640415,
      "learning_rate": 0.06658028690421101,
      "loss": 0.0002,
      "step": 36110
    },
    {
      "epoch": 16.714484035168905,
      "grad_norm": 0.0007191806216724217,
      "learning_rate": 0.0665710319296622,
      "loss": 0.0021,
      "step": 36120
    },
    {
      "epoch": 16.719111522443313,
      "grad_norm": 0.03998973220586777,
      "learning_rate": 0.06656177695511338,
      "loss": 0.0001,
      "step": 36130
    },
    {
      "epoch": 16.723739009717722,
      "grad_norm": 1.8691558837890625,
      "learning_rate": 0.06655252198056455,
      "loss": 0.001,
      "step": 36140
    },
    {
      "epoch": 16.728366496992134,
      "grad_norm": 0.05439678207039833,
      "learning_rate": 0.06654326700601575,
      "loss": 0.0001,
      "step": 36150
    },
    {
      "epoch": 16.732993984266542,
      "grad_norm": 0.019936317577958107,
      "learning_rate": 0.06653401203146692,
      "loss": 0.0001,
      "step": 36160
    },
    {
      "epoch": 16.737621471540955,
      "grad_norm": 0.004250271711498499,
      "learning_rate": 0.0665247570569181,
      "loss": 0.0004,
      "step": 36170
    },
    {
      "epoch": 16.742248958815363,
      "grad_norm": 0.005259782541543245,
      "learning_rate": 0.06651550208236927,
      "loss": 0.0002,
      "step": 36180
    },
    {
      "epoch": 16.74687644608977,
      "grad_norm": 0.0014588594203814864,
      "learning_rate": 0.06650624710782045,
      "loss": 0.0008,
      "step": 36190
    },
    {
      "epoch": 16.751503933364184,
      "grad_norm": 0.005055574234575033,
      "learning_rate": 0.06649699213327163,
      "loss": 0.001,
      "step": 36200
    },
    {
      "epoch": 16.756131420638592,
      "grad_norm": 0.014782406389713287,
      "learning_rate": 0.06648773715872282,
      "loss": 0.0009,
      "step": 36210
    },
    {
      "epoch": 16.760758907913004,
      "grad_norm": 0.006132438313215971,
      "learning_rate": 0.066478482184174,
      "loss": 0.0004,
      "step": 36220
    },
    {
      "epoch": 16.765386395187413,
      "grad_norm": 0.0015310615999624133,
      "learning_rate": 0.06646922720962517,
      "loss": 0.0005,
      "step": 36230
    },
    {
      "epoch": 16.770013882461825,
      "grad_norm": 0.001451514195650816,
      "learning_rate": 0.06645997223507637,
      "loss": 0.0001,
      "step": 36240
    },
    {
      "epoch": 16.774641369736234,
      "grad_norm": 0.006062375381588936,
      "learning_rate": 0.06645071726052754,
      "loss": 0.0002,
      "step": 36250
    },
    {
      "epoch": 16.779268857010642,
      "grad_norm": 0.005248106084764004,
      "learning_rate": 0.06644146228597872,
      "loss": 0.0026,
      "step": 36260
    },
    {
      "epoch": 16.783896344285054,
      "grad_norm": 0.001579011557623744,
      "learning_rate": 0.06643220731142989,
      "loss": 0.0001,
      "step": 36270
    },
    {
      "epoch": 16.788523831559463,
      "grad_norm": 0.08178175985813141,
      "learning_rate": 0.06642295233688107,
      "loss": 0.0003,
      "step": 36280
    },
    {
      "epoch": 16.793151318833875,
      "grad_norm": 0.07314085960388184,
      "learning_rate": 0.06641369736233226,
      "loss": 0.0001,
      "step": 36290
    },
    {
      "epoch": 16.797778806108283,
      "grad_norm": 0.013053078204393387,
      "learning_rate": 0.06640444238778344,
      "loss": 0.0001,
      "step": 36300
    },
    {
      "epoch": 16.802406293382692,
      "grad_norm": 0.09160622209310532,
      "learning_rate": 0.06639518741323462,
      "loss": 0.0005,
      "step": 36310
    },
    {
      "epoch": 16.807033780657104,
      "grad_norm": 0.028623312711715698,
      "learning_rate": 0.06638593243868579,
      "loss": 0.0001,
      "step": 36320
    },
    {
      "epoch": 16.811661267931512,
      "grad_norm": 0.03433430194854736,
      "learning_rate": 0.06637667746413697,
      "loss": 0.0001,
      "step": 36330
    },
    {
      "epoch": 16.816288755205925,
      "grad_norm": 0.05087416246533394,
      "learning_rate": 0.06636742248958816,
      "loss": 0.0003,
      "step": 36340
    },
    {
      "epoch": 16.820916242480333,
      "grad_norm": 0.001517997938208282,
      "learning_rate": 0.06635816751503934,
      "loss": 0.0001,
      "step": 36350
    },
    {
      "epoch": 16.82554372975474,
      "grad_norm": 0.004851509816944599,
      "learning_rate": 0.06634891254049051,
      "loss": 0.0004,
      "step": 36360
    },
    {
      "epoch": 16.830171217029154,
      "grad_norm": 0.1898581087589264,
      "learning_rate": 0.06633965756594169,
      "loss": 0.0003,
      "step": 36370
    },
    {
      "epoch": 16.834798704303562,
      "grad_norm": 0.0005389564903452992,
      "learning_rate": 0.06633040259139288,
      "loss": 0.0008,
      "step": 36380
    },
    {
      "epoch": 16.839426191577974,
      "grad_norm": 0.023152245208621025,
      "learning_rate": 0.06632114761684406,
      "loss": 0.0001,
      "step": 36390
    },
    {
      "epoch": 16.844053678852383,
      "grad_norm": 0.2656770944595337,
      "learning_rate": 0.06631189264229524,
      "loss": 0.0011,
      "step": 36400
    },
    {
      "epoch": 16.84868116612679,
      "grad_norm": 0.00631286483258009,
      "learning_rate": 0.06630263766774641,
      "loss": 0.0001,
      "step": 36410
    },
    {
      "epoch": 16.853308653401204,
      "grad_norm": 0.05695334076881409,
      "learning_rate": 0.0662933826931976,
      "loss": 0.0019,
      "step": 36420
    },
    {
      "epoch": 16.857936140675612,
      "grad_norm": 0.0054824333637952805,
      "learning_rate": 0.06628412771864878,
      "loss": 0.0011,
      "step": 36430
    },
    {
      "epoch": 16.862563627950024,
      "grad_norm": 0.00856118742376566,
      "learning_rate": 0.06627487274409996,
      "loss": 0.0045,
      "step": 36440
    },
    {
      "epoch": 16.867191115224433,
      "grad_norm": 0.010047586634755135,
      "learning_rate": 0.06626561776955113,
      "loss": 0.0001,
      "step": 36450
    },
    {
      "epoch": 16.871818602498845,
      "grad_norm": 0.5661605000495911,
      "learning_rate": 0.06625636279500231,
      "loss": 0.0009,
      "step": 36460
    },
    {
      "epoch": 16.876446089773253,
      "grad_norm": 0.007824227213859558,
      "learning_rate": 0.0662471078204535,
      "loss": 0.0002,
      "step": 36470
    },
    {
      "epoch": 16.881073577047662,
      "grad_norm": 0.01702306792140007,
      "learning_rate": 0.06623785284590468,
      "loss": 0.0001,
      "step": 36480
    },
    {
      "epoch": 16.885701064322074,
      "grad_norm": 0.008925551548600197,
      "learning_rate": 0.06622859787135586,
      "loss": 0.0002,
      "step": 36490
    },
    {
      "epoch": 16.890328551596482,
      "grad_norm": 0.2167702615261078,
      "learning_rate": 0.06621934289680703,
      "loss": 0.0004,
      "step": 36500
    },
    {
      "epoch": 16.894956038870895,
      "grad_norm": 0.980405330657959,
      "learning_rate": 0.06621008792225822,
      "loss": 0.0004,
      "step": 36510
    },
    {
      "epoch": 16.899583526145303,
      "grad_norm": 0.006579192820936441,
      "learning_rate": 0.0662008329477094,
      "loss": 0.0001,
      "step": 36520
    },
    {
      "epoch": 16.90421101341971,
      "grad_norm": 0.059913549572229385,
      "learning_rate": 0.06619157797316058,
      "loss": 0.0015,
      "step": 36530
    },
    {
      "epoch": 16.908838500694124,
      "grad_norm": 0.011244452558457851,
      "learning_rate": 0.06618232299861175,
      "loss": 0.0002,
      "step": 36540
    },
    {
      "epoch": 16.913465987968532,
      "grad_norm": 1.3155101537704468,
      "learning_rate": 0.06617306802406293,
      "loss": 0.0004,
      "step": 36550
    },
    {
      "epoch": 16.918093475242944,
      "grad_norm": 0.003156533231958747,
      "learning_rate": 0.06616381304951412,
      "loss": 0.0001,
      "step": 36560
    },
    {
      "epoch": 16.922720962517353,
      "grad_norm": 0.00895183626562357,
      "learning_rate": 0.0661545580749653,
      "loss": 0.0002,
      "step": 36570
    },
    {
      "epoch": 16.92734844979176,
      "grad_norm": 0.03249978646636009,
      "learning_rate": 0.06614530310041648,
      "loss": 0.0001,
      "step": 36580
    },
    {
      "epoch": 16.931975937066174,
      "grad_norm": 0.03446808084845543,
      "learning_rate": 0.06613604812586765,
      "loss": 0.0002,
      "step": 36590
    },
    {
      "epoch": 16.936603424340582,
      "grad_norm": 0.08247032016515732,
      "learning_rate": 0.06612679315131884,
      "loss": 0.0001,
      "step": 36600
    },
    {
      "epoch": 16.941230911614994,
      "grad_norm": 0.0006472432287409902,
      "learning_rate": 0.06611753817677002,
      "loss": 0.0005,
      "step": 36610
    },
    {
      "epoch": 16.945858398889403,
      "grad_norm": 0.004521834198385477,
      "learning_rate": 0.0661082832022212,
      "loss": 0.0001,
      "step": 36620
    },
    {
      "epoch": 16.95048588616381,
      "grad_norm": 0.018404709175229073,
      "learning_rate": 0.06609902822767237,
      "loss": 0.0,
      "step": 36630
    },
    {
      "epoch": 16.955113373438223,
      "grad_norm": 0.008763454854488373,
      "learning_rate": 0.06608977325312355,
      "loss": 0.0002,
      "step": 36640
    },
    {
      "epoch": 16.959740860712632,
      "grad_norm": 0.01983707584440708,
      "learning_rate": 0.06608051827857474,
      "loss": 0.0002,
      "step": 36650
    },
    {
      "epoch": 16.964368347987044,
      "grad_norm": 0.004554152954369783,
      "learning_rate": 0.06607126330402592,
      "loss": 0.0001,
      "step": 36660
    },
    {
      "epoch": 16.968995835261452,
      "grad_norm": 0.18462812900543213,
      "learning_rate": 0.0660620083294771,
      "loss": 0.0004,
      "step": 36670
    },
    {
      "epoch": 16.973623322535865,
      "grad_norm": 3.2729711532592773,
      "learning_rate": 0.06605275335492827,
      "loss": 0.0126,
      "step": 36680
    },
    {
      "epoch": 16.978250809810273,
      "grad_norm": 0.17277856171131134,
      "learning_rate": 0.06604349838037946,
      "loss": 0.0002,
      "step": 36690
    },
    {
      "epoch": 16.98287829708468,
      "grad_norm": 0.0037487302906811237,
      "learning_rate": 0.06603424340583064,
      "loss": 0.0006,
      "step": 36700
    },
    {
      "epoch": 16.987505784359094,
      "grad_norm": 0.022915378212928772,
      "learning_rate": 0.06602498843128182,
      "loss": 0.0001,
      "step": 36710
    },
    {
      "epoch": 16.992133271633502,
      "grad_norm": 0.009425358846783638,
      "learning_rate": 0.06601573345673299,
      "loss": 0.0002,
      "step": 36720
    },
    {
      "epoch": 16.996760758907914,
      "grad_norm": 0.0008314349688589573,
      "learning_rate": 0.06600647848218417,
      "loss": 0.0002,
      "step": 36730
    },
    {
      "epoch": 17.0,
      "eval_accuracy_branch1": 0.9919118779849022,
      "eval_accuracy_branch2": 0.5008473270682483,
      "eval_f1_branch1": 0.9920089727250201,
      "eval_f1_branch2": 0.49946592791454847,
      "eval_loss": 0.01606873609125614,
      "eval_precision_branch1": 0.992730364223786,
      "eval_precision_branch2": 0.5008567854670676,
      "eval_recall_branch1": 0.9913852477488121,
      "eval_recall_branch2": 0.5008473270682483,
      "eval_runtime": 29.9243,
      "eval_samples_per_second": 433.829,
      "eval_steps_per_second": 54.237,
      "step": 36737
    },
    {
      "epoch": 17.001388246182323,
      "grad_norm": 0.004107134882360697,
      "learning_rate": 0.06599722350763536,
      "loss": 0.0,
      "step": 36740
    },
    {
      "epoch": 17.00601573345673,
      "grad_norm": 0.06668341159820557,
      "learning_rate": 0.06598796853308654,
      "loss": 0.0001,
      "step": 36750
    },
    {
      "epoch": 17.010643220731144,
      "grad_norm": 0.07419110834598541,
      "learning_rate": 0.06597871355853772,
      "loss": 0.0003,
      "step": 36760
    },
    {
      "epoch": 17.015270708005552,
      "grad_norm": 0.0017345832893624902,
      "learning_rate": 0.0659694585839889,
      "loss": 0.0004,
      "step": 36770
    },
    {
      "epoch": 17.019898195279964,
      "grad_norm": 0.01260941568762064,
      "learning_rate": 0.06596020360944008,
      "loss": 0.0001,
      "step": 36780
    },
    {
      "epoch": 17.024525682554373,
      "grad_norm": 0.009166593663394451,
      "learning_rate": 0.06595094863489125,
      "loss": 0.003,
      "step": 36790
    },
    {
      "epoch": 17.02915316982878,
      "grad_norm": 0.015784529969096184,
      "learning_rate": 0.06594169366034244,
      "loss": 0.0002,
      "step": 36800
    },
    {
      "epoch": 17.033780657103193,
      "grad_norm": 0.0018836373928934336,
      "learning_rate": 0.06593243868579361,
      "loss": 0.0002,
      "step": 36810
    },
    {
      "epoch": 17.038408144377602,
      "grad_norm": 0.016135182231664658,
      "learning_rate": 0.0659231837112448,
      "loss": 0.0002,
      "step": 36820
    },
    {
      "epoch": 17.043035631652014,
      "grad_norm": 0.001215234398841858,
      "learning_rate": 0.06591392873669598,
      "loss": 0.0,
      "step": 36830
    },
    {
      "epoch": 17.047663118926422,
      "grad_norm": 0.005428051110357046,
      "learning_rate": 0.06590467376214716,
      "loss": 0.0004,
      "step": 36840
    },
    {
      "epoch": 17.052290606200835,
      "grad_norm": 0.014217575080692768,
      "learning_rate": 0.06589541878759834,
      "loss": 0.0003,
      "step": 36850
    },
    {
      "epoch": 17.056918093475243,
      "grad_norm": 0.2044883668422699,
      "learning_rate": 0.06588616381304951,
      "loss": 0.0001,
      "step": 36860
    },
    {
      "epoch": 17.06154558074965,
      "grad_norm": 0.05879034847021103,
      "learning_rate": 0.0658769088385007,
      "loss": 0.0001,
      "step": 36870
    },
    {
      "epoch": 17.066173068024064,
      "grad_norm": 0.0004689718771260232,
      "learning_rate": 0.06586765386395187,
      "loss": 0.0002,
      "step": 36880
    },
    {
      "epoch": 17.070800555298472,
      "grad_norm": 0.0010315467370674014,
      "learning_rate": 0.06585839888940306,
      "loss": 0.0,
      "step": 36890
    },
    {
      "epoch": 17.075428042572884,
      "grad_norm": 0.14099904894828796,
      "learning_rate": 0.06584914391485423,
      "loss": 0.0001,
      "step": 36900
    },
    {
      "epoch": 17.080055529847293,
      "grad_norm": 0.04766423627734184,
      "learning_rate": 0.06583988894030542,
      "loss": 0.0001,
      "step": 36910
    },
    {
      "epoch": 17.0846830171217,
      "grad_norm": 0.00895979255437851,
      "learning_rate": 0.0658306339657566,
      "loss": 0.0001,
      "step": 36920
    },
    {
      "epoch": 17.089310504396114,
      "grad_norm": 0.0007849496905691922,
      "learning_rate": 0.06582137899120778,
      "loss": 0.0001,
      "step": 36930
    },
    {
      "epoch": 17.093937991670522,
      "grad_norm": 0.001956954598426819,
      "learning_rate": 0.06581212401665897,
      "loss": 0.0,
      "step": 36940
    },
    {
      "epoch": 17.098565478944934,
      "grad_norm": 0.5842224359512329,
      "learning_rate": 0.06580286904211013,
      "loss": 0.0002,
      "step": 36950
    },
    {
      "epoch": 17.103192966219343,
      "grad_norm": 0.1118617132306099,
      "learning_rate": 0.06579361406756132,
      "loss": 0.0002,
      "step": 36960
    },
    {
      "epoch": 17.10782045349375,
      "grad_norm": 0.007038845214992762,
      "learning_rate": 0.06578435909301249,
      "loss": 0.0,
      "step": 36970
    },
    {
      "epoch": 17.112447940768163,
      "grad_norm": 0.004747265018522739,
      "learning_rate": 0.06577510411846368,
      "loss": 0.0043,
      "step": 36980
    },
    {
      "epoch": 17.117075428042572,
      "grad_norm": 0.015990911051630974,
      "learning_rate": 0.06576584914391485,
      "loss": 0.0002,
      "step": 36990
    },
    {
      "epoch": 17.121702915316984,
      "grad_norm": 0.002577477367594838,
      "learning_rate": 0.06575659416936604,
      "loss": 0.0001,
      "step": 37000
    },
    {
      "epoch": 17.126330402591392,
      "grad_norm": 0.0005775512545369565,
      "learning_rate": 0.06574733919481722,
      "loss": 0.0,
      "step": 37010
    },
    {
      "epoch": 17.130957889865805,
      "grad_norm": 0.0006399938138201833,
      "learning_rate": 0.06573808422026839,
      "loss": 0.0002,
      "step": 37020
    },
    {
      "epoch": 17.135585377140213,
      "grad_norm": 0.0020953412167727947,
      "learning_rate": 0.06572882924571959,
      "loss": 0.0001,
      "step": 37030
    },
    {
      "epoch": 17.14021286441462,
      "grad_norm": 0.000885838526301086,
      "learning_rate": 0.06571957427117076,
      "loss": 0.0001,
      "step": 37040
    },
    {
      "epoch": 17.144840351689034,
      "grad_norm": 0.0011657398426905274,
      "learning_rate": 0.06571031929662194,
      "loss": 0.0016,
      "step": 37050
    },
    {
      "epoch": 17.149467838963442,
      "grad_norm": 0.005540069658309221,
      "learning_rate": 0.06570106432207311,
      "loss": 0.0001,
      "step": 37060
    },
    {
      "epoch": 17.154095326237854,
      "grad_norm": 0.012797423638403416,
      "learning_rate": 0.0656918093475243,
      "loss": 0.0001,
      "step": 37070
    },
    {
      "epoch": 17.158722813512263,
      "grad_norm": 0.5920747518539429,
      "learning_rate": 0.06568255437297547,
      "loss": 0.0003,
      "step": 37080
    },
    {
      "epoch": 17.16335030078667,
      "grad_norm": 0.13095301389694214,
      "learning_rate": 0.06567329939842666,
      "loss": 0.0001,
      "step": 37090
    },
    {
      "epoch": 17.167977788061084,
      "grad_norm": 0.004126852843910456,
      "learning_rate": 0.06566404442387784,
      "loss": 0.0001,
      "step": 37100
    },
    {
      "epoch": 17.172605275335492,
      "grad_norm": 0.006864821072667837,
      "learning_rate": 0.06565478944932901,
      "loss": 0.0,
      "step": 37110
    },
    {
      "epoch": 17.177232762609904,
      "grad_norm": 0.0011391573352739215,
      "learning_rate": 0.0656455344747802,
      "loss": 0.0001,
      "step": 37120
    },
    {
      "epoch": 17.181860249884313,
      "grad_norm": 0.027202477678656578,
      "learning_rate": 0.06563627950023138,
      "loss": 0.0009,
      "step": 37130
    },
    {
      "epoch": 17.18648773715872,
      "grad_norm": 0.0032470286823809147,
      "learning_rate": 0.06562702452568256,
      "loss": 0.0001,
      "step": 37140
    },
    {
      "epoch": 17.191115224433133,
      "grad_norm": 0.033861540257930756,
      "learning_rate": 0.06561776955113373,
      "loss": 0.0001,
      "step": 37150
    },
    {
      "epoch": 17.195742711707542,
      "grad_norm": 0.006690057460218668,
      "learning_rate": 0.06560851457658493,
      "loss": 0.0001,
      "step": 37160
    },
    {
      "epoch": 17.200370198981954,
      "grad_norm": 0.009701014496386051,
      "learning_rate": 0.0655992596020361,
      "loss": 0.0001,
      "step": 37170
    },
    {
      "epoch": 17.204997686256362,
      "grad_norm": 0.0033624551724642515,
      "learning_rate": 0.06559000462748728,
      "loss": 0.0003,
      "step": 37180
    },
    {
      "epoch": 17.20962517353077,
      "grad_norm": 0.016441265121102333,
      "learning_rate": 0.06558074965293846,
      "loss": 0.0001,
      "step": 37190
    },
    {
      "epoch": 17.214252660805183,
      "grad_norm": 0.4984232187271118,
      "learning_rate": 0.06557149467838963,
      "loss": 0.0001,
      "step": 37200
    },
    {
      "epoch": 17.21888014807959,
      "grad_norm": 0.001847011037170887,
      "learning_rate": 0.06556223970384083,
      "loss": 0.0001,
      "step": 37210
    },
    {
      "epoch": 17.223507635354004,
      "grad_norm": 0.007275498937815428,
      "learning_rate": 0.065552984729292,
      "loss": 0.0,
      "step": 37220
    },
    {
      "epoch": 17.228135122628412,
      "grad_norm": 0.0013662786222994328,
      "learning_rate": 0.06554372975474318,
      "loss": 0.0,
      "step": 37230
    },
    {
      "epoch": 17.232762609902824,
      "grad_norm": 0.14556966722011566,
      "learning_rate": 0.06553447478019435,
      "loss": 0.0002,
      "step": 37240
    },
    {
      "epoch": 17.237390097177233,
      "grad_norm": 0.07000979781150818,
      "learning_rate": 0.06552521980564553,
      "loss": 0.001,
      "step": 37250
    },
    {
      "epoch": 17.24201758445164,
      "grad_norm": 0.049026984721422195,
      "learning_rate": 0.06551596483109672,
      "loss": 0.0002,
      "step": 37260
    },
    {
      "epoch": 17.246645071726054,
      "grad_norm": 0.004846612922847271,
      "learning_rate": 0.0655067098565479,
      "loss": 0.0001,
      "step": 37270
    },
    {
      "epoch": 17.251272559000462,
      "grad_norm": 0.002693743444979191,
      "learning_rate": 0.06549745488199908,
      "loss": 0.0001,
      "step": 37280
    },
    {
      "epoch": 17.255900046274874,
      "grad_norm": 0.022376812994480133,
      "learning_rate": 0.06548819990745025,
      "loss": 0.0001,
      "step": 37290
    },
    {
      "epoch": 17.260527533549283,
      "grad_norm": 0.00141096708830446,
      "learning_rate": 0.06547894493290145,
      "loss": 0.0001,
      "step": 37300
    },
    {
      "epoch": 17.26515502082369,
      "grad_norm": 0.014889981597661972,
      "learning_rate": 0.06546968995835262,
      "loss": 0.0001,
      "step": 37310
    },
    {
      "epoch": 17.269782508098103,
      "grad_norm": 0.004245329182595015,
      "learning_rate": 0.0654604349838038,
      "loss": 0.0002,
      "step": 37320
    },
    {
      "epoch": 17.274409995372512,
      "grad_norm": 0.00733431288972497,
      "learning_rate": 0.06545118000925497,
      "loss": 0.0001,
      "step": 37330
    },
    {
      "epoch": 17.279037482646924,
      "grad_norm": 0.0024950960651040077,
      "learning_rate": 0.06544192503470615,
      "loss": 0.0001,
      "step": 37340
    },
    {
      "epoch": 17.283664969921332,
      "grad_norm": 4.262235641479492,
      "learning_rate": 0.06543267006015734,
      "loss": 0.0016,
      "step": 37350
    },
    {
      "epoch": 17.28829245719574,
      "grad_norm": 0.004290373995900154,
      "learning_rate": 0.06542341508560852,
      "loss": 0.0,
      "step": 37360
    },
    {
      "epoch": 17.292919944470153,
      "grad_norm": 0.0017007440328598022,
      "learning_rate": 0.0654141601110597,
      "loss": 0.0008,
      "step": 37370
    },
    {
      "epoch": 17.29754743174456,
      "grad_norm": 0.061298489570617676,
      "learning_rate": 0.06540490513651087,
      "loss": 0.0002,
      "step": 37380
    },
    {
      "epoch": 17.302174919018974,
      "grad_norm": 0.25642287731170654,
      "learning_rate": 0.06539565016196207,
      "loss": 0.0001,
      "step": 37390
    },
    {
      "epoch": 17.306802406293382,
      "grad_norm": 0.009387587197124958,
      "learning_rate": 0.06538639518741324,
      "loss": 0.0001,
      "step": 37400
    },
    {
      "epoch": 17.311429893567794,
      "grad_norm": 3.0442585945129395,
      "learning_rate": 0.06537714021286442,
      "loss": 0.0005,
      "step": 37410
    },
    {
      "epoch": 17.316057380842203,
      "grad_norm": 0.040253013372421265,
      "learning_rate": 0.06536788523831559,
      "loss": 0.0001,
      "step": 37420
    },
    {
      "epoch": 17.32068486811661,
      "grad_norm": 0.0018519588047638535,
      "learning_rate": 0.06535863026376677,
      "loss": 0.0001,
      "step": 37430
    },
    {
      "epoch": 17.325312355391024,
      "grad_norm": 0.0052918619476258755,
      "learning_rate": 0.06534937528921796,
      "loss": 0.0001,
      "step": 37440
    },
    {
      "epoch": 17.329939842665432,
      "grad_norm": 0.002115207491442561,
      "learning_rate": 0.06534012031466914,
      "loss": 0.0,
      "step": 37450
    },
    {
      "epoch": 17.334567329939844,
      "grad_norm": 0.004459974355995655,
      "learning_rate": 0.06533086534012032,
      "loss": 0.0,
      "step": 37460
    },
    {
      "epoch": 17.339194817214253,
      "grad_norm": 0.023805538192391396,
      "learning_rate": 0.06532161036557149,
      "loss": 0.0,
      "step": 37470
    },
    {
      "epoch": 17.34382230448866,
      "grad_norm": 0.00021944454056210816,
      "learning_rate": 0.06531235539102268,
      "loss": 0.0002,
      "step": 37480
    },
    {
      "epoch": 17.348449791763073,
      "grad_norm": 0.004622380714863539,
      "learning_rate": 0.06530310041647386,
      "loss": 0.0001,
      "step": 37490
    },
    {
      "epoch": 17.353077279037482,
      "grad_norm": 0.002082609571516514,
      "learning_rate": 0.06529384544192504,
      "loss": 0.0001,
      "step": 37500
    },
    {
      "epoch": 17.357704766311894,
      "grad_norm": 0.008432630449533463,
      "learning_rate": 0.06528459046737621,
      "loss": 0.0006,
      "step": 37510
    },
    {
      "epoch": 17.362332253586302,
      "grad_norm": 0.0029011168517172337,
      "learning_rate": 0.0652753354928274,
      "loss": 0.0002,
      "step": 37520
    },
    {
      "epoch": 17.36695974086071,
      "grad_norm": 0.006233635358512402,
      "learning_rate": 0.06526608051827858,
      "loss": 0.0,
      "step": 37530
    },
    {
      "epoch": 17.371587228135123,
      "grad_norm": 0.04558069631457329,
      "learning_rate": 0.06525682554372976,
      "loss": 0.0009,
      "step": 37540
    },
    {
      "epoch": 17.37621471540953,
      "grad_norm": 0.0013763729948550463,
      "learning_rate": 0.06524757056918094,
      "loss": 0.0008,
      "step": 37550
    },
    {
      "epoch": 17.380842202683944,
      "grad_norm": 0.005181363318115473,
      "learning_rate": 0.06523831559463211,
      "loss": 0.0001,
      "step": 37560
    },
    {
      "epoch": 17.385469689958352,
      "grad_norm": 0.005060561466962099,
      "learning_rate": 0.0652290606200833,
      "loss": 0.0,
      "step": 37570
    },
    {
      "epoch": 17.39009717723276,
      "grad_norm": 0.012713965959846973,
      "learning_rate": 0.06521980564553448,
      "loss": 0.0002,
      "step": 37580
    },
    {
      "epoch": 17.394724664507173,
      "grad_norm": 0.0008406621054746211,
      "learning_rate": 0.06521055067098566,
      "loss": 0.0001,
      "step": 37590
    },
    {
      "epoch": 17.39935215178158,
      "grad_norm": 0.6696165204048157,
      "learning_rate": 0.06520129569643683,
      "loss": 0.0005,
      "step": 37600
    },
    {
      "epoch": 17.403979639055994,
      "grad_norm": 0.05803936347365379,
      "learning_rate": 0.06519204072188801,
      "loss": 0.0002,
      "step": 37610
    },
    {
      "epoch": 17.408607126330402,
      "grad_norm": 0.01416794117540121,
      "learning_rate": 0.0651827857473392,
      "loss": 0.0001,
      "step": 37620
    },
    {
      "epoch": 17.413234613604814,
      "grad_norm": 0.12122803181409836,
      "learning_rate": 0.06517353077279038,
      "loss": 0.0001,
      "step": 37630
    },
    {
      "epoch": 17.417862100879223,
      "grad_norm": 0.0010044650407508016,
      "learning_rate": 0.06516427579824156,
      "loss": 0.0004,
      "step": 37640
    },
    {
      "epoch": 17.42248958815363,
      "grad_norm": 0.062130872160196304,
      "learning_rate": 0.06515502082369273,
      "loss": 0.0002,
      "step": 37650
    },
    {
      "epoch": 17.427117075428043,
      "grad_norm": 0.0010210385080426931,
      "learning_rate": 0.06514576584914392,
      "loss": 0.0,
      "step": 37660
    },
    {
      "epoch": 17.431744562702452,
      "grad_norm": 0.7631186246871948,
      "learning_rate": 0.0651365108745951,
      "loss": 0.0002,
      "step": 37670
    },
    {
      "epoch": 17.436372049976864,
      "grad_norm": 0.006739039905369282,
      "learning_rate": 0.06512725590004628,
      "loss": 0.0003,
      "step": 37680
    },
    {
      "epoch": 17.440999537251272,
      "grad_norm": 0.0035048704594373703,
      "learning_rate": 0.06511800092549745,
      "loss": 0.0,
      "step": 37690
    },
    {
      "epoch": 17.44562702452568,
      "grad_norm": 0.014283287338912487,
      "learning_rate": 0.06510874595094863,
      "loss": 0.0001,
      "step": 37700
    },
    {
      "epoch": 17.450254511800093,
      "grad_norm": 0.014803110621869564,
      "learning_rate": 0.06509949097639982,
      "loss": 0.0004,
      "step": 37710
    },
    {
      "epoch": 17.4548819990745,
      "grad_norm": 0.0009237526101060212,
      "learning_rate": 0.065090236001851,
      "loss": 0.0002,
      "step": 37720
    },
    {
      "epoch": 17.459509486348914,
      "grad_norm": 0.0005756664904765785,
      "learning_rate": 0.06508098102730218,
      "loss": 0.0001,
      "step": 37730
    },
    {
      "epoch": 17.464136973623322,
      "grad_norm": 0.009214472956955433,
      "learning_rate": 0.06507172605275335,
      "loss": 0.0001,
      "step": 37740
    },
    {
      "epoch": 17.46876446089773,
      "grad_norm": 0.0020510987378656864,
      "learning_rate": 0.06506247107820454,
      "loss": 0.0,
      "step": 37750
    },
    {
      "epoch": 17.473391948172143,
      "grad_norm": 0.020442867651581764,
      "learning_rate": 0.06505321610365572,
      "loss": 0.0001,
      "step": 37760
    },
    {
      "epoch": 17.47801943544655,
      "grad_norm": 0.012605135329067707,
      "learning_rate": 0.0650439611291069,
      "loss": 0.0001,
      "step": 37770
    },
    {
      "epoch": 17.482646922720964,
      "grad_norm": 0.02046096883714199,
      "learning_rate": 0.06503470615455807,
      "loss": 0.0001,
      "step": 37780
    },
    {
      "epoch": 17.487274409995372,
      "grad_norm": 0.006954585667699575,
      "learning_rate": 0.06502545118000926,
      "loss": 0.0001,
      "step": 37790
    },
    {
      "epoch": 17.491901897269784,
      "grad_norm": 0.01142165344208479,
      "learning_rate": 0.06501619620546044,
      "loss": 0.0001,
      "step": 37800
    },
    {
      "epoch": 17.496529384544193,
      "grad_norm": 0.0013749570352956653,
      "learning_rate": 0.06500694123091162,
      "loss": 0.0001,
      "step": 37810
    },
    {
      "epoch": 17.5011568718186,
      "grad_norm": 0.027965374290943146,
      "learning_rate": 0.0649976862563628,
      "loss": 0.0001,
      "step": 37820
    },
    {
      "epoch": 17.505784359093013,
      "grad_norm": 0.06326963752508163,
      "learning_rate": 0.06498843128181397,
      "loss": 0.0002,
      "step": 37830
    },
    {
      "epoch": 17.510411846367422,
      "grad_norm": 0.0038383787032216787,
      "learning_rate": 0.06497917630726516,
      "loss": 0.0001,
      "step": 37840
    },
    {
      "epoch": 17.515039333641834,
      "grad_norm": 0.04179536923766136,
      "learning_rate": 0.06496992133271634,
      "loss": 0.0003,
      "step": 37850
    },
    {
      "epoch": 17.519666820916243,
      "grad_norm": 0.006937210913747549,
      "learning_rate": 0.06496066635816752,
      "loss": 0.0023,
      "step": 37860
    },
    {
      "epoch": 17.52429430819065,
      "grad_norm": 0.00908844918012619,
      "learning_rate": 0.06495141138361869,
      "loss": 0.0004,
      "step": 37870
    },
    {
      "epoch": 17.528921795465063,
      "grad_norm": 0.07373077422380447,
      "learning_rate": 0.06494215640906988,
      "loss": 0.0005,
      "step": 37880
    },
    {
      "epoch": 17.53354928273947,
      "grad_norm": 0.0001028289771056734,
      "learning_rate": 0.06493290143452106,
      "loss": 0.0001,
      "step": 37890
    },
    {
      "epoch": 17.538176770013884,
      "grad_norm": 1.3928860425949097,
      "learning_rate": 0.06492364645997224,
      "loss": 0.0005,
      "step": 37900
    },
    {
      "epoch": 17.542804257288292,
      "grad_norm": 0.00047145175631158054,
      "learning_rate": 0.06491439148542343,
      "loss": 0.0002,
      "step": 37910
    },
    {
      "epoch": 17.5474317445627,
      "grad_norm": 0.0006200510542839766,
      "learning_rate": 0.0649051365108746,
      "loss": 0.0001,
      "step": 37920
    },
    {
      "epoch": 17.552059231837113,
      "grad_norm": 0.015437341295182705,
      "learning_rate": 0.06489588153632578,
      "loss": 0.0014,
      "step": 37930
    },
    {
      "epoch": 17.55668671911152,
      "grad_norm": 0.8112993240356445,
      "learning_rate": 0.06488662656177695,
      "loss": 0.0002,
      "step": 37940
    },
    {
      "epoch": 17.561314206385934,
      "grad_norm": 0.58985435962677,
      "learning_rate": 0.06487737158722814,
      "loss": 0.0008,
      "step": 37950
    },
    {
      "epoch": 17.565941693660342,
      "grad_norm": 0.05463129281997681,
      "learning_rate": 0.06486811661267931,
      "loss": 0.0017,
      "step": 37960
    },
    {
      "epoch": 17.570569180934754,
      "grad_norm": 0.0005270230467431247,
      "learning_rate": 0.0648588616381305,
      "loss": 0.0001,
      "step": 37970
    },
    {
      "epoch": 17.575196668209163,
      "grad_norm": 0.005669873673468828,
      "learning_rate": 0.06484960666358168,
      "loss": 0.0001,
      "step": 37980
    },
    {
      "epoch": 17.57982415548357,
      "grad_norm": 0.0019633949268609285,
      "learning_rate": 0.06484035168903286,
      "loss": 0.0001,
      "step": 37990
    },
    {
      "epoch": 17.584451642757983,
      "grad_norm": 0.002450463827699423,
      "learning_rate": 0.06483109671448405,
      "loss": 0.0001,
      "step": 38000
    },
    {
      "epoch": 17.589079130032392,
      "grad_norm": 0.06555663794279099,
      "learning_rate": 0.06482184173993522,
      "loss": 0.0003,
      "step": 38010
    },
    {
      "epoch": 17.593706617306804,
      "grad_norm": 0.005834624171257019,
      "learning_rate": 0.0648125867653864,
      "loss": 0.0006,
      "step": 38020
    },
    {
      "epoch": 17.598334104581213,
      "grad_norm": 0.0025274227373301983,
      "learning_rate": 0.06480333179083757,
      "loss": 0.0001,
      "step": 38030
    },
    {
      "epoch": 17.60296159185562,
      "grad_norm": 0.003323383629322052,
      "learning_rate": 0.06479407681628876,
      "loss": 0.0002,
      "step": 38040
    },
    {
      "epoch": 17.607589079130033,
      "grad_norm": 0.005819297395646572,
      "learning_rate": 0.06478482184173993,
      "loss": 0.0,
      "step": 38050
    },
    {
      "epoch": 17.61221656640444,
      "grad_norm": 0.052014999091625214,
      "learning_rate": 0.06477556686719112,
      "loss": 0.0002,
      "step": 38060
    },
    {
      "epoch": 17.616844053678854,
      "grad_norm": 0.021524662151932716,
      "learning_rate": 0.0647663118926423,
      "loss": 0.0004,
      "step": 38070
    },
    {
      "epoch": 17.621471540953262,
      "grad_norm": 0.03260777145624161,
      "learning_rate": 0.06475705691809348,
      "loss": 0.0002,
      "step": 38080
    },
    {
      "epoch": 17.62609902822767,
      "grad_norm": 0.004708084277808666,
      "learning_rate": 0.06474780194354467,
      "loss": 0.0001,
      "step": 38090
    },
    {
      "epoch": 17.630726515502083,
      "grad_norm": 0.0019976983312517405,
      "learning_rate": 0.06473854696899584,
      "loss": 0.0001,
      "step": 38100
    },
    {
      "epoch": 17.63535400277649,
      "grad_norm": 0.011884169653058052,
      "learning_rate": 0.06472929199444702,
      "loss": 0.0008,
      "step": 38110
    },
    {
      "epoch": 17.639981490050904,
      "grad_norm": 0.014848262071609497,
      "learning_rate": 0.06472003701989819,
      "loss": 0.0002,
      "step": 38120
    },
    {
      "epoch": 17.644608977325312,
      "grad_norm": 0.006135724484920502,
      "learning_rate": 0.06471078204534939,
      "loss": 0.0003,
      "step": 38130
    },
    {
      "epoch": 17.649236464599724,
      "grad_norm": 0.021003274247050285,
      "learning_rate": 0.06470152707080055,
      "loss": 0.0,
      "step": 38140
    },
    {
      "epoch": 17.653863951874133,
      "grad_norm": 0.8622227907180786,
      "learning_rate": 0.06469227209625174,
      "loss": 0.0006,
      "step": 38150
    },
    {
      "epoch": 17.65849143914854,
      "grad_norm": 0.06145266070961952,
      "learning_rate": 0.06468301712170292,
      "loss": 0.0002,
      "step": 38160
    },
    {
      "epoch": 17.663118926422953,
      "grad_norm": 0.002656947122886777,
      "learning_rate": 0.06467376214715409,
      "loss": 0.0022,
      "step": 38170
    },
    {
      "epoch": 17.667746413697362,
      "grad_norm": 0.005317757837474346,
      "learning_rate": 0.06466450717260529,
      "loss": 0.0002,
      "step": 38180
    },
    {
      "epoch": 17.672373900971774,
      "grad_norm": 0.00241377972997725,
      "learning_rate": 0.06465525219805646,
      "loss": 0.0001,
      "step": 38190
    },
    {
      "epoch": 17.677001388246183,
      "grad_norm": 0.001667979871854186,
      "learning_rate": 0.06464599722350764,
      "loss": 0.0001,
      "step": 38200
    },
    {
      "epoch": 17.68162887552059,
      "grad_norm": 0.001868086983449757,
      "learning_rate": 0.06463674224895881,
      "loss": 0.0001,
      "step": 38210
    },
    {
      "epoch": 17.686256362795003,
      "grad_norm": 0.014483340084552765,
      "learning_rate": 0.06462748727441,
      "loss": 0.0005,
      "step": 38220
    },
    {
      "epoch": 17.69088385006941,
      "grad_norm": 0.01133157592266798,
      "learning_rate": 0.06461823229986118,
      "loss": 0.0004,
      "step": 38230
    },
    {
      "epoch": 17.695511337343824,
      "grad_norm": 0.01277715340256691,
      "learning_rate": 0.06460897732531236,
      "loss": 0.0001,
      "step": 38240
    },
    {
      "epoch": 17.700138824618232,
      "grad_norm": 0.0003880993463099003,
      "learning_rate": 0.06459972235076354,
      "loss": 0.0001,
      "step": 38250
    },
    {
      "epoch": 17.70476631189264,
      "grad_norm": 0.023137114942073822,
      "learning_rate": 0.06459046737621471,
      "loss": 0.0002,
      "step": 38260
    },
    {
      "epoch": 17.709393799167053,
      "grad_norm": 5.1699442863464355,
      "learning_rate": 0.06458121240166591,
      "loss": 0.0018,
      "step": 38270
    },
    {
      "epoch": 17.71402128644146,
      "grad_norm": 0.01750997081398964,
      "learning_rate": 0.06457195742711708,
      "loss": 0.001,
      "step": 38280
    },
    {
      "epoch": 17.718648773715874,
      "grad_norm": 0.013017499819397926,
      "learning_rate": 0.06456270245256826,
      "loss": 0.0,
      "step": 38290
    },
    {
      "epoch": 17.723276260990282,
      "grad_norm": 0.059174928814172745,
      "learning_rate": 0.06455344747801943,
      "loss": 0.0001,
      "step": 38300
    },
    {
      "epoch": 17.72790374826469,
      "grad_norm": 0.0036912737414240837,
      "learning_rate": 0.06454419250347063,
      "loss": 0.0001,
      "step": 38310
    },
    {
      "epoch": 17.732531235539103,
      "grad_norm": 0.03078138455748558,
      "learning_rate": 0.0645349375289218,
      "loss": 0.0001,
      "step": 38320
    },
    {
      "epoch": 17.73715872281351,
      "grad_norm": 0.5028526782989502,
      "learning_rate": 0.06452568255437298,
      "loss": 0.0003,
      "step": 38330
    },
    {
      "epoch": 17.741786210087923,
      "grad_norm": 0.008444334380328655,
      "learning_rate": 0.06451642757982416,
      "loss": 0.0002,
      "step": 38340
    },
    {
      "epoch": 17.746413697362332,
      "grad_norm": 0.010053865611553192,
      "learning_rate": 0.06450717260527533,
      "loss": 0.0002,
      "step": 38350
    },
    {
      "epoch": 17.751041184636744,
      "grad_norm": 0.002804012503474951,
      "learning_rate": 0.06449791763072653,
      "loss": 0.0001,
      "step": 38360
    },
    {
      "epoch": 17.755668671911153,
      "grad_norm": 0.01810077577829361,
      "learning_rate": 0.0644886626561777,
      "loss": 0.0002,
      "step": 38370
    },
    {
      "epoch": 17.76029615918556,
      "grad_norm": 0.010484252125024796,
      "learning_rate": 0.06447940768162888,
      "loss": 0.0001,
      "step": 38380
    },
    {
      "epoch": 17.764923646459973,
      "grad_norm": 0.3437609076499939,
      "learning_rate": 0.06447015270708005,
      "loss": 0.0001,
      "step": 38390
    },
    {
      "epoch": 17.76955113373438,
      "grad_norm": 0.0372912734746933,
      "learning_rate": 0.06446089773253123,
      "loss": 0.0001,
      "step": 38400
    },
    {
      "epoch": 17.774178621008794,
      "grad_norm": 0.0011154836975038052,
      "learning_rate": 0.06445164275798242,
      "loss": 0.0002,
      "step": 38410
    },
    {
      "epoch": 17.778806108283202,
      "grad_norm": 0.00069288513623178,
      "learning_rate": 0.0644423877834336,
      "loss": 0.0001,
      "step": 38420
    },
    {
      "epoch": 17.78343359555761,
      "grad_norm": 0.4445587694644928,
      "learning_rate": 0.06443313280888478,
      "loss": 0.0003,
      "step": 38430
    },
    {
      "epoch": 17.788061082832023,
      "grad_norm": 0.0028912480920553207,
      "learning_rate": 0.06442387783433595,
      "loss": 0.0002,
      "step": 38440
    },
    {
      "epoch": 17.79268857010643,
      "grad_norm": 0.0065662856213748455,
      "learning_rate": 0.06441462285978715,
      "loss": 0.0001,
      "step": 38450
    },
    {
      "epoch": 17.797316057380844,
      "grad_norm": 0.000518346147146076,
      "learning_rate": 0.06440536788523832,
      "loss": 0.0011,
      "step": 38460
    },
    {
      "epoch": 17.801943544655252,
      "grad_norm": 0.11348655074834824,
      "learning_rate": 0.0643961129106895,
      "loss": 0.0005,
      "step": 38470
    },
    {
      "epoch": 17.80657103192966,
      "grad_norm": 0.019653599709272385,
      "learning_rate": 0.06438685793614067,
      "loss": 0.0002,
      "step": 38480
    },
    {
      "epoch": 17.811198519204073,
      "grad_norm": 0.1317853033542633,
      "learning_rate": 0.06437760296159185,
      "loss": 0.0001,
      "step": 38490
    },
    {
      "epoch": 17.81582600647848,
      "grad_norm": 0.005063008517026901,
      "learning_rate": 0.06436834798704304,
      "loss": 0.0001,
      "step": 38500
    },
    {
      "epoch": 17.820453493752893,
      "grad_norm": 0.006514014210551977,
      "learning_rate": 0.06435909301249422,
      "loss": 0.0004,
      "step": 38510
    },
    {
      "epoch": 17.825080981027302,
      "grad_norm": 0.006288453936576843,
      "learning_rate": 0.0643498380379454,
      "loss": 0.0001,
      "step": 38520
    },
    {
      "epoch": 17.82970846830171,
      "grad_norm": 0.005812000948935747,
      "learning_rate": 0.06434058306339657,
      "loss": 0.0001,
      "step": 38530
    },
    {
      "epoch": 17.834335955576123,
      "grad_norm": 0.0021676768083125353,
      "learning_rate": 0.06433132808884777,
      "loss": 0.0001,
      "step": 38540
    },
    {
      "epoch": 17.83896344285053,
      "grad_norm": 0.0017180561553686857,
      "learning_rate": 0.06432207311429894,
      "loss": 0.0001,
      "step": 38550
    },
    {
      "epoch": 17.843590930124943,
      "grad_norm": 0.039563603699207306,
      "learning_rate": 0.06431281813975012,
      "loss": 0.0003,
      "step": 38560
    },
    {
      "epoch": 17.84821841739935,
      "grad_norm": 0.0012563630007207394,
      "learning_rate": 0.06430356316520129,
      "loss": 0.0077,
      "step": 38570
    },
    {
      "epoch": 17.852845904673764,
      "grad_norm": 5.552707195281982,
      "learning_rate": 0.06429430819065247,
      "loss": 0.0008,
      "step": 38580
    },
    {
      "epoch": 17.857473391948172,
      "grad_norm": 0.09899928420782089,
      "learning_rate": 0.06428505321610366,
      "loss": 0.0004,
      "step": 38590
    },
    {
      "epoch": 17.86210087922258,
      "grad_norm": 0.01758722774684429,
      "learning_rate": 0.06427579824155484,
      "loss": 0.0022,
      "step": 38600
    },
    {
      "epoch": 17.866728366496993,
      "grad_norm": 0.05356310307979584,
      "learning_rate": 0.06426654326700602,
      "loss": 0.0011,
      "step": 38610
    },
    {
      "epoch": 17.8713558537714,
      "grad_norm": 0.007915066555142403,
      "learning_rate": 0.0642572882924572,
      "loss": 0.0001,
      "step": 38620
    },
    {
      "epoch": 17.875983341045814,
      "grad_norm": 0.0011271160328760743,
      "learning_rate": 0.06424803331790838,
      "loss": 0.0,
      "step": 38630
    },
    {
      "epoch": 17.880610828320222,
      "grad_norm": 0.0158042274415493,
      "learning_rate": 0.06423877834335956,
      "loss": 0.0005,
      "step": 38640
    },
    {
      "epoch": 17.88523831559463,
      "grad_norm": 0.4225495159626007,
      "learning_rate": 0.06422952336881074,
      "loss": 0.0002,
      "step": 38650
    },
    {
      "epoch": 17.889865802869043,
      "grad_norm": 0.06182720139622688,
      "learning_rate": 0.06422026839426191,
      "loss": 0.0003,
      "step": 38660
    },
    {
      "epoch": 17.89449329014345,
      "grad_norm": 0.006121843587607145,
      "learning_rate": 0.0642110134197131,
      "loss": 0.0002,
      "step": 38670
    },
    {
      "epoch": 17.899120777417863,
      "grad_norm": 0.032825108617544174,
      "learning_rate": 0.06420175844516428,
      "loss": 0.0004,
      "step": 38680
    },
    {
      "epoch": 17.903748264692272,
      "grad_norm": 0.13158412277698517,
      "learning_rate": 0.06419250347061546,
      "loss": 0.0006,
      "step": 38690
    },
    {
      "epoch": 17.90837575196668,
      "grad_norm": 0.22387690842151642,
      "learning_rate": 0.06418324849606664,
      "loss": 0.0003,
      "step": 38700
    },
    {
      "epoch": 17.913003239241093,
      "grad_norm": 0.06733720004558563,
      "learning_rate": 0.06417399352151781,
      "loss": 0.0005,
      "step": 38710
    },
    {
      "epoch": 17.9176307265155,
      "grad_norm": 0.004086140543222427,
      "learning_rate": 0.064164738546969,
      "loss": 0.0002,
      "step": 38720
    },
    {
      "epoch": 17.922258213789913,
      "grad_norm": 0.004723330494016409,
      "learning_rate": 0.06415548357242018,
      "loss": 0.0001,
      "step": 38730
    },
    {
      "epoch": 17.92688570106432,
      "grad_norm": 0.01805775798857212,
      "learning_rate": 0.06414622859787136,
      "loss": 0.0009,
      "step": 38740
    },
    {
      "epoch": 17.931513188338734,
      "grad_norm": 0.0019134945468977094,
      "learning_rate": 0.06413697362332253,
      "loss": 0.0001,
      "step": 38750
    },
    {
      "epoch": 17.936140675613142,
      "grad_norm": 0.00650427769869566,
      "learning_rate": 0.06412771864877372,
      "loss": 0.0002,
      "step": 38760
    },
    {
      "epoch": 17.94076816288755,
      "grad_norm": 0.011737275868654251,
      "learning_rate": 0.0641184636742249,
      "loss": 0.0008,
      "step": 38770
    },
    {
      "epoch": 17.945395650161963,
      "grad_norm": 0.004772832151502371,
      "learning_rate": 0.06410920869967608,
      "loss": 0.0001,
      "step": 38780
    },
    {
      "epoch": 17.95002313743637,
      "grad_norm": 0.014423676766455173,
      "learning_rate": 0.06409995372512726,
      "loss": 0.0001,
      "step": 38790
    },
    {
      "epoch": 17.954650624710784,
      "grad_norm": 4.049963474273682,
      "learning_rate": 0.06409069875057843,
      "loss": 0.0006,
      "step": 38800
    },
    {
      "epoch": 17.959278111985192,
      "grad_norm": 0.007295684888958931,
      "learning_rate": 0.06408144377602962,
      "loss": 0.0004,
      "step": 38810
    },
    {
      "epoch": 17.9639055992596,
      "grad_norm": 0.001761681865900755,
      "learning_rate": 0.0640721888014808,
      "loss": 0.0001,
      "step": 38820
    },
    {
      "epoch": 17.968533086534013,
      "grad_norm": 0.04189804568886757,
      "learning_rate": 0.06406293382693198,
      "loss": 0.0001,
      "step": 38830
    },
    {
      "epoch": 17.97316057380842,
      "grad_norm": 0.0016607593279331923,
      "learning_rate": 0.06405367885238315,
      "loss": 0.0001,
      "step": 38840
    },
    {
      "epoch": 17.977788061082833,
      "grad_norm": 0.0062564099207520485,
      "learning_rate": 0.06404442387783434,
      "loss": 0.0,
      "step": 38850
    },
    {
      "epoch": 17.982415548357242,
      "grad_norm": 0.0013276357203722,
      "learning_rate": 0.06403516890328552,
      "loss": 0.0001,
      "step": 38860
    },
    {
      "epoch": 17.98704303563165,
      "grad_norm": 0.06188800185918808,
      "learning_rate": 0.0640259139287367,
      "loss": 0.0002,
      "step": 38870
    },
    {
      "epoch": 17.991670522906063,
      "grad_norm": 0.26534566283226013,
      "learning_rate": 0.06401665895418789,
      "loss": 0.0001,
      "step": 38880
    },
    {
      "epoch": 17.99629801018047,
      "grad_norm": 4.297122001647949,
      "learning_rate": 0.06400740397963905,
      "loss": 0.0008,
      "step": 38890
    },
    {
      "epoch": 18.0,
      "eval_accuracy_branch1": 0.9889077183792944,
      "eval_accuracy_branch2": 0.49869049453088893,
      "eval_f1_branch1": 0.9901608948217525,
      "eval_f1_branch2": 0.4982283100180471,
      "eval_loss": 0.02287992835044861,
      "eval_precision_branch1": 0.9904489726064825,
      "eval_precision_branch2": 0.4986856519194828,
      "eval_recall_branch1": 0.9900482607767114,
      "eval_recall_branch2": 0.49869049453088893,
      "eval_runtime": 28.972,
      "eval_samples_per_second": 448.088,
      "eval_steps_per_second": 56.02,
      "step": 38898
    },
    {
      "epoch": 18.000925497454883,
      "grad_norm": 0.5273340344429016,
      "learning_rate": 0.06399814900509024,
      "loss": 0.1406,
      "step": 38900
    },
    {
      "epoch": 18.00555298472929,
      "grad_norm": 0.00814136117696762,
      "learning_rate": 0.06398889403054142,
      "loss": 0.0,
      "step": 38910
    },
    {
      "epoch": 18.010180472003704,
      "grad_norm": 0.002118695294484496,
      "learning_rate": 0.0639796390559926,
      "loss": 0.0001,
      "step": 38920
    },
    {
      "epoch": 18.014807959278112,
      "grad_norm": 0.03438606113195419,
      "learning_rate": 0.06397038408144377,
      "loss": 0.005,
      "step": 38930
    },
    {
      "epoch": 18.01943544655252,
      "grad_norm": 0.001180032268166542,
      "learning_rate": 0.06396112910689496,
      "loss": 0.0002,
      "step": 38940
    },
    {
      "epoch": 18.024062933826933,
      "grad_norm": 0.00028374631074257195,
      "learning_rate": 0.06395187413234614,
      "loss": 0.0001,
      "step": 38950
    },
    {
      "epoch": 18.02869042110134,
      "grad_norm": 0.0018830677727237344,
      "learning_rate": 0.06394261915779732,
      "loss": 0.0001,
      "step": 38960
    },
    {
      "epoch": 18.033317908375754,
      "grad_norm": 0.014250458218157291,
      "learning_rate": 0.0639333641832485,
      "loss": 0.0001,
      "step": 38970
    },
    {
      "epoch": 18.037945395650162,
      "grad_norm": 2.472743511199951,
      "learning_rate": 0.06392410920869968,
      "loss": 0.0011,
      "step": 38980
    },
    {
      "epoch": 18.04257288292457,
      "grad_norm": 0.004461290314793587,
      "learning_rate": 0.06391485423415086,
      "loss": 0.0001,
      "step": 38990
    },
    {
      "epoch": 18.047200370198983,
      "grad_norm": 0.0018734920304268599,
      "learning_rate": 0.06390559925960203,
      "loss": 0.0002,
      "step": 39000
    },
    {
      "epoch": 18.05182785747339,
      "grad_norm": 0.0021234210580587387,
      "learning_rate": 0.06389634428505322,
      "loss": 0.0001,
      "step": 39010
    },
    {
      "epoch": 18.056455344747803,
      "grad_norm": 0.004055298399180174,
      "learning_rate": 0.0638870893105044,
      "loss": 0.0031,
      "step": 39020
    },
    {
      "epoch": 18.061082832022212,
      "grad_norm": 0.0339394249022007,
      "learning_rate": 0.06387783433595558,
      "loss": 0.0001,
      "step": 39030
    },
    {
      "epoch": 18.06571031929662,
      "grad_norm": 0.005982936359941959,
      "learning_rate": 0.06386857936140676,
      "loss": 0.0008,
      "step": 39040
    },
    {
      "epoch": 18.070337806571033,
      "grad_norm": 0.42884361743927,
      "learning_rate": 0.06385932438685794,
      "loss": 0.0002,
      "step": 39050
    },
    {
      "epoch": 18.07496529384544,
      "grad_norm": 0.004569556098431349,
      "learning_rate": 0.06385006941230913,
      "loss": 0.0001,
      "step": 39060
    },
    {
      "epoch": 18.079592781119853,
      "grad_norm": 0.026683960109949112,
      "learning_rate": 0.0638408144377603,
      "loss": 0.0001,
      "step": 39070
    },
    {
      "epoch": 18.08422026839426,
      "grad_norm": 0.004034239798784256,
      "learning_rate": 0.06383155946321148,
      "loss": 0.0003,
      "step": 39080
    },
    {
      "epoch": 18.08884775566867,
      "grad_norm": 0.0010892297141253948,
      "learning_rate": 0.06382230448866265,
      "loss": 0.0001,
      "step": 39090
    },
    {
      "epoch": 18.093475242943082,
      "grad_norm": 0.1258162409067154,
      "learning_rate": 0.06381304951411385,
      "loss": 0.0001,
      "step": 39100
    },
    {
      "epoch": 18.09810273021749,
      "grad_norm": 0.05661001428961754,
      "learning_rate": 0.06380379453956501,
      "loss": 0.0002,
      "step": 39110
    },
    {
      "epoch": 18.102730217491903,
      "grad_norm": 0.1748325675725937,
      "learning_rate": 0.0637945395650162,
      "loss": 0.0001,
      "step": 39120
    },
    {
      "epoch": 18.10735770476631,
      "grad_norm": 0.011249312199652195,
      "learning_rate": 0.06378528459046738,
      "loss": 0.0001,
      "step": 39130
    },
    {
      "epoch": 18.111985192040724,
      "grad_norm": 0.03849795088171959,
      "learning_rate": 0.06377602961591856,
      "loss": 0.0001,
      "step": 39140
    },
    {
      "epoch": 18.116612679315132,
      "grad_norm": 0.005765931680798531,
      "learning_rate": 0.06376677464136975,
      "loss": 0.0012,
      "step": 39150
    },
    {
      "epoch": 18.12124016658954,
      "grad_norm": 0.0045502204447984695,
      "learning_rate": 0.06375751966682092,
      "loss": 0.0001,
      "step": 39160
    },
    {
      "epoch": 18.125867653863953,
      "grad_norm": 0.006004345137625933,
      "learning_rate": 0.0637482646922721,
      "loss": 0.0008,
      "step": 39170
    },
    {
      "epoch": 18.13049514113836,
      "grad_norm": 0.007871020585298538,
      "learning_rate": 0.06373900971772327,
      "loss": 0.0001,
      "step": 39180
    },
    {
      "epoch": 18.135122628412773,
      "grad_norm": 0.001282348996028304,
      "learning_rate": 0.06372975474317447,
      "loss": 0.0005,
      "step": 39190
    },
    {
      "epoch": 18.139750115687182,
      "grad_norm": 0.005589517764747143,
      "learning_rate": 0.06372049976862564,
      "loss": 0.0041,
      "step": 39200
    },
    {
      "epoch": 18.14437760296159,
      "grad_norm": 0.000932393129914999,
      "learning_rate": 0.06371124479407682,
      "loss": 0.0003,
      "step": 39210
    },
    {
      "epoch": 18.149005090236003,
      "grad_norm": 0.0033491619396954775,
      "learning_rate": 0.063701989819528,
      "loss": 0.0006,
      "step": 39220
    },
    {
      "epoch": 18.15363257751041,
      "grad_norm": 0.01094423234462738,
      "learning_rate": 0.06369273484497917,
      "loss": 0.0001,
      "step": 39230
    },
    {
      "epoch": 18.158260064784823,
      "grad_norm": 0.004490325693041086,
      "learning_rate": 0.06368347987043037,
      "loss": 0.0001,
      "step": 39240
    },
    {
      "epoch": 18.16288755205923,
      "grad_norm": 0.0010652071796357632,
      "learning_rate": 0.06367422489588154,
      "loss": 0.0004,
      "step": 39250
    },
    {
      "epoch": 18.16751503933364,
      "grad_norm": 0.1985037475824356,
      "learning_rate": 0.06366496992133272,
      "loss": 0.0001,
      "step": 39260
    },
    {
      "epoch": 18.172142526608052,
      "grad_norm": 0.00039519675192423165,
      "learning_rate": 0.06365571494678389,
      "loss": 0.0002,
      "step": 39270
    },
    {
      "epoch": 18.17677001388246,
      "grad_norm": 0.0013624754501506686,
      "learning_rate": 0.06364645997223509,
      "loss": 0.0001,
      "step": 39280
    },
    {
      "epoch": 18.181397501156873,
      "grad_norm": 0.007715110667049885,
      "learning_rate": 0.06363720499768626,
      "loss": 0.0001,
      "step": 39290
    },
    {
      "epoch": 18.18602498843128,
      "grad_norm": 0.016526855528354645,
      "learning_rate": 0.06362795002313744,
      "loss": 0.0001,
      "step": 39300
    },
    {
      "epoch": 18.190652475705694,
      "grad_norm": 0.04482677951455116,
      "learning_rate": 0.06361869504858862,
      "loss": 0.0003,
      "step": 39310
    },
    {
      "epoch": 18.195279962980102,
      "grad_norm": 0.001628160011023283,
      "learning_rate": 0.06360944007403979,
      "loss": 0.001,
      "step": 39320
    },
    {
      "epoch": 18.19990745025451,
      "grad_norm": 0.00811746809631586,
      "learning_rate": 0.06360018509949099,
      "loss": 0.002,
      "step": 39330
    },
    {
      "epoch": 18.204534937528923,
      "grad_norm": 0.0710354670882225,
      "learning_rate": 0.06359093012494216,
      "loss": 0.0005,
      "step": 39340
    },
    {
      "epoch": 18.20916242480333,
      "grad_norm": 0.0012656196486204863,
      "learning_rate": 0.06358167515039334,
      "loss": 0.0001,
      "step": 39350
    },
    {
      "epoch": 18.213789912077743,
      "grad_norm": 0.011292151175439358,
      "learning_rate": 0.06357242017584451,
      "loss": 0.0001,
      "step": 39360
    },
    {
      "epoch": 18.218417399352152,
      "grad_norm": 0.005852133501321077,
      "learning_rate": 0.06356316520129571,
      "loss": 0.0002,
      "step": 39370
    },
    {
      "epoch": 18.22304488662656,
      "grad_norm": 0.013092349283397198,
      "learning_rate": 0.06355391022674688,
      "loss": 0.0,
      "step": 39380
    },
    {
      "epoch": 18.227672373900973,
      "grad_norm": 0.0006396827520802617,
      "learning_rate": 0.06354465525219806,
      "loss": 0.0045,
      "step": 39390
    },
    {
      "epoch": 18.23229986117538,
      "grad_norm": 0.013971003703773022,
      "learning_rate": 0.06353540027764924,
      "loss": 0.0003,
      "step": 39400
    },
    {
      "epoch": 18.236927348449793,
      "grad_norm": 0.022610198706388474,
      "learning_rate": 0.06352614530310041,
      "loss": 0.0001,
      "step": 39410
    },
    {
      "epoch": 18.2415548357242,
      "grad_norm": 0.00073317188071087,
      "learning_rate": 0.06351689032855161,
      "loss": 0.0004,
      "step": 39420
    },
    {
      "epoch": 18.24618232299861,
      "grad_norm": 0.02442663349211216,
      "learning_rate": 0.06350763535400278,
      "loss": 0.0002,
      "step": 39430
    },
    {
      "epoch": 18.250809810273022,
      "grad_norm": 0.02177719958126545,
      "learning_rate": 0.06349838037945396,
      "loss": 0.0001,
      "step": 39440
    },
    {
      "epoch": 18.25543729754743,
      "grad_norm": 0.0263295229524374,
      "learning_rate": 0.06348912540490513,
      "loss": 0.0002,
      "step": 39450
    },
    {
      "epoch": 18.260064784821843,
      "grad_norm": 0.039740189909935,
      "learning_rate": 0.06347987043035631,
      "loss": 0.0001,
      "step": 39460
    },
    {
      "epoch": 18.26469227209625,
      "grad_norm": 0.006111482158303261,
      "learning_rate": 0.0634706154558075,
      "loss": 0.0003,
      "step": 39470
    },
    {
      "epoch": 18.26931975937066,
      "grad_norm": 0.08546832203865051,
      "learning_rate": 0.06346136048125868,
      "loss": 0.0001,
      "step": 39480
    },
    {
      "epoch": 18.273947246645072,
      "grad_norm": 0.03629877045750618,
      "learning_rate": 0.06345210550670986,
      "loss": 0.0,
      "step": 39490
    },
    {
      "epoch": 18.27857473391948,
      "grad_norm": 0.010036018677055836,
      "learning_rate": 0.06344285053216103,
      "loss": 0.0001,
      "step": 39500
    },
    {
      "epoch": 18.283202221193893,
      "grad_norm": 0.19146326184272766,
      "learning_rate": 0.06343359555761223,
      "loss": 0.0001,
      "step": 39510
    },
    {
      "epoch": 18.2878297084683,
      "grad_norm": 0.041111189872026443,
      "learning_rate": 0.0634243405830634,
      "loss": 0.0001,
      "step": 39520
    },
    {
      "epoch": 18.292457195742713,
      "grad_norm": 0.0015445973258465528,
      "learning_rate": 0.06341508560851458,
      "loss": 0.0001,
      "step": 39530
    },
    {
      "epoch": 18.297084683017122,
      "grad_norm": 0.001338962814770639,
      "learning_rate": 0.06340583063396575,
      "loss": 0.0001,
      "step": 39540
    },
    {
      "epoch": 18.30171217029153,
      "grad_norm": 0.0011429182486608624,
      "learning_rate": 0.06339657565941693,
      "loss": 0.0001,
      "step": 39550
    },
    {
      "epoch": 18.306339657565943,
      "grad_norm": 0.7232847809791565,
      "learning_rate": 0.06338732068486812,
      "loss": 0.0003,
      "step": 39560
    },
    {
      "epoch": 18.31096714484035,
      "grad_norm": 0.0009675163892097771,
      "learning_rate": 0.0633780657103193,
      "loss": 0.0002,
      "step": 39570
    },
    {
      "epoch": 18.315594632114763,
      "grad_norm": 0.0023147775791585445,
      "learning_rate": 0.06336881073577048,
      "loss": 0.0,
      "step": 39580
    },
    {
      "epoch": 18.32022211938917,
      "grad_norm": 0.04517567157745361,
      "learning_rate": 0.06335955576122165,
      "loss": 0.0001,
      "step": 39590
    },
    {
      "epoch": 18.32484960666358,
      "grad_norm": 0.0017128221224993467,
      "learning_rate": 0.06335030078667285,
      "loss": 0.0,
      "step": 39600
    },
    {
      "epoch": 18.329477093937992,
      "grad_norm": 0.009533638134598732,
      "learning_rate": 0.06334104581212402,
      "loss": 0.0002,
      "step": 39610
    },
    {
      "epoch": 18.3341045812124,
      "grad_norm": 0.0222410187125206,
      "learning_rate": 0.0633317908375752,
      "loss": 0.0001,
      "step": 39620
    },
    {
      "epoch": 18.338732068486813,
      "grad_norm": 0.0017661943566054106,
      "learning_rate": 0.06332253586302637,
      "loss": 0.0001,
      "step": 39630
    },
    {
      "epoch": 18.34335955576122,
      "grad_norm": 0.001270396402105689,
      "learning_rate": 0.06331328088847755,
      "loss": 0.0002,
      "step": 39640
    },
    {
      "epoch": 18.34798704303563,
      "grad_norm": 0.00988195464015007,
      "learning_rate": 0.06330402591392874,
      "loss": 0.0003,
      "step": 39650
    },
    {
      "epoch": 18.352614530310042,
      "grad_norm": 0.0011522042332217097,
      "learning_rate": 0.06329477093937992,
      "loss": 0.0004,
      "step": 39660
    },
    {
      "epoch": 18.35724201758445,
      "grad_norm": 0.25667592883110046,
      "learning_rate": 0.0632855159648311,
      "loss": 0.0004,
      "step": 39670
    },
    {
      "epoch": 18.361869504858863,
      "grad_norm": 0.0006415193784050643,
      "learning_rate": 0.06327626099028227,
      "loss": 0.0001,
      "step": 39680
    },
    {
      "epoch": 18.36649699213327,
      "grad_norm": 0.0130764190107584,
      "learning_rate": 0.06326700601573346,
      "loss": 0.0001,
      "step": 39690
    },
    {
      "epoch": 18.371124479407683,
      "grad_norm": 0.007887031883001328,
      "learning_rate": 0.06325775104118464,
      "loss": 0.0012,
      "step": 39700
    },
    {
      "epoch": 18.375751966682092,
      "grad_norm": 0.029208233579993248,
      "learning_rate": 0.06324849606663582,
      "loss": 0.0003,
      "step": 39710
    },
    {
      "epoch": 18.3803794539565,
      "grad_norm": 0.17183591425418854,
      "learning_rate": 0.06323924109208699,
      "loss": 0.0001,
      "step": 39720
    },
    {
      "epoch": 18.385006941230913,
      "grad_norm": 0.014503132551908493,
      "learning_rate": 0.06322998611753818,
      "loss": 0.0002,
      "step": 39730
    },
    {
      "epoch": 18.38963442850532,
      "grad_norm": 0.002630294766277075,
      "learning_rate": 0.06322073114298936,
      "loss": 0.0001,
      "step": 39740
    },
    {
      "epoch": 18.394261915779733,
      "grad_norm": 0.21531245112419128,
      "learning_rate": 0.06321147616844054,
      "loss": 0.0001,
      "step": 39750
    },
    {
      "epoch": 18.39888940305414,
      "grad_norm": 0.0009123861673288047,
      "learning_rate": 0.06320222119389172,
      "loss": 0.0001,
      "step": 39760
    },
    {
      "epoch": 18.40351689032855,
      "grad_norm": 0.42570269107818604,
      "learning_rate": 0.0631929662193429,
      "loss": 0.0002,
      "step": 39770
    },
    {
      "epoch": 18.408144377602962,
      "grad_norm": 0.0013130459701642394,
      "learning_rate": 0.06318371124479408,
      "loss": 0.0001,
      "step": 39780
    },
    {
      "epoch": 18.41277186487737,
      "grad_norm": 0.000725921243429184,
      "learning_rate": 0.06317445627024526,
      "loss": 0.0,
      "step": 39790
    },
    {
      "epoch": 18.417399352151783,
      "grad_norm": 0.009204116649925709,
      "learning_rate": 0.06316520129569644,
      "loss": 0.0002,
      "step": 39800
    },
    {
      "epoch": 18.42202683942619,
      "grad_norm": 0.005313594359904528,
      "learning_rate": 0.06315594632114761,
      "loss": 0.0001,
      "step": 39810
    },
    {
      "epoch": 18.4266543267006,
      "grad_norm": 0.4219343066215515,
      "learning_rate": 0.0631466913465988,
      "loss": 0.0032,
      "step": 39820
    },
    {
      "epoch": 18.431281813975012,
      "grad_norm": 0.028029484674334526,
      "learning_rate": 0.06313743637204998,
      "loss": 0.0001,
      "step": 39830
    },
    {
      "epoch": 18.43590930124942,
      "grad_norm": 0.0021280415821820498,
      "learning_rate": 0.06312818139750116,
      "loss": 0.0001,
      "step": 39840
    },
    {
      "epoch": 18.440536788523833,
      "grad_norm": 0.024400262162089348,
      "learning_rate": 0.06311892642295235,
      "loss": 0.0001,
      "step": 39850
    },
    {
      "epoch": 18.44516427579824,
      "grad_norm": 0.007010855711996555,
      "learning_rate": 0.06310967144840351,
      "loss": 0.0,
      "step": 39860
    },
    {
      "epoch": 18.449791763072653,
      "grad_norm": 0.02142307348549366,
      "learning_rate": 0.0631004164738547,
      "loss": 0.0003,
      "step": 39870
    },
    {
      "epoch": 18.454419250347062,
      "grad_norm": 0.0734417736530304,
      "learning_rate": 0.06309116149930588,
      "loss": 0.0003,
      "step": 39880
    },
    {
      "epoch": 18.45904673762147,
      "grad_norm": 0.01469370536506176,
      "learning_rate": 0.06308190652475706,
      "loss": 0.0018,
      "step": 39890
    },
    {
      "epoch": 18.463674224895883,
      "grad_norm": 0.006511863321065903,
      "learning_rate": 0.06307265155020823,
      "loss": 0.0008,
      "step": 39900
    },
    {
      "epoch": 18.46830171217029,
      "grad_norm": 0.008346149697899818,
      "learning_rate": 0.06306339657565942,
      "loss": 0.0001,
      "step": 39910
    },
    {
      "epoch": 18.472929199444703,
      "grad_norm": 0.003209154587239027,
      "learning_rate": 0.0630541416011106,
      "loss": 0.0002,
      "step": 39920
    },
    {
      "epoch": 18.47755668671911,
      "grad_norm": 0.0013972579035907984,
      "learning_rate": 0.06304488662656178,
      "loss": 0.0001,
      "step": 39930
    },
    {
      "epoch": 18.48218417399352,
      "grad_norm": 0.0002875459613278508,
      "learning_rate": 0.06303563165201297,
      "loss": 0.0,
      "step": 39940
    },
    {
      "epoch": 18.486811661267932,
      "grad_norm": 0.0952458381652832,
      "learning_rate": 0.06302637667746414,
      "loss": 0.0003,
      "step": 39950
    },
    {
      "epoch": 18.49143914854234,
      "grad_norm": 0.019144639372825623,
      "learning_rate": 0.06301712170291532,
      "loss": 0.0004,
      "step": 39960
    },
    {
      "epoch": 18.496066635816753,
      "grad_norm": 0.00191917410120368,
      "learning_rate": 0.0630078667283665,
      "loss": 0.0003,
      "step": 39970
    },
    {
      "epoch": 18.50069412309116,
      "grad_norm": 0.004634172655642033,
      "learning_rate": 0.06299861175381768,
      "loss": 0.0034,
      "step": 39980
    },
    {
      "epoch": 18.50532161036557,
      "grad_norm": 0.0028617018833756447,
      "learning_rate": 0.06298935677926885,
      "loss": 0.0001,
      "step": 39990
    },
    {
      "epoch": 18.509949097639982,
      "grad_norm": 0.009101918898522854,
      "learning_rate": 0.06298010180472004,
      "loss": 0.0006,
      "step": 40000
    },
    {
      "epoch": 18.51457658491439,
      "grad_norm": 0.010114218108355999,
      "learning_rate": 0.06297084683017122,
      "loss": 0.0005,
      "step": 40010
    },
    {
      "epoch": 18.519204072188803,
      "grad_norm": 0.0015782687114551663,
      "learning_rate": 0.0629615918556224,
      "loss": 0.0001,
      "step": 40020
    },
    {
      "epoch": 18.52383155946321,
      "grad_norm": 0.06881861388683319,
      "learning_rate": 0.06295233688107359,
      "loss": 0.0009,
      "step": 40030
    },
    {
      "epoch": 18.52845904673762,
      "grad_norm": 0.019327839836478233,
      "learning_rate": 0.06294308190652476,
      "loss": 0.0002,
      "step": 40040
    },
    {
      "epoch": 18.533086534012032,
      "grad_norm": 0.30665647983551025,
      "learning_rate": 0.06293382693197594,
      "loss": 0.0001,
      "step": 40050
    },
    {
      "epoch": 18.53771402128644,
      "grad_norm": 0.04867353290319443,
      "learning_rate": 0.06292457195742712,
      "loss": 0.0001,
      "step": 40060
    },
    {
      "epoch": 18.542341508560853,
      "grad_norm": 0.03326893225312233,
      "learning_rate": 0.0629153169828783,
      "loss": 0.0001,
      "step": 40070
    },
    {
      "epoch": 18.54696899583526,
      "grad_norm": 0.00039878132520243526,
      "learning_rate": 0.06290606200832947,
      "loss": 0.0004,
      "step": 40080
    },
    {
      "epoch": 18.551596483109673,
      "grad_norm": 0.005000660195946693,
      "learning_rate": 0.06289680703378066,
      "loss": 0.0012,
      "step": 40090
    },
    {
      "epoch": 18.55622397038408,
      "grad_norm": 0.0015182856004685163,
      "learning_rate": 0.06288755205923184,
      "loss": 0.0002,
      "step": 40100
    },
    {
      "epoch": 18.56085145765849,
      "grad_norm": 0.0020143985748291016,
      "learning_rate": 0.06287829708468302,
      "loss": 0.0001,
      "step": 40110
    },
    {
      "epoch": 18.565478944932902,
      "grad_norm": 0.11380023509263992,
      "learning_rate": 0.06286904211013421,
      "loss": 0.0001,
      "step": 40120
    },
    {
      "epoch": 18.57010643220731,
      "grad_norm": 0.0004502406227402389,
      "learning_rate": 0.06285978713558538,
      "loss": 0.0001,
      "step": 40130
    },
    {
      "epoch": 18.574733919481723,
      "grad_norm": 0.3113408386707306,
      "learning_rate": 0.06285053216103656,
      "loss": 0.0001,
      "step": 40140
    },
    {
      "epoch": 18.57936140675613,
      "grad_norm": 0.010957672260701656,
      "learning_rate": 0.06284127718648773,
      "loss": 0.0,
      "step": 40150
    },
    {
      "epoch": 18.58398889403054,
      "grad_norm": 0.004279208369553089,
      "learning_rate": 0.06283202221193893,
      "loss": 0.0,
      "step": 40160
    },
    {
      "epoch": 18.588616381304952,
      "grad_norm": 0.1659844070672989,
      "learning_rate": 0.0628227672373901,
      "loss": 0.0002,
      "step": 40170
    },
    {
      "epoch": 18.59324386857936,
      "grad_norm": 0.002350644441321492,
      "learning_rate": 0.06281351226284128,
      "loss": 0.0001,
      "step": 40180
    },
    {
      "epoch": 18.597871355853773,
      "grad_norm": 0.020879752933979034,
      "learning_rate": 0.06280425728829246,
      "loss": 0.0002,
      "step": 40190
    },
    {
      "epoch": 18.60249884312818,
      "grad_norm": 0.0011063809506595135,
      "learning_rate": 0.06279500231374364,
      "loss": 0.0001,
      "step": 40200
    },
    {
      "epoch": 18.60712633040259,
      "grad_norm": 4.054355144500732,
      "learning_rate": 0.06278574733919483,
      "loss": 0.0018,
      "step": 40210
    },
    {
      "epoch": 18.611753817677002,
      "grad_norm": 0.0008998316479846835,
      "learning_rate": 0.062776492364646,
      "loss": 0.0001,
      "step": 40220
    },
    {
      "epoch": 18.61638130495141,
      "grad_norm": 0.0023717929143458605,
      "learning_rate": 0.06276723739009718,
      "loss": 0.0002,
      "step": 40230
    },
    {
      "epoch": 18.621008792225823,
      "grad_norm": 0.0007934403838589787,
      "learning_rate": 0.06275798241554835,
      "loss": 0.0034,
      "step": 40240
    },
    {
      "epoch": 18.62563627950023,
      "grad_norm": 0.005078540649265051,
      "learning_rate": 0.06274872744099955,
      "loss": 0.0,
      "step": 40250
    },
    {
      "epoch": 18.63026376677464,
      "grad_norm": 0.07839742302894592,
      "learning_rate": 0.06273947246645072,
      "loss": 0.0001,
      "step": 40260
    },
    {
      "epoch": 18.63489125404905,
      "grad_norm": 0.06541937589645386,
      "learning_rate": 0.0627302174919019,
      "loss": 0.0002,
      "step": 40270
    },
    {
      "epoch": 18.63951874132346,
      "grad_norm": 0.022435743361711502,
      "learning_rate": 0.06272096251735308,
      "loss": 0.0004,
      "step": 40280
    },
    {
      "epoch": 18.644146228597872,
      "grad_norm": 0.01683305762708187,
      "learning_rate": 0.06271170754280427,
      "loss": 0.0001,
      "step": 40290
    },
    {
      "epoch": 18.64877371587228,
      "grad_norm": 0.07076150923967361,
      "learning_rate": 0.06270245256825545,
      "loss": 0.0001,
      "step": 40300
    },
    {
      "epoch": 18.653401203146693,
      "grad_norm": 0.029556913301348686,
      "learning_rate": 0.06269319759370662,
      "loss": 0.0002,
      "step": 40310
    },
    {
      "epoch": 18.6580286904211,
      "grad_norm": 0.0009857575641945004,
      "learning_rate": 0.0626839426191578,
      "loss": 0.0005,
      "step": 40320
    },
    {
      "epoch": 18.66265617769551,
      "grad_norm": 0.010016149841248989,
      "learning_rate": 0.06267468764460897,
      "loss": 0.0001,
      "step": 40330
    },
    {
      "epoch": 18.667283664969922,
      "grad_norm": 0.0034731763880699873,
      "learning_rate": 0.06266543267006017,
      "loss": 0.0001,
      "step": 40340
    },
    {
      "epoch": 18.67191115224433,
      "grad_norm": 0.05735769495368004,
      "learning_rate": 0.06265617769551134,
      "loss": 0.0,
      "step": 40350
    },
    {
      "epoch": 18.676538639518743,
      "grad_norm": 0.001962261274456978,
      "learning_rate": 0.06264692272096252,
      "loss": 0.0001,
      "step": 40360
    },
    {
      "epoch": 18.68116612679315,
      "grad_norm": 0.021199960261583328,
      "learning_rate": 0.0626376677464137,
      "loss": 0.0001,
      "step": 40370
    },
    {
      "epoch": 18.68579361406756,
      "grad_norm": 0.1471889317035675,
      "learning_rate": 0.06262841277186487,
      "loss": 0.0001,
      "step": 40380
    },
    {
      "epoch": 18.690421101341972,
      "grad_norm": 0.002277050167322159,
      "learning_rate": 0.06261915779731607,
      "loss": 0.0001,
      "step": 40390
    },
    {
      "epoch": 18.69504858861638,
      "grad_norm": 0.001161394058726728,
      "learning_rate": 0.06260990282276724,
      "loss": 0.0002,
      "step": 40400
    },
    {
      "epoch": 18.699676075890793,
      "grad_norm": 0.036497268825769424,
      "learning_rate": 0.06260064784821842,
      "loss": 0.0001,
      "step": 40410
    },
    {
      "epoch": 18.7043035631652,
      "grad_norm": 0.004233524668961763,
      "learning_rate": 0.06259139287366959,
      "loss": 0.0001,
      "step": 40420
    },
    {
      "epoch": 18.70893105043961,
      "grad_norm": 0.22840338945388794,
      "learning_rate": 0.06258213789912079,
      "loss": 0.0001,
      "step": 40430
    },
    {
      "epoch": 18.71355853771402,
      "grad_norm": 0.0814272090792656,
      "learning_rate": 0.06257288292457196,
      "loss": 0.0003,
      "step": 40440
    },
    {
      "epoch": 18.71818602498843,
      "grad_norm": 0.10193893313407898,
      "learning_rate": 0.06256362795002314,
      "loss": 0.0014,
      "step": 40450
    },
    {
      "epoch": 18.722813512262842,
      "grad_norm": 0.002105945721268654,
      "learning_rate": 0.06255437297547432,
      "loss": 0.0001,
      "step": 40460
    },
    {
      "epoch": 18.72744099953725,
      "grad_norm": 0.004581759683787823,
      "learning_rate": 0.06254511800092549,
      "loss": 0.0002,
      "step": 40470
    },
    {
      "epoch": 18.732068486811663,
      "grad_norm": 0.015510929748415947,
      "learning_rate": 0.06253586302637669,
      "loss": 0.0003,
      "step": 40480
    },
    {
      "epoch": 18.73669597408607,
      "grad_norm": 0.0013768940698355436,
      "learning_rate": 0.06252660805182786,
      "loss": 0.0002,
      "step": 40490
    },
    {
      "epoch": 18.74132346136048,
      "grad_norm": 0.005618193652480841,
      "learning_rate": 0.06251735307727904,
      "loss": 0.0002,
      "step": 40500
    },
    {
      "epoch": 18.745950948634892,
      "grad_norm": 0.021117139607667923,
      "learning_rate": 0.06250809810273021,
      "loss": 0.0001,
      "step": 40510
    },
    {
      "epoch": 18.7505784359093,
      "grad_norm": 0.9371466040611267,
      "learning_rate": 0.06249884312818141,
      "loss": 0.0002,
      "step": 40520
    },
    {
      "epoch": 18.755205923183713,
      "grad_norm": 0.13202042877674103,
      "learning_rate": 0.062489588153632585,
      "loss": 0.0003,
      "step": 40530
    },
    {
      "epoch": 18.75983341045812,
      "grad_norm": 0.03639044985175133,
      "learning_rate": 0.06248033317908376,
      "loss": 0.0005,
      "step": 40540
    },
    {
      "epoch": 18.76446089773253,
      "grad_norm": 0.0011018187506124377,
      "learning_rate": 0.06247107820453494,
      "loss": 0.0002,
      "step": 40550
    },
    {
      "epoch": 18.769088385006942,
      "grad_norm": 0.003563252044841647,
      "learning_rate": 0.06246182322998611,
      "loss": 0.0057,
      "step": 40560
    },
    {
      "epoch": 18.77371587228135,
      "grad_norm": 0.007454574108123779,
      "learning_rate": 0.0624525682554373,
      "loss": 0.0001,
      "step": 40570
    },
    {
      "epoch": 18.778343359555763,
      "grad_norm": 0.0629769116640091,
      "learning_rate": 0.06244331328088848,
      "loss": 0.0018,
      "step": 40580
    },
    {
      "epoch": 18.78297084683017,
      "grad_norm": 0.0014447900466620922,
      "learning_rate": 0.06243405830633966,
      "loss": 0.0,
      "step": 40590
    },
    {
      "epoch": 18.78759833410458,
      "grad_norm": 0.0016537479823455215,
      "learning_rate": 0.06242480333179084,
      "loss": 0.0001,
      "step": 40600
    },
    {
      "epoch": 18.79222582137899,
      "grad_norm": 0.032217055559158325,
      "learning_rate": 0.062415548357242015,
      "loss": 0.0002,
      "step": 40610
    },
    {
      "epoch": 18.7968533086534,
      "grad_norm": 0.3785495162010193,
      "learning_rate": 0.062406293382693205,
      "loss": 0.0003,
      "step": 40620
    },
    {
      "epoch": 18.801480795927812,
      "grad_norm": 0.06452839821577072,
      "learning_rate": 0.06239703840814438,
      "loss": 0.0004,
      "step": 40630
    },
    {
      "epoch": 18.80610828320222,
      "grad_norm": 0.004741476383060217,
      "learning_rate": 0.06238778343359556,
      "loss": 0.0002,
      "step": 40640
    },
    {
      "epoch": 18.810735770476633,
      "grad_norm": 0.42827895283699036,
      "learning_rate": 0.062378528459046734,
      "loss": 0.0003,
      "step": 40650
    },
    {
      "epoch": 18.81536325775104,
      "grad_norm": 0.01212015189230442,
      "learning_rate": 0.062369273484497924,
      "loss": 0.0001,
      "step": 40660
    },
    {
      "epoch": 18.81999074502545,
      "grad_norm": 0.4077351987361908,
      "learning_rate": 0.0623600185099491,
      "loss": 0.0003,
      "step": 40670
    },
    {
      "epoch": 18.824618232299862,
      "grad_norm": 0.004690124653279781,
      "learning_rate": 0.06235076353540028,
      "loss": 0.0002,
      "step": 40680
    },
    {
      "epoch": 18.82924571957427,
      "grad_norm": 0.03994182497262955,
      "learning_rate": 0.06234150856085146,
      "loss": 0.0001,
      "step": 40690
    },
    {
      "epoch": 18.833873206848683,
      "grad_norm": 0.07032237946987152,
      "learning_rate": 0.062332253586302636,
      "loss": 0.0001,
      "step": 40700
    },
    {
      "epoch": 18.83850069412309,
      "grad_norm": 0.02770855277776718,
      "learning_rate": 0.062322998611753826,
      "loss": 0.0002,
      "step": 40710
    },
    {
      "epoch": 18.8431281813975,
      "grad_norm": 0.020126715302467346,
      "learning_rate": 0.062313743637205,
      "loss": 0.0001,
      "step": 40720
    },
    {
      "epoch": 18.847755668671912,
      "grad_norm": 0.01083382684737444,
      "learning_rate": 0.06230448866265618,
      "loss": 0.0002,
      "step": 40730
    },
    {
      "epoch": 18.85238315594632,
      "grad_norm": 0.02771933190524578,
      "learning_rate": 0.062295233688107354,
      "loss": 0.0,
      "step": 40740
    },
    {
      "epoch": 18.857010643220733,
      "grad_norm": 0.018892228603363037,
      "learning_rate": 0.062285978713558544,
      "loss": 0.0012,
      "step": 40750
    },
    {
      "epoch": 18.86163813049514,
      "grad_norm": 0.9300011992454529,
      "learning_rate": 0.06227672373900972,
      "loss": 0.0002,
      "step": 40760
    },
    {
      "epoch": 18.86626561776955,
      "grad_norm": 0.001982391346246004,
      "learning_rate": 0.062267468764460904,
      "loss": 0.0003,
      "step": 40770
    },
    {
      "epoch": 18.87089310504396,
      "grad_norm": 0.0025750123895704746,
      "learning_rate": 0.06225821378991208,
      "loss": 0.0061,
      "step": 40780
    },
    {
      "epoch": 18.87552059231837,
      "grad_norm": 0.008510629646480083,
      "learning_rate": 0.062248958815363256,
      "loss": 0.0001,
      "step": 40790
    },
    {
      "epoch": 18.880148079592782,
      "grad_norm": 0.01682240702211857,
      "learning_rate": 0.062239703840814446,
      "loss": 0.0033,
      "step": 40800
    },
    {
      "epoch": 18.88477556686719,
      "grad_norm": 0.0043352688662707806,
      "learning_rate": 0.06223044886626562,
      "loss": 0.0001,
      "step": 40810
    },
    {
      "epoch": 18.889403054141603,
      "grad_norm": 0.002169763669371605,
      "learning_rate": 0.0622211938917168,
      "loss": 0.0001,
      "step": 40820
    },
    {
      "epoch": 18.89403054141601,
      "grad_norm": 0.0735139325261116,
      "learning_rate": 0.062211938917167975,
      "loss": 0.0001,
      "step": 40830
    },
    {
      "epoch": 18.89865802869042,
      "grad_norm": 0.0038686690386384726,
      "learning_rate": 0.06220268394261916,
      "loss": 0.0001,
      "step": 40840
    },
    {
      "epoch": 18.903285515964832,
      "grad_norm": 0.07596737146377563,
      "learning_rate": 0.06219342896807034,
      "loss": 0.0001,
      "step": 40850
    },
    {
      "epoch": 18.90791300323924,
      "grad_norm": 0.00745662534609437,
      "learning_rate": 0.062184173993521524,
      "loss": 0.0033,
      "step": 40860
    },
    {
      "epoch": 18.912540490513653,
      "grad_norm": 0.06697504967451096,
      "learning_rate": 0.0621749190189727,
      "loss": 0.0001,
      "step": 40870
    },
    {
      "epoch": 18.91716797778806,
      "grad_norm": 0.6403787136077881,
      "learning_rate": 0.06216566404442388,
      "loss": 0.0009,
      "step": 40880
    },
    {
      "epoch": 18.92179546506247,
      "grad_norm": 0.011853323318064213,
      "learning_rate": 0.06215640906987507,
      "loss": 0.0001,
      "step": 40890
    },
    {
      "epoch": 18.926422952336882,
      "grad_norm": 0.04002346470952034,
      "learning_rate": 0.06214715409532624,
      "loss": 0.0001,
      "step": 40900
    },
    {
      "epoch": 18.93105043961129,
      "grad_norm": 0.44156306982040405,
      "learning_rate": 0.06213789912077742,
      "loss": 0.0002,
      "step": 40910
    },
    {
      "epoch": 18.935677926885703,
      "grad_norm": 0.006044529844075441,
      "learning_rate": 0.062128644146228595,
      "loss": 0.0001,
      "step": 40920
    },
    {
      "epoch": 18.94030541416011,
      "grad_norm": 0.007799180224537849,
      "learning_rate": 0.06211938917167978,
      "loss": 0.0001,
      "step": 40930
    },
    {
      "epoch": 18.94493290143452,
      "grad_norm": 0.28142279386520386,
      "learning_rate": 0.06211013419713096,
      "loss": 0.0001,
      "step": 40940
    },
    {
      "epoch": 18.94956038870893,
      "grad_norm": 0.009629909880459309,
      "learning_rate": 0.062100879222582145,
      "loss": 0.0001,
      "step": 40950
    },
    {
      "epoch": 18.95418787598334,
      "grad_norm": 0.040160004049539566,
      "learning_rate": 0.06209162424803332,
      "loss": 0.0129,
      "step": 40960
    },
    {
      "epoch": 18.958815363257752,
      "grad_norm": 0.0643971785902977,
      "learning_rate": 0.0620823692734845,
      "loss": 0.0001,
      "step": 40970
    },
    {
      "epoch": 18.96344285053216,
      "grad_norm": 0.021709712222218513,
      "learning_rate": 0.06207311429893569,
      "loss": 0.0001,
      "step": 40980
    },
    {
      "epoch": 18.96807033780657,
      "grad_norm": 0.0008435823838226497,
      "learning_rate": 0.062063859324386864,
      "loss": 0.0004,
      "step": 40990
    },
    {
      "epoch": 18.97269782508098,
      "grad_norm": 0.012907417491078377,
      "learning_rate": 0.06205460434983804,
      "loss": 0.0005,
      "step": 41000
    },
    {
      "epoch": 18.97732531235539,
      "grad_norm": 0.232740581035614,
      "learning_rate": 0.062045349375289216,
      "loss": 0.0003,
      "step": 41010
    },
    {
      "epoch": 18.981952799629802,
      "grad_norm": 0.003297951305285096,
      "learning_rate": 0.0620360944007404,
      "loss": 0.0003,
      "step": 41020
    },
    {
      "epoch": 18.98658028690421,
      "grad_norm": 0.003336670808494091,
      "learning_rate": 0.06202683942619158,
      "loss": 0.0001,
      "step": 41030
    },
    {
      "epoch": 18.991207774178623,
      "grad_norm": 0.004571306519210339,
      "learning_rate": 0.062017584451642765,
      "loss": 0.0001,
      "step": 41040
    },
    {
      "epoch": 18.99583526145303,
      "grad_norm": 0.008453181944787502,
      "learning_rate": 0.06200832947709394,
      "loss": 0.0001,
      "step": 41050
    },
    {
      "epoch": 19.0,
      "eval_accuracy_branch1": 0.9901401941149284,
      "eval_accuracy_branch2": 0.5003081189339085,
      "eval_f1_branch1": 0.9911465491915792,
      "eval_f1_branch2": 0.49969894036382667,
      "eval_loss": 0.018489986658096313,
      "eval_precision_branch1": 0.9912630329676829,
      "eval_precision_branch2": 0.5003096269708132,
      "eval_recall_branch1": 0.991141712434003,
      "eval_recall_branch2": 0.5003081189339085,
      "eval_runtime": 28.8405,
      "eval_samples_per_second": 450.13,
      "eval_steps_per_second": 56.275,
      "step": 41059
    },
    {
      "epoch": 19.00046274872744,
      "grad_norm": 0.00838775560259819,
      "learning_rate": 0.06199907450254512,
      "loss": 0.0002,
      "step": 41060
    },
    {
      "epoch": 19.005090236001852,
      "grad_norm": 0.003538005519658327,
      "learning_rate": 0.061989819527996294,
      "loss": 0.0,
      "step": 41070
    },
    {
      "epoch": 19.00971772327626,
      "grad_norm": 0.0028033608105033636,
      "learning_rate": 0.061980564553447484,
      "loss": 0.0,
      "step": 41080
    },
    {
      "epoch": 19.014345210550673,
      "grad_norm": 0.0021016474347561598,
      "learning_rate": 0.06197130957889866,
      "loss": 0.0168,
      "step": 41090
    },
    {
      "epoch": 19.01897269782508,
      "grad_norm": 0.05685506761074066,
      "learning_rate": 0.06196205460434984,
      "loss": 0.0001,
      "step": 41100
    },
    {
      "epoch": 19.02360018509949,
      "grad_norm": 0.0010644325520843267,
      "learning_rate": 0.06195279962980102,
      "loss": 0.0002,
      "step": 41110
    },
    {
      "epoch": 19.0282276723739,
      "grad_norm": 0.008107373490929604,
      "learning_rate": 0.0619435446552522,
      "loss": 0.0005,
      "step": 41120
    },
    {
      "epoch": 19.03285515964831,
      "grad_norm": 1.960288405418396,
      "learning_rate": 0.061934289680703386,
      "loss": 0.0004,
      "step": 41130
    },
    {
      "epoch": 19.037482646922722,
      "grad_norm": 1.602971076965332,
      "learning_rate": 0.06192503470615456,
      "loss": 0.0004,
      "step": 41140
    },
    {
      "epoch": 19.04211013419713,
      "grad_norm": 4.98906946182251,
      "learning_rate": 0.06191577973160574,
      "loss": 0.0012,
      "step": 41150
    },
    {
      "epoch": 19.04673762147154,
      "grad_norm": 0.0029534981586039066,
      "learning_rate": 0.061906524757056915,
      "loss": 0.0004,
      "step": 41160
    },
    {
      "epoch": 19.05136510874595,
      "grad_norm": 0.02658557891845703,
      "learning_rate": 0.061897269782508105,
      "loss": 0.0003,
      "step": 41170
    },
    {
      "epoch": 19.05599259602036,
      "grad_norm": 0.0005442739930003881,
      "learning_rate": 0.06188801480795928,
      "loss": 0.0001,
      "step": 41180
    },
    {
      "epoch": 19.060620083294772,
      "grad_norm": 0.021079646423459053,
      "learning_rate": 0.06187875983341046,
      "loss": 0.0002,
      "step": 41190
    },
    {
      "epoch": 19.06524757056918,
      "grad_norm": 0.0021189884282648563,
      "learning_rate": 0.06186950485886164,
      "loss": 0.0075,
      "step": 41200
    },
    {
      "epoch": 19.06987505784359,
      "grad_norm": 0.0025681941770017147,
      "learning_rate": 0.06186024988431282,
      "loss": 0.0001,
      "step": 41210
    },
    {
      "epoch": 19.074502545118,
      "grad_norm": 0.00394689803943038,
      "learning_rate": 0.06185099490976401,
      "loss": 0.0001,
      "step": 41220
    },
    {
      "epoch": 19.07913003239241,
      "grad_norm": 0.0009019936551339924,
      "learning_rate": 0.06184173993521518,
      "loss": 0.0059,
      "step": 41230
    },
    {
      "epoch": 19.083757519666822,
      "grad_norm": 0.0588318295776844,
      "learning_rate": 0.06183248496066636,
      "loss": 0.001,
      "step": 41240
    },
    {
      "epoch": 19.08838500694123,
      "grad_norm": 0.0013645669678226113,
      "learning_rate": 0.061823229986117535,
      "loss": 0.0002,
      "step": 41250
    },
    {
      "epoch": 19.093012494215643,
      "grad_norm": 0.0031547651160508394,
      "learning_rate": 0.061813975011568725,
      "loss": 0.0002,
      "step": 41260
    },
    {
      "epoch": 19.09763998149005,
      "grad_norm": 0.0048178560100495815,
      "learning_rate": 0.0618047200370199,
      "loss": 0.0002,
      "step": 41270
    },
    {
      "epoch": 19.10226746876446,
      "grad_norm": 0.016711758449673653,
      "learning_rate": 0.06179546506247108,
      "loss": 0.0001,
      "step": 41280
    },
    {
      "epoch": 19.10689495603887,
      "grad_norm": 0.03487836569547653,
      "learning_rate": 0.06178621008792226,
      "loss": 0.0021,
      "step": 41290
    },
    {
      "epoch": 19.11152244331328,
      "grad_norm": 0.012261067517101765,
      "learning_rate": 0.06177695511337344,
      "loss": 0.0003,
      "step": 41300
    },
    {
      "epoch": 19.116149930587692,
      "grad_norm": 0.0492485836148262,
      "learning_rate": 0.06176770013882463,
      "loss": 0.0001,
      "step": 41310
    },
    {
      "epoch": 19.1207774178621,
      "grad_norm": 0.020581254735589027,
      "learning_rate": 0.0617584451642758,
      "loss": 0.0001,
      "step": 41320
    },
    {
      "epoch": 19.12540490513651,
      "grad_norm": 0.0014243931509554386,
      "learning_rate": 0.06174919018972698,
      "loss": 0.0001,
      "step": 41330
    },
    {
      "epoch": 19.13003239241092,
      "grad_norm": 3.304273843765259,
      "learning_rate": 0.061739935215178156,
      "loss": 0.0008,
      "step": 41340
    },
    {
      "epoch": 19.13465987968533,
      "grad_norm": 0.0014177034609019756,
      "learning_rate": 0.061730680240629346,
      "loss": 0.0001,
      "step": 41350
    },
    {
      "epoch": 19.139287366959742,
      "grad_norm": 0.0009441489819437265,
      "learning_rate": 0.06172142526608052,
      "loss": 0.0009,
      "step": 41360
    },
    {
      "epoch": 19.14391485423415,
      "grad_norm": 0.21259747445583344,
      "learning_rate": 0.0617121702915317,
      "loss": 0.0001,
      "step": 41370
    },
    {
      "epoch": 19.14854234150856,
      "grad_norm": 0.04049970954656601,
      "learning_rate": 0.06170291531698288,
      "loss": 0.0003,
      "step": 41380
    },
    {
      "epoch": 19.15316982878297,
      "grad_norm": 0.0004535418702289462,
      "learning_rate": 0.06169366034243406,
      "loss": 0.0002,
      "step": 41390
    },
    {
      "epoch": 19.15779731605738,
      "grad_norm": 0.033391911536455154,
      "learning_rate": 0.06168440536788525,
      "loss": 0.0001,
      "step": 41400
    },
    {
      "epoch": 19.162424803331792,
      "grad_norm": 0.02384793572127819,
      "learning_rate": 0.061675150393336424,
      "loss": 0.0002,
      "step": 41410
    },
    {
      "epoch": 19.1670522906062,
      "grad_norm": 0.0016452758572995663,
      "learning_rate": 0.0616658954187876,
      "loss": 0.0,
      "step": 41420
    },
    {
      "epoch": 19.171679777880613,
      "grad_norm": 0.003776548197492957,
      "learning_rate": 0.061656640444238776,
      "loss": 0.0,
      "step": 41430
    },
    {
      "epoch": 19.17630726515502,
      "grad_norm": 0.001095480634830892,
      "learning_rate": 0.061647385469689966,
      "loss": 0.0003,
      "step": 41440
    },
    {
      "epoch": 19.18093475242943,
      "grad_norm": 0.004248552490025759,
      "learning_rate": 0.06163813049514114,
      "loss": 0.0001,
      "step": 41450
    },
    {
      "epoch": 19.18556223970384,
      "grad_norm": 0.0026696512941271067,
      "learning_rate": 0.06162887552059232,
      "loss": 0.0002,
      "step": 41460
    },
    {
      "epoch": 19.19018972697825,
      "grad_norm": 0.002367474837228656,
      "learning_rate": 0.0616196205460435,
      "loss": 0.0001,
      "step": 41470
    },
    {
      "epoch": 19.194817214252662,
      "grad_norm": 0.0039048618637025356,
      "learning_rate": 0.06161036557149468,
      "loss": 0.0009,
      "step": 41480
    },
    {
      "epoch": 19.19944470152707,
      "grad_norm": 0.011759940534830093,
      "learning_rate": 0.06160111059694587,
      "loss": 0.0012,
      "step": 41490
    },
    {
      "epoch": 19.20407218880148,
      "grad_norm": 0.0021897919941693544,
      "learning_rate": 0.061591855622397045,
      "loss": 0.0001,
      "step": 41500
    },
    {
      "epoch": 19.20869967607589,
      "grad_norm": 0.0029754952993243933,
      "learning_rate": 0.06158260064784822,
      "loss": 0.0001,
      "step": 41510
    },
    {
      "epoch": 19.2133271633503,
      "grad_norm": 0.00534049840644002,
      "learning_rate": 0.0615733456732994,
      "loss": 0.0035,
      "step": 41520
    },
    {
      "epoch": 19.217954650624712,
      "grad_norm": 0.00598937226459384,
      "learning_rate": 0.06156409069875057,
      "loss": 0.0,
      "step": 41530
    },
    {
      "epoch": 19.22258213789912,
      "grad_norm": 0.016591330990195274,
      "learning_rate": 0.06155483572420176,
      "loss": 0.0002,
      "step": 41540
    },
    {
      "epoch": 19.22720962517353,
      "grad_norm": 0.008344346657395363,
      "learning_rate": 0.06154558074965294,
      "loss": 0.0001,
      "step": 41550
    },
    {
      "epoch": 19.23183711244794,
      "grad_norm": 0.03284486010670662,
      "learning_rate": 0.06153632577510412,
      "loss": 0.0002,
      "step": 41560
    },
    {
      "epoch": 19.23646459972235,
      "grad_norm": 0.011094007641077042,
      "learning_rate": 0.0615270708005553,
      "loss": 0.0082,
      "step": 41570
    },
    {
      "epoch": 19.241092086996762,
      "grad_norm": 0.005057988688349724,
      "learning_rate": 0.06151781582600649,
      "loss": 0.0003,
      "step": 41580
    },
    {
      "epoch": 19.24571957427117,
      "grad_norm": 0.02112800069153309,
      "learning_rate": 0.061508560851457665,
      "loss": 0.0001,
      "step": 41590
    },
    {
      "epoch": 19.250347061545583,
      "grad_norm": 0.0005263763596303761,
      "learning_rate": 0.06149930587690884,
      "loss": 0.0,
      "step": 41600
    },
    {
      "epoch": 19.25497454881999,
      "grad_norm": 0.01089677307754755,
      "learning_rate": 0.06149005090236002,
      "loss": 0.0005,
      "step": 41610
    },
    {
      "epoch": 19.2596020360944,
      "grad_norm": 0.11862517148256302,
      "learning_rate": 0.061480795927811194,
      "loss": 0.0003,
      "step": 41620
    },
    {
      "epoch": 19.26422952336881,
      "grad_norm": 0.008540785871446133,
      "learning_rate": 0.061471540953262384,
      "loss": 0.0005,
      "step": 41630
    },
    {
      "epoch": 19.26885701064322,
      "grad_norm": 0.010689981281757355,
      "learning_rate": 0.06146228597871356,
      "loss": 0.0009,
      "step": 41640
    },
    {
      "epoch": 19.273484497917632,
      "grad_norm": 0.0003830877540167421,
      "learning_rate": 0.06145303100416474,
      "loss": 0.0001,
      "step": 41650
    },
    {
      "epoch": 19.27811198519204,
      "grad_norm": 0.0005170099320821464,
      "learning_rate": 0.06144377602961592,
      "loss": 0.0004,
      "step": 41660
    },
    {
      "epoch": 19.28273947246645,
      "grad_norm": 0.006156104616820812,
      "learning_rate": 0.06143452105506711,
      "loss": 0.0001,
      "step": 41670
    },
    {
      "epoch": 19.28736695974086,
      "grad_norm": 0.051124367862939835,
      "learning_rate": 0.061425266080518286,
      "loss": 0.0009,
      "step": 41680
    },
    {
      "epoch": 19.29199444701527,
      "grad_norm": 0.007922343909740448,
      "learning_rate": 0.06141601110596946,
      "loss": 0.0004,
      "step": 41690
    },
    {
      "epoch": 19.296621934289682,
      "grad_norm": 0.01436562929302454,
      "learning_rate": 0.06140675613142064,
      "loss": 0.0002,
      "step": 41700
    },
    {
      "epoch": 19.30124942156409,
      "grad_norm": 0.010478210635483265,
      "learning_rate": 0.061397501156871814,
      "loss": 0.0,
      "step": 41710
    },
    {
      "epoch": 19.3058769088385,
      "grad_norm": 0.6686068773269653,
      "learning_rate": 0.061388246182323004,
      "loss": 0.0004,
      "step": 41720
    },
    {
      "epoch": 19.31050439611291,
      "grad_norm": 0.014248511753976345,
      "learning_rate": 0.06137899120777418,
      "loss": 0.0001,
      "step": 41730
    },
    {
      "epoch": 19.31513188338732,
      "grad_norm": 0.6951643824577332,
      "learning_rate": 0.061369736233225364,
      "loss": 0.0003,
      "step": 41740
    },
    {
      "epoch": 19.319759370661732,
      "grad_norm": 0.010968445800244808,
      "learning_rate": 0.06136048125867654,
      "loss": 0.0,
      "step": 41750
    },
    {
      "epoch": 19.32438685793614,
      "grad_norm": 0.02857598103582859,
      "learning_rate": 0.061351226284127716,
      "loss": 0.0002,
      "step": 41760
    },
    {
      "epoch": 19.32901434521055,
      "grad_norm": 0.0008815945475362241,
      "learning_rate": 0.061341971309578906,
      "loss": 0.0002,
      "step": 41770
    },
    {
      "epoch": 19.33364183248496,
      "grad_norm": 0.0009624829399399459,
      "learning_rate": 0.06133271633503008,
      "loss": 0.0002,
      "step": 41780
    },
    {
      "epoch": 19.33826931975937,
      "grad_norm": 0.012428408488631248,
      "learning_rate": 0.06132346136048126,
      "loss": 0.0009,
      "step": 41790
    },
    {
      "epoch": 19.34289680703378,
      "grad_norm": 0.018884917721152306,
      "learning_rate": 0.061314206385932435,
      "loss": 0.0001,
      "step": 41800
    },
    {
      "epoch": 19.34752429430819,
      "grad_norm": 0.015856293961405754,
      "learning_rate": 0.061304951411383625,
      "loss": 0.0009,
      "step": 41810
    },
    {
      "epoch": 19.352151781582602,
      "grad_norm": 0.0013068434782326221,
      "learning_rate": 0.0612956964368348,
      "loss": 0.0001,
      "step": 41820
    },
    {
      "epoch": 19.35677926885701,
      "grad_norm": 0.0001610359613550827,
      "learning_rate": 0.061286441462285984,
      "loss": 0.0002,
      "step": 41830
    },
    {
      "epoch": 19.36140675613142,
      "grad_norm": 0.003096874337643385,
      "learning_rate": 0.06127718648773716,
      "loss": 0.0019,
      "step": 41840
    },
    {
      "epoch": 19.36603424340583,
      "grad_norm": 0.005441875196993351,
      "learning_rate": 0.06126793151318834,
      "loss": 0.0001,
      "step": 41850
    },
    {
      "epoch": 19.37066173068024,
      "grad_norm": 0.01036166399717331,
      "learning_rate": 0.06125867653863953,
      "loss": 0.0001,
      "step": 41860
    },
    {
      "epoch": 19.375289217954652,
      "grad_norm": 0.03149789199233055,
      "learning_rate": 0.0612494215640907,
      "loss": 0.0001,
      "step": 41870
    },
    {
      "epoch": 19.37991670522906,
      "grad_norm": 0.008537924848496914,
      "learning_rate": 0.06124016658954188,
      "loss": 0.0004,
      "step": 41880
    },
    {
      "epoch": 19.38454419250347,
      "grad_norm": 0.004211473278701305,
      "learning_rate": 0.061230911614993055,
      "loss": 0.0008,
      "step": 41890
    },
    {
      "epoch": 19.38917167977788,
      "grad_norm": 0.03253772482275963,
      "learning_rate": 0.061221656640444246,
      "loss": 0.0002,
      "step": 41900
    },
    {
      "epoch": 19.39379916705229,
      "grad_norm": 0.0035801148042082787,
      "learning_rate": 0.06121240166589542,
      "loss": 0.0014,
      "step": 41910
    },
    {
      "epoch": 19.398426654326702,
      "grad_norm": 0.16866415739059448,
      "learning_rate": 0.061203146691346605,
      "loss": 0.0011,
      "step": 41920
    },
    {
      "epoch": 19.40305414160111,
      "grad_norm": 0.009186872281134129,
      "learning_rate": 0.06119389171679778,
      "loss": 0.0002,
      "step": 41930
    },
    {
      "epoch": 19.40768162887552,
      "grad_norm": 0.0109178451821208,
      "learning_rate": 0.06118463674224896,
      "loss": 0.0002,
      "step": 41940
    },
    {
      "epoch": 19.41230911614993,
      "grad_norm": 0.04174131527543068,
      "learning_rate": 0.06117538176770015,
      "loss": 0.0004,
      "step": 41950
    },
    {
      "epoch": 19.41693660342434,
      "grad_norm": 0.004127207212150097,
      "learning_rate": 0.061166126793151324,
      "loss": 0.0003,
      "step": 41960
    },
    {
      "epoch": 19.42156409069875,
      "grad_norm": 0.02944635972380638,
      "learning_rate": 0.0611568718186025,
      "loss": 0.0001,
      "step": 41970
    },
    {
      "epoch": 19.42619157797316,
      "grad_norm": 0.018345756456255913,
      "learning_rate": 0.061147616844053676,
      "loss": 0.0002,
      "step": 41980
    },
    {
      "epoch": 19.430819065247572,
      "grad_norm": 0.0012489092769101262,
      "learning_rate": 0.06113836186950486,
      "loss": 0.001,
      "step": 41990
    },
    {
      "epoch": 19.43544655252198,
      "grad_norm": 0.005109074991196394,
      "learning_rate": 0.06112910689495604,
      "loss": 0.0006,
      "step": 42000
    },
    {
      "epoch": 19.44007403979639,
      "grad_norm": 0.02468307688832283,
      "learning_rate": 0.061119851920407225,
      "loss": 0.0003,
      "step": 42010
    },
    {
      "epoch": 19.4447015270708,
      "grad_norm": 0.06312044709920883,
      "learning_rate": 0.0611105969458584,
      "loss": 0.0002,
      "step": 42020
    },
    {
      "epoch": 19.44932901434521,
      "grad_norm": 0.00828662607818842,
      "learning_rate": 0.06110134197130958,
      "loss": 0.0001,
      "step": 42030
    },
    {
      "epoch": 19.453956501619622,
      "grad_norm": 0.003456107107922435,
      "learning_rate": 0.06109208699676077,
      "loss": 0.0,
      "step": 42040
    },
    {
      "epoch": 19.45858398889403,
      "grad_norm": 0.0012712323805317283,
      "learning_rate": 0.061082832022211944,
      "loss": 0.0001,
      "step": 42050
    },
    {
      "epoch": 19.46321147616844,
      "grad_norm": 0.002930160379037261,
      "learning_rate": 0.06107357704766312,
      "loss": 0.0001,
      "step": 42060
    },
    {
      "epoch": 19.46783896344285,
      "grad_norm": 0.0011628420324996114,
      "learning_rate": 0.0610643220731143,
      "loss": 0.0001,
      "step": 42070
    },
    {
      "epoch": 19.47246645071726,
      "grad_norm": 0.0015417439863085747,
      "learning_rate": 0.06105506709856548,
      "loss": 0.0003,
      "step": 42080
    },
    {
      "epoch": 19.477093937991672,
      "grad_norm": 0.004091726150363684,
      "learning_rate": 0.06104581212401666,
      "loss": 0.0001,
      "step": 42090
    },
    {
      "epoch": 19.48172142526608,
      "grad_norm": 0.07009515911340714,
      "learning_rate": 0.061036557149467846,
      "loss": 0.0016,
      "step": 42100
    },
    {
      "epoch": 19.48634891254049,
      "grad_norm": 0.01285499520599842,
      "learning_rate": 0.06102730217491902,
      "loss": 0.0001,
      "step": 42110
    },
    {
      "epoch": 19.4909763998149,
      "grad_norm": 4.543746471405029,
      "learning_rate": 0.0610180472003702,
      "loss": 0.0012,
      "step": 42120
    },
    {
      "epoch": 19.49560388708931,
      "grad_norm": 0.10064376890659332,
      "learning_rate": 0.06100879222582139,
      "loss": 0.0001,
      "step": 42130
    },
    {
      "epoch": 19.50023137436372,
      "grad_norm": 0.07348304241895676,
      "learning_rate": 0.060999537251272565,
      "loss": 0.0001,
      "step": 42140
    },
    {
      "epoch": 19.50485886163813,
      "grad_norm": 0.001926342025399208,
      "learning_rate": 0.06099028227672374,
      "loss": 0.0015,
      "step": 42150
    },
    {
      "epoch": 19.50948634891254,
      "grad_norm": 0.038069769740104675,
      "learning_rate": 0.06098102730217492,
      "loss": 0.0001,
      "step": 42160
    },
    {
      "epoch": 19.51411383618695,
      "grad_norm": 0.0013649951433762908,
      "learning_rate": 0.0609717723276261,
      "loss": 0.0001,
      "step": 42170
    },
    {
      "epoch": 19.51874132346136,
      "grad_norm": 0.004999186377972364,
      "learning_rate": 0.060962517353077283,
      "loss": 0.0001,
      "step": 42180
    },
    {
      "epoch": 19.52336881073577,
      "grad_norm": 0.0015731784515082836,
      "learning_rate": 0.06095326237852847,
      "loss": 0.0002,
      "step": 42190
    },
    {
      "epoch": 19.52799629801018,
      "grad_norm": 0.027962185442447662,
      "learning_rate": 0.06094400740397964,
      "loss": 0.0,
      "step": 42200
    },
    {
      "epoch": 19.532623785284592,
      "grad_norm": 0.008923574350774288,
      "learning_rate": 0.06093475242943082,
      "loss": 0.0002,
      "step": 42210
    },
    {
      "epoch": 19.537251272559,
      "grad_norm": 0.0012083182809874415,
      "learning_rate": 0.060925497454881995,
      "loss": 0.0002,
      "step": 42220
    },
    {
      "epoch": 19.54187875983341,
      "grad_norm": 0.00027459365082904696,
      "learning_rate": 0.060916242480333185,
      "loss": 0.0,
      "step": 42230
    },
    {
      "epoch": 19.54650624710782,
      "grad_norm": 0.025077899917960167,
      "learning_rate": 0.06090698750578436,
      "loss": 0.0006,
      "step": 42240
    },
    {
      "epoch": 19.55113373438223,
      "grad_norm": 0.06736872345209122,
      "learning_rate": 0.06089773253123554,
      "loss": 0.0001,
      "step": 42250
    },
    {
      "epoch": 19.555761221656642,
      "grad_norm": 0.6545383334159851,
      "learning_rate": 0.06088847755668672,
      "loss": 0.0003,
      "step": 42260
    },
    {
      "epoch": 19.56038870893105,
      "grad_norm": 0.008292147889733315,
      "learning_rate": 0.060879222582137904,
      "loss": 0.0003,
      "step": 42270
    },
    {
      "epoch": 19.56501619620546,
      "grad_norm": 0.0012444063322618604,
      "learning_rate": 0.06086996760758909,
      "loss": 0.0003,
      "step": 42280
    },
    {
      "epoch": 19.56964368347987,
      "grad_norm": 0.0007306794868782163,
      "learning_rate": 0.06086071263304026,
      "loss": 0.0004,
      "step": 42290
    },
    {
      "epoch": 19.57427117075428,
      "grad_norm": 0.01705164648592472,
      "learning_rate": 0.06085145765849144,
      "loss": 0.0001,
      "step": 42300
    },
    {
      "epoch": 19.57889865802869,
      "grad_norm": 0.004781123250722885,
      "learning_rate": 0.060842202683942616,
      "loss": 0.0001,
      "step": 42310
    },
    {
      "epoch": 19.5835261453031,
      "grad_norm": 0.014005963690578938,
      "learning_rate": 0.060832947709393806,
      "loss": 0.0002,
      "step": 42320
    },
    {
      "epoch": 19.58815363257751,
      "grad_norm": 0.0025212487671524286,
      "learning_rate": 0.06082369273484498,
      "loss": 0.0,
      "step": 42330
    },
    {
      "epoch": 19.59278111985192,
      "grad_norm": 0.01138467900454998,
      "learning_rate": 0.06081443776029616,
      "loss": 0.0001,
      "step": 42340
    },
    {
      "epoch": 19.59740860712633,
      "grad_norm": 0.0075925118289887905,
      "learning_rate": 0.06080518278574734,
      "loss": 0.0003,
      "step": 42350
    },
    {
      "epoch": 19.60203609440074,
      "grad_norm": 0.0014596321852877736,
      "learning_rate": 0.060795927811198525,
      "loss": 0.0001,
      "step": 42360
    },
    {
      "epoch": 19.60666358167515,
      "grad_norm": 0.008561179041862488,
      "learning_rate": 0.06078667283664971,
      "loss": 0.0,
      "step": 42370
    },
    {
      "epoch": 19.611291068949562,
      "grad_norm": 0.0053877197206020355,
      "learning_rate": 0.060777417862100884,
      "loss": 0.0,
      "step": 42380
    },
    {
      "epoch": 19.61591855622397,
      "grad_norm": 0.0016279248520731926,
      "learning_rate": 0.06076816288755206,
      "loss": 0.0011,
      "step": 42390
    },
    {
      "epoch": 19.62054604349838,
      "grad_norm": 0.001794185722246766,
      "learning_rate": 0.060758907913003236,
      "loss": 0.0008,
      "step": 42400
    },
    {
      "epoch": 19.62517353077279,
      "grad_norm": 0.010499842464923859,
      "learning_rate": 0.060749652938454426,
      "loss": 0.0002,
      "step": 42410
    },
    {
      "epoch": 19.6298010180472,
      "grad_norm": 0.0021189621184021235,
      "learning_rate": 0.0607403979639056,
      "loss": 0.0,
      "step": 42420
    },
    {
      "epoch": 19.634428505321612,
      "grad_norm": 0.968410074710846,
      "learning_rate": 0.06073114298935678,
      "loss": 0.0003,
      "step": 42430
    },
    {
      "epoch": 19.63905599259602,
      "grad_norm": 0.017277052626013756,
      "learning_rate": 0.06072188801480796,
      "loss": 0.0002,
      "step": 42440
    },
    {
      "epoch": 19.64368347987043,
      "grad_norm": 0.021651268005371094,
      "learning_rate": 0.06071263304025914,
      "loss": 0.0001,
      "step": 42450
    },
    {
      "epoch": 19.64831096714484,
      "grad_norm": 0.0018282188102602959,
      "learning_rate": 0.06070337806571033,
      "loss": 0.0001,
      "step": 42460
    },
    {
      "epoch": 19.65293845441925,
      "grad_norm": 0.0013176077045500278,
      "learning_rate": 0.060694123091161505,
      "loss": 0.0,
      "step": 42470
    },
    {
      "epoch": 19.65756594169366,
      "grad_norm": 0.016169531270861626,
      "learning_rate": 0.06068486811661268,
      "loss": 0.0001,
      "step": 42480
    },
    {
      "epoch": 19.66219342896807,
      "grad_norm": 0.17150920629501343,
      "learning_rate": 0.06067561314206386,
      "loss": 0.0011,
      "step": 42490
    },
    {
      "epoch": 19.66682091624248,
      "grad_norm": 0.009166927076876163,
      "learning_rate": 0.06066635816751505,
      "loss": 0.0001,
      "step": 42500
    },
    {
      "epoch": 19.67144840351689,
      "grad_norm": 0.0010009093675762415,
      "learning_rate": 0.06065710319296622,
      "loss": 0.0001,
      "step": 42510
    },
    {
      "epoch": 19.6760758907913,
      "grad_norm": 0.007215184159576893,
      "learning_rate": 0.0606478482184174,
      "loss": 0.0008,
      "step": 42520
    },
    {
      "epoch": 19.68070337806571,
      "grad_norm": 0.026081396266818047,
      "learning_rate": 0.06063859324386858,
      "loss": 0.0001,
      "step": 42530
    },
    {
      "epoch": 19.68533086534012,
      "grad_norm": 0.04736872389912605,
      "learning_rate": 0.06062933826931976,
      "loss": 0.0001,
      "step": 42540
    },
    {
      "epoch": 19.689958352614532,
      "grad_norm": 0.001925513963215053,
      "learning_rate": 0.06062008329477095,
      "loss": 0.0001,
      "step": 42550
    },
    {
      "epoch": 19.69458583988894,
      "grad_norm": 0.03983594849705696,
      "learning_rate": 0.060610828320222125,
      "loss": 0.0018,
      "step": 42560
    },
    {
      "epoch": 19.69921332716335,
      "grad_norm": 1.2009776830673218,
      "learning_rate": 0.0606015733456733,
      "loss": 0.0006,
      "step": 42570
    },
    {
      "epoch": 19.70384081443776,
      "grad_norm": 0.0030839431565254927,
      "learning_rate": 0.06059231837112448,
      "loss": 0.0002,
      "step": 42580
    },
    {
      "epoch": 19.70846830171217,
      "grad_norm": 0.002747531747445464,
      "learning_rate": 0.06058306339657567,
      "loss": 0.0,
      "step": 42590
    },
    {
      "epoch": 19.713095788986582,
      "grad_norm": 0.1269259750843048,
      "learning_rate": 0.060573808422026844,
      "loss": 0.0001,
      "step": 42600
    },
    {
      "epoch": 19.71772327626099,
      "grad_norm": 0.03559666499495506,
      "learning_rate": 0.06056455344747802,
      "loss": 0.0002,
      "step": 42610
    },
    {
      "epoch": 19.7223507635354,
      "grad_norm": 0.12932083010673523,
      "learning_rate": 0.0605552984729292,
      "loss": 0.0003,
      "step": 42620
    },
    {
      "epoch": 19.72697825080981,
      "grad_norm": 0.0035242149606347084,
      "learning_rate": 0.06054604349838038,
      "loss": 0.0,
      "step": 42630
    },
    {
      "epoch": 19.73160573808422,
      "grad_norm": 0.0046415324322879314,
      "learning_rate": 0.06053678852383157,
      "loss": 0.0004,
      "step": 42640
    },
    {
      "epoch": 19.73623322535863,
      "grad_norm": 0.009088232181966305,
      "learning_rate": 0.060527533549282746,
      "loss": 0.0001,
      "step": 42650
    },
    {
      "epoch": 19.74086071263304,
      "grad_norm": 0.013122920878231525,
      "learning_rate": 0.06051827857473392,
      "loss": 0.0002,
      "step": 42660
    },
    {
      "epoch": 19.74548819990745,
      "grad_norm": 0.1732671856880188,
      "learning_rate": 0.0605090236001851,
      "loss": 0.0001,
      "step": 42670
    },
    {
      "epoch": 19.75011568718186,
      "grad_norm": 0.009393657557666302,
      "learning_rate": 0.060499768625636274,
      "loss": 0.0001,
      "step": 42680
    },
    {
      "epoch": 19.75474317445627,
      "grad_norm": 0.08988019824028015,
      "learning_rate": 0.060490513651087464,
      "loss": 0.0001,
      "step": 42690
    },
    {
      "epoch": 19.75937066173068,
      "grad_norm": 0.1559758335351944,
      "learning_rate": 0.06048125867653864,
      "loss": 0.0001,
      "step": 42700
    },
    {
      "epoch": 19.76399814900509,
      "grad_norm": 0.037750933319330215,
      "learning_rate": 0.060472003701989824,
      "loss": 0.0016,
      "step": 42710
    },
    {
      "epoch": 19.7686256362795,
      "grad_norm": 0.02199961058795452,
      "learning_rate": 0.060462748727441,
      "loss": 0.0002,
      "step": 42720
    },
    {
      "epoch": 19.77325312355391,
      "grad_norm": 0.0026795421727001667,
      "learning_rate": 0.06045349375289219,
      "loss": 0.0001,
      "step": 42730
    },
    {
      "epoch": 19.77788061082832,
      "grad_norm": 0.0036916753742843866,
      "learning_rate": 0.060444238778343366,
      "loss": 0.0002,
      "step": 42740
    },
    {
      "epoch": 19.78250809810273,
      "grad_norm": 0.0019250976620242,
      "learning_rate": 0.06043498380379454,
      "loss": 0.0008,
      "step": 42750
    },
    {
      "epoch": 19.78713558537714,
      "grad_norm": 0.01353000570088625,
      "learning_rate": 0.06042572882924572,
      "loss": 0.0008,
      "step": 42760
    },
    {
      "epoch": 19.791763072651552,
      "grad_norm": 0.0014139869017526507,
      "learning_rate": 0.060416473854696895,
      "loss": 0.002,
      "step": 42770
    },
    {
      "epoch": 19.79639055992596,
      "grad_norm": 0.06661536544561386,
      "learning_rate": 0.060407218880148085,
      "loss": 0.0001,
      "step": 42780
    },
    {
      "epoch": 19.80101804720037,
      "grad_norm": 0.005554158706218004,
      "learning_rate": 0.06039796390559926,
      "loss": 0.0002,
      "step": 42790
    },
    {
      "epoch": 19.80564553447478,
      "grad_norm": 0.018795836716890335,
      "learning_rate": 0.060388708931050444,
      "loss": 0.0001,
      "step": 42800
    },
    {
      "epoch": 19.81027302174919,
      "grad_norm": 0.0035637510009109974,
      "learning_rate": 0.06037945395650162,
      "loss": 0.0001,
      "step": 42810
    },
    {
      "epoch": 19.8149005090236,
      "grad_norm": 0.2664582133293152,
      "learning_rate": 0.06037019898195281,
      "loss": 0.0002,
      "step": 42820
    },
    {
      "epoch": 19.81952799629801,
      "grad_norm": 0.6970067024230957,
      "learning_rate": 0.06036094400740399,
      "loss": 0.0019,
      "step": 42830
    },
    {
      "epoch": 19.82415548357242,
      "grad_norm": 0.003990111872553825,
      "learning_rate": 0.06035168903285516,
      "loss": 0.0016,
      "step": 42840
    },
    {
      "epoch": 19.82878297084683,
      "grad_norm": 0.018096184358000755,
      "learning_rate": 0.06034243405830634,
      "loss": 0.0,
      "step": 42850
    },
    {
      "epoch": 19.83341045812124,
      "grad_norm": 0.07810617238283157,
      "learning_rate": 0.060333179083757515,
      "loss": 0.0001,
      "step": 42860
    },
    {
      "epoch": 19.83803794539565,
      "grad_norm": 0.0019149412401020527,
      "learning_rate": 0.060323924109208706,
      "loss": 0.0003,
      "step": 42870
    },
    {
      "epoch": 19.84266543267006,
      "grad_norm": 0.0015063497703522444,
      "learning_rate": 0.06031466913465988,
      "loss": 0.0001,
      "step": 42880
    },
    {
      "epoch": 19.84729291994447,
      "grad_norm": 0.006924123037606478,
      "learning_rate": 0.060305414160111065,
      "loss": 0.0001,
      "step": 42890
    },
    {
      "epoch": 19.85192040721888,
      "grad_norm": 0.7027593851089478,
      "learning_rate": 0.06029615918556224,
      "loss": 0.0002,
      "step": 42900
    },
    {
      "epoch": 19.85654789449329,
      "grad_norm": 0.004377615638077259,
      "learning_rate": 0.06028690421101342,
      "loss": 0.0003,
      "step": 42910
    },
    {
      "epoch": 19.8611753817677,
      "grad_norm": 0.04220206290483475,
      "learning_rate": 0.06027764923646461,
      "loss": 0.0004,
      "step": 42920
    },
    {
      "epoch": 19.86580286904211,
      "grad_norm": 0.0006684657419100404,
      "learning_rate": 0.060268394261915784,
      "loss": 0.0024,
      "step": 42930
    },
    {
      "epoch": 19.87043035631652,
      "grad_norm": 0.0013185902498662472,
      "learning_rate": 0.06025913928736696,
      "loss": 0.0,
      "step": 42940
    },
    {
      "epoch": 19.87505784359093,
      "grad_norm": 0.002116773510351777,
      "learning_rate": 0.060249884312818136,
      "loss": 0.0001,
      "step": 42950
    },
    {
      "epoch": 19.87968533086534,
      "grad_norm": 0.0016095100436359644,
      "learning_rate": 0.060240629338269326,
      "loss": 0.0001,
      "step": 42960
    },
    {
      "epoch": 19.88431281813975,
      "grad_norm": 0.0019364007748663425,
      "learning_rate": 0.0602313743637205,
      "loss": 0.0001,
      "step": 42970
    },
    {
      "epoch": 19.88894030541416,
      "grad_norm": 0.0005846807034686208,
      "learning_rate": 0.060222119389171685,
      "loss": 0.0002,
      "step": 42980
    },
    {
      "epoch": 19.89356779268857,
      "grad_norm": 0.00937865674495697,
      "learning_rate": 0.06021286441462286,
      "loss": 0.0006,
      "step": 42990
    },
    {
      "epoch": 19.89819527996298,
      "grad_norm": 0.008572353981435299,
      "learning_rate": 0.06020360944007404,
      "loss": 0.0001,
      "step": 43000
    },
    {
      "epoch": 19.90282276723739,
      "grad_norm": 0.010906169191002846,
      "learning_rate": 0.06019435446552523,
      "loss": 0.0002,
      "step": 43010
    },
    {
      "epoch": 19.9074502545118,
      "grad_norm": 0.28908205032348633,
      "learning_rate": 0.060185099490976404,
      "loss": 0.0001,
      "step": 43020
    },
    {
      "epoch": 19.91207774178621,
      "grad_norm": 0.006169020663946867,
      "learning_rate": 0.06017584451642758,
      "loss": 0.0001,
      "step": 43030
    },
    {
      "epoch": 19.91670522906062,
      "grad_norm": 0.000614701013546437,
      "learning_rate": 0.06016658954187876,
      "loss": 0.0002,
      "step": 43040
    },
    {
      "epoch": 19.92133271633503,
      "grad_norm": 0.10177399218082428,
      "learning_rate": 0.06015733456732995,
      "loss": 0.0002,
      "step": 43050
    },
    {
      "epoch": 19.92596020360944,
      "grad_norm": 0.003689859062433243,
      "learning_rate": 0.06014807959278112,
      "loss": 0.0,
      "step": 43060
    },
    {
      "epoch": 19.93058769088385,
      "grad_norm": 0.07061151415109634,
      "learning_rate": 0.060138824618232306,
      "loss": 0.0004,
      "step": 43070
    },
    {
      "epoch": 19.93521517815826,
      "grad_norm": 0.006769987288862467,
      "learning_rate": 0.06012956964368348,
      "loss": 0.0006,
      "step": 43080
    },
    {
      "epoch": 19.93984266543267,
      "grad_norm": 0.025949006900191307,
      "learning_rate": 0.06012031466913466,
      "loss": 0.0001,
      "step": 43090
    },
    {
      "epoch": 19.94447015270708,
      "grad_norm": 0.07643058896064758,
      "learning_rate": 0.06011105969458585,
      "loss": 0.0001,
      "step": 43100
    },
    {
      "epoch": 19.94909763998149,
      "grad_norm": 0.059664905071258545,
      "learning_rate": 0.060101804720037025,
      "loss": 0.0001,
      "step": 43110
    },
    {
      "epoch": 19.9537251272559,
      "grad_norm": 0.000971707166172564,
      "learning_rate": 0.0600925497454882,
      "loss": 0.0001,
      "step": 43120
    },
    {
      "epoch": 19.95835261453031,
      "grad_norm": 0.0400589220225811,
      "learning_rate": 0.06008329477093938,
      "loss": 0.0002,
      "step": 43130
    },
    {
      "epoch": 19.96298010180472,
      "grad_norm": 0.005839439108967781,
      "learning_rate": 0.06007403979639056,
      "loss": 0.0001,
      "step": 43140
    },
    {
      "epoch": 19.96760758907913,
      "grad_norm": 0.01702886074781418,
      "learning_rate": 0.060064784821841744,
      "loss": 0.0001,
      "step": 43150
    },
    {
      "epoch": 19.97223507635354,
      "grad_norm": 0.028951631858944893,
      "learning_rate": 0.06005552984729293,
      "loss": 0.0,
      "step": 43160
    },
    {
      "epoch": 19.97686256362795,
      "grad_norm": 0.008170124143362045,
      "learning_rate": 0.0600462748727441,
      "loss": 0.0007,
      "step": 43170
    },
    {
      "epoch": 19.98149005090236,
      "grad_norm": 0.006306284107267857,
      "learning_rate": 0.06003701989819528,
      "loss": 0.0001,
      "step": 43180
    },
    {
      "epoch": 19.98611753817677,
      "grad_norm": 0.003372060600668192,
      "learning_rate": 0.06002776492364647,
      "loss": 0.0002,
      "step": 43190
    },
    {
      "epoch": 19.99074502545118,
      "grad_norm": 0.0071272780187428,
      "learning_rate": 0.060018509949097645,
      "loss": 0.0001,
      "step": 43200
    },
    {
      "epoch": 19.99537251272559,
      "grad_norm": 0.1428174525499344,
      "learning_rate": 0.06000925497454882,
      "loss": 0.0003,
      "step": 43210
    },
    {
      "epoch": 20.0,
      "grad_norm": 61.65272903442383,
      "learning_rate": 0.06,
      "loss": 0.1373,
      "step": 43220
    },
    {
      "epoch": 20.0,
      "eval_accuracy_branch1": 0.988676629178863,
      "eval_accuracy_branch2": 0.4995378215991373,
      "eval_f1_branch1": 0.9897379548566954,
      "eval_f1_branch2": 0.4989004351912427,
      "eval_loss": 0.02457459643483162,
      "eval_precision_branch1": 0.9903454966049765,
      "eval_precision_branch2": 0.49953545805510435,
      "eval_recall_branch1": 0.9893466262112148,
      "eval_recall_branch2": 0.4995378215991373,
      "eval_runtime": 29.1576,
      "eval_samples_per_second": 445.235,
      "eval_steps_per_second": 55.663,
      "step": 43220
    },
    {
      "epoch": 20.00462748727441,
      "grad_norm": 0.0036231274716556072,
      "learning_rate": 0.05999074502545118,
      "loss": 0.0001,
      "step": 43230
    },
    {
      "epoch": 20.00925497454882,
      "grad_norm": 0.005665950011461973,
      "learning_rate": 0.059981490050902364,
      "loss": 0.0005,
      "step": 43240
    },
    {
      "epoch": 20.01388246182323,
      "grad_norm": 0.001234463881701231,
      "learning_rate": 0.05997223507635355,
      "loss": 0.0001,
      "step": 43250
    },
    {
      "epoch": 20.01850994909764,
      "grad_norm": 0.02311614900827408,
      "learning_rate": 0.05996298010180472,
      "loss": 0.0001,
      "step": 43260
    },
    {
      "epoch": 20.02313743637205,
      "grad_norm": 0.007271580398082733,
      "learning_rate": 0.0599537251272559,
      "loss": 0.0004,
      "step": 43270
    },
    {
      "epoch": 20.02776492364646,
      "grad_norm": 0.02513059601187706,
      "learning_rate": 0.05994447015270709,
      "loss": 0.0003,
      "step": 43280
    },
    {
      "epoch": 20.03239241092087,
      "grad_norm": 0.11389932781457901,
      "learning_rate": 0.059935215178158266,
      "loss": 0.0002,
      "step": 43290
    },
    {
      "epoch": 20.03701989819528,
      "grad_norm": 0.045746542513370514,
      "learning_rate": 0.05992596020360944,
      "loss": 0.0006,
      "step": 43300
    },
    {
      "epoch": 20.04164738546969,
      "grad_norm": 0.004013323225080967,
      "learning_rate": 0.05991670522906062,
      "loss": 0.0001,
      "step": 43310
    },
    {
      "epoch": 20.0462748727441,
      "grad_norm": 0.006711156573146582,
      "learning_rate": 0.0599074502545118,
      "loss": 0.0011,
      "step": 43320
    },
    {
      "epoch": 20.05090236001851,
      "grad_norm": 0.005765165202319622,
      "learning_rate": 0.059898195279962985,
      "loss": 0.0002,
      "step": 43330
    },
    {
      "epoch": 20.05552984729292,
      "grad_norm": 0.005433393642306328,
      "learning_rate": 0.05988894030541417,
      "loss": 0.0002,
      "step": 43340
    },
    {
      "epoch": 20.06015733456733,
      "grad_norm": 0.0009506630012765527,
      "learning_rate": 0.059879685330865344,
      "loss": 0.0,
      "step": 43350
    },
    {
      "epoch": 20.06478482184174,
      "grad_norm": 0.18605615198612213,
      "learning_rate": 0.05987043035631652,
      "loss": 0.0001,
      "step": 43360
    },
    {
      "epoch": 20.06941230911615,
      "grad_norm": 0.0036336351186037064,
      "learning_rate": 0.059861175381767696,
      "loss": 0.0001,
      "step": 43370
    },
    {
      "epoch": 20.07403979639056,
      "grad_norm": 0.02105936035513878,
      "learning_rate": 0.059851920407218887,
      "loss": 0.0004,
      "step": 43380
    },
    {
      "epoch": 20.07866728366497,
      "grad_norm": 0.001186856534332037,
      "learning_rate": 0.05984266543267006,
      "loss": 0.0004,
      "step": 43390
    },
    {
      "epoch": 20.08329477093938,
      "grad_norm": 0.01484356913715601,
      "learning_rate": 0.05983341045812124,
      "loss": 0.0002,
      "step": 43400
    },
    {
      "epoch": 20.08792225821379,
      "grad_norm": 0.03345869854092598,
      "learning_rate": 0.05982415548357242,
      "loss": 0.0,
      "step": 43410
    },
    {
      "epoch": 20.0925497454882,
      "grad_norm": 0.0031742840074002743,
      "learning_rate": 0.059814900509023605,
      "loss": 0.0,
      "step": 43420
    },
    {
      "epoch": 20.09717723276261,
      "grad_norm": 0.0006570171099156141,
      "learning_rate": 0.05980564553447479,
      "loss": 0.0005,
      "step": 43430
    },
    {
      "epoch": 20.10180472003702,
      "grad_norm": 0.009527276270091534,
      "learning_rate": 0.059796390559925965,
      "loss": 0.0002,
      "step": 43440
    },
    {
      "epoch": 20.10643220731143,
      "grad_norm": 0.0158566702157259,
      "learning_rate": 0.05978713558537714,
      "loss": 0.0002,
      "step": 43450
    },
    {
      "epoch": 20.11105969458584,
      "grad_norm": 0.0011669324012473226,
      "learning_rate": 0.05977788061082832,
      "loss": 0.0001,
      "step": 43460
    },
    {
      "epoch": 20.11568718186025,
      "grad_norm": 0.809126079082489,
      "learning_rate": 0.05976862563627951,
      "loss": 0.0001,
      "step": 43470
    },
    {
      "epoch": 20.12031466913466,
      "grad_norm": 0.00923888385295868,
      "learning_rate": 0.05975937066173068,
      "loss": 0.0001,
      "step": 43480
    },
    {
      "epoch": 20.12494215640907,
      "grad_norm": 0.004012363031506538,
      "learning_rate": 0.05975011568718186,
      "loss": 0.0001,
      "step": 43490
    },
    {
      "epoch": 20.129569643683478,
      "grad_norm": 0.006843116134405136,
      "learning_rate": 0.05974086071263304,
      "loss": 0.0113,
      "step": 43500
    },
    {
      "epoch": 20.13419713095789,
      "grad_norm": 8.060535430908203,
      "learning_rate": 0.059731605738084226,
      "loss": 0.0022,
      "step": 43510
    },
    {
      "epoch": 20.1388246182323,
      "grad_norm": 0.004565007984638214,
      "learning_rate": 0.05972235076353541,
      "loss": 0.0001,
      "step": 43520
    },
    {
      "epoch": 20.14345210550671,
      "grad_norm": 0.03966633975505829,
      "learning_rate": 0.059713095788986585,
      "loss": 0.0001,
      "step": 43530
    },
    {
      "epoch": 20.14807959278112,
      "grad_norm": 0.009757273830473423,
      "learning_rate": 0.05970384081443776,
      "loss": 0.0021,
      "step": 43540
    },
    {
      "epoch": 20.15270708005553,
      "grad_norm": 0.008960803970694542,
      "learning_rate": 0.05969458583988894,
      "loss": 0.0001,
      "step": 43550
    },
    {
      "epoch": 20.15733456732994,
      "grad_norm": 0.0108482139185071,
      "learning_rate": 0.05968533086534013,
      "loss": 0.0005,
      "step": 43560
    },
    {
      "epoch": 20.16196205460435,
      "grad_norm": 0.0012772673508152366,
      "learning_rate": 0.059676075890791304,
      "loss": 0.0002,
      "step": 43570
    },
    {
      "epoch": 20.16658954187876,
      "grad_norm": 0.014414775185286999,
      "learning_rate": 0.05966682091624248,
      "loss": 0.0002,
      "step": 43580
    },
    {
      "epoch": 20.17121702915317,
      "grad_norm": 0.008366186171770096,
      "learning_rate": 0.05965756594169366,
      "loss": 0.0001,
      "step": 43590
    },
    {
      "epoch": 20.17584451642758,
      "grad_norm": 0.01055077649652958,
      "learning_rate": 0.05964831096714484,
      "loss": 0.0002,
      "step": 43600
    },
    {
      "epoch": 20.18047200370199,
      "grad_norm": 0.004682527855038643,
      "learning_rate": 0.05963905599259603,
      "loss": 0.0001,
      "step": 43610
    },
    {
      "epoch": 20.1850994909764,
      "grad_norm": 0.5645293593406677,
      "learning_rate": 0.059629801018047206,
      "loss": 0.0002,
      "step": 43620
    },
    {
      "epoch": 20.18972697825081,
      "grad_norm": 0.014633976854383945,
      "learning_rate": 0.05962054604349838,
      "loss": 0.0027,
      "step": 43630
    },
    {
      "epoch": 20.19435446552522,
      "grad_norm": 0.6212075352668762,
      "learning_rate": 0.05961129106894956,
      "loss": 0.0005,
      "step": 43640
    },
    {
      "epoch": 20.19898195279963,
      "grad_norm": 0.007333668414503336,
      "learning_rate": 0.05960203609440075,
      "loss": 0.0002,
      "step": 43650
    },
    {
      "epoch": 20.20360944007404,
      "grad_norm": 0.29245612025260925,
      "learning_rate": 0.059592781119851924,
      "loss": 0.0002,
      "step": 43660
    },
    {
      "epoch": 20.208236927348448,
      "grad_norm": 0.03949921950697899,
      "learning_rate": 0.0595835261453031,
      "loss": 0.0031,
      "step": 43670
    },
    {
      "epoch": 20.21286441462286,
      "grad_norm": 0.00042902352288365364,
      "learning_rate": 0.059574271170754284,
      "loss": 0.0008,
      "step": 43680
    },
    {
      "epoch": 20.21749190189727,
      "grad_norm": 0.021600140258669853,
      "learning_rate": 0.05956501619620546,
      "loss": 0.0001,
      "step": 43690
    },
    {
      "epoch": 20.22211938917168,
      "grad_norm": 0.18907558917999268,
      "learning_rate": 0.05955576122165665,
      "loss": 0.0001,
      "step": 43700
    },
    {
      "epoch": 20.22674687644609,
      "grad_norm": 0.0030336957424879074,
      "learning_rate": 0.059546506247107826,
      "loss": 0.0001,
      "step": 43710
    },
    {
      "epoch": 20.2313743637205,
      "grad_norm": 0.08285827934741974,
      "learning_rate": 0.059537251272559,
      "loss": 0.0001,
      "step": 43720
    },
    {
      "epoch": 20.23600185099491,
      "grad_norm": 0.0014339227927848697,
      "learning_rate": 0.05952799629801018,
      "loss": 0.0001,
      "step": 43730
    },
    {
      "epoch": 20.24062933826932,
      "grad_norm": 0.0006448122439906001,
      "learning_rate": 0.059518741323461355,
      "loss": 0.0,
      "step": 43740
    },
    {
      "epoch": 20.24525682554373,
      "grad_norm": 0.0030458192341029644,
      "learning_rate": 0.059509486348912545,
      "loss": 0.0001,
      "step": 43750
    },
    {
      "epoch": 20.24988431281814,
      "grad_norm": 0.2077271044254303,
      "learning_rate": 0.05950023137436372,
      "loss": 0.0002,
      "step": 43760
    },
    {
      "epoch": 20.25451180009255,
      "grad_norm": 0.0005353617598302662,
      "learning_rate": 0.059490976399814904,
      "loss": 0.0024,
      "step": 43770
    },
    {
      "epoch": 20.25913928736696,
      "grad_norm": 0.007265933323651552,
      "learning_rate": 0.05948172142526608,
      "loss": 0.0055,
      "step": 43780
    },
    {
      "epoch": 20.26376677464137,
      "grad_norm": 0.4379189610481262,
      "learning_rate": 0.05947246645071727,
      "loss": 0.0002,
      "step": 43790
    },
    {
      "epoch": 20.26839426191578,
      "grad_norm": 0.001825663959607482,
      "learning_rate": 0.05946321147616845,
      "loss": 0.0006,
      "step": 43800
    },
    {
      "epoch": 20.27302174919019,
      "grad_norm": 0.00728648854419589,
      "learning_rate": 0.05945395650161962,
      "loss": 0.0025,
      "step": 43810
    },
    {
      "epoch": 20.2776492364646,
      "grad_norm": 0.12178634107112885,
      "learning_rate": 0.0594447015270708,
      "loss": 0.0002,
      "step": 43820
    },
    {
      "epoch": 20.28227672373901,
      "grad_norm": 0.0026061860844492912,
      "learning_rate": 0.059435446552521976,
      "loss": 0.0001,
      "step": 43830
    },
    {
      "epoch": 20.286904211013418,
      "grad_norm": 0.0028246776200830936,
      "learning_rate": 0.059426191577973166,
      "loss": 0.0002,
      "step": 43840
    },
    {
      "epoch": 20.29153169828783,
      "grad_norm": 0.023008447140455246,
      "learning_rate": 0.05941693660342434,
      "loss": 0.0001,
      "step": 43850
    },
    {
      "epoch": 20.29615918556224,
      "grad_norm": 0.38733917474746704,
      "learning_rate": 0.059407681628875525,
      "loss": 0.0012,
      "step": 43860
    },
    {
      "epoch": 20.30078667283665,
      "grad_norm": 0.03829900547862053,
      "learning_rate": 0.0593984266543267,
      "loss": 0.0001,
      "step": 43870
    },
    {
      "epoch": 20.30541416011106,
      "grad_norm": 0.03369837999343872,
      "learning_rate": 0.05938917167977789,
      "loss": 0.0002,
      "step": 43880
    },
    {
      "epoch": 20.310041647385468,
      "grad_norm": 0.05323612317442894,
      "learning_rate": 0.05937991670522907,
      "loss": 0.0001,
      "step": 43890
    },
    {
      "epoch": 20.31466913465988,
      "grad_norm": 0.0010591159807518125,
      "learning_rate": 0.059370661730680244,
      "loss": 0.0008,
      "step": 43900
    },
    {
      "epoch": 20.31929662193429,
      "grad_norm": 0.00012219119525980204,
      "learning_rate": 0.05936140675613142,
      "loss": 0.0003,
      "step": 43910
    },
    {
      "epoch": 20.3239241092087,
      "grad_norm": 0.003863681573420763,
      "learning_rate": 0.059352151781582596,
      "loss": 0.0004,
      "step": 43920
    },
    {
      "epoch": 20.32855159648311,
      "grad_norm": 0.03177531063556671,
      "learning_rate": 0.059342896807033786,
      "loss": 0.0001,
      "step": 43930
    },
    {
      "epoch": 20.33317908375752,
      "grad_norm": 0.0005541436257772148,
      "learning_rate": 0.05933364183248496,
      "loss": 0.0001,
      "step": 43940
    },
    {
      "epoch": 20.33780657103193,
      "grad_norm": 0.0019610587041825056,
      "learning_rate": 0.059324386857936146,
      "loss": 0.0002,
      "step": 43950
    },
    {
      "epoch": 20.34243405830634,
      "grad_norm": 0.003053374355658889,
      "learning_rate": 0.05931513188338732,
      "loss": 0.0002,
      "step": 43960
    },
    {
      "epoch": 20.34706154558075,
      "grad_norm": 0.0006398287368938327,
      "learning_rate": 0.0593058769088385,
      "loss": 0.0002,
      "step": 43970
    },
    {
      "epoch": 20.35168903285516,
      "grad_norm": 0.0008682597544975579,
      "learning_rate": 0.05929662193428969,
      "loss": 0.0001,
      "step": 43980
    },
    {
      "epoch": 20.35631652012957,
      "grad_norm": 0.0032041824888437986,
      "learning_rate": 0.059287366959740864,
      "loss": 0.0001,
      "step": 43990
    },
    {
      "epoch": 20.36094400740398,
      "grad_norm": 0.20966535806655884,
      "learning_rate": 0.05927811198519204,
      "loss": 0.0006,
      "step": 44000
    },
    {
      "epoch": 20.365571494678388,
      "grad_norm": 0.25423282384872437,
      "learning_rate": 0.05926885701064322,
      "loss": 0.0002,
      "step": 44010
    },
    {
      "epoch": 20.3701989819528,
      "grad_norm": 0.04106033220887184,
      "learning_rate": 0.05925960203609441,
      "loss": 0.0002,
      "step": 44020
    },
    {
      "epoch": 20.37482646922721,
      "grad_norm": 0.0026571061462163925,
      "learning_rate": 0.05925034706154558,
      "loss": 0.0004,
      "step": 44030
    },
    {
      "epoch": 20.37945395650162,
      "grad_norm": 0.035867173224687576,
      "learning_rate": 0.059241092086996766,
      "loss": 0.0002,
      "step": 44040
    },
    {
      "epoch": 20.38408144377603,
      "grad_norm": 0.06711757183074951,
      "learning_rate": 0.05923183711244794,
      "loss": 0.0002,
      "step": 44050
    },
    {
      "epoch": 20.388708931050438,
      "grad_norm": 0.004893012344837189,
      "learning_rate": 0.05922258213789912,
      "loss": 0.0007,
      "step": 44060
    },
    {
      "epoch": 20.39333641832485,
      "grad_norm": 0.08743728697299957,
      "learning_rate": 0.05921332716335031,
      "loss": 0.0001,
      "step": 44070
    },
    {
      "epoch": 20.39796390559926,
      "grad_norm": 0.03028048574924469,
      "learning_rate": 0.059204072188801485,
      "loss": 0.0001,
      "step": 44080
    },
    {
      "epoch": 20.40259139287367,
      "grad_norm": 0.003985347226262093,
      "learning_rate": 0.05919481721425266,
      "loss": 0.0002,
      "step": 44090
    },
    {
      "epoch": 20.40721888014808,
      "grad_norm": 0.02809874154627323,
      "learning_rate": 0.05918556223970384,
      "loss": 0.0001,
      "step": 44100
    },
    {
      "epoch": 20.41184636742249,
      "grad_norm": 0.00910872034728527,
      "learning_rate": 0.05917630726515503,
      "loss": 0.0001,
      "step": 44110
    },
    {
      "epoch": 20.4164738546969,
      "grad_norm": 0.0009729735320433974,
      "learning_rate": 0.059167052290606204,
      "loss": 0.0,
      "step": 44120
    },
    {
      "epoch": 20.42110134197131,
      "grad_norm": 0.0026376056484878063,
      "learning_rate": 0.05915779731605739,
      "loss": 0.0001,
      "step": 44130
    },
    {
      "epoch": 20.42572882924572,
      "grad_norm": 0.2146262675523758,
      "learning_rate": 0.05914854234150856,
      "loss": 0.0001,
      "step": 44140
    },
    {
      "epoch": 20.43035631652013,
      "grad_norm": 0.00042147975182160735,
      "learning_rate": 0.05913928736695974,
      "loss": 0.0005,
      "step": 44150
    },
    {
      "epoch": 20.43498380379454,
      "grad_norm": 0.06946499645709991,
      "learning_rate": 0.05913003239241093,
      "loss": 0.0017,
      "step": 44160
    },
    {
      "epoch": 20.43961129106895,
      "grad_norm": 0.0023779303301125765,
      "learning_rate": 0.059120777417862105,
      "loss": 0.0001,
      "step": 44170
    },
    {
      "epoch": 20.444238778343358,
      "grad_norm": 0.05122477933764458,
      "learning_rate": 0.05911152244331328,
      "loss": 0.0012,
      "step": 44180
    },
    {
      "epoch": 20.44886626561777,
      "grad_norm": 0.01874546706676483,
      "learning_rate": 0.05910226746876446,
      "loss": 0.0058,
      "step": 44190
    },
    {
      "epoch": 20.45349375289218,
      "grad_norm": 0.3276788890361786,
      "learning_rate": 0.05909301249421564,
      "loss": 0.0019,
      "step": 44200
    },
    {
      "epoch": 20.45812124016659,
      "grad_norm": 0.43404415249824524,
      "learning_rate": 0.059083757519666824,
      "loss": 0.0024,
      "step": 44210
    },
    {
      "epoch": 20.462748727441,
      "grad_norm": 0.003084990195930004,
      "learning_rate": 0.05907450254511801,
      "loss": 0.0,
      "step": 44220
    },
    {
      "epoch": 20.467376214715408,
      "grad_norm": 0.0016470495611429214,
      "learning_rate": 0.05906524757056918,
      "loss": 0.001,
      "step": 44230
    },
    {
      "epoch": 20.47200370198982,
      "grad_norm": 0.0018817676464095712,
      "learning_rate": 0.05905599259602036,
      "loss": 0.0001,
      "step": 44240
    },
    {
      "epoch": 20.47663118926423,
      "grad_norm": 0.019872980192303658,
      "learning_rate": 0.05904673762147155,
      "loss": 0.0003,
      "step": 44250
    },
    {
      "epoch": 20.48125867653864,
      "grad_norm": 0.005988238845020533,
      "learning_rate": 0.059037482646922726,
      "loss": 0.0001,
      "step": 44260
    },
    {
      "epoch": 20.48588616381305,
      "grad_norm": 0.0041907234117388725,
      "learning_rate": 0.0590282276723739,
      "loss": 0.0021,
      "step": 44270
    },
    {
      "epoch": 20.49051365108746,
      "grad_norm": 0.02350548282265663,
      "learning_rate": 0.05901897269782508,
      "loss": 0.0004,
      "step": 44280
    },
    {
      "epoch": 20.49514113836187,
      "grad_norm": 0.0019410535460337996,
      "learning_rate": 0.05900971772327626,
      "loss": 0.0001,
      "step": 44290
    },
    {
      "epoch": 20.49976862563628,
      "grad_norm": 0.003558230819180608,
      "learning_rate": 0.059000462748727445,
      "loss": 0.0003,
      "step": 44300
    },
    {
      "epoch": 20.50439611291069,
      "grad_norm": 0.0023145433515310287,
      "learning_rate": 0.05899120777417863,
      "loss": 0.0001,
      "step": 44310
    },
    {
      "epoch": 20.5090236001851,
      "grad_norm": 0.002865717513486743,
      "learning_rate": 0.058981952799629804,
      "loss": 0.0021,
      "step": 44320
    },
    {
      "epoch": 20.51365108745951,
      "grad_norm": 0.8505904078483582,
      "learning_rate": 0.05897269782508098,
      "loss": 0.0002,
      "step": 44330
    },
    {
      "epoch": 20.51827857473392,
      "grad_norm": 0.0015523347537964582,
      "learning_rate": 0.05896344285053217,
      "loss": 0.0001,
      "step": 44340
    },
    {
      "epoch": 20.522906062008328,
      "grad_norm": 0.015175198204815388,
      "learning_rate": 0.05895418787598335,
      "loss": 0.0005,
      "step": 44350
    },
    {
      "epoch": 20.52753354928274,
      "grad_norm": 0.0010510107968002558,
      "learning_rate": 0.05894493290143452,
      "loss": 0.0001,
      "step": 44360
    },
    {
      "epoch": 20.53216103655715,
      "grad_norm": 0.0043513537384569645,
      "learning_rate": 0.0589356779268857,
      "loss": 0.0,
      "step": 44370
    },
    {
      "epoch": 20.53678852383156,
      "grad_norm": 0.09783603996038437,
      "learning_rate": 0.05892642295233688,
      "loss": 0.0001,
      "step": 44380
    },
    {
      "epoch": 20.54141601110597,
      "grad_norm": 0.050751637667417526,
      "learning_rate": 0.058917167977788065,
      "loss": 0.0001,
      "step": 44390
    },
    {
      "epoch": 20.546043498380378,
      "grad_norm": 0.008748013526201248,
      "learning_rate": 0.05890791300323925,
      "loss": 0.0001,
      "step": 44400
    },
    {
      "epoch": 20.55067098565479,
      "grad_norm": 0.012204831466078758,
      "learning_rate": 0.058898658028690425,
      "loss": 0.0002,
      "step": 44410
    },
    {
      "epoch": 20.5552984729292,
      "grad_norm": 0.005591940600425005,
      "learning_rate": 0.0588894030541416,
      "loss": 0.0002,
      "step": 44420
    },
    {
      "epoch": 20.55992596020361,
      "grad_norm": 0.004160573240369558,
      "learning_rate": 0.05888014807959278,
      "loss": 0.0002,
      "step": 44430
    },
    {
      "epoch": 20.56455344747802,
      "grad_norm": 0.018535206094384193,
      "learning_rate": 0.05887089310504397,
      "loss": 0.0002,
      "step": 44440
    },
    {
      "epoch": 20.56918093475243,
      "grad_norm": 0.02152925543487072,
      "learning_rate": 0.05886163813049514,
      "loss": 0.0001,
      "step": 44450
    },
    {
      "epoch": 20.57380842202684,
      "grad_norm": 0.014891228638589382,
      "learning_rate": 0.05885238315594632,
      "loss": 0.0,
      "step": 44460
    },
    {
      "epoch": 20.57843590930125,
      "grad_norm": 0.046257417649030685,
      "learning_rate": 0.0588431281813975,
      "loss": 0.0003,
      "step": 44470
    },
    {
      "epoch": 20.58306339657566,
      "grad_norm": 0.044257987290620804,
      "learning_rate": 0.058833873206848686,
      "loss": 0.0001,
      "step": 44480
    },
    {
      "epoch": 20.58769088385007,
      "grad_norm": 0.006533230189234018,
      "learning_rate": 0.05882461823229987,
      "loss": 0.0003,
      "step": 44490
    },
    {
      "epoch": 20.59231837112448,
      "grad_norm": 0.02213224396109581,
      "learning_rate": 0.058815363257751045,
      "loss": 0.0051,
      "step": 44500
    },
    {
      "epoch": 20.59694585839889,
      "grad_norm": 0.002660720609128475,
      "learning_rate": 0.05880610828320222,
      "loss": 0.0001,
      "step": 44510
    },
    {
      "epoch": 20.601573345673298,
      "grad_norm": 0.003489494789391756,
      "learning_rate": 0.0587968533086534,
      "loss": 0.0001,
      "step": 44520
    },
    {
      "epoch": 20.60620083294771,
      "grad_norm": 0.0019309588242322206,
      "learning_rate": 0.05878759833410459,
      "loss": 0.0001,
      "step": 44530
    },
    {
      "epoch": 20.61082832022212,
      "grad_norm": 0.02376645617187023,
      "learning_rate": 0.058778343359555764,
      "loss": 0.0001,
      "step": 44540
    },
    {
      "epoch": 20.61545580749653,
      "grad_norm": 0.024561159312725067,
      "learning_rate": 0.05876908838500694,
      "loss": 0.0003,
      "step": 44550
    },
    {
      "epoch": 20.62008329477094,
      "grad_norm": 0.0072845895774662495,
      "learning_rate": 0.05875983341045812,
      "loss": 0.0236,
      "step": 44560
    },
    {
      "epoch": 20.624710782045348,
      "grad_norm": 0.004277714993804693,
      "learning_rate": 0.058750578435909306,
      "loss": 0.0001,
      "step": 44570
    },
    {
      "epoch": 20.62933826931976,
      "grad_norm": 0.0046189455315470695,
      "learning_rate": 0.05874132346136049,
      "loss": 0.0028,
      "step": 44580
    },
    {
      "epoch": 20.63396575659417,
      "grad_norm": 0.0061525506898760796,
      "learning_rate": 0.058732068486811666,
      "loss": 0.0003,
      "step": 44590
    },
    {
      "epoch": 20.63859324386858,
      "grad_norm": 0.037918590009212494,
      "learning_rate": 0.05872281351226284,
      "loss": 0.0001,
      "step": 44600
    },
    {
      "epoch": 20.64322073114299,
      "grad_norm": 0.003646165831014514,
      "learning_rate": 0.05871355853771402,
      "loss": 0.0001,
      "step": 44610
    },
    {
      "epoch": 20.647848218417398,
      "grad_norm": 0.041945286095142365,
      "learning_rate": 0.05870430356316521,
      "loss": 0.0005,
      "step": 44620
    },
    {
      "epoch": 20.65247570569181,
      "grad_norm": 0.1443716287612915,
      "learning_rate": 0.058695048588616384,
      "loss": 0.0003,
      "step": 44630
    },
    {
      "epoch": 20.65710319296622,
      "grad_norm": 0.001662562950514257,
      "learning_rate": 0.05868579361406756,
      "loss": 0.0002,
      "step": 44640
    },
    {
      "epoch": 20.66173068024063,
      "grad_norm": 0.021655643358826637,
      "learning_rate": 0.058676538639518744,
      "loss": 0.0002,
      "step": 44650
    },
    {
      "epoch": 20.66635816751504,
      "grad_norm": 0.0010369149968028069,
      "learning_rate": 0.05866728366496992,
      "loss": 0.0001,
      "step": 44660
    },
    {
      "epoch": 20.67098565478945,
      "grad_norm": 0.007738562300801277,
      "learning_rate": 0.05865802869042111,
      "loss": 0.0,
      "step": 44670
    },
    {
      "epoch": 20.67561314206386,
      "grad_norm": 0.0017482867697253823,
      "learning_rate": 0.058648773715872286,
      "loss": 0.0001,
      "step": 44680
    },
    {
      "epoch": 20.680240629338268,
      "grad_norm": 0.0027526486665010452,
      "learning_rate": 0.05863951874132346,
      "loss": 0.0001,
      "step": 44690
    },
    {
      "epoch": 20.68486811661268,
      "grad_norm": 0.0023642098531126976,
      "learning_rate": 0.05863026376677464,
      "loss": 0.0016,
      "step": 44700
    },
    {
      "epoch": 20.68949560388709,
      "grad_norm": 0.2960371673107147,
      "learning_rate": 0.05862100879222583,
      "loss": 0.0005,
      "step": 44710
    },
    {
      "epoch": 20.6941230911615,
      "grad_norm": 0.003928257618099451,
      "learning_rate": 0.058611753817677005,
      "loss": 0.0003,
      "step": 44720
    },
    {
      "epoch": 20.69875057843591,
      "grad_norm": 0.14520783722400665,
      "learning_rate": 0.05860249884312818,
      "loss": 0.0001,
      "step": 44730
    },
    {
      "epoch": 20.703378065710318,
      "grad_norm": 0.025653531774878502,
      "learning_rate": 0.058593243868579364,
      "loss": 0.0,
      "step": 44740
    },
    {
      "epoch": 20.70800555298473,
      "grad_norm": 0.0049373432993888855,
      "learning_rate": 0.05858398889403054,
      "loss": 0.0001,
      "step": 44750
    },
    {
      "epoch": 20.71263304025914,
      "grad_norm": 0.0011936709051951766,
      "learning_rate": 0.05857473391948173,
      "loss": 0.0,
      "step": 44760
    },
    {
      "epoch": 20.71726052753355,
      "grad_norm": 0.002333566313609481,
      "learning_rate": 0.05856547894493291,
      "loss": 0.0005,
      "step": 44770
    },
    {
      "epoch": 20.72188801480796,
      "grad_norm": 0.0011034440249204636,
      "learning_rate": 0.05855622397038408,
      "loss": 0.0002,
      "step": 44780
    },
    {
      "epoch": 20.726515502082368,
      "grad_norm": 0.0007196193328127265,
      "learning_rate": 0.05854696899583526,
      "loss": 0.0,
      "step": 44790
    },
    {
      "epoch": 20.73114298935678,
      "grad_norm": 0.03136201575398445,
      "learning_rate": 0.05853771402128645,
      "loss": 0.0001,
      "step": 44800
    },
    {
      "epoch": 20.73577047663119,
      "grad_norm": 0.28282418847084045,
      "learning_rate": 0.058528459046737626,
      "loss": 0.0001,
      "step": 44810
    },
    {
      "epoch": 20.7403979639056,
      "grad_norm": 0.010073551908135414,
      "learning_rate": 0.0585192040721888,
      "loss": 0.0001,
      "step": 44820
    },
    {
      "epoch": 20.74502545118001,
      "grad_norm": 0.014418207108974457,
      "learning_rate": 0.058509949097639985,
      "loss": 0.0,
      "step": 44830
    },
    {
      "epoch": 20.749652938454417,
      "grad_norm": 0.25732406973838806,
      "learning_rate": 0.05850069412309116,
      "loss": 0.0001,
      "step": 44840
    },
    {
      "epoch": 20.75428042572883,
      "grad_norm": 0.0008442953694611788,
      "learning_rate": 0.05849143914854235,
      "loss": 0.0001,
      "step": 44850
    },
    {
      "epoch": 20.758907913003238,
      "grad_norm": 0.005566578824073076,
      "learning_rate": 0.05848218417399353,
      "loss": 0.0,
      "step": 44860
    },
    {
      "epoch": 20.76353540027765,
      "grad_norm": 0.020991671830415726,
      "learning_rate": 0.058472929199444704,
      "loss": 0.0001,
      "step": 44870
    },
    {
      "epoch": 20.76816288755206,
      "grad_norm": 0.03549684211611748,
      "learning_rate": 0.05846367422489588,
      "loss": 0.0001,
      "step": 44880
    },
    {
      "epoch": 20.77279037482647,
      "grad_norm": 0.01089448481798172,
      "learning_rate": 0.058454419250347056,
      "loss": 0.0007,
      "step": 44890
    },
    {
      "epoch": 20.77741786210088,
      "grad_norm": 0.016497543081641197,
      "learning_rate": 0.058445164275798246,
      "loss": 0.0002,
      "step": 44900
    },
    {
      "epoch": 20.782045349375288,
      "grad_norm": 0.008249330334365368,
      "learning_rate": 0.05843590930124942,
      "loss": 0.0001,
      "step": 44910
    },
    {
      "epoch": 20.7866728366497,
      "grad_norm": 0.007571379654109478,
      "learning_rate": 0.058426654326700606,
      "loss": 0.0004,
      "step": 44920
    },
    {
      "epoch": 20.79130032392411,
      "grad_norm": 0.012970322743058205,
      "learning_rate": 0.05841739935215178,
      "loss": 0.0001,
      "step": 44930
    },
    {
      "epoch": 20.79592781119852,
      "grad_norm": 0.015613559633493423,
      "learning_rate": 0.05840814437760297,
      "loss": 0.0001,
      "step": 44940
    },
    {
      "epoch": 20.80055529847293,
      "grad_norm": 0.002096479292958975,
      "learning_rate": 0.05839888940305415,
      "loss": 0.0001,
      "step": 44950
    },
    {
      "epoch": 20.805182785747338,
      "grad_norm": 0.0004286071634851396,
      "learning_rate": 0.058389634428505324,
      "loss": 0.0006,
      "step": 44960
    },
    {
      "epoch": 20.80981027302175,
      "grad_norm": 0.00047671046922914684,
      "learning_rate": 0.0583803794539565,
      "loss": 0.001,
      "step": 44970
    },
    {
      "epoch": 20.81443776029616,
      "grad_norm": 0.8387027978897095,
      "learning_rate": 0.05837112447940768,
      "loss": 0.0002,
      "step": 44980
    },
    {
      "epoch": 20.81906524757057,
      "grad_norm": 0.001280972152017057,
      "learning_rate": 0.05836186950485887,
      "loss": 0.0001,
      "step": 44990
    },
    {
      "epoch": 20.82369273484498,
      "grad_norm": 0.011148976162075996,
      "learning_rate": 0.05835261453031004,
      "loss": 0.0004,
      "step": 45000
    },
    {
      "epoch": 20.828320222119387,
      "grad_norm": 0.0934467613697052,
      "learning_rate": 0.058343359555761226,
      "loss": 0.0003,
      "step": 45010
    },
    {
      "epoch": 20.8329477093938,
      "grad_norm": 0.0003792797215282917,
      "learning_rate": 0.0583341045812124,
      "loss": 0.0002,
      "step": 45020
    },
    {
      "epoch": 20.837575196668208,
      "grad_norm": 0.0012725440319627523,
      "learning_rate": 0.05832484960666359,
      "loss": 0.0005,
      "step": 45030
    },
    {
      "epoch": 20.84220268394262,
      "grad_norm": 0.014592651277780533,
      "learning_rate": 0.05831559463211477,
      "loss": 0.0002,
      "step": 45040
    },
    {
      "epoch": 20.84683017121703,
      "grad_norm": 0.0008999681449495256,
      "learning_rate": 0.058306339657565945,
      "loss": 0.0001,
      "step": 45050
    },
    {
      "epoch": 20.85145765849144,
      "grad_norm": 0.0038529851008206606,
      "learning_rate": 0.05829708468301712,
      "loss": 0.0009,
      "step": 45060
    },
    {
      "epoch": 20.85608514576585,
      "grad_norm": 0.008435778319835663,
      "learning_rate": 0.0582878297084683,
      "loss": 0.0013,
      "step": 45070
    },
    {
      "epoch": 20.860712633040258,
      "grad_norm": 0.004943245090544224,
      "learning_rate": 0.05827857473391949,
      "loss": 0.0002,
      "step": 45080
    },
    {
      "epoch": 20.86534012031467,
      "grad_norm": 0.001190724316984415,
      "learning_rate": 0.058269319759370664,
      "loss": 0.0001,
      "step": 45090
    },
    {
      "epoch": 20.86996760758908,
      "grad_norm": 0.0024321055971086025,
      "learning_rate": 0.05826006478482185,
      "loss": 0.0001,
      "step": 45100
    },
    {
      "epoch": 20.87459509486349,
      "grad_norm": 0.037388235330581665,
      "learning_rate": 0.05825080981027302,
      "loss": 0.0001,
      "step": 45110
    },
    {
      "epoch": 20.8792225821379,
      "grad_norm": 0.0032907030545175076,
      "learning_rate": 0.0582415548357242,
      "loss": 0.0002,
      "step": 45120
    },
    {
      "epoch": 20.883850069412308,
      "grad_norm": 0.018752319738268852,
      "learning_rate": 0.05823229986117539,
      "loss": 0.0001,
      "step": 45130
    },
    {
      "epoch": 20.88847755668672,
      "grad_norm": 0.0008797042537480593,
      "learning_rate": 0.058223044886626565,
      "loss": 0.0003,
      "step": 45140
    },
    {
      "epoch": 20.89310504396113,
      "grad_norm": 0.030599722638726234,
      "learning_rate": 0.05821378991207774,
      "loss": 0.0001,
      "step": 45150
    },
    {
      "epoch": 20.89773253123554,
      "grad_norm": 0.03692824766039848,
      "learning_rate": 0.05820453493752892,
      "loss": 0.0001,
      "step": 45160
    },
    {
      "epoch": 20.90236001850995,
      "grad_norm": 0.05502522364258766,
      "learning_rate": 0.05819527996298011,
      "loss": 0.0005,
      "step": 45170
    },
    {
      "epoch": 20.906987505784357,
      "grad_norm": 0.003810121212154627,
      "learning_rate": 0.058186024988431284,
      "loss": 0.0001,
      "step": 45180
    },
    {
      "epoch": 20.91161499305877,
      "grad_norm": 0.023147763684391975,
      "learning_rate": 0.05817677001388247,
      "loss": 0.0001,
      "step": 45190
    },
    {
      "epoch": 20.916242480333178,
      "grad_norm": 0.0027816486544907093,
      "learning_rate": 0.058167515039333643,
      "loss": 0.0004,
      "step": 45200
    },
    {
      "epoch": 20.92086996760759,
      "grad_norm": 0.04999126121401787,
      "learning_rate": 0.05815826006478482,
      "loss": 0.0001,
      "step": 45210
    },
    {
      "epoch": 20.925497454882,
      "grad_norm": 0.06983598321676254,
      "learning_rate": 0.05814900509023601,
      "loss": 0.0015,
      "step": 45220
    },
    {
      "epoch": 20.93012494215641,
      "grad_norm": 0.4703250527381897,
      "learning_rate": 0.058139750115687186,
      "loss": 0.0002,
      "step": 45230
    },
    {
      "epoch": 20.93475242943082,
      "grad_norm": 0.023653045296669006,
      "learning_rate": 0.05813049514113836,
      "loss": 0.0001,
      "step": 45240
    },
    {
      "epoch": 20.939379916705228,
      "grad_norm": 0.007125334348529577,
      "learning_rate": 0.05812124016658954,
      "loss": 0.0001,
      "step": 45250
    },
    {
      "epoch": 20.94400740397964,
      "grad_norm": 0.0008101592538878322,
      "learning_rate": 0.05811198519204073,
      "loss": 0.0001,
      "step": 45260
    },
    {
      "epoch": 20.94863489125405,
      "grad_norm": 0.015902051702141762,
      "learning_rate": 0.058102730217491905,
      "loss": 0.0001,
      "step": 45270
    },
    {
      "epoch": 20.95326237852846,
      "grad_norm": 0.014162116684019566,
      "learning_rate": 0.05809347524294309,
      "loss": 0.0001,
      "step": 45280
    },
    {
      "epoch": 20.95788986580287,
      "grad_norm": 0.001935147913172841,
      "learning_rate": 0.058084220268394264,
      "loss": 0.0001,
      "step": 45290
    },
    {
      "epoch": 20.962517353077278,
      "grad_norm": 0.0006297819782048464,
      "learning_rate": 0.05807496529384544,
      "loss": 0.0001,
      "step": 45300
    },
    {
      "epoch": 20.96714484035169,
      "grad_norm": 0.015326760709285736,
      "learning_rate": 0.05806571031929663,
      "loss": 0.0003,
      "step": 45310
    },
    {
      "epoch": 20.9717723276261,
      "grad_norm": 0.002552605466917157,
      "learning_rate": 0.05805645534474781,
      "loss": 0.0085,
      "step": 45320
    },
    {
      "epoch": 20.97639981490051,
      "grad_norm": 0.028138576075434685,
      "learning_rate": 0.05804720037019898,
      "loss": 0.0001,
      "step": 45330
    },
    {
      "epoch": 20.98102730217492,
      "grad_norm": 0.014569510705769062,
      "learning_rate": 0.05803794539565016,
      "loss": 0.0002,
      "step": 45340
    },
    {
      "epoch": 20.985654789449327,
      "grad_norm": 0.06998135894536972,
      "learning_rate": 0.05802869042110134,
      "loss": 0.0044,
      "step": 45350
    },
    {
      "epoch": 20.99028227672374,
      "grad_norm": 0.38825541734695435,
      "learning_rate": 0.058019435446552525,
      "loss": 0.0006,
      "step": 45360
    },
    {
      "epoch": 20.994909763998148,
      "grad_norm": 0.7757432460784912,
      "learning_rate": 0.05801018047200371,
      "loss": 0.0062,
      "step": 45370
    },
    {
      "epoch": 20.99953725127256,
      "grad_norm": 0.008281663991510868,
      "learning_rate": 0.058000925497454885,
      "loss": 0.0015,
      "step": 45380
    },
    {
      "epoch": 21.0,
      "eval_accuracy_branch1": 0.9907564319827453,
      "eval_accuracy_branch2": 0.5,
      "eval_f1_branch1": 0.9909387827977792,
      "eval_f1_branch2": 0.49997856402386687,
      "eval_loss": 0.022824717685580254,
      "eval_precision_branch1": 0.9910164604128856,
      "eval_precision_branch2": 0.5,
      "eval_recall_branch1": 0.9910058663215704,
      "eval_recall_branch2": 0.5,
      "eval_runtime": 29.0015,
      "eval_samples_per_second": 447.632,
      "eval_steps_per_second": 55.963,
      "step": 45381
    },
    {
      "epoch": 21.00416473854697,
      "grad_norm": 0.0008836009656079113,
      "learning_rate": 0.05799167052290606,
      "loss": 0.0146,
      "step": 45390
    },
    {
      "epoch": 21.008792225821377,
      "grad_norm": 0.018001440912485123,
      "learning_rate": 0.05798241554835725,
      "loss": 0.0046,
      "step": 45400
    },
    {
      "epoch": 21.01341971309579,
      "grad_norm": 0.0013284286251291633,
      "learning_rate": 0.05797316057380843,
      "loss": 0.0002,
      "step": 45410
    },
    {
      "epoch": 21.018047200370198,
      "grad_norm": 0.022806664928793907,
      "learning_rate": 0.0579639055992596,
      "loss": 0.0002,
      "step": 45420
    },
    {
      "epoch": 21.02267468764461,
      "grad_norm": 0.015500791370868683,
      "learning_rate": 0.05795465062471078,
      "loss": 0.0031,
      "step": 45430
    },
    {
      "epoch": 21.02730217491902,
      "grad_norm": 0.013071232475340366,
      "learning_rate": 0.05794539565016196,
      "loss": 0.0002,
      "step": 45440
    },
    {
      "epoch": 21.03192966219343,
      "grad_norm": 0.1461925208568573,
      "learning_rate": 0.057936140675613146,
      "loss": 0.0001,
      "step": 45450
    },
    {
      "epoch": 21.03655714946784,
      "grad_norm": 0.008410300128161907,
      "learning_rate": 0.05792688570106433,
      "loss": 0.0004,
      "step": 45460
    },
    {
      "epoch": 21.041184636742248,
      "grad_norm": 0.00230779848061502,
      "learning_rate": 0.057917630726515505,
      "loss": 0.0002,
      "step": 45470
    },
    {
      "epoch": 21.04581212401666,
      "grad_norm": 0.0188040342181921,
      "learning_rate": 0.05790837575196668,
      "loss": 0.0001,
      "step": 45480
    },
    {
      "epoch": 21.05043961129107,
      "grad_norm": 0.023598503321409225,
      "learning_rate": 0.05789912077741787,
      "loss": 0.0001,
      "step": 45490
    },
    {
      "epoch": 21.05506709856548,
      "grad_norm": 0.011606394313275814,
      "learning_rate": 0.05788986580286905,
      "loss": 0.0001,
      "step": 45500
    },
    {
      "epoch": 21.05969458583989,
      "grad_norm": 0.001827063737437129,
      "learning_rate": 0.057880610828320224,
      "loss": 0.0001,
      "step": 45510
    },
    {
      "epoch": 21.064322073114297,
      "grad_norm": 0.0012299248483031988,
      "learning_rate": 0.0578713558537714,
      "loss": 0.0001,
      "step": 45520
    },
    {
      "epoch": 21.06894956038871,
      "grad_norm": 0.058023713529109955,
      "learning_rate": 0.05786210087922258,
      "loss": 0.0001,
      "step": 45530
    },
    {
      "epoch": 21.073577047663118,
      "grad_norm": 0.010179772973060608,
      "learning_rate": 0.057852845904673766,
      "loss": 0.0006,
      "step": 45540
    },
    {
      "epoch": 21.07820453493753,
      "grad_norm": 0.0020485285203903913,
      "learning_rate": 0.05784359093012495,
      "loss": 0.0001,
      "step": 45550
    },
    {
      "epoch": 21.08283202221194,
      "grad_norm": 0.01370681170374155,
      "learning_rate": 0.057834335955576126,
      "loss": 0.0,
      "step": 45560
    },
    {
      "epoch": 21.087459509486347,
      "grad_norm": 0.006753398105502129,
      "learning_rate": 0.0578250809810273,
      "loss": 0.0001,
      "step": 45570
    },
    {
      "epoch": 21.09208699676076,
      "grad_norm": 0.01446607243269682,
      "learning_rate": 0.05781582600647848,
      "loss": 0.0001,
      "step": 45580
    },
    {
      "epoch": 21.096714484035168,
      "grad_norm": 0.09753050655126572,
      "learning_rate": 0.05780657103192967,
      "loss": 0.0076,
      "step": 45590
    },
    {
      "epoch": 21.10134197130958,
      "grad_norm": 0.0020068546291440725,
      "learning_rate": 0.057797316057380844,
      "loss": 0.0,
      "step": 45600
    },
    {
      "epoch": 21.10596945858399,
      "grad_norm": 0.004386689513921738,
      "learning_rate": 0.05778806108283202,
      "loss": 0.0001,
      "step": 45610
    },
    {
      "epoch": 21.1105969458584,
      "grad_norm": 0.09288300573825836,
      "learning_rate": 0.057778806108283204,
      "loss": 0.0002,
      "step": 45620
    },
    {
      "epoch": 21.11522443313281,
      "grad_norm": 0.06378570199012756,
      "learning_rate": 0.05776955113373439,
      "loss": 0.0001,
      "step": 45630
    },
    {
      "epoch": 21.119851920407218,
      "grad_norm": 0.3810686469078064,
      "learning_rate": 0.05776029615918557,
      "loss": 0.0001,
      "step": 45640
    },
    {
      "epoch": 21.12447940768163,
      "grad_norm": 0.0014323786599561572,
      "learning_rate": 0.057751041184636746,
      "loss": 0.0003,
      "step": 45650
    },
    {
      "epoch": 21.12910689495604,
      "grad_norm": 0.029811931774020195,
      "learning_rate": 0.05774178621008792,
      "loss": 0.0002,
      "step": 45660
    },
    {
      "epoch": 21.13373438223045,
      "grad_norm": 0.001700992346741259,
      "learning_rate": 0.0577325312355391,
      "loss": 0.0001,
      "step": 45670
    },
    {
      "epoch": 21.13836186950486,
      "grad_norm": 0.00441144872456789,
      "learning_rate": 0.05772327626099029,
      "loss": 0.0007,
      "step": 45680
    },
    {
      "epoch": 21.142989356779267,
      "grad_norm": 0.0013324724277481437,
      "learning_rate": 0.057714021286441465,
      "loss": 0.0,
      "step": 45690
    },
    {
      "epoch": 21.14761684405368,
      "grad_norm": 0.002822731388732791,
      "learning_rate": 0.05770476631189264,
      "loss": 0.0001,
      "step": 45700
    },
    {
      "epoch": 21.152244331328088,
      "grad_norm": 0.08397670090198517,
      "learning_rate": 0.057695511337343824,
      "loss": 0.0002,
      "step": 45710
    },
    {
      "epoch": 21.1568718186025,
      "grad_norm": 0.029026169329881668,
      "learning_rate": 0.05768625636279501,
      "loss": 0.0031,
      "step": 45720
    },
    {
      "epoch": 21.16149930587691,
      "grad_norm": 0.0023302605841308832,
      "learning_rate": 0.05767700138824619,
      "loss": 0.0001,
      "step": 45730
    },
    {
      "epoch": 21.166126793151317,
      "grad_norm": 0.03334146738052368,
      "learning_rate": 0.05766774641369737,
      "loss": 0.0001,
      "step": 45740
    },
    {
      "epoch": 21.17075428042573,
      "grad_norm": 0.0003762504493352026,
      "learning_rate": 0.05765849143914854,
      "loss": 0.0007,
      "step": 45750
    },
    {
      "epoch": 21.175381767700138,
      "grad_norm": 0.012322530150413513,
      "learning_rate": 0.05764923646459972,
      "loss": 0.0001,
      "step": 45760
    },
    {
      "epoch": 21.18000925497455,
      "grad_norm": 0.24398978054523468,
      "learning_rate": 0.05763998149005091,
      "loss": 0.0004,
      "step": 45770
    },
    {
      "epoch": 21.18463674224896,
      "grad_norm": 0.05363178625702858,
      "learning_rate": 0.057630726515502086,
      "loss": 0.0001,
      "step": 45780
    },
    {
      "epoch": 21.189264229523367,
      "grad_norm": 0.2387179583311081,
      "learning_rate": 0.05762147154095326,
      "loss": 0.0001,
      "step": 45790
    },
    {
      "epoch": 21.19389171679778,
      "grad_norm": 0.027101051062345505,
      "learning_rate": 0.057612216566404445,
      "loss": 0.0001,
      "step": 45800
    },
    {
      "epoch": 21.198519204072188,
      "grad_norm": 0.026500023901462555,
      "learning_rate": 0.05760296159185562,
      "loss": 0.0002,
      "step": 45810
    },
    {
      "epoch": 21.2031466913466,
      "grad_norm": 0.001211724360473454,
      "learning_rate": 0.05759370661730681,
      "loss": 0.0001,
      "step": 45820
    },
    {
      "epoch": 21.20777417862101,
      "grad_norm": 0.006478847935795784,
      "learning_rate": 0.05758445164275799,
      "loss": 0.0002,
      "step": 45830
    },
    {
      "epoch": 21.21240166589542,
      "grad_norm": 0.03539218753576279,
      "learning_rate": 0.057575196668209164,
      "loss": 0.0003,
      "step": 45840
    },
    {
      "epoch": 21.21702915316983,
      "grad_norm": 0.0009555203141644597,
      "learning_rate": 0.05756594169366034,
      "loss": 0.0,
      "step": 45850
    },
    {
      "epoch": 21.221656640444237,
      "grad_norm": 0.014832594431936741,
      "learning_rate": 0.05755668671911153,
      "loss": 0.0011,
      "step": 45860
    },
    {
      "epoch": 21.22628412771865,
      "grad_norm": 0.0057645742781460285,
      "learning_rate": 0.057547431744562706,
      "loss": 0.0001,
      "step": 45870
    },
    {
      "epoch": 21.230911614993058,
      "grad_norm": 0.07473041117191315,
      "learning_rate": 0.05753817677001388,
      "loss": 0.0001,
      "step": 45880
    },
    {
      "epoch": 21.23553910226747,
      "grad_norm": 0.6529947519302368,
      "learning_rate": 0.057528921795465066,
      "loss": 0.0002,
      "step": 45890
    },
    {
      "epoch": 21.24016658954188,
      "grad_norm": 0.23573638498783112,
      "learning_rate": 0.05751966682091624,
      "loss": 0.0001,
      "step": 45900
    },
    {
      "epoch": 21.244794076816287,
      "grad_norm": 0.07085123658180237,
      "learning_rate": 0.05751041184636743,
      "loss": 0.0002,
      "step": 45910
    },
    {
      "epoch": 21.2494215640907,
      "grad_norm": 0.0006921326857991517,
      "learning_rate": 0.05750115687181861,
      "loss": 0.0004,
      "step": 45920
    },
    {
      "epoch": 21.254049051365108,
      "grad_norm": 0.00233611767180264,
      "learning_rate": 0.057491901897269784,
      "loss": 0.0001,
      "step": 45930
    },
    {
      "epoch": 21.25867653863952,
      "grad_norm": 0.0023471261374652386,
      "learning_rate": 0.05748264692272096,
      "loss": 0.0001,
      "step": 45940
    },
    {
      "epoch": 21.26330402591393,
      "grad_norm": 0.07803395390510559,
      "learning_rate": 0.05747339194817215,
      "loss": 0.0007,
      "step": 45950
    },
    {
      "epoch": 21.267931513188337,
      "grad_norm": 0.014774919487535954,
      "learning_rate": 0.05746413697362333,
      "loss": 0.0002,
      "step": 45960
    },
    {
      "epoch": 21.27255900046275,
      "grad_norm": 0.011726915836334229,
      "learning_rate": 0.0574548819990745,
      "loss": 0.0001,
      "step": 45970
    },
    {
      "epoch": 21.277186487737158,
      "grad_norm": 0.001271930756047368,
      "learning_rate": 0.057445627024525686,
      "loss": 0.0001,
      "step": 45980
    },
    {
      "epoch": 21.28181397501157,
      "grad_norm": 0.016973119229078293,
      "learning_rate": 0.05743637204997686,
      "loss": 0.0003,
      "step": 45990
    },
    {
      "epoch": 21.28644146228598,
      "grad_norm": 0.04980707913637161,
      "learning_rate": 0.05742711707542805,
      "loss": 0.0001,
      "step": 46000
    },
    {
      "epoch": 21.29106894956039,
      "grad_norm": 0.0004218214889988303,
      "learning_rate": 0.05741786210087923,
      "loss": 0.0005,
      "step": 46010
    },
    {
      "epoch": 21.2956964368348,
      "grad_norm": 0.0029478624928742647,
      "learning_rate": 0.057408607126330405,
      "loss": 0.0002,
      "step": 46020
    },
    {
      "epoch": 21.300323924109207,
      "grad_norm": 0.01879834197461605,
      "learning_rate": 0.05739935215178158,
      "loss": 0.0003,
      "step": 46030
    },
    {
      "epoch": 21.30495141138362,
      "grad_norm": 0.02226249687373638,
      "learning_rate": 0.05739009717723276,
      "loss": 0.0,
      "step": 46040
    },
    {
      "epoch": 21.309578898658028,
      "grad_norm": 0.0029085513669997454,
      "learning_rate": 0.05738084220268395,
      "loss": 0.0001,
      "step": 46050
    },
    {
      "epoch": 21.31420638593244,
      "grad_norm": 8.598321914672852,
      "learning_rate": 0.057371587228135124,
      "loss": 0.0023,
      "step": 46060
    },
    {
      "epoch": 21.31883387320685,
      "grad_norm": 0.0036280974745750427,
      "learning_rate": 0.05736233225358631,
      "loss": 0.0002,
      "step": 46070
    },
    {
      "epoch": 21.323461360481257,
      "grad_norm": 0.016280928626656532,
      "learning_rate": 0.05735307727903748,
      "loss": 0.0001,
      "step": 46080
    },
    {
      "epoch": 21.32808884775567,
      "grad_norm": 0.0015440903371199965,
      "learning_rate": 0.05734382230448867,
      "loss": 0.0003,
      "step": 46090
    },
    {
      "epoch": 21.332716335030078,
      "grad_norm": 0.003721968736499548,
      "learning_rate": 0.05733456732993985,
      "loss": 0.0001,
      "step": 46100
    },
    {
      "epoch": 21.33734382230449,
      "grad_norm": 0.018041817471385002,
      "learning_rate": 0.057325312355391025,
      "loss": 0.0002,
      "step": 46110
    },
    {
      "epoch": 21.3419713095789,
      "grad_norm": 1.3417081832885742,
      "learning_rate": 0.0573160573808422,
      "loss": 0.0003,
      "step": 46120
    },
    {
      "epoch": 21.346598796853307,
      "grad_norm": 0.006137560587376356,
      "learning_rate": 0.05730680240629338,
      "loss": 0.0001,
      "step": 46130
    },
    {
      "epoch": 21.35122628412772,
      "grad_norm": 0.010961943306028843,
      "learning_rate": 0.05729754743174457,
      "loss": 0.0002,
      "step": 46140
    },
    {
      "epoch": 21.355853771402128,
      "grad_norm": 0.0086141312494874,
      "learning_rate": 0.057288292457195744,
      "loss": 0.0001,
      "step": 46150
    },
    {
      "epoch": 21.36048125867654,
      "grad_norm": 0.0002783333184197545,
      "learning_rate": 0.05727903748264693,
      "loss": 0.0001,
      "step": 46160
    },
    {
      "epoch": 21.36510874595095,
      "grad_norm": 0.38023853302001953,
      "learning_rate": 0.057269782508098104,
      "loss": 0.0003,
      "step": 46170
    },
    {
      "epoch": 21.36973623322536,
      "grad_norm": 0.0016229351749643683,
      "learning_rate": 0.057260527533549294,
      "loss": 0.006,
      "step": 46180
    },
    {
      "epoch": 21.37436372049977,
      "grad_norm": 0.027805902063846588,
      "learning_rate": 0.05725127255900047,
      "loss": 0.0001,
      "step": 46190
    },
    {
      "epoch": 21.378991207774177,
      "grad_norm": 0.006219544913619757,
      "learning_rate": 0.057242017584451646,
      "loss": 0.0072,
      "step": 46200
    },
    {
      "epoch": 21.38361869504859,
      "grad_norm": 0.0011369631392881274,
      "learning_rate": 0.05723276260990282,
      "loss": 0.0001,
      "step": 46210
    },
    {
      "epoch": 21.388246182322998,
      "grad_norm": 0.03690642863512039,
      "learning_rate": 0.057223507635354,
      "loss": 0.0001,
      "step": 46220
    },
    {
      "epoch": 21.39287366959741,
      "grad_norm": 0.0036449970211833715,
      "learning_rate": 0.05721425266080519,
      "loss": 0.0001,
      "step": 46230
    },
    {
      "epoch": 21.39750115687182,
      "grad_norm": 0.47868311405181885,
      "learning_rate": 0.057204997686256365,
      "loss": 0.0004,
      "step": 46240
    },
    {
      "epoch": 21.402128644146227,
      "grad_norm": 0.21155980229377747,
      "learning_rate": 0.05719574271170755,
      "loss": 0.0003,
      "step": 46250
    },
    {
      "epoch": 21.40675613142064,
      "grad_norm": 0.004487443715333939,
      "learning_rate": 0.057186487737158724,
      "loss": 0.0003,
      "step": 46260
    },
    {
      "epoch": 21.411383618695048,
      "grad_norm": 0.007559651974588633,
      "learning_rate": 0.0571772327626099,
      "loss": 0.0,
      "step": 46270
    },
    {
      "epoch": 21.41601110596946,
      "grad_norm": 0.14575695991516113,
      "learning_rate": 0.05716797778806109,
      "loss": 0.0001,
      "step": 46280
    },
    {
      "epoch": 21.42063859324387,
      "grad_norm": 0.0037858313880860806,
      "learning_rate": 0.05715872281351227,
      "loss": 0.0001,
      "step": 46290
    },
    {
      "epoch": 21.425266080518277,
      "grad_norm": 0.010723195038735867,
      "learning_rate": 0.05714946783896344,
      "loss": 0.0005,
      "step": 46300
    },
    {
      "epoch": 21.42989356779269,
      "grad_norm": 0.004530816804617643,
      "learning_rate": 0.05714021286441462,
      "loss": 0.0003,
      "step": 46310
    },
    {
      "epoch": 21.434521055067098,
      "grad_norm": 0.000992947956547141,
      "learning_rate": 0.05713095788986581,
      "loss": 0.0003,
      "step": 46320
    },
    {
      "epoch": 21.43914854234151,
      "grad_norm": 0.0005301525234244764,
      "learning_rate": 0.057121702915316985,
      "loss": 0.0002,
      "step": 46330
    },
    {
      "epoch": 21.44377602961592,
      "grad_norm": 0.0005312219727784395,
      "learning_rate": 0.05711244794076817,
      "loss": 0.0112,
      "step": 46340
    },
    {
      "epoch": 21.448403516890327,
      "grad_norm": 0.012767555192112923,
      "learning_rate": 0.057103192966219345,
      "loss": 0.0001,
      "step": 46350
    },
    {
      "epoch": 21.45303100416474,
      "grad_norm": 0.04963980242609978,
      "learning_rate": 0.05709393799167052,
      "loss": 0.0001,
      "step": 46360
    },
    {
      "epoch": 21.457658491439147,
      "grad_norm": 0.007121826056391001,
      "learning_rate": 0.05708468301712171,
      "loss": 0.0001,
      "step": 46370
    },
    {
      "epoch": 21.46228597871356,
      "grad_norm": 0.028720693662762642,
      "learning_rate": 0.05707542804257289,
      "loss": 0.0002,
      "step": 46380
    },
    {
      "epoch": 21.466913465987968,
      "grad_norm": 0.2043907344341278,
      "learning_rate": 0.05706617306802406,
      "loss": 0.0002,
      "step": 46390
    },
    {
      "epoch": 21.47154095326238,
      "grad_norm": 0.004018442239612341,
      "learning_rate": 0.05705691809347524,
      "loss": 0.0001,
      "step": 46400
    },
    {
      "epoch": 21.47616844053679,
      "grad_norm": 0.01232115738093853,
      "learning_rate": 0.05704766311892643,
      "loss": 0.0001,
      "step": 46410
    },
    {
      "epoch": 21.480795927811197,
      "grad_norm": 0.016025329008698463,
      "learning_rate": 0.057038408144377606,
      "loss": 0.0001,
      "step": 46420
    },
    {
      "epoch": 21.48542341508561,
      "grad_norm": 0.0026225880719721317,
      "learning_rate": 0.05702915316982879,
      "loss": 0.0002,
      "step": 46430
    },
    {
      "epoch": 21.490050902360018,
      "grad_norm": 0.007605481427162886,
      "learning_rate": 0.057019898195279965,
      "loss": 0.0002,
      "step": 46440
    },
    {
      "epoch": 21.49467838963443,
      "grad_norm": 0.025303885340690613,
      "learning_rate": 0.05701064322073114,
      "loss": 0.0002,
      "step": 46450
    },
    {
      "epoch": 21.49930587690884,
      "grad_norm": 0.0017089620232582092,
      "learning_rate": 0.05700138824618233,
      "loss": 0.0003,
      "step": 46460
    },
    {
      "epoch": 21.503933364183247,
      "grad_norm": 0.007498312741518021,
      "learning_rate": 0.05699213327163351,
      "loss": 0.0018,
      "step": 46470
    },
    {
      "epoch": 21.50856085145766,
      "grad_norm": 0.004079726524651051,
      "learning_rate": 0.056982878297084684,
      "loss": 0.0003,
      "step": 46480
    },
    {
      "epoch": 21.513188338732068,
      "grad_norm": 0.0007711654179729521,
      "learning_rate": 0.05697362332253586,
      "loss": 0.0001,
      "step": 46490
    },
    {
      "epoch": 21.51781582600648,
      "grad_norm": 0.001010686275549233,
      "learning_rate": 0.05696436834798704,
      "loss": 0.0001,
      "step": 46500
    },
    {
      "epoch": 21.52244331328089,
      "grad_norm": 0.0028003398329019547,
      "learning_rate": 0.056955113373438226,
      "loss": 0.0002,
      "step": 46510
    },
    {
      "epoch": 21.527070800555297,
      "grad_norm": 0.0765642523765564,
      "learning_rate": 0.05694585839888941,
      "loss": 0.0001,
      "step": 46520
    },
    {
      "epoch": 21.53169828782971,
      "grad_norm": 0.03643007203936577,
      "learning_rate": 0.056936603424340586,
      "loss": 0.0001,
      "step": 46530
    },
    {
      "epoch": 21.536325775104117,
      "grad_norm": 0.005158359184861183,
      "learning_rate": 0.05692734844979176,
      "loss": 0.0,
      "step": 46540
    },
    {
      "epoch": 21.54095326237853,
      "grad_norm": 0.0005823519313707948,
      "learning_rate": 0.05691809347524295,
      "loss": 0.0001,
      "step": 46550
    },
    {
      "epoch": 21.545580749652938,
      "grad_norm": 0.008927124552428722,
      "learning_rate": 0.05690883850069413,
      "loss": 0.0001,
      "step": 46560
    },
    {
      "epoch": 21.550208236927347,
      "grad_norm": 0.007503223605453968,
      "learning_rate": 0.056899583526145305,
      "loss": 0.0002,
      "step": 46570
    },
    {
      "epoch": 21.55483572420176,
      "grad_norm": 0.002415055874735117,
      "learning_rate": 0.05689032855159648,
      "loss": 0.0001,
      "step": 46580
    },
    {
      "epoch": 21.559463211476167,
      "grad_norm": 0.015664856880903244,
      "learning_rate": 0.056881073577047664,
      "loss": 0.0001,
      "step": 46590
    },
    {
      "epoch": 21.56409069875058,
      "grad_norm": 0.27183106541633606,
      "learning_rate": 0.05687181860249885,
      "loss": 0.0004,
      "step": 46600
    },
    {
      "epoch": 21.568718186024988,
      "grad_norm": 0.002487289486452937,
      "learning_rate": 0.05686256362795003,
      "loss": 0.0001,
      "step": 46610
    },
    {
      "epoch": 21.5733456732994,
      "grad_norm": 0.05871523171663284,
      "learning_rate": 0.056853308653401206,
      "loss": 0.006,
      "step": 46620
    },
    {
      "epoch": 21.57797316057381,
      "grad_norm": 0.016479233279824257,
      "learning_rate": 0.05684405367885238,
      "loss": 0.0001,
      "step": 46630
    },
    {
      "epoch": 21.582600647848217,
      "grad_norm": 0.010754945687949657,
      "learning_rate": 0.05683479870430357,
      "loss": 0.0001,
      "step": 46640
    },
    {
      "epoch": 21.58722813512263,
      "grad_norm": 0.008424297906458378,
      "learning_rate": 0.05682554372975475,
      "loss": 0.0099,
      "step": 46650
    },
    {
      "epoch": 21.591855622397038,
      "grad_norm": 0.0003072014660574496,
      "learning_rate": 0.056816288755205925,
      "loss": 0.0001,
      "step": 46660
    },
    {
      "epoch": 21.59648310967145,
      "grad_norm": 0.002286296570673585,
      "learning_rate": 0.0568070337806571,
      "loss": 0.0001,
      "step": 46670
    },
    {
      "epoch": 21.60111059694586,
      "grad_norm": 0.25694403052330017,
      "learning_rate": 0.056797778806108284,
      "loss": 0.0003,
      "step": 46680
    },
    {
      "epoch": 21.605738084220267,
      "grad_norm": 0.004243341740220785,
      "learning_rate": 0.05678852383155947,
      "loss": 0.0,
      "step": 46690
    },
    {
      "epoch": 21.61036557149468,
      "grad_norm": 0.0051035201177001,
      "learning_rate": 0.05677926885701065,
      "loss": 0.0002,
      "step": 46700
    },
    {
      "epoch": 21.614993058769087,
      "grad_norm": 0.001306123798713088,
      "learning_rate": 0.05677001388246183,
      "loss": 0.0001,
      "step": 46710
    },
    {
      "epoch": 21.6196205460435,
      "grad_norm": 0.09801120311021805,
      "learning_rate": 0.056760758907913,
      "loss": 0.0001,
      "step": 46720
    },
    {
      "epoch": 21.624248033317908,
      "grad_norm": 0.0826076790690422,
      "learning_rate": 0.05675150393336418,
      "loss": 0.0004,
      "step": 46730
    },
    {
      "epoch": 21.628875520592317,
      "grad_norm": 0.020240221172571182,
      "learning_rate": 0.05674224895881537,
      "loss": 0.0002,
      "step": 46740
    },
    {
      "epoch": 21.63350300786673,
      "grad_norm": 0.001217702403664589,
      "learning_rate": 0.056732993984266546,
      "loss": 0.0,
      "step": 46750
    },
    {
      "epoch": 21.638130495141137,
      "grad_norm": 0.0028283074498176575,
      "learning_rate": 0.05672373900971772,
      "loss": 0.0002,
      "step": 46760
    },
    {
      "epoch": 21.64275798241555,
      "grad_norm": 0.01226294506341219,
      "learning_rate": 0.056714484035168905,
      "loss": 0.0002,
      "step": 46770
    },
    {
      "epoch": 21.647385469689958,
      "grad_norm": 0.0006187164690345526,
      "learning_rate": 0.05670522906062009,
      "loss": 0.0002,
      "step": 46780
    },
    {
      "epoch": 21.65201295696437,
      "grad_norm": 0.0054347217082977295,
      "learning_rate": 0.05669597408607127,
      "loss": 0.0001,
      "step": 46790
    },
    {
      "epoch": 21.65664044423878,
      "grad_norm": 0.01700199954211712,
      "learning_rate": 0.05668671911152245,
      "loss": 0.0001,
      "step": 46800
    },
    {
      "epoch": 21.661267931513187,
      "grad_norm": 0.018882550299167633,
      "learning_rate": 0.056677464136973624,
      "loss": 0.0002,
      "step": 46810
    },
    {
      "epoch": 21.6658954187876,
      "grad_norm": 0.003995956853032112,
      "learning_rate": 0.0566682091624248,
      "loss": 0.0001,
      "step": 46820
    },
    {
      "epoch": 21.670522906062008,
      "grad_norm": 0.008255352266132832,
      "learning_rate": 0.05665895418787599,
      "loss": 0.0001,
      "step": 46830
    },
    {
      "epoch": 21.67515039333642,
      "grad_norm": 0.00019276529201306403,
      "learning_rate": 0.056649699213327166,
      "loss": 0.0001,
      "step": 46840
    },
    {
      "epoch": 21.67977788061083,
      "grad_norm": 0.0013656220398843288,
      "learning_rate": 0.05664044423877834,
      "loss": 0.0,
      "step": 46850
    },
    {
      "epoch": 21.684405367885237,
      "grad_norm": 1.6383700370788574,
      "learning_rate": 0.056631189264229526,
      "loss": 0.0039,
      "step": 46860
    },
    {
      "epoch": 21.68903285515965,
      "grad_norm": 0.019870270043611526,
      "learning_rate": 0.05662193428968071,
      "loss": 0.0001,
      "step": 46870
    },
    {
      "epoch": 21.693660342434057,
      "grad_norm": 0.011563107371330261,
      "learning_rate": 0.05661267931513189,
      "loss": 0.0003,
      "step": 46880
    },
    {
      "epoch": 21.69828782970847,
      "grad_norm": 0.004019131418317556,
      "learning_rate": 0.05660342434058307,
      "loss": 0.0002,
      "step": 46890
    },
    {
      "epoch": 21.702915316982878,
      "grad_norm": 0.1389746516942978,
      "learning_rate": 0.056594169366034244,
      "loss": 0.0003,
      "step": 46900
    },
    {
      "epoch": 21.707542804257287,
      "grad_norm": 0.11596377193927765,
      "learning_rate": 0.05658491439148542,
      "loss": 0.0003,
      "step": 46910
    },
    {
      "epoch": 21.7121702915317,
      "grad_norm": 0.005919108632951975,
      "learning_rate": 0.05657565941693661,
      "loss": 0.0001,
      "step": 46920
    },
    {
      "epoch": 21.716797778806107,
      "grad_norm": 0.007560666650533676,
      "learning_rate": 0.05656640444238779,
      "loss": 0.0005,
      "step": 46930
    },
    {
      "epoch": 21.72142526608052,
      "grad_norm": 0.00744035467505455,
      "learning_rate": 0.05655714946783896,
      "loss": 0.0001,
      "step": 46940
    },
    {
      "epoch": 21.726052753354928,
      "grad_norm": 0.00396905280649662,
      "learning_rate": 0.056547894493290146,
      "loss": 0.0,
      "step": 46950
    },
    {
      "epoch": 21.73068024062934,
      "grad_norm": 0.06422925740480423,
      "learning_rate": 0.05653863951874132,
      "loss": 0.0001,
      "step": 46960
    },
    {
      "epoch": 21.73530772790375,
      "grad_norm": 0.007693497929722071,
      "learning_rate": 0.05652938454419251,
      "loss": 0.0007,
      "step": 46970
    },
    {
      "epoch": 21.739935215178157,
      "grad_norm": 0.038022153079509735,
      "learning_rate": 0.05652012956964369,
      "loss": 0.0002,
      "step": 46980
    },
    {
      "epoch": 21.74456270245257,
      "grad_norm": 0.0037765081506222486,
      "learning_rate": 0.056510874595094865,
      "loss": 0.0001,
      "step": 46990
    },
    {
      "epoch": 21.749190189726978,
      "grad_norm": 0.0017455448396503925,
      "learning_rate": 0.05650161962054604,
      "loss": 0.0002,
      "step": 47000
    },
    {
      "epoch": 21.75381767700139,
      "grad_norm": 0.01045465748757124,
      "learning_rate": 0.05649236464599723,
      "loss": 0.0001,
      "step": 47010
    },
    {
      "epoch": 21.7584451642758,
      "grad_norm": 0.0002769734710454941,
      "learning_rate": 0.05648310967144841,
      "loss": 0.0001,
      "step": 47020
    },
    {
      "epoch": 21.763072651550207,
      "grad_norm": 0.2386264055967331,
      "learning_rate": 0.056473854696899584,
      "loss": 0.0004,
      "step": 47030
    },
    {
      "epoch": 21.76770013882462,
      "grad_norm": 0.004173845052719116,
      "learning_rate": 0.05646459972235077,
      "loss": 0.0001,
      "step": 47040
    },
    {
      "epoch": 21.772327626099027,
      "grad_norm": 0.00771197397261858,
      "learning_rate": 0.05645534474780194,
      "loss": 0.0008,
      "step": 47050
    },
    {
      "epoch": 21.77695511337344,
      "grad_norm": 0.0008603668538853526,
      "learning_rate": 0.05644608977325313,
      "loss": 0.0001,
      "step": 47060
    },
    {
      "epoch": 21.781582600647848,
      "grad_norm": 0.0018229453125968575,
      "learning_rate": 0.05643683479870431,
      "loss": 0.0001,
      "step": 47070
    },
    {
      "epoch": 21.786210087922257,
      "grad_norm": 0.00520545756444335,
      "learning_rate": 0.056427579824155485,
      "loss": 0.0002,
      "step": 47080
    },
    {
      "epoch": 21.79083757519667,
      "grad_norm": 0.005128710996359587,
      "learning_rate": 0.05641832484960666,
      "loss": 0.0005,
      "step": 47090
    },
    {
      "epoch": 21.795465062471077,
      "grad_norm": 0.049532700330019,
      "learning_rate": 0.05640906987505785,
      "loss": 0.0001,
      "step": 47100
    },
    {
      "epoch": 21.80009254974549,
      "grad_norm": 0.01677960716187954,
      "learning_rate": 0.05639981490050903,
      "loss": 0.0001,
      "step": 47110
    },
    {
      "epoch": 21.804720037019898,
      "grad_norm": 0.005165382754057646,
      "learning_rate": 0.056390559925960204,
      "loss": 0.0001,
      "step": 47120
    },
    {
      "epoch": 21.80934752429431,
      "grad_norm": 0.0028626855928450823,
      "learning_rate": 0.05638130495141139,
      "loss": 0.0001,
      "step": 47130
    },
    {
      "epoch": 21.81397501156872,
      "grad_norm": 0.012205859646201134,
      "learning_rate": 0.056372049976862564,
      "loss": 0.0001,
      "step": 47140
    },
    {
      "epoch": 21.818602498843127,
      "grad_norm": 0.06681222468614578,
      "learning_rate": 0.056362795002313754,
      "loss": 0.0002,
      "step": 47150
    },
    {
      "epoch": 21.82322998611754,
      "grad_norm": 0.0016174439806491137,
      "learning_rate": 0.05635354002776493,
      "loss": 0.0017,
      "step": 47160
    },
    {
      "epoch": 21.827857473391948,
      "grad_norm": 0.06143360212445259,
      "learning_rate": 0.056344285053216106,
      "loss": 0.0001,
      "step": 47170
    },
    {
      "epoch": 21.83248496066636,
      "grad_norm": 0.0008804058888927102,
      "learning_rate": 0.05633503007866728,
      "loss": 0.0001,
      "step": 47180
    },
    {
      "epoch": 21.83711244794077,
      "grad_norm": 0.005277085117995739,
      "learning_rate": 0.05632577510411846,
      "loss": 0.0006,
      "step": 47190
    },
    {
      "epoch": 21.841739935215177,
      "grad_norm": 0.007968677207827568,
      "learning_rate": 0.05631652012956965,
      "loss": 0.0001,
      "step": 47200
    },
    {
      "epoch": 21.84636742248959,
      "grad_norm": 0.0015218426706269383,
      "learning_rate": 0.056307265155020825,
      "loss": 0.0,
      "step": 47210
    },
    {
      "epoch": 21.850994909763997,
      "grad_norm": 0.024384042248129845,
      "learning_rate": 0.05629801018047201,
      "loss": 0.0002,
      "step": 47220
    },
    {
      "epoch": 21.85562239703841,
      "grad_norm": 0.008221215568482876,
      "learning_rate": 0.056288755205923184,
      "loss": 0.0,
      "step": 47230
    },
    {
      "epoch": 21.860249884312818,
      "grad_norm": 0.0025933871511369944,
      "learning_rate": 0.056279500231374374,
      "loss": 0.0002,
      "step": 47240
    },
    {
      "epoch": 21.864877371587227,
      "grad_norm": 0.01585085690021515,
      "learning_rate": 0.05627024525682555,
      "loss": 0.0003,
      "step": 47250
    },
    {
      "epoch": 21.86950485886164,
      "grad_norm": 0.7248743176460266,
      "learning_rate": 0.05626099028227673,
      "loss": 0.0002,
      "step": 47260
    },
    {
      "epoch": 21.874132346136047,
      "grad_norm": 0.0022991292644292116,
      "learning_rate": 0.0562517353077279,
      "loss": 0.0003,
      "step": 47270
    },
    {
      "epoch": 21.87875983341046,
      "grad_norm": 0.0037475377321243286,
      "learning_rate": 0.05624248033317908,
      "loss": 0.0002,
      "step": 47280
    },
    {
      "epoch": 21.883387320684868,
      "grad_norm": 0.0005641794996336102,
      "learning_rate": 0.05623322535863027,
      "loss": 0.0,
      "step": 47290
    },
    {
      "epoch": 21.888014807959276,
      "grad_norm": 0.00033466413151472807,
      "learning_rate": 0.056223970384081445,
      "loss": 0.0001,
      "step": 47300
    },
    {
      "epoch": 21.89264229523369,
      "grad_norm": 0.0018470081267878413,
      "learning_rate": 0.05621471540953263,
      "loss": 0.0002,
      "step": 47310
    },
    {
      "epoch": 21.897269782508097,
      "grad_norm": 0.0385885015130043,
      "learning_rate": 0.056205460434983805,
      "loss": 0.0013,
      "step": 47320
    },
    {
      "epoch": 21.90189726978251,
      "grad_norm": 0.016519226133823395,
      "learning_rate": 0.056196205460434995,
      "loss": 0.0002,
      "step": 47330
    },
    {
      "epoch": 21.906524757056918,
      "grad_norm": 0.1659288853406906,
      "learning_rate": 0.05618695048588617,
      "loss": 0.0001,
      "step": 47340
    },
    {
      "epoch": 21.91115224433133,
      "grad_norm": 0.002085441956296563,
      "learning_rate": 0.05617769551133735,
      "loss": 0.0001,
      "step": 47350
    },
    {
      "epoch": 21.91577973160574,
      "grad_norm": 0.0007375860004685819,
      "learning_rate": 0.05616844053678852,
      "loss": 0.0001,
      "step": 47360
    },
    {
      "epoch": 21.920407218880147,
      "grad_norm": 0.006647980771958828,
      "learning_rate": 0.0561591855622397,
      "loss": 0.0078,
      "step": 47370
    },
    {
      "epoch": 21.92503470615456,
      "grad_norm": 0.10110831260681152,
      "learning_rate": 0.05614993058769089,
      "loss": 0.0001,
      "step": 47380
    },
    {
      "epoch": 21.929662193428967,
      "grad_norm": 0.040889572352170944,
      "learning_rate": 0.056140675613142066,
      "loss": 0.0001,
      "step": 47390
    },
    {
      "epoch": 21.93428968070338,
      "grad_norm": 0.012614909559488297,
      "learning_rate": 0.05613142063859325,
      "loss": 0.0003,
      "step": 47400
    },
    {
      "epoch": 21.938917167977788,
      "grad_norm": 0.00043311133049428463,
      "learning_rate": 0.056122165664044425,
      "loss": 0.0001,
      "step": 47410
    },
    {
      "epoch": 21.943544655252197,
      "grad_norm": 0.023958072066307068,
      "learning_rate": 0.0561129106894956,
      "loss": 0.0001,
      "step": 47420
    },
    {
      "epoch": 21.94817214252661,
      "grad_norm": 0.037290506064891815,
      "learning_rate": 0.05610365571494679,
      "loss": 0.0003,
      "step": 47430
    },
    {
      "epoch": 21.952799629801017,
      "grad_norm": 0.12769140303134918,
      "learning_rate": 0.05609440074039797,
      "loss": 0.0001,
      "step": 47440
    },
    {
      "epoch": 21.95742711707543,
      "grad_norm": 0.0005340998177416623,
      "learning_rate": 0.056085145765849144,
      "loss": 0.0001,
      "step": 47450
    },
    {
      "epoch": 21.962054604349838,
      "grad_norm": 0.027541717514395714,
      "learning_rate": 0.05607589079130032,
      "loss": 0.0001,
      "step": 47460
    },
    {
      "epoch": 21.966682091624246,
      "grad_norm": 0.007041723933070898,
      "learning_rate": 0.05606663581675151,
      "loss": 0.0002,
      "step": 47470
    },
    {
      "epoch": 21.97130957889866,
      "grad_norm": 0.013860972598195076,
      "learning_rate": 0.056057380842202686,
      "loss": 0.0002,
      "step": 47480
    },
    {
      "epoch": 21.975937066173067,
      "grad_norm": 0.0013402767945080996,
      "learning_rate": 0.05604812586765387,
      "loss": 0.0001,
      "step": 47490
    },
    {
      "epoch": 21.98056455344748,
      "grad_norm": 0.12077893316745758,
      "learning_rate": 0.056038870893105046,
      "loss": 0.0001,
      "step": 47500
    },
    {
      "epoch": 21.985192040721888,
      "grad_norm": 0.015355522744357586,
      "learning_rate": 0.05602961591855622,
      "loss": 0.0015,
      "step": 47510
    },
    {
      "epoch": 21.989819527996296,
      "grad_norm": 0.0037221533711999655,
      "learning_rate": 0.05602036094400741,
      "loss": 0.0001,
      "step": 47520
    },
    {
      "epoch": 21.99444701527071,
      "grad_norm": 0.0016634662169963121,
      "learning_rate": 0.05601110596945859,
      "loss": 0.0002,
      "step": 47530
    },
    {
      "epoch": 21.999074502545117,
      "grad_norm": 0.0031426523346453905,
      "learning_rate": 0.056001850994909765,
      "loss": 0.0001,
      "step": 47540
    },
    {
      "epoch": 22.0,
      "eval_accuracy_branch1": 0.9877522723771376,
      "eval_accuracy_branch2": 0.4996148513326144,
      "eval_f1_branch1": 0.9885137908567565,
      "eval_f1_branch2": 0.49831817067416495,
      "eval_loss": 0.026179932057857513,
      "eval_precision_branch1": 0.9888298884379942,
      "eval_precision_branch2": 0.49961082780995814,
      "eval_recall_branch1": 0.9884003781508391,
      "eval_recall_branch2": 0.4996148513326144,
      "eval_runtime": 38.2498,
      "eval_samples_per_second": 339.401,
      "eval_steps_per_second": 42.432,
      "step": 47542
    },
    {
      "epoch": 22.00370198981953,
      "grad_norm": 0.014275714755058289,
      "learning_rate": 0.05599259602036094,
      "loss": 0.1209,
      "step": 47550
    },
    {
      "epoch": 22.008329477093937,
      "grad_norm": 0.009544024243950844,
      "learning_rate": 0.05598334104581213,
      "loss": 0.0002,
      "step": 47560
    },
    {
      "epoch": 22.01295696436835,
      "grad_norm": 0.03334521874785423,
      "learning_rate": 0.05597408607126331,
      "loss": 0.0001,
      "step": 47570
    },
    {
      "epoch": 22.017584451642758,
      "grad_norm": 0.014766485430300236,
      "learning_rate": 0.05596483109671449,
      "loss": 0.0001,
      "step": 47580
    },
    {
      "epoch": 22.022211938917167,
      "grad_norm": 0.10867829620838165,
      "learning_rate": 0.055955576122165666,
      "loss": 0.0002,
      "step": 47590
    },
    {
      "epoch": 22.02683942619158,
      "grad_norm": 0.002407716354355216,
      "learning_rate": 0.05594632114761684,
      "loss": 0.0002,
      "step": 47600
    },
    {
      "epoch": 22.031466913465987,
      "grad_norm": 0.011673993431031704,
      "learning_rate": 0.05593706617306803,
      "loss": 0.0001,
      "step": 47610
    },
    {
      "epoch": 22.0360944007404,
      "grad_norm": 0.017301319167017937,
      "learning_rate": 0.05592781119851921,
      "loss": 0.0002,
      "step": 47620
    },
    {
      "epoch": 22.040721888014808,
      "grad_norm": 0.008838660083711147,
      "learning_rate": 0.055918556223970385,
      "loss": 0.0003,
      "step": 47630
    },
    {
      "epoch": 22.045349375289216,
      "grad_norm": 0.0020047470461577177,
      "learning_rate": 0.05590930124942156,
      "loss": 0.001,
      "step": 47640
    },
    {
      "epoch": 22.04997686256363,
      "grad_norm": 0.0009146188967861235,
      "learning_rate": 0.055900046274872744,
      "loss": 0.0001,
      "step": 47650
    },
    {
      "epoch": 22.054604349838037,
      "grad_norm": 0.14623640477657318,
      "learning_rate": 0.05589079130032393,
      "loss": 0.0001,
      "step": 47660
    },
    {
      "epoch": 22.05923183711245,
      "grad_norm": 0.5965056419372559,
      "learning_rate": 0.05588153632577511,
      "loss": 0.0001,
      "step": 47670
    },
    {
      "epoch": 22.063859324386858,
      "grad_norm": 0.01001589186489582,
      "learning_rate": 0.05587228135122629,
      "loss": 0.0001,
      "step": 47680
    },
    {
      "epoch": 22.068486811661266,
      "grad_norm": 0.08214625716209412,
      "learning_rate": 0.05586302637667746,
      "loss": 0.0015,
      "step": 47690
    },
    {
      "epoch": 22.07311429893568,
      "grad_norm": 0.0031335623934865,
      "learning_rate": 0.05585377140212865,
      "loss": 0.0002,
      "step": 47700
    },
    {
      "epoch": 22.077741786210087,
      "grad_norm": 0.001439473475329578,
      "learning_rate": 0.05584451642757983,
      "loss": 0.0022,
      "step": 47710
    },
    {
      "epoch": 22.0823692734845,
      "grad_norm": 0.005090018268674612,
      "learning_rate": 0.055835261453031006,
      "loss": 0.0003,
      "step": 47720
    },
    {
      "epoch": 22.086996760758907,
      "grad_norm": 0.03914175182580948,
      "learning_rate": 0.05582600647848218,
      "loss": 0.0001,
      "step": 47730
    },
    {
      "epoch": 22.09162424803332,
      "grad_norm": 0.0054666949436068535,
      "learning_rate": 0.055816751503933365,
      "loss": 0.0002,
      "step": 47740
    },
    {
      "epoch": 22.096251735307728,
      "grad_norm": 0.0016215805662795901,
      "learning_rate": 0.05580749652938455,
      "loss": 0.0001,
      "step": 47750
    },
    {
      "epoch": 22.100879222582137,
      "grad_norm": 0.0037341865245252848,
      "learning_rate": 0.05579824155483573,
      "loss": 0.0001,
      "step": 47760
    },
    {
      "epoch": 22.10550670985655,
      "grad_norm": 0.021757369861006737,
      "learning_rate": 0.05578898658028691,
      "loss": 0.0001,
      "step": 47770
    },
    {
      "epoch": 22.110134197130957,
      "grad_norm": 0.004205813631415367,
      "learning_rate": 0.055779731605738084,
      "loss": 0.0001,
      "step": 47780
    },
    {
      "epoch": 22.11476168440537,
      "grad_norm": 0.005167149938642979,
      "learning_rate": 0.055770476631189274,
      "loss": 0.0,
      "step": 47790
    },
    {
      "epoch": 22.119389171679778,
      "grad_norm": 0.001652686158195138,
      "learning_rate": 0.05576122165664045,
      "loss": 0.0001,
      "step": 47800
    },
    {
      "epoch": 22.124016658954186,
      "grad_norm": 0.0023983537685126066,
      "learning_rate": 0.055751966682091626,
      "loss": 0.0001,
      "step": 47810
    },
    {
      "epoch": 22.1286441462286,
      "grad_norm": 0.0022296220995485783,
      "learning_rate": 0.0557427117075428,
      "loss": 0.0001,
      "step": 47820
    },
    {
      "epoch": 22.133271633503007,
      "grad_norm": 0.01088025327771902,
      "learning_rate": 0.055733456732993986,
      "loss": 0.0002,
      "step": 47830
    },
    {
      "epoch": 22.13789912077742,
      "grad_norm": 0.156751811504364,
      "learning_rate": 0.05572420175844517,
      "loss": 0.0002,
      "step": 47840
    },
    {
      "epoch": 22.142526608051828,
      "grad_norm": 0.002417475450783968,
      "learning_rate": 0.05571494678389635,
      "loss": 0.0007,
      "step": 47850
    },
    {
      "epoch": 22.147154095326236,
      "grad_norm": 0.0023846544791013002,
      "learning_rate": 0.05570569180934753,
      "loss": 0.0001,
      "step": 47860
    },
    {
      "epoch": 22.15178158260065,
      "grad_norm": 0.013137508183717728,
      "learning_rate": 0.055696436834798704,
      "loss": 0.0004,
      "step": 47870
    },
    {
      "epoch": 22.156409069875057,
      "grad_norm": 0.005920125637203455,
      "learning_rate": 0.05568718186024988,
      "loss": 0.0023,
      "step": 47880
    },
    {
      "epoch": 22.16103655714947,
      "grad_norm": 0.0329730249941349,
      "learning_rate": 0.05567792688570107,
      "loss": 0.0005,
      "step": 47890
    },
    {
      "epoch": 22.165664044423877,
      "grad_norm": 0.004927363246679306,
      "learning_rate": 0.05566867191115225,
      "loss": 0.0001,
      "step": 47900
    },
    {
      "epoch": 22.17029153169829,
      "grad_norm": 0.012889477424323559,
      "learning_rate": 0.05565941693660342,
      "loss": 0.0001,
      "step": 47910
    },
    {
      "epoch": 22.174919018972698,
      "grad_norm": 0.003126152092590928,
      "learning_rate": 0.055650161962054606,
      "loss": 0.0004,
      "step": 47920
    },
    {
      "epoch": 22.179546506247107,
      "grad_norm": 0.015179540030658245,
      "learning_rate": 0.05564090698750579,
      "loss": 0.0001,
      "step": 47930
    },
    {
      "epoch": 22.18417399352152,
      "grad_norm": 0.02572002448141575,
      "learning_rate": 0.05563165201295697,
      "loss": 0.0005,
      "step": 47940
    },
    {
      "epoch": 22.188801480795927,
      "grad_norm": 0.029463423416018486,
      "learning_rate": 0.05562239703840815,
      "loss": 0.0001,
      "step": 47950
    },
    {
      "epoch": 22.19342896807034,
      "grad_norm": 0.015702031552791595,
      "learning_rate": 0.055613142063859325,
      "loss": 0.0001,
      "step": 47960
    },
    {
      "epoch": 22.198056455344748,
      "grad_norm": 0.013516657054424286,
      "learning_rate": 0.0556038870893105,
      "loss": 0.0002,
      "step": 47970
    },
    {
      "epoch": 22.202683942619156,
      "grad_norm": 0.00579489441588521,
      "learning_rate": 0.05559463211476169,
      "loss": 0.0,
      "step": 47980
    },
    {
      "epoch": 22.20731142989357,
      "grad_norm": 0.007297690026462078,
      "learning_rate": 0.05558537714021287,
      "loss": 0.0002,
      "step": 47990
    },
    {
      "epoch": 22.211938917167977,
      "grad_norm": 0.0010147696593776345,
      "learning_rate": 0.055576122165664044,
      "loss": 0.0,
      "step": 48000
    },
    {
      "epoch": 22.21656640444239,
      "grad_norm": 0.00167061947286129,
      "learning_rate": 0.05556686719111523,
      "loss": 0.0001,
      "step": 48010
    },
    {
      "epoch": 22.221193891716798,
      "grad_norm": 0.007067626807838678,
      "learning_rate": 0.05555761221656641,
      "loss": 0.0001,
      "step": 48020
    },
    {
      "epoch": 22.225821378991206,
      "grad_norm": 0.0012923363829031587,
      "learning_rate": 0.05554835724201759,
      "loss": 0.0001,
      "step": 48030
    },
    {
      "epoch": 22.23044886626562,
      "grad_norm": 0.0038296303246170282,
      "learning_rate": 0.05553910226746877,
      "loss": 0.0002,
      "step": 48040
    },
    {
      "epoch": 22.235076353540027,
      "grad_norm": 0.001387140597216785,
      "learning_rate": 0.055529847292919945,
      "loss": 0.0001,
      "step": 48050
    },
    {
      "epoch": 22.23970384081444,
      "grad_norm": 0.007462599780410528,
      "learning_rate": 0.05552059231837112,
      "loss": 0.0,
      "step": 48060
    },
    {
      "epoch": 22.244331328088847,
      "grad_norm": 0.03634370118379593,
      "learning_rate": 0.05551133734382231,
      "loss": 0.0006,
      "step": 48070
    },
    {
      "epoch": 22.248958815363256,
      "grad_norm": 0.014770551584661007,
      "learning_rate": 0.05550208236927349,
      "loss": 0.0001,
      "step": 48080
    },
    {
      "epoch": 22.253586302637668,
      "grad_norm": 0.011484985239803791,
      "learning_rate": 0.055492827394724664,
      "loss": 0.0,
      "step": 48090
    },
    {
      "epoch": 22.258213789912077,
      "grad_norm": 0.018873728811740875,
      "learning_rate": 0.05548357242017585,
      "loss": 0.0001,
      "step": 48100
    },
    {
      "epoch": 22.26284127718649,
      "grad_norm": 0.00727782491594553,
      "learning_rate": 0.055474317445627024,
      "loss": 0.0002,
      "step": 48110
    },
    {
      "epoch": 22.267468764460897,
      "grad_norm": 0.013697434216737747,
      "learning_rate": 0.055465062471078214,
      "loss": 0.0001,
      "step": 48120
    },
    {
      "epoch": 22.27209625173531,
      "grad_norm": 0.01693052612245083,
      "learning_rate": 0.05545580749652939,
      "loss": 0.0001,
      "step": 48130
    },
    {
      "epoch": 22.276723739009718,
      "grad_norm": 0.0679256021976471,
      "learning_rate": 0.055446552521980566,
      "loss": 0.0001,
      "step": 48140
    },
    {
      "epoch": 22.281351226284126,
      "grad_norm": 0.0003944145573768765,
      "learning_rate": 0.05543729754743174,
      "loss": 0.0001,
      "step": 48150
    },
    {
      "epoch": 22.28597871355854,
      "grad_norm": 0.026505447924137115,
      "learning_rate": 0.05542804257288293,
      "loss": 0.0001,
      "step": 48160
    },
    {
      "epoch": 22.290606200832947,
      "grad_norm": 0.004603513982146978,
      "learning_rate": 0.05541878759833411,
      "loss": 0.0001,
      "step": 48170
    },
    {
      "epoch": 22.29523368810736,
      "grad_norm": 0.003914276137948036,
      "learning_rate": 0.055409532623785285,
      "loss": 0.0003,
      "step": 48180
    },
    {
      "epoch": 22.299861175381768,
      "grad_norm": 0.041707657277584076,
      "learning_rate": 0.05540027764923647,
      "loss": 0.0001,
      "step": 48190
    },
    {
      "epoch": 22.304488662656176,
      "grad_norm": 0.0009533876436762512,
      "learning_rate": 0.055391022674687644,
      "loss": 0.0016,
      "step": 48200
    },
    {
      "epoch": 22.30911614993059,
      "grad_norm": 0.012280293740332127,
      "learning_rate": 0.055381767700138834,
      "loss": 0.0018,
      "step": 48210
    },
    {
      "epoch": 22.313743637204997,
      "grad_norm": 0.027712902054190636,
      "learning_rate": 0.05537251272559001,
      "loss": 0.0002,
      "step": 48220
    },
    {
      "epoch": 22.31837112447941,
      "grad_norm": 0.1733824759721756,
      "learning_rate": 0.05536325775104119,
      "loss": 0.0001,
      "step": 48230
    },
    {
      "epoch": 22.322998611753817,
      "grad_norm": 0.007029843050986528,
      "learning_rate": 0.05535400277649236,
      "loss": 0.0001,
      "step": 48240
    },
    {
      "epoch": 22.327626099028226,
      "grad_norm": 0.02035374566912651,
      "learning_rate": 0.05534474780194355,
      "loss": 0.0003,
      "step": 48250
    },
    {
      "epoch": 22.332253586302638,
      "grad_norm": 0.007414249703288078,
      "learning_rate": 0.05533549282739473,
      "loss": 0.0001,
      "step": 48260
    },
    {
      "epoch": 22.336881073577047,
      "grad_norm": 0.033016789704561234,
      "learning_rate": 0.055326237852845905,
      "loss": 0.0006,
      "step": 48270
    },
    {
      "epoch": 22.34150856085146,
      "grad_norm": 0.0051068938337266445,
      "learning_rate": 0.05531698287829709,
      "loss": 0.0005,
      "step": 48280
    },
    {
      "epoch": 22.346136048125867,
      "grad_norm": 0.013453000225126743,
      "learning_rate": 0.055307727903748265,
      "loss": 0.0001,
      "step": 48290
    },
    {
      "epoch": 22.35076353540028,
      "grad_norm": 1.132359504699707,
      "learning_rate": 0.055298472929199455,
      "loss": 0.0002,
      "step": 48300
    },
    {
      "epoch": 22.355391022674688,
      "grad_norm": 0.017052192240953445,
      "learning_rate": 0.05528921795465063,
      "loss": 0.0003,
      "step": 48310
    },
    {
      "epoch": 22.360018509949096,
      "grad_norm": 0.05204613134264946,
      "learning_rate": 0.05527996298010181,
      "loss": 0.0002,
      "step": 48320
    },
    {
      "epoch": 22.36464599722351,
      "grad_norm": 0.034737538546323776,
      "learning_rate": 0.05527070800555298,
      "loss": 0.0002,
      "step": 48330
    },
    {
      "epoch": 22.369273484497917,
      "grad_norm": 0.0008254374843090773,
      "learning_rate": 0.05526145303100416,
      "loss": 0.0001,
      "step": 48340
    },
    {
      "epoch": 22.37390097177233,
      "grad_norm": 0.001504083746112883,
      "learning_rate": 0.05525219805645535,
      "loss": 0.0002,
      "step": 48350
    },
    {
      "epoch": 22.378528459046738,
      "grad_norm": 0.09442514181137085,
      "learning_rate": 0.055242943081906526,
      "loss": 0.0001,
      "step": 48360
    },
    {
      "epoch": 22.383155946321146,
      "grad_norm": 0.02383376657962799,
      "learning_rate": 0.05523368810735771,
      "loss": 0.0002,
      "step": 48370
    },
    {
      "epoch": 22.38778343359556,
      "grad_norm": 0.15553376078605652,
      "learning_rate": 0.055224433132808885,
      "loss": 0.0001,
      "step": 48380
    },
    {
      "epoch": 22.392410920869967,
      "grad_norm": 0.024238169193267822,
      "learning_rate": 0.055215178158260075,
      "loss": 0.0001,
      "step": 48390
    },
    {
      "epoch": 22.39703840814438,
      "grad_norm": 0.00033211123081855476,
      "learning_rate": 0.05520592318371125,
      "loss": 0.0001,
      "step": 48400
    },
    {
      "epoch": 22.401665895418787,
      "grad_norm": 0.4195901155471802,
      "learning_rate": 0.05519666820916243,
      "loss": 0.0003,
      "step": 48410
    },
    {
      "epoch": 22.406293382693196,
      "grad_norm": 0.009064720012247562,
      "learning_rate": 0.055187413234613604,
      "loss": 0.0001,
      "step": 48420
    },
    {
      "epoch": 22.410920869967608,
      "grad_norm": 0.006780869327485561,
      "learning_rate": 0.05517815826006478,
      "loss": 0.002,
      "step": 48430
    },
    {
      "epoch": 22.415548357242017,
      "grad_norm": 0.008622659370303154,
      "learning_rate": 0.05516890328551597,
      "loss": 0.0001,
      "step": 48440
    },
    {
      "epoch": 22.42017584451643,
      "grad_norm": 0.01318562962114811,
      "learning_rate": 0.055159648310967146,
      "loss": 0.001,
      "step": 48450
    },
    {
      "epoch": 22.424803331790837,
      "grad_norm": 0.05513080582022667,
      "learning_rate": 0.05515039333641833,
      "loss": 0.0004,
      "step": 48460
    },
    {
      "epoch": 22.429430819065246,
      "grad_norm": 0.014840837568044662,
      "learning_rate": 0.055141138361869506,
      "loss": 0.0001,
      "step": 48470
    },
    {
      "epoch": 22.434058306339658,
      "grad_norm": 0.006737329065799713,
      "learning_rate": 0.055131883387320696,
      "loss": 0.0001,
      "step": 48480
    },
    {
      "epoch": 22.438685793614066,
      "grad_norm": 0.09741822630167007,
      "learning_rate": 0.05512262841277187,
      "loss": 0.0002,
      "step": 48490
    },
    {
      "epoch": 22.44331328088848,
      "grad_norm": 0.03332289308309555,
      "learning_rate": 0.05511337343822305,
      "loss": 0.0001,
      "step": 48500
    },
    {
      "epoch": 22.447940768162887,
      "grad_norm": 0.002310610143467784,
      "learning_rate": 0.055104118463674225,
      "loss": 0.0002,
      "step": 48510
    },
    {
      "epoch": 22.4525682554373,
      "grad_norm": 0.0010585858253762126,
      "learning_rate": 0.0550948634891254,
      "loss": 0.0001,
      "step": 48520
    },
    {
      "epoch": 22.457195742711708,
      "grad_norm": 0.0030283129308372736,
      "learning_rate": 0.05508560851457659,
      "loss": 0.0002,
      "step": 48530
    },
    {
      "epoch": 22.461823229986116,
      "grad_norm": 0.009944402612745762,
      "learning_rate": 0.05507635354002777,
      "loss": 0.0,
      "step": 48540
    },
    {
      "epoch": 22.46645071726053,
      "grad_norm": 0.010119524784386158,
      "learning_rate": 0.05506709856547895,
      "loss": 0.0021,
      "step": 48550
    },
    {
      "epoch": 22.471078204534937,
      "grad_norm": 0.0008719724137336016,
      "learning_rate": 0.055057843590930126,
      "loss": 0.0005,
      "step": 48560
    },
    {
      "epoch": 22.47570569180935,
      "grad_norm": 4.064022541046143,
      "learning_rate": 0.0550485886163813,
      "loss": 0.001,
      "step": 48570
    },
    {
      "epoch": 22.480333179083757,
      "grad_norm": 0.46435990929603577,
      "learning_rate": 0.05503933364183249,
      "loss": 0.0002,
      "step": 48580
    },
    {
      "epoch": 22.484960666358166,
      "grad_norm": 0.012012886814773083,
      "learning_rate": 0.05503007866728367,
      "loss": 0.0001,
      "step": 48590
    },
    {
      "epoch": 22.489588153632578,
      "grad_norm": 0.0005224355263635516,
      "learning_rate": 0.055020823692734845,
      "loss": 0.0001,
      "step": 48600
    },
    {
      "epoch": 22.494215640906987,
      "grad_norm": 0.001146996975876391,
      "learning_rate": 0.05501156871818602,
      "loss": 0.0001,
      "step": 48610
    },
    {
      "epoch": 22.4988431281814,
      "grad_norm": 0.019655073061585426,
      "learning_rate": 0.05500231374363721,
      "loss": 0.0006,
      "step": 48620
    },
    {
      "epoch": 22.503470615455807,
      "grad_norm": 0.08052846044301987,
      "learning_rate": 0.05499305876908839,
      "loss": 0.0002,
      "step": 48630
    },
    {
      "epoch": 22.508098102730216,
      "grad_norm": 0.0008826483390294015,
      "learning_rate": 0.05498380379453957,
      "loss": 0.0001,
      "step": 48640
    },
    {
      "epoch": 22.512725590004628,
      "grad_norm": 0.05544963479042053,
      "learning_rate": 0.05497454881999075,
      "loss": 0.0014,
      "step": 48650
    },
    {
      "epoch": 22.517353077279036,
      "grad_norm": 0.0018141546752303839,
      "learning_rate": 0.05496529384544192,
      "loss": 0.0001,
      "step": 48660
    },
    {
      "epoch": 22.52198056455345,
      "grad_norm": 0.0024425722658634186,
      "learning_rate": 0.05495603887089311,
      "loss": 0.0001,
      "step": 48670
    },
    {
      "epoch": 22.526608051827857,
      "grad_norm": 0.007927990518510342,
      "learning_rate": 0.05494678389634429,
      "loss": 0.0001,
      "step": 48680
    },
    {
      "epoch": 22.53123553910227,
      "grad_norm": 0.0033753528259694576,
      "learning_rate": 0.054937528921795466,
      "loss": 0.0003,
      "step": 48690
    },
    {
      "epoch": 22.535863026376678,
      "grad_norm": 0.01443544588983059,
      "learning_rate": 0.05492827394724664,
      "loss": 0.0001,
      "step": 48700
    },
    {
      "epoch": 22.540490513651086,
      "grad_norm": 0.0009610082488507032,
      "learning_rate": 0.05491901897269783,
      "loss": 0.0001,
      "step": 48710
    },
    {
      "epoch": 22.5451180009255,
      "grad_norm": 0.0008966581663116813,
      "learning_rate": 0.05490976399814901,
      "loss": 0.0008,
      "step": 48720
    },
    {
      "epoch": 22.549745488199907,
      "grad_norm": 0.011821434833109379,
      "learning_rate": 0.05490050902360019,
      "loss": 0.0001,
      "step": 48730
    },
    {
      "epoch": 22.55437297547432,
      "grad_norm": 0.007217722479254007,
      "learning_rate": 0.05489125404905137,
      "loss": 0.0003,
      "step": 48740
    },
    {
      "epoch": 22.559000462748728,
      "grad_norm": 0.0022868900559842587,
      "learning_rate": 0.054881999074502544,
      "loss": 0.0001,
      "step": 48750
    },
    {
      "epoch": 22.563627950023136,
      "grad_norm": 0.0027609546668827534,
      "learning_rate": 0.054872744099953734,
      "loss": 0.0001,
      "step": 48760
    },
    {
      "epoch": 22.568255437297548,
      "grad_norm": 0.1311059445142746,
      "learning_rate": 0.05486348912540491,
      "loss": 0.0004,
      "step": 48770
    },
    {
      "epoch": 22.572882924571957,
      "grad_norm": 0.015032917261123657,
      "learning_rate": 0.054854234150856086,
      "loss": 0.0001,
      "step": 48780
    },
    {
      "epoch": 22.57751041184637,
      "grad_norm": 0.036459457129240036,
      "learning_rate": 0.05484497917630726,
      "loss": 0.0002,
      "step": 48790
    },
    {
      "epoch": 22.582137899120777,
      "grad_norm": 2.6689624786376953,
      "learning_rate": 0.054835724201758446,
      "loss": 0.0011,
      "step": 48800
    },
    {
      "epoch": 22.586765386395186,
      "grad_norm": 0.000614218763075769,
      "learning_rate": 0.05482646922720963,
      "loss": 0.0001,
      "step": 48810
    },
    {
      "epoch": 22.591392873669598,
      "grad_norm": 0.0019898652099072933,
      "learning_rate": 0.05481721425266081,
      "loss": 0.0,
      "step": 48820
    },
    {
      "epoch": 22.596020360944006,
      "grad_norm": 0.009050714783370495,
      "learning_rate": 0.05480795927811199,
      "loss": 0.0,
      "step": 48830
    },
    {
      "epoch": 22.60064784821842,
      "grad_norm": 0.015286021865904331,
      "learning_rate": 0.054798704303563164,
      "loss": 0.0001,
      "step": 48840
    },
    {
      "epoch": 22.605275335492827,
      "grad_norm": 0.0014607025077566504,
      "learning_rate": 0.054789449329014354,
      "loss": 0.0001,
      "step": 48850
    },
    {
      "epoch": 22.60990282276724,
      "grad_norm": 0.0010981649393215775,
      "learning_rate": 0.05478019435446553,
      "loss": 0.0001,
      "step": 48860
    },
    {
      "epoch": 22.614530310041648,
      "grad_norm": 0.0006845365860499442,
      "learning_rate": 0.05477093937991671,
      "loss": 0.0001,
      "step": 48870
    },
    {
      "epoch": 22.619157797316056,
      "grad_norm": 0.005140581168234348,
      "learning_rate": 0.05476168440536788,
      "loss": 0.0001,
      "step": 48880
    },
    {
      "epoch": 22.62378528459047,
      "grad_norm": 0.027925748378038406,
      "learning_rate": 0.054752429430819066,
      "loss": 0.0002,
      "step": 48890
    },
    {
      "epoch": 22.628412771864877,
      "grad_norm": 0.011380836367607117,
      "learning_rate": 0.05474317445627025,
      "loss": 0.0001,
      "step": 48900
    },
    {
      "epoch": 22.63304025913929,
      "grad_norm": 0.0014253222616389394,
      "learning_rate": 0.05473391948172143,
      "loss": 0.0001,
      "step": 48910
    },
    {
      "epoch": 22.637667746413698,
      "grad_norm": 0.010783839039504528,
      "learning_rate": 0.05472466450717261,
      "loss": 0.0001,
      "step": 48920
    },
    {
      "epoch": 22.642295233688106,
      "grad_norm": 0.018193960189819336,
      "learning_rate": 0.054715409532623785,
      "loss": 0.0001,
      "step": 48930
    },
    {
      "epoch": 22.646922720962518,
      "grad_norm": 0.03335031867027283,
      "learning_rate": 0.05470615455807496,
      "loss": 0.0001,
      "step": 48940
    },
    {
      "epoch": 22.651550208236927,
      "grad_norm": 0.0018260980723425746,
      "learning_rate": 0.05469689958352615,
      "loss": 0.0002,
      "step": 48950
    },
    {
      "epoch": 22.65617769551134,
      "grad_norm": 0.003905282588675618,
      "learning_rate": 0.05468764460897733,
      "loss": 0.0001,
      "step": 48960
    },
    {
      "epoch": 22.660805182785747,
      "grad_norm": 0.000488099962240085,
      "learning_rate": 0.054678389634428504,
      "loss": 0.0002,
      "step": 48970
    },
    {
      "epoch": 22.665432670060156,
      "grad_norm": 0.010966330766677856,
      "learning_rate": 0.05466913465987969,
      "loss": 0.0001,
      "step": 48980
    },
    {
      "epoch": 22.670060157334568,
      "grad_norm": 0.00200658873654902,
      "learning_rate": 0.05465987968533087,
      "loss": 0.0,
      "step": 48990
    },
    {
      "epoch": 22.674687644608976,
      "grad_norm": 0.024479100480675697,
      "learning_rate": 0.05465062471078205,
      "loss": 0.0,
      "step": 49000
    },
    {
      "epoch": 22.67931513188339,
      "grad_norm": 0.17496751248836517,
      "learning_rate": 0.05464136973623323,
      "loss": 0.0008,
      "step": 49010
    },
    {
      "epoch": 22.683942619157797,
      "grad_norm": 0.0033065229654312134,
      "learning_rate": 0.054632114761684406,
      "loss": 0.0001,
      "step": 49020
    },
    {
      "epoch": 22.688570106432206,
      "grad_norm": 0.005106318276375532,
      "learning_rate": 0.05462285978713558,
      "loss": 0.0019,
      "step": 49030
    },
    {
      "epoch": 22.693197593706618,
      "grad_norm": 0.0030255280435085297,
      "learning_rate": 0.05461360481258677,
      "loss": 0.0003,
      "step": 49040
    },
    {
      "epoch": 22.697825080981026,
      "grad_norm": 0.005955276545137167,
      "learning_rate": 0.05460434983803795,
      "loss": 0.0,
      "step": 49050
    },
    {
      "epoch": 22.70245256825544,
      "grad_norm": 0.9741035103797913,
      "learning_rate": 0.054595094863489124,
      "loss": 0.0003,
      "step": 49060
    },
    {
      "epoch": 22.707080055529847,
      "grad_norm": 0.0035217360127717257,
      "learning_rate": 0.05458583988894031,
      "loss": 0.0003,
      "step": 49070
    },
    {
      "epoch": 22.71170754280426,
      "grad_norm": 0.013585472479462624,
      "learning_rate": 0.05457658491439149,
      "loss": 0.0001,
      "step": 49080
    },
    {
      "epoch": 22.716335030078668,
      "grad_norm": 1.340813159942627,
      "learning_rate": 0.054567329939842674,
      "loss": 0.0006,
      "step": 49090
    },
    {
      "epoch": 22.720962517353076,
      "grad_norm": 0.0427885465323925,
      "learning_rate": 0.05455807496529385,
      "loss": 0.0001,
      "step": 49100
    },
    {
      "epoch": 22.725590004627488,
      "grad_norm": 0.44253626465797424,
      "learning_rate": 0.054548819990745026,
      "loss": 0.0002,
      "step": 49110
    },
    {
      "epoch": 22.730217491901897,
      "grad_norm": 0.01055227592587471,
      "learning_rate": 0.0545395650161962,
      "loss": 0.0001,
      "step": 49120
    },
    {
      "epoch": 22.73484497917631,
      "grad_norm": 0.003907261416316032,
      "learning_rate": 0.05453031004164739,
      "loss": 0.0008,
      "step": 49130
    },
    {
      "epoch": 22.739472466450717,
      "grad_norm": 0.003876353381201625,
      "learning_rate": 0.05452105506709857,
      "loss": 0.0001,
      "step": 49140
    },
    {
      "epoch": 22.744099953725126,
      "grad_norm": 0.06245700642466545,
      "learning_rate": 0.054511800092549745,
      "loss": 0.0001,
      "step": 49150
    },
    {
      "epoch": 22.748727440999538,
      "grad_norm": 0.009327792562544346,
      "learning_rate": 0.05450254511800093,
      "loss": 0.0001,
      "step": 49160
    },
    {
      "epoch": 22.753354928273946,
      "grad_norm": 0.0022424531634896994,
      "learning_rate": 0.054493290143452104,
      "loss": 0.0007,
      "step": 49170
    },
    {
      "epoch": 22.75798241554836,
      "grad_norm": 0.07372314482927322,
      "learning_rate": 0.054484035168903294,
      "loss": 0.0001,
      "step": 49180
    },
    {
      "epoch": 22.762609902822767,
      "grad_norm": 0.021015329286456108,
      "learning_rate": 0.05447478019435447,
      "loss": 0.0002,
      "step": 49190
    },
    {
      "epoch": 22.767237390097176,
      "grad_norm": 0.05868079513311386,
      "learning_rate": 0.05446552521980565,
      "loss": 0.0001,
      "step": 49200
    },
    {
      "epoch": 22.771864877371588,
      "grad_norm": 0.003218060825020075,
      "learning_rate": 0.05445627024525682,
      "loss": 0.0001,
      "step": 49210
    },
    {
      "epoch": 22.776492364645996,
      "grad_norm": 0.00048337067710235715,
      "learning_rate": 0.05444701527070801,
      "loss": 0.0001,
      "step": 49220
    },
    {
      "epoch": 22.78111985192041,
      "grad_norm": 0.004037890583276749,
      "learning_rate": 0.05443776029615919,
      "loss": 0.0001,
      "step": 49230
    },
    {
      "epoch": 22.785747339194817,
      "grad_norm": 0.002158145885914564,
      "learning_rate": 0.054428505321610365,
      "loss": 0.0002,
      "step": 49240
    },
    {
      "epoch": 22.790374826469225,
      "grad_norm": 0.004556782077997923,
      "learning_rate": 0.05441925034706155,
      "loss": 0.0001,
      "step": 49250
    },
    {
      "epoch": 22.795002313743638,
      "grad_norm": 0.10555591434240341,
      "learning_rate": 0.054409995372512725,
      "loss": 0.0001,
      "step": 49260
    },
    {
      "epoch": 22.799629801018046,
      "grad_norm": 0.10756445676088333,
      "learning_rate": 0.054400740397963915,
      "loss": 0.0003,
      "step": 49270
    },
    {
      "epoch": 22.804257288292458,
      "grad_norm": 0.0032521553803235292,
      "learning_rate": 0.05439148542341509,
      "loss": 0.0,
      "step": 49280
    },
    {
      "epoch": 22.808884775566867,
      "grad_norm": 0.059116579592227936,
      "learning_rate": 0.05438223044886627,
      "loss": 0.0002,
      "step": 49290
    },
    {
      "epoch": 22.81351226284128,
      "grad_norm": 0.00353707536123693,
      "learning_rate": 0.05437297547431744,
      "loss": 0.0006,
      "step": 49300
    },
    {
      "epoch": 22.818139750115687,
      "grad_norm": 0.14555704593658447,
      "learning_rate": 0.054363720499768634,
      "loss": 0.0001,
      "step": 49310
    },
    {
      "epoch": 22.822767237390096,
      "grad_norm": 0.014520631171762943,
      "learning_rate": 0.05435446552521981,
      "loss": 0.0004,
      "step": 49320
    },
    {
      "epoch": 22.827394724664508,
      "grad_norm": 0.027136322110891342,
      "learning_rate": 0.054345210550670986,
      "loss": 0.0001,
      "step": 49330
    },
    {
      "epoch": 22.832022211938916,
      "grad_norm": 0.004025025758892298,
      "learning_rate": 0.05433595557612217,
      "loss": 0.0,
      "step": 49340
    },
    {
      "epoch": 22.83664969921333,
      "grad_norm": 0.004042987711727619,
      "learning_rate": 0.054326700601573345,
      "loss": 0.0001,
      "step": 49350
    },
    {
      "epoch": 22.841277186487737,
      "grad_norm": 0.039883702993392944,
      "learning_rate": 0.054317445627024535,
      "loss": 0.0001,
      "step": 49360
    },
    {
      "epoch": 22.845904673762146,
      "grad_norm": 0.010789550840854645,
      "learning_rate": 0.05430819065247571,
      "loss": 0.0015,
      "step": 49370
    },
    {
      "epoch": 22.850532161036558,
      "grad_norm": 0.00038228847552090883,
      "learning_rate": 0.05429893567792689,
      "loss": 0.0001,
      "step": 49380
    },
    {
      "epoch": 22.855159648310966,
      "grad_norm": 0.006703279912471771,
      "learning_rate": 0.054289680703378064,
      "loss": 0.0,
      "step": 49390
    },
    {
      "epoch": 22.85978713558538,
      "grad_norm": 0.01039087399840355,
      "learning_rate": 0.05428042572882924,
      "loss": 0.0013,
      "step": 49400
    },
    {
      "epoch": 22.864414622859787,
      "grad_norm": 0.02782263047993183,
      "learning_rate": 0.05427117075428043,
      "loss": 0.0001,
      "step": 49410
    },
    {
      "epoch": 22.869042110134195,
      "grad_norm": 0.27613651752471924,
      "learning_rate": 0.054261915779731607,
      "loss": 0.0002,
      "step": 49420
    },
    {
      "epoch": 22.873669597408608,
      "grad_norm": 0.0022303874138742685,
      "learning_rate": 0.05425266080518279,
      "loss": 0.0001,
      "step": 49430
    },
    {
      "epoch": 22.878297084683016,
      "grad_norm": 0.008866558782756329,
      "learning_rate": 0.054243405830633966,
      "loss": 0.0008,
      "step": 49440
    },
    {
      "epoch": 22.882924571957428,
      "grad_norm": 0.0016751316143199801,
      "learning_rate": 0.054234150856085156,
      "loss": 0.0003,
      "step": 49450
    },
    {
      "epoch": 22.887552059231837,
      "grad_norm": 0.0693235844373703,
      "learning_rate": 0.05422489588153633,
      "loss": 0.0002,
      "step": 49460
    },
    {
      "epoch": 22.89217954650625,
      "grad_norm": 0.004327727947384119,
      "learning_rate": 0.05421564090698751,
      "loss": 0.0,
      "step": 49470
    },
    {
      "epoch": 22.896807033780657,
      "grad_norm": 0.02931893803179264,
      "learning_rate": 0.054206385932438685,
      "loss": 0.0001,
      "step": 49480
    },
    {
      "epoch": 22.901434521055066,
      "grad_norm": 0.0019038383616134524,
      "learning_rate": 0.05419713095788986,
      "loss": 0.0001,
      "step": 49490
    },
    {
      "epoch": 22.906062008329478,
      "grad_norm": 0.021664736792445183,
      "learning_rate": 0.05418787598334105,
      "loss": 0.0,
      "step": 49500
    },
    {
      "epoch": 22.910689495603886,
      "grad_norm": 0.0013224119320511818,
      "learning_rate": 0.05417862100879223,
      "loss": 0.0001,
      "step": 49510
    },
    {
      "epoch": 22.9153169828783,
      "grad_norm": 0.02040402777493,
      "learning_rate": 0.05416936603424341,
      "loss": 0.0,
      "step": 49520
    },
    {
      "epoch": 22.919944470152707,
      "grad_norm": 0.055033400654792786,
      "learning_rate": 0.054160111059694586,
      "loss": 0.0001,
      "step": 49530
    },
    {
      "epoch": 22.924571957427116,
      "grad_norm": 0.0006824489100836217,
      "learning_rate": 0.05415085608514578,
      "loss": 0.0001,
      "step": 49540
    },
    {
      "epoch": 22.929199444701528,
      "grad_norm": 0.001817653188481927,
      "learning_rate": 0.05414160111059695,
      "loss": 0.0003,
      "step": 49550
    },
    {
      "epoch": 22.933826931975936,
      "grad_norm": 0.004940016660839319,
      "learning_rate": 0.05413234613604813,
      "loss": 0.0001,
      "step": 49560
    },
    {
      "epoch": 22.93845441925035,
      "grad_norm": 0.0010712327202782035,
      "learning_rate": 0.054123091161499305,
      "loss": 0.0,
      "step": 49570
    },
    {
      "epoch": 22.943081906524757,
      "grad_norm": 0.005656776018440723,
      "learning_rate": 0.05411383618695048,
      "loss": 0.0005,
      "step": 49580
    },
    {
      "epoch": 22.947709393799165,
      "grad_norm": 0.0017041816608980298,
      "learning_rate": 0.05410458121240167,
      "loss": 0.0001,
      "step": 49590
    },
    {
      "epoch": 22.952336881073578,
      "grad_norm": 0.009985704906284809,
      "learning_rate": 0.05409532623785285,
      "loss": 0.0,
      "step": 49600
    },
    {
      "epoch": 22.956964368347986,
      "grad_norm": 0.030142148956656456,
      "learning_rate": 0.05408607126330403,
      "loss": 0.0001,
      "step": 49610
    },
    {
      "epoch": 22.961591855622398,
      "grad_norm": 0.00204283744096756,
      "learning_rate": 0.05407681628875521,
      "loss": 0.0002,
      "step": 49620
    },
    {
      "epoch": 22.966219342896807,
      "grad_norm": 0.014865058474242687,
      "learning_rate": 0.05406756131420638,
      "loss": 0.0001,
      "step": 49630
    },
    {
      "epoch": 22.97084683017122,
      "grad_norm": 0.04638823866844177,
      "learning_rate": 0.05405830633965757,
      "loss": 0.0001,
      "step": 49640
    },
    {
      "epoch": 22.975474317445627,
      "grad_norm": 0.2454918920993805,
      "learning_rate": 0.05404905136510875,
      "loss": 0.0002,
      "step": 49650
    },
    {
      "epoch": 22.980101804720036,
      "grad_norm": 0.0028295954689383507,
      "learning_rate": 0.054039796390559926,
      "loss": 0.0001,
      "step": 49660
    },
    {
      "epoch": 22.984729291994448,
      "grad_norm": 0.0032678046263754368,
      "learning_rate": 0.0540305414160111,
      "loss": 0.0,
      "step": 49670
    },
    {
      "epoch": 22.989356779268856,
      "grad_norm": 0.0028883093036711216,
      "learning_rate": 0.05402128644146229,
      "loss": 0.0001,
      "step": 49680
    },
    {
      "epoch": 22.99398426654327,
      "grad_norm": 0.0443444699048996,
      "learning_rate": 0.05401203146691347,
      "loss": 0.0002,
      "step": 49690
    },
    {
      "epoch": 22.998611753817677,
      "grad_norm": 0.047310858964920044,
      "learning_rate": 0.05400277649236465,
      "loss": 0.0008,
      "step": 49700
    },
    {
      "epoch": 23.0,
      "eval_accuracy_branch1": 0.982976428901556,
      "eval_accuracy_branch2": 0.49884455399784317,
      "eval_f1_branch1": 0.9842996696729898,
      "eval_f1_branch2": 0.4971301593924568,
      "eval_loss": 0.029994750395417213,
      "eval_precision_branch1": 0.9842757643343762,
      "eval_precision_branch2": 0.4988285794702443,
      "eval_recall_branch1": 0.9846469072016759,
      "eval_recall_branch2": 0.49884455399784317,
      "eval_runtime": 30.4673,
      "eval_samples_per_second": 426.096,
      "eval_steps_per_second": 53.27,
      "step": 49703
    },
    {
      "epoch": 23.003239241092086,
      "grad_norm": 0.0014699938474223018,
      "learning_rate": 0.05399352151781583,
      "loss": 0.0001,
      "step": 49710
    },
    {
      "epoch": 23.007866728366498,
      "grad_norm": 0.0016294911038130522,
      "learning_rate": 0.053984266543267004,
      "loss": 0.0001,
      "step": 49720
    },
    {
      "epoch": 23.012494215640906,
      "grad_norm": 0.07522054761648178,
      "learning_rate": 0.053975011568718194,
      "loss": 0.0006,
      "step": 49730
    },
    {
      "epoch": 23.01712170291532,
      "grad_norm": 0.004454720765352249,
      "learning_rate": 0.05396575659416937,
      "loss": 0.0003,
      "step": 49740
    },
    {
      "epoch": 23.021749190189727,
      "grad_norm": 1.7192885875701904,
      "learning_rate": 0.053956501619620546,
      "loss": 0.0005,
      "step": 49750
    },
    {
      "epoch": 23.026376677464135,
      "grad_norm": 0.005379538983106613,
      "learning_rate": 0.05394724664507172,
      "loss": 0.0016,
      "step": 49760
    },
    {
      "epoch": 23.031004164738548,
      "grad_norm": 0.000912180810701102,
      "learning_rate": 0.05393799167052291,
      "loss": 0.0002,
      "step": 49770
    },
    {
      "epoch": 23.035631652012956,
      "grad_norm": 0.005807977635413408,
      "learning_rate": 0.05392873669597409,
      "loss": 0.0001,
      "step": 49780
    },
    {
      "epoch": 23.040259139287368,
      "grad_norm": 0.007625467609614134,
      "learning_rate": 0.05391948172142527,
      "loss": 0.0012,
      "step": 49790
    },
    {
      "epoch": 23.044886626561777,
      "grad_norm": 0.003186949295923114,
      "learning_rate": 0.05391022674687645,
      "loss": 0.0001,
      "step": 49800
    },
    {
      "epoch": 23.049514113836185,
      "grad_norm": 0.40070387721061707,
      "learning_rate": 0.053900971772327624,
      "loss": 0.0002,
      "step": 49810
    },
    {
      "epoch": 23.054141601110597,
      "grad_norm": 0.0021897924598306417,
      "learning_rate": 0.053891716797778814,
      "loss": 0.0001,
      "step": 49820
    },
    {
      "epoch": 23.058769088385006,
      "grad_norm": 0.0153033547103405,
      "learning_rate": 0.05388246182322999,
      "loss": 0.0002,
      "step": 49830
    },
    {
      "epoch": 23.063396575659418,
      "grad_norm": 0.009300828911364079,
      "learning_rate": 0.05387320684868117,
      "loss": 0.0001,
      "step": 49840
    },
    {
      "epoch": 23.068024062933826,
      "grad_norm": 0.6140556931495667,
      "learning_rate": 0.05386395187413234,
      "loss": 0.0002,
      "step": 49850
    },
    {
      "epoch": 23.07265155020824,
      "grad_norm": 1.5083194971084595,
      "learning_rate": 0.053854696899583526,
      "loss": 0.0005,
      "step": 49860
    },
    {
      "epoch": 23.077279037482647,
      "grad_norm": 0.10823091864585876,
      "learning_rate": 0.05384544192503471,
      "loss": 0.0004,
      "step": 49870
    },
    {
      "epoch": 23.081906524757056,
      "grad_norm": 0.01364685595035553,
      "learning_rate": 0.05383618695048589,
      "loss": 0.0001,
      "step": 49880
    },
    {
      "epoch": 23.086534012031468,
      "grad_norm": 0.033364564180374146,
      "learning_rate": 0.05382693197593707,
      "loss": 0.0004,
      "step": 49890
    },
    {
      "epoch": 23.091161499305876,
      "grad_norm": 0.016617918387055397,
      "learning_rate": 0.053817677001388245,
      "loss": 0.0001,
      "step": 49900
    },
    {
      "epoch": 23.09578898658029,
      "grad_norm": 0.002752270782366395,
      "learning_rate": 0.053808422026839435,
      "loss": 0.0004,
      "step": 49910
    },
    {
      "epoch": 23.100416473854697,
      "grad_norm": 0.00587447639554739,
      "learning_rate": 0.05379916705229061,
      "loss": 0.0,
      "step": 49920
    },
    {
      "epoch": 23.105043961129105,
      "grad_norm": 0.006204549223184586,
      "learning_rate": 0.05378991207774179,
      "loss": 0.0001,
      "step": 49930
    },
    {
      "epoch": 23.109671448403518,
      "grad_norm": 0.008589322678744793,
      "learning_rate": 0.053780657103192964,
      "loss": 0.0002,
      "step": 49940
    },
    {
      "epoch": 23.114298935677926,
      "grad_norm": 0.005748669616878033,
      "learning_rate": 0.05377140212864415,
      "loss": 0.0013,
      "step": 49950
    },
    {
      "epoch": 23.118926422952338,
      "grad_norm": 0.0009263125248253345,
      "learning_rate": 0.05376214715409533,
      "loss": 0.0001,
      "step": 49960
    },
    {
      "epoch": 23.123553910226747,
      "grad_norm": 1.8260924816131592,
      "learning_rate": 0.05375289217954651,
      "loss": 0.0008,
      "step": 49970
    },
    {
      "epoch": 23.128181397501155,
      "grad_norm": 0.00017386219406034797,
      "learning_rate": 0.05374363720499769,
      "loss": 0.0014,
      "step": 49980
    },
    {
      "epoch": 23.132808884775567,
      "grad_norm": 0.0028735268861055374,
      "learning_rate": 0.053734382230448866,
      "loss": 0.0001,
      "step": 49990
    },
    {
      "epoch": 23.137436372049976,
      "grad_norm": 0.022894449532032013,
      "learning_rate": 0.053725127255900056,
      "loss": 0.0001,
      "step": 50000
    },
    {
      "epoch": 23.142063859324388,
      "grad_norm": 0.08086860179901123,
      "learning_rate": 0.05371587228135123,
      "loss": 0.0001,
      "step": 50010
    },
    {
      "epoch": 23.146691346598796,
      "grad_norm": 0.0025952723808586597,
      "learning_rate": 0.05370661730680241,
      "loss": 0.0003,
      "step": 50020
    },
    {
      "epoch": 23.15131883387321,
      "grad_norm": 0.10264009237289429,
      "learning_rate": 0.053697362332253584,
      "loss": 0.0002,
      "step": 50030
    },
    {
      "epoch": 23.155946321147617,
      "grad_norm": 0.00749775068834424,
      "learning_rate": 0.05368810735770477,
      "loss": 0.0001,
      "step": 50040
    },
    {
      "epoch": 23.160573808422026,
      "grad_norm": 0.002073025330901146,
      "learning_rate": 0.05367885238315595,
      "loss": 0.0001,
      "step": 50050
    },
    {
      "epoch": 23.165201295696438,
      "grad_norm": 0.002895815297961235,
      "learning_rate": 0.053669597408607134,
      "loss": 0.0,
      "step": 50060
    },
    {
      "epoch": 23.169828782970846,
      "grad_norm": 0.0012193969450891018,
      "learning_rate": 0.05366034243405831,
      "loss": 0.0001,
      "step": 50070
    },
    {
      "epoch": 23.17445627024526,
      "grad_norm": 0.005979322828352451,
      "learning_rate": 0.053651087459509486,
      "loss": 0.0001,
      "step": 50080
    },
    {
      "epoch": 23.179083757519667,
      "grad_norm": 0.004313769284635782,
      "learning_rate": 0.05364183248496066,
      "loss": 0.0002,
      "step": 50090
    },
    {
      "epoch": 23.183711244794075,
      "grad_norm": 0.04480624943971634,
      "learning_rate": 0.05363257751041185,
      "loss": 0.0001,
      "step": 50100
    },
    {
      "epoch": 23.188338732068488,
      "grad_norm": 0.00891344528645277,
      "learning_rate": 0.05362332253586303,
      "loss": 0.0001,
      "step": 50110
    },
    {
      "epoch": 23.192966219342896,
      "grad_norm": 0.35851410031318665,
      "learning_rate": 0.053614067561314205,
      "loss": 0.0003,
      "step": 50120
    },
    {
      "epoch": 23.197593706617308,
      "grad_norm": 0.006767602637410164,
      "learning_rate": 0.05360481258676539,
      "loss": 0.0,
      "step": 50130
    },
    {
      "epoch": 23.202221193891717,
      "grad_norm": 0.05655960738658905,
      "learning_rate": 0.05359555761221657,
      "loss": 0.0001,
      "step": 50140
    },
    {
      "epoch": 23.206848681166125,
      "grad_norm": 0.02468910813331604,
      "learning_rate": 0.053586302637667754,
      "loss": 0.0001,
      "step": 50150
    },
    {
      "epoch": 23.211476168440537,
      "grad_norm": 0.007182787172496319,
      "learning_rate": 0.05357704766311893,
      "loss": 0.0002,
      "step": 50160
    },
    {
      "epoch": 23.216103655714946,
      "grad_norm": 0.00629964005202055,
      "learning_rate": 0.05356779268857011,
      "loss": 0.0,
      "step": 50170
    },
    {
      "epoch": 23.220731142989358,
      "grad_norm": 0.10939379781484604,
      "learning_rate": 0.05355853771402128,
      "loss": 0.0002,
      "step": 50180
    },
    {
      "epoch": 23.225358630263766,
      "grad_norm": 0.0010225035948678851,
      "learning_rate": 0.05354928273947247,
      "loss": 0.0001,
      "step": 50190
    },
    {
      "epoch": 23.229986117538175,
      "grad_norm": 0.49146586656570435,
      "learning_rate": 0.05354002776492365,
      "loss": 0.0003,
      "step": 50200
    },
    {
      "epoch": 23.234613604812587,
      "grad_norm": 0.025112265720963478,
      "learning_rate": 0.053530772790374825,
      "loss": 0.0002,
      "step": 50210
    },
    {
      "epoch": 23.239241092086996,
      "grad_norm": 0.013370274566113949,
      "learning_rate": 0.05352151781582601,
      "loss": 0.0001,
      "step": 50220
    },
    {
      "epoch": 23.243868579361408,
      "grad_norm": 0.012182009406387806,
      "learning_rate": 0.05351226284127719,
      "loss": 0.0001,
      "step": 50230
    },
    {
      "epoch": 23.248496066635816,
      "grad_norm": 0.0009492608369328082,
      "learning_rate": 0.053503007866728375,
      "loss": 0.0002,
      "step": 50240
    },
    {
      "epoch": 23.25312355391023,
      "grad_norm": 0.017409445717930794,
      "learning_rate": 0.05349375289217955,
      "loss": 0.0004,
      "step": 50250
    },
    {
      "epoch": 23.257751041184637,
      "grad_norm": 0.024750880897045135,
      "learning_rate": 0.05348449791763073,
      "loss": 0.0001,
      "step": 50260
    },
    {
      "epoch": 23.262378528459045,
      "grad_norm": 0.0562199130654335,
      "learning_rate": 0.0534752429430819,
      "loss": 0.0001,
      "step": 50270
    },
    {
      "epoch": 23.267006015733458,
      "grad_norm": 0.004282827489078045,
      "learning_rate": 0.053465987968533094,
      "loss": 0.0003,
      "step": 50280
    },
    {
      "epoch": 23.271633503007866,
      "grad_norm": 0.006861584726721048,
      "learning_rate": 0.05345673299398427,
      "loss": 0.0,
      "step": 50290
    },
    {
      "epoch": 23.276260990282278,
      "grad_norm": 0.004265503492206335,
      "learning_rate": 0.053447478019435446,
      "loss": 0.0001,
      "step": 50300
    },
    {
      "epoch": 23.280888477556687,
      "grad_norm": 0.1366259604692459,
      "learning_rate": 0.05343822304488663,
      "loss": 0.0002,
      "step": 50310
    },
    {
      "epoch": 23.285515964831095,
      "grad_norm": 0.07398442924022675,
      "learning_rate": 0.053428968070337805,
      "loss": 0.0002,
      "step": 50320
    },
    {
      "epoch": 23.290143452105507,
      "grad_norm": 0.0023460504598915577,
      "learning_rate": 0.053419713095788995,
      "loss": 0.0001,
      "step": 50330
    },
    {
      "epoch": 23.294770939379916,
      "grad_norm": 0.0010511713335290551,
      "learning_rate": 0.05341045812124017,
      "loss": 0.0,
      "step": 50340
    },
    {
      "epoch": 23.299398426654328,
      "grad_norm": 0.028628727421164513,
      "learning_rate": 0.05340120314669135,
      "loss": 0.0001,
      "step": 50350
    },
    {
      "epoch": 23.304025913928736,
      "grad_norm": 0.007188648451119661,
      "learning_rate": 0.053391948172142524,
      "loss": 0.0001,
      "step": 50360
    },
    {
      "epoch": 23.308653401203145,
      "grad_norm": 0.003926038276404142,
      "learning_rate": 0.053382693197593714,
      "loss": 0.0001,
      "step": 50370
    },
    {
      "epoch": 23.313280888477557,
      "grad_norm": 0.0026333723217248917,
      "learning_rate": 0.05337343822304489,
      "loss": 0.0001,
      "step": 50380
    },
    {
      "epoch": 23.317908375751966,
      "grad_norm": 0.34140709042549133,
      "learning_rate": 0.05336418324849607,
      "loss": 0.0001,
      "step": 50390
    },
    {
      "epoch": 23.322535863026378,
      "grad_norm": 0.08850182592868805,
      "learning_rate": 0.05335492827394725,
      "loss": 0.0004,
      "step": 50400
    },
    {
      "epoch": 23.327163350300786,
      "grad_norm": 1.1908704042434692,
      "learning_rate": 0.053345673299398426,
      "loss": 0.0003,
      "step": 50410
    },
    {
      "epoch": 23.3317908375752,
      "grad_norm": 0.00500880740582943,
      "learning_rate": 0.053336418324849616,
      "loss": 0.0006,
      "step": 50420
    },
    {
      "epoch": 23.336418324849607,
      "grad_norm": 0.021015390753746033,
      "learning_rate": 0.05332716335030079,
      "loss": 0.0003,
      "step": 50430
    },
    {
      "epoch": 23.341045812124015,
      "grad_norm": 0.0006940740277059376,
      "learning_rate": 0.05331790837575197,
      "loss": 0.0,
      "step": 50440
    },
    {
      "epoch": 23.345673299398428,
      "grad_norm": 0.008373389951884747,
      "learning_rate": 0.053308653401203145,
      "loss": 0.0002,
      "step": 50450
    },
    {
      "epoch": 23.350300786672836,
      "grad_norm": 0.004611387383192778,
      "learning_rate": 0.053299398426654335,
      "loss": 0.0002,
      "step": 50460
    },
    {
      "epoch": 23.354928273947248,
      "grad_norm": 0.3392180800437927,
      "learning_rate": 0.05329014345210551,
      "loss": 0.0002,
      "step": 50470
    },
    {
      "epoch": 23.359555761221657,
      "grad_norm": 0.004819557536393404,
      "learning_rate": 0.05328088847755669,
      "loss": 0.0001,
      "step": 50480
    },
    {
      "epoch": 23.364183248496065,
      "grad_norm": 0.004178780596703291,
      "learning_rate": 0.05327163350300787,
      "loss": 0.0002,
      "step": 50490
    },
    {
      "epoch": 23.368810735770477,
      "grad_norm": 0.006781863048672676,
      "learning_rate": 0.053262378528459046,
      "loss": 0.0003,
      "step": 50500
    },
    {
      "epoch": 23.373438223044886,
      "grad_norm": 0.0038530011661350727,
      "learning_rate": 0.05325312355391024,
      "loss": 0.0002,
      "step": 50510
    },
    {
      "epoch": 23.378065710319298,
      "grad_norm": 0.008528592064976692,
      "learning_rate": 0.05324386857936141,
      "loss": 0.0001,
      "step": 50520
    },
    {
      "epoch": 23.382693197593706,
      "grad_norm": 0.0008956495439633727,
      "learning_rate": 0.05323461360481259,
      "loss": 0.0,
      "step": 50530
    },
    {
      "epoch": 23.387320684868115,
      "grad_norm": 0.00627842266112566,
      "learning_rate": 0.053225358630263765,
      "loss": 0.0001,
      "step": 50540
    },
    {
      "epoch": 23.391948172142527,
      "grad_norm": 0.020693577826023102,
      "learning_rate": 0.05321610365571494,
      "loss": 0.0001,
      "step": 50550
    },
    {
      "epoch": 23.396575659416936,
      "grad_norm": 0.029123030602931976,
      "learning_rate": 0.05320684868116613,
      "loss": 0.0031,
      "step": 50560
    },
    {
      "epoch": 23.401203146691348,
      "grad_norm": 0.00574831385165453,
      "learning_rate": 0.05319759370661731,
      "loss": 0.0005,
      "step": 50570
    },
    {
      "epoch": 23.405830633965756,
      "grad_norm": 0.0015687071718275547,
      "learning_rate": 0.05318833873206849,
      "loss": 0.0051,
      "step": 50580
    },
    {
      "epoch": 23.41045812124017,
      "grad_norm": 0.19040006399154663,
      "learning_rate": 0.05317908375751967,
      "loss": 0.0011,
      "step": 50590
    },
    {
      "epoch": 23.415085608514577,
      "grad_norm": 0.0013492703437805176,
      "learning_rate": 0.05316982878297086,
      "loss": 0.0,
      "step": 50600
    },
    {
      "epoch": 23.419713095788985,
      "grad_norm": 0.05597361549735069,
      "learning_rate": 0.05316057380842203,
      "loss": 0.0002,
      "step": 50610
    },
    {
      "epoch": 23.424340583063398,
      "grad_norm": 0.03804565221071243,
      "learning_rate": 0.05315131883387321,
      "loss": 0.0001,
      "step": 50620
    },
    {
      "epoch": 23.428968070337806,
      "grad_norm": 0.003640064736828208,
      "learning_rate": 0.053142063859324386,
      "loss": 0.0,
      "step": 50630
    },
    {
      "epoch": 23.433595557612218,
      "grad_norm": 0.018033787608146667,
      "learning_rate": 0.05313280888477556,
      "loss": 0.0001,
      "step": 50640
    },
    {
      "epoch": 23.438223044886627,
      "grad_norm": 0.028621148318052292,
      "learning_rate": 0.05312355391022675,
      "loss": 0.0003,
      "step": 50650
    },
    {
      "epoch": 23.442850532161035,
      "grad_norm": 0.0005848961300216615,
      "learning_rate": 0.05311429893567793,
      "loss": 0.0007,
      "step": 50660
    },
    {
      "epoch": 23.447478019435447,
      "grad_norm": 0.00396442087367177,
      "learning_rate": 0.05310504396112911,
      "loss": 0.0002,
      "step": 50670
    },
    {
      "epoch": 23.452105506709856,
      "grad_norm": 0.001986484043300152,
      "learning_rate": 0.05309578898658029,
      "loss": 0.0001,
      "step": 50680
    },
    {
      "epoch": 23.456732993984268,
      "grad_norm": 0.0009137715678662062,
      "learning_rate": 0.05308653401203148,
      "loss": 0.0001,
      "step": 50690
    },
    {
      "epoch": 23.461360481258676,
      "grad_norm": 0.5907593965530396,
      "learning_rate": 0.053077279037482654,
      "loss": 0.0003,
      "step": 50700
    },
    {
      "epoch": 23.465987968533085,
      "grad_norm": 0.03140774741768837,
      "learning_rate": 0.05306802406293383,
      "loss": 0.0001,
      "step": 50710
    },
    {
      "epoch": 23.470615455807497,
      "grad_norm": 0.04010455682873726,
      "learning_rate": 0.053058769088385006,
      "loss": 0.0,
      "step": 50720
    },
    {
      "epoch": 23.475242943081906,
      "grad_norm": 0.001056079170666635,
      "learning_rate": 0.05304951411383618,
      "loss": 0.0001,
      "step": 50730
    },
    {
      "epoch": 23.479870430356318,
      "grad_norm": 0.004066367167979479,
      "learning_rate": 0.05304025913928737,
      "loss": 0.0,
      "step": 50740
    },
    {
      "epoch": 23.484497917630726,
      "grad_norm": 0.004061320796608925,
      "learning_rate": 0.05303100416473855,
      "loss": 0.0003,
      "step": 50750
    },
    {
      "epoch": 23.489125404905135,
      "grad_norm": 0.0026957839727401733,
      "learning_rate": 0.05302174919018973,
      "loss": 0.0003,
      "step": 50760
    },
    {
      "epoch": 23.493752892179547,
      "grad_norm": 0.03817290440201759,
      "learning_rate": 0.05301249421564091,
      "loss": 0.0001,
      "step": 50770
    },
    {
      "epoch": 23.498380379453955,
      "grad_norm": 0.1281769573688507,
      "learning_rate": 0.053003239241092084,
      "loss": 0.0002,
      "step": 50780
    },
    {
      "epoch": 23.503007866728368,
      "grad_norm": 0.008016328327357769,
      "learning_rate": 0.052993984266543274,
      "loss": 0.0001,
      "step": 50790
    },
    {
      "epoch": 23.507635354002776,
      "grad_norm": 0.014336520805954933,
      "learning_rate": 0.05298472929199445,
      "loss": 0.0001,
      "step": 50800
    },
    {
      "epoch": 23.512262841277188,
      "grad_norm": 0.004072464536875486,
      "learning_rate": 0.05297547431744563,
      "loss": 0.0004,
      "step": 50810
    },
    {
      "epoch": 23.516890328551597,
      "grad_norm": 0.005880409386008978,
      "learning_rate": 0.0529662193428968,
      "loss": 0.0001,
      "step": 50820
    },
    {
      "epoch": 23.521517815826005,
      "grad_norm": 0.12550275027751923,
      "learning_rate": 0.05295696436834799,
      "loss": 0.0001,
      "step": 50830
    },
    {
      "epoch": 23.526145303100417,
      "grad_norm": 0.007820477709174156,
      "learning_rate": 0.05294770939379917,
      "loss": 0.0014,
      "step": 50840
    },
    {
      "epoch": 23.530772790374826,
      "grad_norm": 0.42932751774787903,
      "learning_rate": 0.05293845441925035,
      "loss": 0.0001,
      "step": 50850
    },
    {
      "epoch": 23.535400277649238,
      "grad_norm": 0.2755923271179199,
      "learning_rate": 0.05292919944470153,
      "loss": 0.0002,
      "step": 50860
    },
    {
      "epoch": 23.540027764923646,
      "grad_norm": 0.011708968318998814,
      "learning_rate": 0.052919944470152705,
      "loss": 0.0002,
      "step": 50870
    },
    {
      "epoch": 23.544655252198055,
      "grad_norm": 0.08952268213033676,
      "learning_rate": 0.052910689495603895,
      "loss": 0.0001,
      "step": 50880
    },
    {
      "epoch": 23.549282739472467,
      "grad_norm": 0.0190381221473217,
      "learning_rate": 0.05290143452105507,
      "loss": 0.0001,
      "step": 50890
    },
    {
      "epoch": 23.553910226746876,
      "grad_norm": 0.005903423763811588,
      "learning_rate": 0.05289217954650625,
      "loss": 0.0001,
      "step": 50900
    },
    {
      "epoch": 23.558537714021288,
      "grad_norm": 0.007178375963121653,
      "learning_rate": 0.052882924571957424,
      "loss": 0.0004,
      "step": 50910
    },
    {
      "epoch": 23.563165201295696,
      "grad_norm": 0.006341638509184122,
      "learning_rate": 0.052873669597408614,
      "loss": 0.0001,
      "step": 50920
    },
    {
      "epoch": 23.567792688570105,
      "grad_norm": 0.0005217024590820074,
      "learning_rate": 0.05286441462285979,
      "loss": 0.0,
      "step": 50930
    },
    {
      "epoch": 23.572420175844517,
      "grad_norm": 0.0032817802857607603,
      "learning_rate": 0.05285515964831097,
      "loss": 0.0001,
      "step": 50940
    },
    {
      "epoch": 23.577047663118925,
      "grad_norm": 0.0543137788772583,
      "learning_rate": 0.05284590467376215,
      "loss": 0.0002,
      "step": 50950
    },
    {
      "epoch": 23.581675150393338,
      "grad_norm": 0.0014467454748228192,
      "learning_rate": 0.052836649699213326,
      "loss": 0.0028,
      "step": 50960
    },
    {
      "epoch": 23.586302637667746,
      "grad_norm": 0.004356793127954006,
      "learning_rate": 0.052827394724664516,
      "loss": 0.0004,
      "step": 50970
    },
    {
      "epoch": 23.590930124942155,
      "grad_norm": 0.0026454937178641558,
      "learning_rate": 0.05281813975011569,
      "loss": 0.0001,
      "step": 50980
    },
    {
      "epoch": 23.595557612216567,
      "grad_norm": 0.001457075821235776,
      "learning_rate": 0.05280888477556687,
      "loss": 0.0002,
      "step": 50990
    },
    {
      "epoch": 23.600185099490975,
      "grad_norm": 0.0018900793511420488,
      "learning_rate": 0.052799629801018044,
      "loss": 0.0001,
      "step": 51000
    },
    {
      "epoch": 23.604812586765387,
      "grad_norm": 0.0039684404619038105,
      "learning_rate": 0.05279037482646923,
      "loss": 0.0001,
      "step": 51010
    },
    {
      "epoch": 23.609440074039796,
      "grad_norm": 0.0017085840227082372,
      "learning_rate": 0.05278111985192041,
      "loss": 0.0001,
      "step": 51020
    },
    {
      "epoch": 23.614067561314208,
      "grad_norm": 0.0018344928976148367,
      "learning_rate": 0.052771864877371594,
      "loss": 0.0001,
      "step": 51030
    },
    {
      "epoch": 23.618695048588616,
      "grad_norm": 0.0022019646130502224,
      "learning_rate": 0.05276260990282277,
      "loss": 0.0057,
      "step": 51040
    },
    {
      "epoch": 23.623322535863025,
      "grad_norm": 0.07861223816871643,
      "learning_rate": 0.052753354928273946,
      "loss": 0.0002,
      "step": 51050
    },
    {
      "epoch": 23.627950023137437,
      "grad_norm": 0.0004875904705841094,
      "learning_rate": 0.052744099953725136,
      "loss": 0.0,
      "step": 51060
    },
    {
      "epoch": 23.632577510411846,
      "grad_norm": 0.001074891071766615,
      "learning_rate": 0.05273484497917631,
      "loss": 0.0002,
      "step": 51070
    },
    {
      "epoch": 23.637204997686258,
      "grad_norm": 0.01964620314538479,
      "learning_rate": 0.05272559000462749,
      "loss": 0.0002,
      "step": 51080
    },
    {
      "epoch": 23.641832484960666,
      "grad_norm": 0.0015853677177801728,
      "learning_rate": 0.052716335030078665,
      "loss": 0.0002,
      "step": 51090
    },
    {
      "epoch": 23.646459972235075,
      "grad_norm": 0.0004749649087898433,
      "learning_rate": 0.05270708005552985,
      "loss": 0.0001,
      "step": 51100
    },
    {
      "epoch": 23.651087459509487,
      "grad_norm": 0.018368184566497803,
      "learning_rate": 0.05269782508098103,
      "loss": 0.0007,
      "step": 51110
    },
    {
      "epoch": 23.655714946783895,
      "grad_norm": 0.000951118883676827,
      "learning_rate": 0.052688570106432214,
      "loss": 0.0,
      "step": 51120
    },
    {
      "epoch": 23.660342434058308,
      "grad_norm": 0.0004620588733814657,
      "learning_rate": 0.05267931513188339,
      "loss": 0.0001,
      "step": 51130
    },
    {
      "epoch": 23.664969921332716,
      "grad_norm": 0.0007751869852654636,
      "learning_rate": 0.05267006015733457,
      "loss": 0.0002,
      "step": 51140
    },
    {
      "epoch": 23.669597408607125,
      "grad_norm": 0.26494455337524414,
      "learning_rate": 0.05266080518278576,
      "loss": 0.0005,
      "step": 51150
    },
    {
      "epoch": 23.674224895881537,
      "grad_norm": 0.005521364510059357,
      "learning_rate": 0.05265155020823693,
      "loss": 0.0003,
      "step": 51160
    },
    {
      "epoch": 23.678852383155945,
      "grad_norm": 0.006584520451724529,
      "learning_rate": 0.05264229523368811,
      "loss": 0.0001,
      "step": 51170
    },
    {
      "epoch": 23.683479870430357,
      "grad_norm": 0.0014676233986392617,
      "learning_rate": 0.052633040259139285,
      "loss": 0.0001,
      "step": 51180
    },
    {
      "epoch": 23.688107357704766,
      "grad_norm": 0.15180926024913788,
      "learning_rate": 0.05262378528459047,
      "loss": 0.0002,
      "step": 51190
    },
    {
      "epoch": 23.692734844979178,
      "grad_norm": 0.002476828871294856,
      "learning_rate": 0.05261453031004165,
      "loss": 0.0001,
      "step": 51200
    },
    {
      "epoch": 23.697362332253586,
      "grad_norm": 0.0007461248314939439,
      "learning_rate": 0.052605275335492835,
      "loss": 0.0,
      "step": 51210
    },
    {
      "epoch": 23.701989819527995,
      "grad_norm": 0.0009008577326312661,
      "learning_rate": 0.05259602036094401,
      "loss": 0.0,
      "step": 51220
    },
    {
      "epoch": 23.706617306802407,
      "grad_norm": 0.0008926792070269585,
      "learning_rate": 0.05258676538639519,
      "loss": 0.0,
      "step": 51230
    },
    {
      "epoch": 23.711244794076816,
      "grad_norm": 0.004823173861950636,
      "learning_rate": 0.052577510411846363,
      "loss": 0.0001,
      "step": 51240
    },
    {
      "epoch": 23.715872281351228,
      "grad_norm": 0.004561814479529858,
      "learning_rate": 0.052568255437297554,
      "loss": 0.0001,
      "step": 51250
    },
    {
      "epoch": 23.720499768625636,
      "grad_norm": 0.011698415502905846,
      "learning_rate": 0.05255900046274873,
      "loss": 0.0001,
      "step": 51260
    },
    {
      "epoch": 23.725127255900045,
      "grad_norm": 0.0007012123242020607,
      "learning_rate": 0.052549745488199906,
      "loss": 0.0001,
      "step": 51270
    },
    {
      "epoch": 23.729754743174457,
      "grad_norm": 0.003976714797317982,
      "learning_rate": 0.05254049051365109,
      "loss": 0.0001,
      "step": 51280
    },
    {
      "epoch": 23.734382230448865,
      "grad_norm": 0.002733140252530575,
      "learning_rate": 0.05253123553910227,
      "loss": 0.0001,
      "step": 51290
    },
    {
      "epoch": 23.739009717723278,
      "grad_norm": 0.0031198763754218817,
      "learning_rate": 0.052521980564553455,
      "loss": 0.0001,
      "step": 51300
    },
    {
      "epoch": 23.743637204997686,
      "grad_norm": 0.47454261779785156,
      "learning_rate": 0.05251272559000463,
      "loss": 0.0008,
      "step": 51310
    },
    {
      "epoch": 23.748264692272095,
      "grad_norm": 0.0052419546991586685,
      "learning_rate": 0.05250347061545581,
      "loss": 0.0,
      "step": 51320
    },
    {
      "epoch": 23.752892179546507,
      "grad_norm": 0.011183627881109715,
      "learning_rate": 0.052494215640906984,
      "loss": 0.0002,
      "step": 51330
    },
    {
      "epoch": 23.757519666820915,
      "grad_norm": 0.1238797977566719,
      "learning_rate": 0.052484960666358174,
      "loss": 0.001,
      "step": 51340
    },
    {
      "epoch": 23.762147154095327,
      "grad_norm": 0.0007692421786487103,
      "learning_rate": 0.05247570569180935,
      "loss": 0.0001,
      "step": 51350
    },
    {
      "epoch": 23.766774641369736,
      "grad_norm": 0.0018816430820152164,
      "learning_rate": 0.05246645071726053,
      "loss": 0.0001,
      "step": 51360
    },
    {
      "epoch": 23.771402128644148,
      "grad_norm": 0.016964800655841827,
      "learning_rate": 0.05245719574271171,
      "loss": 0.0001,
      "step": 51370
    },
    {
      "epoch": 23.776029615918556,
      "grad_norm": 0.004094691481441259,
      "learning_rate": 0.05244794076816289,
      "loss": 0.0001,
      "step": 51380
    },
    {
      "epoch": 23.780657103192965,
      "grad_norm": 0.004402387887239456,
      "learning_rate": 0.052438685793614076,
      "loss": 0.0006,
      "step": 51390
    },
    {
      "epoch": 23.785284590467377,
      "grad_norm": 0.024535229429602623,
      "learning_rate": 0.05242943081906525,
      "loss": 0.0,
      "step": 51400
    },
    {
      "epoch": 23.789912077741786,
      "grad_norm": 0.13808290660381317,
      "learning_rate": 0.05242017584451643,
      "loss": 0.0001,
      "step": 51410
    },
    {
      "epoch": 23.794539565016198,
      "grad_norm": 0.002805673284456134,
      "learning_rate": 0.052410920869967605,
      "loss": 0.0002,
      "step": 51420
    },
    {
      "epoch": 23.799167052290606,
      "grad_norm": 0.02316262014210224,
      "learning_rate": 0.052401665895418795,
      "loss": 0.0001,
      "step": 51430
    },
    {
      "epoch": 23.803794539565015,
      "grad_norm": 0.2926861345767975,
      "learning_rate": 0.05239241092086997,
      "loss": 0.0004,
      "step": 51440
    },
    {
      "epoch": 23.808422026839427,
      "grad_norm": 0.014402303844690323,
      "learning_rate": 0.05238315594632115,
      "loss": 0.0001,
      "step": 51450
    },
    {
      "epoch": 23.813049514113835,
      "grad_norm": 0.005526142194867134,
      "learning_rate": 0.05237390097177233,
      "loss": 0.0001,
      "step": 51460
    },
    {
      "epoch": 23.817677001388248,
      "grad_norm": 0.010645993985235691,
      "learning_rate": 0.052364645997223506,
      "loss": 0.0001,
      "step": 51470
    },
    {
      "epoch": 23.822304488662656,
      "grad_norm": 0.01595374569296837,
      "learning_rate": 0.0523553910226747,
      "loss": 0.0002,
      "step": 51480
    },
    {
      "epoch": 23.826931975937065,
      "grad_norm": 0.052333999425172806,
      "learning_rate": 0.05234613604812587,
      "loss": 0.0001,
      "step": 51490
    },
    {
      "epoch": 23.831559463211477,
      "grad_norm": 0.0067664203234016895,
      "learning_rate": 0.05233688107357705,
      "loss": 0.0,
      "step": 51500
    },
    {
      "epoch": 23.836186950485885,
      "grad_norm": 0.0043575698509812355,
      "learning_rate": 0.052327626099028225,
      "loss": 0.0001,
      "step": 51510
    },
    {
      "epoch": 23.840814437760297,
      "grad_norm": 0.00321244727820158,
      "learning_rate": 0.052318371124479415,
      "loss": 0.0,
      "step": 51520
    },
    {
      "epoch": 23.845441925034706,
      "grad_norm": 0.012950573116540909,
      "learning_rate": 0.05230911614993059,
      "loss": 0.0001,
      "step": 51530
    },
    {
      "epoch": 23.850069412309118,
      "grad_norm": 0.029073867946863174,
      "learning_rate": 0.05229986117538177,
      "loss": 0.0,
      "step": 51540
    },
    {
      "epoch": 23.854696899583526,
      "grad_norm": 0.040310315787792206,
      "learning_rate": 0.05229060620083295,
      "loss": 0.0001,
      "step": 51550
    },
    {
      "epoch": 23.859324386857935,
      "grad_norm": 0.005147256422787905,
      "learning_rate": 0.05228135122628413,
      "loss": 0.0,
      "step": 51560
    },
    {
      "epoch": 23.863951874132347,
      "grad_norm": 0.007131050806492567,
      "learning_rate": 0.05227209625173532,
      "loss": 0.0,
      "step": 51570
    },
    {
      "epoch": 23.868579361406756,
      "grad_norm": 0.00804345216602087,
      "learning_rate": 0.05226284127718649,
      "loss": 0.0,
      "step": 51580
    },
    {
      "epoch": 23.873206848681168,
      "grad_norm": 0.0005648310179822147,
      "learning_rate": 0.05225358630263767,
      "loss": 0.0001,
      "step": 51590
    },
    {
      "epoch": 23.877834335955576,
      "grad_norm": 0.471344918012619,
      "learning_rate": 0.052244331328088846,
      "loss": 0.0001,
      "step": 51600
    },
    {
      "epoch": 23.882461823229985,
      "grad_norm": 0.007382526528090239,
      "learning_rate": 0.052235076353540036,
      "loss": 0.0003,
      "step": 51610
    },
    {
      "epoch": 23.887089310504397,
      "grad_norm": 0.0002042033156612888,
      "learning_rate": 0.05222582137899121,
      "loss": 0.0001,
      "step": 51620
    },
    {
      "epoch": 23.891716797778805,
      "grad_norm": 0.023969857022166252,
      "learning_rate": 0.05221656640444239,
      "loss": 0.0001,
      "step": 51630
    },
    {
      "epoch": 23.896344285053218,
      "grad_norm": 0.501129150390625,
      "learning_rate": 0.05220731142989357,
      "loss": 0.0003,
      "step": 51640
    },
    {
      "epoch": 23.900971772327626,
      "grad_norm": 0.009183238260447979,
      "learning_rate": 0.05219805645534475,
      "loss": 0.0002,
      "step": 51650
    },
    {
      "epoch": 23.905599259602035,
      "grad_norm": 0.0006776624359190464,
      "learning_rate": 0.05218880148079594,
      "loss": 0.0001,
      "step": 51660
    },
    {
      "epoch": 23.910226746876447,
      "grad_norm": 0.03165667876601219,
      "learning_rate": 0.052179546506247114,
      "loss": 0.0002,
      "step": 51670
    },
    {
      "epoch": 23.914854234150855,
      "grad_norm": 0.0013068719999864697,
      "learning_rate": 0.05217029153169829,
      "loss": 0.0001,
      "step": 51680
    },
    {
      "epoch": 23.919481721425267,
      "grad_norm": 0.026192795485258102,
      "learning_rate": 0.052161036557149466,
      "loss": 0.0001,
      "step": 51690
    },
    {
      "epoch": 23.924109208699676,
      "grad_norm": 0.03353678435087204,
      "learning_rate": 0.05215178158260064,
      "loss": 0.0001,
      "step": 51700
    },
    {
      "epoch": 23.928736695974084,
      "grad_norm": 0.0069399564526975155,
      "learning_rate": 0.05214252660805183,
      "loss": 0.0001,
      "step": 51710
    },
    {
      "epoch": 23.933364183248496,
      "grad_norm": 0.0006235969485715032,
      "learning_rate": 0.05213327163350301,
      "loss": 0.0001,
      "step": 51720
    },
    {
      "epoch": 23.937991670522905,
      "grad_norm": 0.0007504377281293273,
      "learning_rate": 0.05212401665895419,
      "loss": 0.0001,
      "step": 51730
    },
    {
      "epoch": 23.942619157797317,
      "grad_norm": 0.034239985048770905,
      "learning_rate": 0.05211476168440537,
      "loss": 0.0001,
      "step": 51740
    },
    {
      "epoch": 23.947246645071726,
      "grad_norm": 0.003296023001894355,
      "learning_rate": 0.05210550670985656,
      "loss": 0.0001,
      "step": 51750
    },
    {
      "epoch": 23.951874132346138,
      "grad_norm": 0.01392656285315752,
      "learning_rate": 0.052096251735307735,
      "loss": 0.0,
      "step": 51760
    },
    {
      "epoch": 23.956501619620546,
      "grad_norm": 0.00651245703920722,
      "learning_rate": 0.05208699676075891,
      "loss": 0.0002,
      "step": 51770
    },
    {
      "epoch": 23.961129106894955,
      "grad_norm": 0.021819602698087692,
      "learning_rate": 0.05207774178621009,
      "loss": 0.0,
      "step": 51780
    },
    {
      "epoch": 23.965756594169367,
      "grad_norm": 0.0029605927411466837,
      "learning_rate": 0.05206848681166126,
      "loss": 0.0001,
      "step": 51790
    },
    {
      "epoch": 23.970384081443775,
      "grad_norm": 0.0014703440247103572,
      "learning_rate": 0.05205923183711245,
      "loss": 0.0001,
      "step": 51800
    },
    {
      "epoch": 23.975011568718188,
      "grad_norm": 0.0011900544632226229,
      "learning_rate": 0.05204997686256363,
      "loss": 0.0001,
      "step": 51810
    },
    {
      "epoch": 23.979639055992596,
      "grad_norm": 0.06487493962049484,
      "learning_rate": 0.05204072188801481,
      "loss": 0.0001,
      "step": 51820
    },
    {
      "epoch": 23.984266543267005,
      "grad_norm": 0.012156752869486809,
      "learning_rate": 0.05203146691346599,
      "loss": 0.0001,
      "step": 51830
    },
    {
      "epoch": 23.988894030541417,
      "grad_norm": 0.0013785713817924261,
      "learning_rate": 0.05202221193891718,
      "loss": 0.0004,
      "step": 51840
    },
    {
      "epoch": 23.993521517815825,
      "grad_norm": 0.0819658637046814,
      "learning_rate": 0.052012956964368355,
      "loss": 0.0001,
      "step": 51850
    },
    {
      "epoch": 23.998149005090237,
      "grad_norm": 0.03935674950480461,
      "learning_rate": 0.05200370198981953,
      "loss": 0.0001,
      "step": 51860
    },
    {
      "epoch": 24.0,
      "eval_accuracy_branch1": 0.9902942535818826,
      "eval_accuracy_branch2": 0.5000770297334771,
      "eval_f1_branch1": 0.9910434041728898,
      "eval_f1_branch2": 0.49972165979402206,
      "eval_loss": 0.01850741170346737,
      "eval_precision_branch1": 0.991672220806279,
      "eval_precision_branch2": 0.5000772492277159,
      "eval_recall_branch1": 0.9905158805221563,
      "eval_recall_branch2": 0.5000770297334771,
      "eval_runtime": 28.9091,
      "eval_samples_per_second": 449.063,
      "eval_steps_per_second": 56.142,
      "step": 51864
    },
    {
      "epoch": 24.002776492364646,
      "grad_norm": 0.0013199124950915575,
      "learning_rate": 0.05199444701527071,
      "loss": 0.2279,
      "step": 51870
    },
    {
      "epoch": 24.007403979639054,
      "grad_norm": 0.0012215638998895884,
      "learning_rate": 0.051985192040721884,
      "loss": 0.0001,
      "step": 51880
    },
    {
      "epoch": 24.012031466913466,
      "grad_norm": 0.001381628680974245,
      "learning_rate": 0.051975937066173074,
      "loss": 0.0001,
      "step": 51890
    },
    {
      "epoch": 24.016658954187875,
      "grad_norm": 0.03634483367204666,
      "learning_rate": 0.05196668209162425,
      "loss": 0.0004,
      "step": 51900
    },
    {
      "epoch": 24.021286441462287,
      "grad_norm": 0.02131275087594986,
      "learning_rate": 0.05195742711707543,
      "loss": 0.0,
      "step": 51910
    },
    {
      "epoch": 24.025913928736696,
      "grad_norm": 0.0041617597453296185,
      "learning_rate": 0.05194817214252661,
      "loss": 0.0,
      "step": 51920
    },
    {
      "epoch": 24.030541416011108,
      "grad_norm": 0.004158482886850834,
      "learning_rate": 0.051938917167977786,
      "loss": 0.0001,
      "step": 51930
    },
    {
      "epoch": 24.035168903285516,
      "grad_norm": 0.0827292874455452,
      "learning_rate": 0.051929662193428976,
      "loss": 0.0002,
      "step": 51940
    },
    {
      "epoch": 24.039796390559925,
      "grad_norm": 0.03916348144412041,
      "learning_rate": 0.05192040721888015,
      "loss": 0.0001,
      "step": 51950
    },
    {
      "epoch": 24.044423877834337,
      "grad_norm": 0.004727036226540804,
      "learning_rate": 0.05191115224433133,
      "loss": 0.0001,
      "step": 51960
    },
    {
      "epoch": 24.049051365108745,
      "grad_norm": 0.004190419800579548,
      "learning_rate": 0.051901897269782504,
      "loss": 0.0002,
      "step": 51970
    },
    {
      "epoch": 24.053678852383158,
      "grad_norm": 0.011945478618144989,
      "learning_rate": 0.051892642295233694,
      "loss": 0.0001,
      "step": 51980
    },
    {
      "epoch": 24.058306339657566,
      "grad_norm": 0.01848234236240387,
      "learning_rate": 0.05188338732068487,
      "loss": 0.0001,
      "step": 51990
    },
    {
      "epoch": 24.062933826931975,
      "grad_norm": 0.027632974088191986,
      "learning_rate": 0.051874132346136054,
      "loss": 0.0003,
      "step": 52000
    },
    {
      "epoch": 24.067561314206387,
      "grad_norm": 0.007651297375559807,
      "learning_rate": 0.05186487737158723,
      "loss": 0.0001,
      "step": 52010
    },
    {
      "epoch": 24.072188801480795,
      "grad_norm": 0.015119628049433231,
      "learning_rate": 0.051855622397038406,
      "loss": 0.0001,
      "step": 52020
    },
    {
      "epoch": 24.076816288755207,
      "grad_norm": 0.015089467167854309,
      "learning_rate": 0.051846367422489596,
      "loss": 0.0008,
      "step": 52030
    },
    {
      "epoch": 24.081443776029616,
      "grad_norm": 0.8111355304718018,
      "learning_rate": 0.05183711244794077,
      "loss": 0.0005,
      "step": 52040
    },
    {
      "epoch": 24.086071263304024,
      "grad_norm": 0.004117122385650873,
      "learning_rate": 0.05182785747339195,
      "loss": 0.0001,
      "step": 52050
    },
    {
      "epoch": 24.090698750578436,
      "grad_norm": 0.008253292180597782,
      "learning_rate": 0.051818602498843125,
      "loss": 0.0004,
      "step": 52060
    },
    {
      "epoch": 24.095326237852845,
      "grad_norm": 0.001003126846626401,
      "learning_rate": 0.051809347524294315,
      "loss": 0.0002,
      "step": 52070
    },
    {
      "epoch": 24.099953725127257,
      "grad_norm": 0.13127076625823975,
      "learning_rate": 0.05180009254974549,
      "loss": 0.0029,
      "step": 52080
    },
    {
      "epoch": 24.104581212401666,
      "grad_norm": 0.003198868129402399,
      "learning_rate": 0.051790837575196674,
      "loss": 0.0001,
      "step": 52090
    },
    {
      "epoch": 24.109208699676074,
      "grad_norm": 0.06568601727485657,
      "learning_rate": 0.05178158260064785,
      "loss": 0.0002,
      "step": 52100
    },
    {
      "epoch": 24.113836186950486,
      "grad_norm": 0.006179410498589277,
      "learning_rate": 0.05177232762609903,
      "loss": 0.0001,
      "step": 52110
    },
    {
      "epoch": 24.118463674224895,
      "grad_norm": 0.01455255039036274,
      "learning_rate": 0.05176307265155022,
      "loss": 0.0003,
      "step": 52120
    },
    {
      "epoch": 24.123091161499307,
      "grad_norm": 0.05787811055779457,
      "learning_rate": 0.05175381767700139,
      "loss": 0.0001,
      "step": 52130
    },
    {
      "epoch": 24.127718648773715,
      "grad_norm": 0.0009907700587064028,
      "learning_rate": 0.05174456270245257,
      "loss": 0.0001,
      "step": 52140
    },
    {
      "epoch": 24.132346136048128,
      "grad_norm": 0.005214313976466656,
      "learning_rate": 0.051735307727903745,
      "loss": 0.0001,
      "step": 52150
    },
    {
      "epoch": 24.136973623322536,
      "grad_norm": 0.0017169404309242964,
      "learning_rate": 0.05172605275335493,
      "loss": 0.0001,
      "step": 52160
    },
    {
      "epoch": 24.141601110596945,
      "grad_norm": 0.034850966185331345,
      "learning_rate": 0.05171679777880611,
      "loss": 0.0,
      "step": 52170
    },
    {
      "epoch": 24.146228597871357,
      "grad_norm": 0.1734377145767212,
      "learning_rate": 0.051707542804257295,
      "loss": 0.0008,
      "step": 52180
    },
    {
      "epoch": 24.150856085145765,
      "grad_norm": 0.0038855348248034716,
      "learning_rate": 0.05169828782970847,
      "loss": 0.0001,
      "step": 52190
    },
    {
      "epoch": 24.155483572420177,
      "grad_norm": 0.2281123846769333,
      "learning_rate": 0.05168903285515965,
      "loss": 0.0001,
      "step": 52200
    },
    {
      "epoch": 24.160111059694586,
      "grad_norm": 0.06576038897037506,
      "learning_rate": 0.05167977788061084,
      "loss": 0.0004,
      "step": 52210
    },
    {
      "epoch": 24.164738546968994,
      "grad_norm": 0.5704306960105896,
      "learning_rate": 0.051670522906062014,
      "loss": 0.0002,
      "step": 52220
    },
    {
      "epoch": 24.169366034243406,
      "grad_norm": 0.0021349755115807056,
      "learning_rate": 0.05166126793151319,
      "loss": 0.0008,
      "step": 52230
    },
    {
      "epoch": 24.173993521517815,
      "grad_norm": 0.011960518546402454,
      "learning_rate": 0.051652012956964366,
      "loss": 0.0002,
      "step": 52240
    },
    {
      "epoch": 24.178621008792227,
      "grad_norm": 0.002356145763769746,
      "learning_rate": 0.05164275798241555,
      "loss": 0.0001,
      "step": 52250
    },
    {
      "epoch": 24.183248496066636,
      "grad_norm": 0.027062905952334404,
      "learning_rate": 0.05163350300786673,
      "loss": 0.0001,
      "step": 52260
    },
    {
      "epoch": 24.187875983341044,
      "grad_norm": 0.0008221580646932125,
      "learning_rate": 0.051624248033317915,
      "loss": 0.0024,
      "step": 52270
    },
    {
      "epoch": 24.192503470615456,
      "grad_norm": 0.03437066450715065,
      "learning_rate": 0.05161499305876909,
      "loss": 0.0001,
      "step": 52280
    },
    {
      "epoch": 24.197130957889865,
      "grad_norm": 0.0037469558883458376,
      "learning_rate": 0.05160573808422027,
      "loss": 0.0016,
      "step": 52290
    },
    {
      "epoch": 24.201758445164277,
      "grad_norm": 0.003371479921042919,
      "learning_rate": 0.05159648310967146,
      "loss": 0.0001,
      "step": 52300
    },
    {
      "epoch": 24.206385932438685,
      "grad_norm": 0.013282488100230694,
      "learning_rate": 0.051587228135122634,
      "loss": 0.0003,
      "step": 52310
    },
    {
      "epoch": 24.211013419713098,
      "grad_norm": 0.004060323815792799,
      "learning_rate": 0.05157797316057381,
      "loss": 0.0018,
      "step": 52320
    },
    {
      "epoch": 24.215640906987506,
      "grad_norm": 0.0030514858663082123,
      "learning_rate": 0.05156871818602499,
      "loss": 0.0001,
      "step": 52330
    },
    {
      "epoch": 24.220268394261915,
      "grad_norm": 0.000863314897287637,
      "learning_rate": 0.05155946321147617,
      "loss": 0.0,
      "step": 52340
    },
    {
      "epoch": 24.224895881536327,
      "grad_norm": 0.13544654846191406,
      "learning_rate": 0.05155020823692735,
      "loss": 0.0001,
      "step": 52350
    },
    {
      "epoch": 24.229523368810735,
      "grad_norm": 0.007191947195678949,
      "learning_rate": 0.051540953262378536,
      "loss": 0.0001,
      "step": 52360
    },
    {
      "epoch": 24.234150856085147,
      "grad_norm": 0.013237299397587776,
      "learning_rate": 0.05153169828782971,
      "loss": 0.0001,
      "step": 52370
    },
    {
      "epoch": 24.238778343359556,
      "grad_norm": 0.0011717068264260888,
      "learning_rate": 0.05152244331328089,
      "loss": 0.0031,
      "step": 52380
    },
    {
      "epoch": 24.243405830633964,
      "grad_norm": 0.03054330125451088,
      "learning_rate": 0.051513188338732065,
      "loss": 0.0002,
      "step": 52390
    },
    {
      "epoch": 24.248033317908376,
      "grad_norm": 0.0034773126244544983,
      "learning_rate": 0.051503933364183255,
      "loss": 0.0,
      "step": 52400
    },
    {
      "epoch": 24.252660805182785,
      "grad_norm": 0.020062070339918137,
      "learning_rate": 0.05149467838963443,
      "loss": 0.0002,
      "step": 52410
    },
    {
      "epoch": 24.257288292457197,
      "grad_norm": 0.05274813622236252,
      "learning_rate": 0.05148542341508561,
      "loss": 0.0002,
      "step": 52420
    },
    {
      "epoch": 24.261915779731606,
      "grad_norm": 0.0017931743059307337,
      "learning_rate": 0.05147616844053679,
      "loss": 0.0001,
      "step": 52430
    },
    {
      "epoch": 24.266543267006014,
      "grad_norm": 0.0011243076296523213,
      "learning_rate": 0.05146691346598797,
      "loss": 0.0001,
      "step": 52440
    },
    {
      "epoch": 24.271170754280426,
      "grad_norm": 0.009786609560251236,
      "learning_rate": 0.05145765849143916,
      "loss": 0.0,
      "step": 52450
    },
    {
      "epoch": 24.275798241554835,
      "grad_norm": 0.016327990218997,
      "learning_rate": 0.05144840351689033,
      "loss": 0.0018,
      "step": 52460
    },
    {
      "epoch": 24.280425728829247,
      "grad_norm": 0.04681850224733353,
      "learning_rate": 0.05143914854234151,
      "loss": 0.001,
      "step": 52470
    },
    {
      "epoch": 24.285053216103655,
      "grad_norm": 0.002226388081908226,
      "learning_rate": 0.051429893567792685,
      "loss": 0.0001,
      "step": 52480
    },
    {
      "epoch": 24.289680703378068,
      "grad_norm": 0.0027763766702264547,
      "learning_rate": 0.051420638593243875,
      "loss": 0.0002,
      "step": 52490
    },
    {
      "epoch": 24.294308190652476,
      "grad_norm": 0.908197820186615,
      "learning_rate": 0.05141138361869505,
      "loss": 0.0007,
      "step": 52500
    },
    {
      "epoch": 24.298935677926885,
      "grad_norm": 0.014352342113852501,
      "learning_rate": 0.05140212864414623,
      "loss": 0.0004,
      "step": 52510
    },
    {
      "epoch": 24.303563165201297,
      "grad_norm": 0.0013051606947556138,
      "learning_rate": 0.05139287366959741,
      "loss": 0.0005,
      "step": 52520
    },
    {
      "epoch": 24.308190652475705,
      "grad_norm": 4.997481346130371,
      "learning_rate": 0.051383618695048594,
      "loss": 0.005,
      "step": 52530
    },
    {
      "epoch": 24.312818139750117,
      "grad_norm": 0.006324837449938059,
      "learning_rate": 0.05137436372049978,
      "loss": 0.0001,
      "step": 52540
    },
    {
      "epoch": 24.317445627024526,
      "grad_norm": 0.003031436586752534,
      "learning_rate": 0.05136510874595095,
      "loss": 0.0001,
      "step": 52550
    },
    {
      "epoch": 24.322073114298934,
      "grad_norm": 0.009440847672522068,
      "learning_rate": 0.05135585377140213,
      "loss": 0.0001,
      "step": 52560
    },
    {
      "epoch": 24.326700601573346,
      "grad_norm": 0.0017001027008518577,
      "learning_rate": 0.051346598796853306,
      "loss": 0.0005,
      "step": 52570
    },
    {
      "epoch": 24.331328088847755,
      "grad_norm": 0.01755438558757305,
      "learning_rate": 0.051337343822304496,
      "loss": 0.0001,
      "step": 52580
    },
    {
      "epoch": 24.335955576122167,
      "grad_norm": 0.02282504178583622,
      "learning_rate": 0.05132808884775567,
      "loss": 0.0004,
      "step": 52590
    },
    {
      "epoch": 24.340583063396576,
      "grad_norm": 0.019970446825027466,
      "learning_rate": 0.05131883387320685,
      "loss": 0.0002,
      "step": 52600
    },
    {
      "epoch": 24.345210550670984,
      "grad_norm": 0.005104162730276585,
      "learning_rate": 0.05130957889865803,
      "loss": 0.0001,
      "step": 52610
    },
    {
      "epoch": 24.349838037945396,
      "grad_norm": 0.13883374631404877,
      "learning_rate": 0.05130032392410921,
      "loss": 0.001,
      "step": 52620
    },
    {
      "epoch": 24.354465525219805,
      "grad_norm": 0.006563196424394846,
      "learning_rate": 0.0512910689495604,
      "loss": 0.0004,
      "step": 52630
    },
    {
      "epoch": 24.359093012494217,
      "grad_norm": 0.008486275561153889,
      "learning_rate": 0.051281813975011574,
      "loss": 0.0001,
      "step": 52640
    },
    {
      "epoch": 24.363720499768625,
      "grad_norm": 0.011831626296043396,
      "learning_rate": 0.05127255900046275,
      "loss": 0.0,
      "step": 52650
    },
    {
      "epoch": 24.368347987043034,
      "grad_norm": 0.002282073488458991,
      "learning_rate": 0.051263304025913926,
      "loss": 0.0001,
      "step": 52660
    },
    {
      "epoch": 24.372975474317446,
      "grad_norm": 0.009380658157169819,
      "learning_rate": 0.051254049051365116,
      "loss": 0.0001,
      "step": 52670
    },
    {
      "epoch": 24.377602961591855,
      "grad_norm": 0.013740818947553635,
      "learning_rate": 0.05124479407681629,
      "loss": 0.0001,
      "step": 52680
    },
    {
      "epoch": 24.382230448866267,
      "grad_norm": 0.0016682954737916589,
      "learning_rate": 0.05123553910226747,
      "loss": 0.0001,
      "step": 52690
    },
    {
      "epoch": 24.386857936140675,
      "grad_norm": 0.16105039417743683,
      "learning_rate": 0.05122628412771865,
      "loss": 0.0002,
      "step": 52700
    },
    {
      "epoch": 24.391485423415087,
      "grad_norm": 0.0016814668197184801,
      "learning_rate": 0.05121702915316983,
      "loss": 0.0,
      "step": 52710
    },
    {
      "epoch": 24.396112910689496,
      "grad_norm": 0.00878390297293663,
      "learning_rate": 0.05120777417862102,
      "loss": 0.0002,
      "step": 52720
    },
    {
      "epoch": 24.400740397963904,
      "grad_norm": 0.017186280339956284,
      "learning_rate": 0.051198519204072195,
      "loss": 0.0005,
      "step": 52730
    },
    {
      "epoch": 24.405367885238316,
      "grad_norm": 0.0007422920898534358,
      "learning_rate": 0.05118926422952337,
      "loss": 0.0003,
      "step": 52740
    },
    {
      "epoch": 24.409995372512725,
      "grad_norm": 0.002389518776908517,
      "learning_rate": 0.05118000925497455,
      "loss": 0.0001,
      "step": 52750
    },
    {
      "epoch": 24.414622859787137,
      "grad_norm": 0.0020364848896861076,
      "learning_rate": 0.05117075428042574,
      "loss": 0.0001,
      "step": 52760
    },
    {
      "epoch": 24.419250347061546,
      "grad_norm": 0.00014603063755203038,
      "learning_rate": 0.05116149930587691,
      "loss": 0.0001,
      "step": 52770
    },
    {
      "epoch": 24.423877834335954,
      "grad_norm": 0.0007332020904868841,
      "learning_rate": 0.05115224433132809,
      "loss": 0.0001,
      "step": 52780
    },
    {
      "epoch": 24.428505321610366,
      "grad_norm": 0.0012788277817890048,
      "learning_rate": 0.05114298935677927,
      "loss": 0.0002,
      "step": 52790
    },
    {
      "epoch": 24.433132808884775,
      "grad_norm": 0.04993375763297081,
      "learning_rate": 0.05113373438223045,
      "loss": 0.0002,
      "step": 52800
    },
    {
      "epoch": 24.437760296159187,
      "grad_norm": 0.363821804523468,
      "learning_rate": 0.05112447940768164,
      "loss": 0.0002,
      "step": 52810
    },
    {
      "epoch": 24.442387783433595,
      "grad_norm": 0.05863816663622856,
      "learning_rate": 0.051115224433132815,
      "loss": 0.0,
      "step": 52820
    },
    {
      "epoch": 24.447015270708004,
      "grad_norm": 0.009776606224477291,
      "learning_rate": 0.05110596945858399,
      "loss": 0.0001,
      "step": 52830
    },
    {
      "epoch": 24.451642757982416,
      "grad_norm": 0.005181611981242895,
      "learning_rate": 0.05109671448403517,
      "loss": 0.0,
      "step": 52840
    },
    {
      "epoch": 24.456270245256825,
      "grad_norm": 0.019397685304284096,
      "learning_rate": 0.051087459509486344,
      "loss": 0.0001,
      "step": 52850
    },
    {
      "epoch": 24.460897732531237,
      "grad_norm": 0.0073225293308496475,
      "learning_rate": 0.051078204534937534,
      "loss": 0.0003,
      "step": 52860
    },
    {
      "epoch": 24.465525219805645,
      "grad_norm": 0.0030668159015476704,
      "learning_rate": 0.05106894956038871,
      "loss": 0.0001,
      "step": 52870
    },
    {
      "epoch": 24.470152707080054,
      "grad_norm": 0.008328082971274853,
      "learning_rate": 0.05105969458583989,
      "loss": 0.0,
      "step": 52880
    },
    {
      "epoch": 24.474780194354466,
      "grad_norm": 0.0018605425721034408,
      "learning_rate": 0.05105043961129107,
      "loss": 0.009,
      "step": 52890
    },
    {
      "epoch": 24.479407681628874,
      "grad_norm": 0.0010895546292886138,
      "learning_rate": 0.05104118463674226,
      "loss": 0.0,
      "step": 52900
    },
    {
      "epoch": 24.484035168903286,
      "grad_norm": 0.02131485939025879,
      "learning_rate": 0.051031929662193436,
      "loss": 0.0001,
      "step": 52910
    },
    {
      "epoch": 24.488662656177695,
      "grad_norm": 0.012999825179576874,
      "learning_rate": 0.05102267468764461,
      "loss": 0.0001,
      "step": 52920
    },
    {
      "epoch": 24.493290143452107,
      "grad_norm": 0.0018271512817591429,
      "learning_rate": 0.05101341971309579,
      "loss": 0.0001,
      "step": 52930
    },
    {
      "epoch": 24.497917630726516,
      "grad_norm": 0.005238136742264032,
      "learning_rate": 0.051004164738546964,
      "loss": 0.0001,
      "step": 52940
    },
    {
      "epoch": 24.502545118000924,
      "grad_norm": 0.8616383075714111,
      "learning_rate": 0.050994909763998154,
      "loss": 0.0002,
      "step": 52950
    },
    {
      "epoch": 24.507172605275336,
      "grad_norm": 0.0011091885389760137,
      "learning_rate": 0.05098565478944933,
      "loss": 0.0001,
      "step": 52960
    },
    {
      "epoch": 24.511800092549745,
      "grad_norm": 0.007264053449034691,
      "learning_rate": 0.050976399814900514,
      "loss": 0.0002,
      "step": 52970
    },
    {
      "epoch": 24.516427579824157,
      "grad_norm": 0.012425091117620468,
      "learning_rate": 0.05096714484035169,
      "loss": 0.0001,
      "step": 52980
    },
    {
      "epoch": 24.521055067098565,
      "grad_norm": 0.006721604615449905,
      "learning_rate": 0.05095788986580288,
      "loss": 0.0004,
      "step": 52990
    },
    {
      "epoch": 24.525682554372974,
      "grad_norm": 0.03147038072347641,
      "learning_rate": 0.050948634891254056,
      "loss": 0.0001,
      "step": 53000
    },
    {
      "epoch": 24.530310041647386,
      "grad_norm": 0.007201654836535454,
      "learning_rate": 0.05093937991670523,
      "loss": 0.0001,
      "step": 53010
    },
    {
      "epoch": 24.534937528921795,
      "grad_norm": 0.034986190497875214,
      "learning_rate": 0.05093012494215641,
      "loss": 0.0004,
      "step": 53020
    },
    {
      "epoch": 24.539565016196207,
      "grad_norm": 0.006736329756677151,
      "learning_rate": 0.050920869967607585,
      "loss": 0.0017,
      "step": 53030
    },
    {
      "epoch": 24.544192503470615,
      "grad_norm": 0.003942573443055153,
      "learning_rate": 0.050911614993058775,
      "loss": 0.0001,
      "step": 53040
    },
    {
      "epoch": 24.548819990745024,
      "grad_norm": 0.00036408958840183914,
      "learning_rate": 0.05090236001850995,
      "loss": 0.0004,
      "step": 53050
    },
    {
      "epoch": 24.553447478019436,
      "grad_norm": 0.0023072045296430588,
      "learning_rate": 0.050893105043961134,
      "loss": 0.0002,
      "step": 53060
    },
    {
      "epoch": 24.558074965293844,
      "grad_norm": 0.13659191131591797,
      "learning_rate": 0.05088385006941231,
      "loss": 0.0002,
      "step": 53070
    },
    {
      "epoch": 24.562702452568256,
      "grad_norm": 0.010932717472314835,
      "learning_rate": 0.05087459509486349,
      "loss": 0.0001,
      "step": 53080
    },
    {
      "epoch": 24.567329939842665,
      "grad_norm": 1.8175352811813354,
      "learning_rate": 0.05086534012031468,
      "loss": 0.0006,
      "step": 53090
    },
    {
      "epoch": 24.571957427117077,
      "grad_norm": 0.01621433161199093,
      "learning_rate": 0.05085608514576585,
      "loss": 0.0004,
      "step": 53100
    },
    {
      "epoch": 24.576584914391486,
      "grad_norm": 0.019660810008645058,
      "learning_rate": 0.05084683017121703,
      "loss": 0.001,
      "step": 53110
    },
    {
      "epoch": 24.581212401665894,
      "grad_norm": 0.004189637489616871,
      "learning_rate": 0.050837575196668205,
      "loss": 0.0003,
      "step": 53120
    },
    {
      "epoch": 24.585839888940306,
      "grad_norm": 0.015868304297327995,
      "learning_rate": 0.050828320222119396,
      "loss": 0.0002,
      "step": 53130
    },
    {
      "epoch": 24.590467376214715,
      "grad_norm": 0.0022657124791294336,
      "learning_rate": 0.05081906524757057,
      "loss": 0.0002,
      "step": 53140
    },
    {
      "epoch": 24.595094863489127,
      "grad_norm": 0.010498895309865475,
      "learning_rate": 0.050809810273021755,
      "loss": 0.0001,
      "step": 53150
    },
    {
      "epoch": 24.599722350763535,
      "grad_norm": 0.005481571890413761,
      "learning_rate": 0.05080055529847293,
      "loss": 0.0001,
      "step": 53160
    },
    {
      "epoch": 24.604349838037944,
      "grad_norm": 0.04121147096157074,
      "learning_rate": 0.05079130032392411,
      "loss": 0.0001,
      "step": 53170
    },
    {
      "epoch": 24.608977325312356,
      "grad_norm": 0.011004701256752014,
      "learning_rate": 0.0507820453493753,
      "loss": 0.0004,
      "step": 53180
    },
    {
      "epoch": 24.613604812586765,
      "grad_norm": 0.017619026824831963,
      "learning_rate": 0.050772790374826474,
      "loss": 0.0001,
      "step": 53190
    },
    {
      "epoch": 24.618232299861177,
      "grad_norm": 0.03026646189391613,
      "learning_rate": 0.05076353540027765,
      "loss": 0.0001,
      "step": 53200
    },
    {
      "epoch": 24.622859787135585,
      "grad_norm": 0.016670167446136475,
      "learning_rate": 0.050754280425728826,
      "loss": 0.0001,
      "step": 53210
    },
    {
      "epoch": 24.627487274409994,
      "grad_norm": 0.004567610565572977,
      "learning_rate": 0.050745025451180016,
      "loss": 0.0001,
      "step": 53220
    },
    {
      "epoch": 24.632114761684406,
      "grad_norm": 0.0007118767243809998,
      "learning_rate": 0.05073577047663119,
      "loss": 0.0001,
      "step": 53230
    },
    {
      "epoch": 24.636742248958814,
      "grad_norm": 0.0029203626327216625,
      "learning_rate": 0.050726515502082375,
      "loss": 0.0001,
      "step": 53240
    },
    {
      "epoch": 24.641369736233226,
      "grad_norm": 0.006644801702350378,
      "learning_rate": 0.05071726052753355,
      "loss": 0.0002,
      "step": 53250
    },
    {
      "epoch": 24.645997223507635,
      "grad_norm": 0.0030765540432184935,
      "learning_rate": 0.05070800555298473,
      "loss": 0.0001,
      "step": 53260
    },
    {
      "epoch": 24.650624710782047,
      "grad_norm": 0.0014554394874721766,
      "learning_rate": 0.05069875057843592,
      "loss": 0.0001,
      "step": 53270
    },
    {
      "epoch": 24.655252198056456,
      "grad_norm": 0.11897175759077072,
      "learning_rate": 0.050689495603887094,
      "loss": 0.0001,
      "step": 53280
    },
    {
      "epoch": 24.659879685330864,
      "grad_norm": 0.0051908111199736595,
      "learning_rate": 0.05068024062933827,
      "loss": 0.0,
      "step": 53290
    },
    {
      "epoch": 24.664507172605276,
      "grad_norm": 0.0008067690650932491,
      "learning_rate": 0.05067098565478945,
      "loss": 0.0001,
      "step": 53300
    },
    {
      "epoch": 24.669134659879685,
      "grad_norm": 0.05433784797787666,
      "learning_rate": 0.05066173068024063,
      "loss": 0.0001,
      "step": 53310
    },
    {
      "epoch": 24.673762147154097,
      "grad_norm": 0.002562244888395071,
      "learning_rate": 0.05065247570569181,
      "loss": 0.0001,
      "step": 53320
    },
    {
      "epoch": 24.678389634428505,
      "grad_norm": 0.2888624668121338,
      "learning_rate": 0.050643220731142996,
      "loss": 0.0001,
      "step": 53330
    },
    {
      "epoch": 24.683017121702914,
      "grad_norm": 0.0007144598639570177,
      "learning_rate": 0.05063396575659417,
      "loss": 0.0001,
      "step": 53340
    },
    {
      "epoch": 24.687644608977326,
      "grad_norm": 0.009716528467833996,
      "learning_rate": 0.05062471078204535,
      "loss": 0.0001,
      "step": 53350
    },
    {
      "epoch": 24.692272096251735,
      "grad_norm": 0.005824538879096508,
      "learning_rate": 0.05061545580749654,
      "loss": 0.0001,
      "step": 53360
    },
    {
      "epoch": 24.696899583526147,
      "grad_norm": 0.0011215182021260262,
      "learning_rate": 0.050606200832947715,
      "loss": 0.0,
      "step": 53370
    },
    {
      "epoch": 24.701527070800555,
      "grad_norm": 0.004487974569201469,
      "learning_rate": 0.05059694585839889,
      "loss": 0.0001,
      "step": 53380
    },
    {
      "epoch": 24.706154558074964,
      "grad_norm": 0.04012962803244591,
      "learning_rate": 0.05058769088385007,
      "loss": 0.0001,
      "step": 53390
    },
    {
      "epoch": 24.710782045349376,
      "grad_norm": 0.0027700625360012054,
      "learning_rate": 0.05057843590930125,
      "loss": 0.0001,
      "step": 53400
    },
    {
      "epoch": 24.715409532623784,
      "grad_norm": 0.004343293607234955,
      "learning_rate": 0.050569180934752433,
      "loss": 0.0001,
      "step": 53410
    },
    {
      "epoch": 24.720037019898196,
      "grad_norm": 0.08441781252622604,
      "learning_rate": 0.05055992596020362,
      "loss": 0.0002,
      "step": 53420
    },
    {
      "epoch": 24.724664507172605,
      "grad_norm": 0.011058193631470203,
      "learning_rate": 0.05055067098565479,
      "loss": 0.0003,
      "step": 53430
    },
    {
      "epoch": 24.729291994447017,
      "grad_norm": 0.003355390392243862,
      "learning_rate": 0.05054141601110597,
      "loss": 0.0001,
      "step": 53440
    },
    {
      "epoch": 24.733919481721426,
      "grad_norm": 0.025793693959712982,
      "learning_rate": 0.05053216103655716,
      "loss": 0.0005,
      "step": 53450
    },
    {
      "epoch": 24.738546968995834,
      "grad_norm": 0.006383617874234915,
      "learning_rate": 0.050522906062008335,
      "loss": 0.0,
      "step": 53460
    },
    {
      "epoch": 24.743174456270246,
      "grad_norm": 0.01625129207968712,
      "learning_rate": 0.05051365108745951,
      "loss": 0.0001,
      "step": 53470
    },
    {
      "epoch": 24.747801943544655,
      "grad_norm": 0.0009601294877938926,
      "learning_rate": 0.05050439611291069,
      "loss": 0.0,
      "step": 53480
    },
    {
      "epoch": 24.752429430819067,
      "grad_norm": 0.0049497648142278194,
      "learning_rate": 0.05049514113836187,
      "loss": 0.0,
      "step": 53490
    },
    {
      "epoch": 24.757056918093475,
      "grad_norm": 0.0016891787527129054,
      "learning_rate": 0.050485886163813054,
      "loss": 0.0001,
      "step": 53500
    },
    {
      "epoch": 24.761684405367884,
      "grad_norm": 0.011852575466036797,
      "learning_rate": 0.05047663118926424,
      "loss": 0.0003,
      "step": 53510
    },
    {
      "epoch": 24.766311892642296,
      "grad_norm": 0.06632932275533676,
      "learning_rate": 0.05046737621471541,
      "loss": 0.0002,
      "step": 53520
    },
    {
      "epoch": 24.770939379916705,
      "grad_norm": 0.43553903698921204,
      "learning_rate": 0.05045812124016659,
      "loss": 0.0003,
      "step": 53530
    },
    {
      "epoch": 24.775566867191117,
      "grad_norm": 0.004169859457761049,
      "learning_rate": 0.050448866265617766,
      "loss": 0.0,
      "step": 53540
    },
    {
      "epoch": 24.780194354465525,
      "grad_norm": 0.003324605757370591,
      "learning_rate": 0.050439611291068956,
      "loss": 0.0001,
      "step": 53550
    },
    {
      "epoch": 24.784821841739934,
      "grad_norm": 0.0460616797208786,
      "learning_rate": 0.05043035631652013,
      "loss": 0.0001,
      "step": 53560
    },
    {
      "epoch": 24.789449329014346,
      "grad_norm": 0.009236833080649376,
      "learning_rate": 0.05042110134197131,
      "loss": 0.0001,
      "step": 53570
    },
    {
      "epoch": 24.794076816288754,
      "grad_norm": 0.00421326095238328,
      "learning_rate": 0.05041184636742249,
      "loss": 0.0,
      "step": 53580
    },
    {
      "epoch": 24.798704303563166,
      "grad_norm": 0.010194476693868637,
      "learning_rate": 0.050402591392873675,
      "loss": 0.0002,
      "step": 53590
    },
    {
      "epoch": 24.803331790837575,
      "grad_norm": 0.0034441419411450624,
      "learning_rate": 0.05039333641832486,
      "loss": 0.0002,
      "step": 53600
    },
    {
      "epoch": 24.807959278111984,
      "grad_norm": 0.006200812757015228,
      "learning_rate": 0.050384081443776034,
      "loss": 0.0001,
      "step": 53610
    },
    {
      "epoch": 24.812586765386396,
      "grad_norm": 0.006961934734135866,
      "learning_rate": 0.05037482646922721,
      "loss": 0.0001,
      "step": 53620
    },
    {
      "epoch": 24.817214252660804,
      "grad_norm": 0.019556885585188866,
      "learning_rate": 0.050365571494678386,
      "loss": 0.0,
      "step": 53630
    },
    {
      "epoch": 24.821841739935216,
      "grad_norm": 0.008127069100737572,
      "learning_rate": 0.050356316520129576,
      "loss": 0.0002,
      "step": 53640
    },
    {
      "epoch": 24.826469227209625,
      "grad_norm": 0.0012912799138575792,
      "learning_rate": 0.05034706154558075,
      "loss": 0.0006,
      "step": 53650
    },
    {
      "epoch": 24.831096714484037,
      "grad_norm": 0.06350690126419067,
      "learning_rate": 0.05033780657103193,
      "loss": 0.0001,
      "step": 53660
    },
    {
      "epoch": 24.835724201758445,
      "grad_norm": 0.004452605731785297,
      "learning_rate": 0.05032855159648311,
      "loss": 0.0001,
      "step": 53670
    },
    {
      "epoch": 24.840351689032854,
      "grad_norm": 0.004015044309198856,
      "learning_rate": 0.050319296621934295,
      "loss": 0.0,
      "step": 53680
    },
    {
      "epoch": 24.844979176307266,
      "grad_norm": 0.005551179405301809,
      "learning_rate": 0.05031004164738548,
      "loss": 0.0002,
      "step": 53690
    },
    {
      "epoch": 24.849606663581675,
      "grad_norm": 0.007341035641729832,
      "learning_rate": 0.050300786672836655,
      "loss": 0.0001,
      "step": 53700
    },
    {
      "epoch": 24.854234150856087,
      "grad_norm": 0.0006881199078634381,
      "learning_rate": 0.05029153169828783,
      "loss": 0.0018,
      "step": 53710
    },
    {
      "epoch": 24.858861638130495,
      "grad_norm": 0.0016831356333568692,
      "learning_rate": 0.05028227672373901,
      "loss": 0.0001,
      "step": 53720
    },
    {
      "epoch": 24.863489125404904,
      "grad_norm": 0.008782466873526573,
      "learning_rate": 0.0502730217491902,
      "loss": 0.0,
      "step": 53730
    },
    {
      "epoch": 24.868116612679316,
      "grad_norm": 0.0004378568846732378,
      "learning_rate": 0.05026376677464137,
      "loss": 0.0001,
      "step": 53740
    },
    {
      "epoch": 24.872744099953724,
      "grad_norm": 0.005964807700365782,
      "learning_rate": 0.05025451180009255,
      "loss": 0.0,
      "step": 53750
    },
    {
      "epoch": 24.877371587228136,
      "grad_norm": 0.014073795638978481,
      "learning_rate": 0.05024525682554373,
      "loss": 0.0001,
      "step": 53760
    },
    {
      "epoch": 24.881999074502545,
      "grad_norm": 0.0015785201685503125,
      "learning_rate": 0.05023600185099491,
      "loss": 0.0005,
      "step": 53770
    },
    {
      "epoch": 24.886626561776954,
      "grad_norm": 0.0005619144067168236,
      "learning_rate": 0.0502267468764461,
      "loss": 0.0001,
      "step": 53780
    },
    {
      "epoch": 24.891254049051366,
      "grad_norm": 0.6230745911598206,
      "learning_rate": 0.050217491901897275,
      "loss": 0.0003,
      "step": 53790
    },
    {
      "epoch": 24.895881536325774,
      "grad_norm": 0.053751036524772644,
      "learning_rate": 0.05020823692734845,
      "loss": 0.0001,
      "step": 53800
    },
    {
      "epoch": 24.900509023600186,
      "grad_norm": 0.00042368221329525113,
      "learning_rate": 0.05019898195279963,
      "loss": 0.0001,
      "step": 53810
    },
    {
      "epoch": 24.905136510874595,
      "grad_norm": 0.0032179048284888268,
      "learning_rate": 0.05018972697825082,
      "loss": 0.0,
      "step": 53820
    },
    {
      "epoch": 24.909763998149003,
      "grad_norm": 2.2688682079315186,
      "learning_rate": 0.050180472003701994,
      "loss": 0.0054,
      "step": 53830
    },
    {
      "epoch": 24.914391485423415,
      "grad_norm": 0.0006760003743693233,
      "learning_rate": 0.05017121702915317,
      "loss": 0.0005,
      "step": 53840
    },
    {
      "epoch": 24.919018972697824,
      "grad_norm": 0.0037873361725360155,
      "learning_rate": 0.05016196205460435,
      "loss": 0.0001,
      "step": 53850
    },
    {
      "epoch": 24.923646459972236,
      "grad_norm": 0.002259747125208378,
      "learning_rate": 0.05015270708005553,
      "loss": 0.0001,
      "step": 53860
    },
    {
      "epoch": 24.928273947246645,
      "grad_norm": 0.2678505480289459,
      "learning_rate": 0.05014345210550672,
      "loss": 0.0003,
      "step": 53870
    },
    {
      "epoch": 24.932901434521057,
      "grad_norm": 0.0036704353988170624,
      "learning_rate": 0.050134197130957896,
      "loss": 0.0,
      "step": 53880
    },
    {
      "epoch": 24.937528921795465,
      "grad_norm": 0.03262706100940704,
      "learning_rate": 0.05012494215640907,
      "loss": 0.0001,
      "step": 53890
    },
    {
      "epoch": 24.942156409069874,
      "grad_norm": 0.0098040122538805,
      "learning_rate": 0.05011568718186025,
      "loss": 0.0001,
      "step": 53900
    },
    {
      "epoch": 24.946783896344286,
      "grad_norm": 0.01294626947492361,
      "learning_rate": 0.05010643220731144,
      "loss": 0.0001,
      "step": 53910
    },
    {
      "epoch": 24.951411383618694,
      "grad_norm": 0.0910162553191185,
      "learning_rate": 0.050097177232762614,
      "loss": 0.0003,
      "step": 53920
    },
    {
      "epoch": 24.956038870893106,
      "grad_norm": 0.006030825898051262,
      "learning_rate": 0.05008792225821379,
      "loss": 0.0005,
      "step": 53930
    },
    {
      "epoch": 24.960666358167515,
      "grad_norm": 0.03483326733112335,
      "learning_rate": 0.050078667283664974,
      "loss": 0.0001,
      "step": 53940
    },
    {
      "epoch": 24.965293845441924,
      "grad_norm": 0.014129236340522766,
      "learning_rate": 0.05006941230911615,
      "loss": 0.0002,
      "step": 53950
    },
    {
      "epoch": 24.969921332716336,
      "grad_norm": 0.00929053034633398,
      "learning_rate": 0.05006015733456734,
      "loss": 0.0,
      "step": 53960
    },
    {
      "epoch": 24.974548819990744,
      "grad_norm": 0.002582346787676215,
      "learning_rate": 0.050050902360018516,
      "loss": 0.0001,
      "step": 53970
    },
    {
      "epoch": 24.979176307265156,
      "grad_norm": 0.03675251081585884,
      "learning_rate": 0.05004164738546969,
      "loss": 0.0003,
      "step": 53980
    },
    {
      "epoch": 24.983803794539565,
      "grad_norm": 0.009324171580374241,
      "learning_rate": 0.05003239241092087,
      "loss": 0.0003,
      "step": 53990
    },
    {
      "epoch": 24.988431281813973,
      "grad_norm": 0.02199910208582878,
      "learning_rate": 0.050023137436372045,
      "loss": 0.0001,
      "step": 54000
    },
    {
      "epoch": 24.993058769088385,
      "grad_norm": 0.0008447985746897757,
      "learning_rate": 0.050013882461823235,
      "loss": 0.0,
      "step": 54010
    },
    {
      "epoch": 24.997686256362794,
      "grad_norm": 0.010152214206755161,
      "learning_rate": 0.05000462748727441,
      "loss": 0.0001,
      "step": 54020
    },
    {
      "epoch": 25.0,
      "eval_accuracy_branch1": 0.9875211831767062,
      "eval_accuracy_branch2": 0.4989986134647974,
      "eval_f1_branch1": 0.9885158469371788,
      "eval_f1_branch2": 0.49890440772213096,
      "eval_loss": 0.025320325046777725,
      "eval_precision_branch1": 0.9886804472506378,
      "eval_precision_branch2": 0.49899785985723943,
      "eval_recall_branch1": 0.9885530026653199,
      "eval_recall_branch2": 0.4989986134647974,
      "eval_runtime": 29.2045,
      "eval_samples_per_second": 444.521,
      "eval_steps_per_second": 55.574,
      "step": 54025
    },
    {
      "epoch": 25.002313743637206,
      "grad_norm": 0.00578918494284153,
      "learning_rate": 0.049995372512725594,
      "loss": 0.0002,
      "step": 54030
    },
    {
      "epoch": 25.006941230911615,
      "grad_norm": 0.12437319755554199,
      "learning_rate": 0.04998611753817678,
      "loss": 0.0001,
      "step": 54040
    },
    {
      "epoch": 25.011568718186027,
      "grad_norm": 0.0007594026392325759,
      "learning_rate": 0.049976862563627954,
      "loss": 0.0001,
      "step": 54050
    },
    {
      "epoch": 25.016196205460435,
      "grad_norm": 0.16139596700668335,
      "learning_rate": 0.04996760758907914,
      "loss": 0.0001,
      "step": 54060
    },
    {
      "epoch": 25.020823692734844,
      "grad_norm": 0.0037907555233687162,
      "learning_rate": 0.04995835261453031,
      "loss": 0.0001,
      "step": 54070
    },
    {
      "epoch": 25.025451180009256,
      "grad_norm": 0.019658900797367096,
      "learning_rate": 0.04994909763998149,
      "loss": 0.0007,
      "step": 54080
    },
    {
      "epoch": 25.030078667283664,
      "grad_norm": 0.13513708114624023,
      "learning_rate": 0.04993984266543267,
      "loss": 0.0002,
      "step": 54090
    },
    {
      "epoch": 25.034706154558076,
      "grad_norm": 0.0008790631545707583,
      "learning_rate": 0.04993058769088385,
      "loss": 0.0001,
      "step": 54100
    },
    {
      "epoch": 25.039333641832485,
      "grad_norm": 0.0016203963896259665,
      "learning_rate": 0.04992133271633503,
      "loss": 0.0001,
      "step": 54110
    },
    {
      "epoch": 25.043961129106894,
      "grad_norm": 0.047698915004730225,
      "learning_rate": 0.049912077741786215,
      "loss": 0.0001,
      "step": 54120
    },
    {
      "epoch": 25.048588616381306,
      "grad_norm": 0.001812553615309298,
      "learning_rate": 0.0499028227672374,
      "loss": 0.0,
      "step": 54130
    },
    {
      "epoch": 25.053216103655714,
      "grad_norm": 0.0007659820839762688,
      "learning_rate": 0.049893567792688574,
      "loss": 0.0001,
      "step": 54140
    },
    {
      "epoch": 25.057843590930126,
      "grad_norm": 0.046438366174697876,
      "learning_rate": 0.04988431281813975,
      "loss": 0.0001,
      "step": 54150
    },
    {
      "epoch": 25.062471078204535,
      "grad_norm": 0.03532226011157036,
      "learning_rate": 0.049875057843590934,
      "loss": 0.0002,
      "step": 54160
    },
    {
      "epoch": 25.067098565478943,
      "grad_norm": 0.03287186846137047,
      "learning_rate": 0.04986580286904211,
      "loss": 0.0001,
      "step": 54170
    },
    {
      "epoch": 25.071726052753355,
      "grad_norm": 0.0020109782926738262,
      "learning_rate": 0.04985654789449329,
      "loss": 0.0001,
      "step": 54180
    },
    {
      "epoch": 25.076353540027764,
      "grad_norm": 0.002649211324751377,
      "learning_rate": 0.04984729291994447,
      "loss": 0.0,
      "step": 54190
    },
    {
      "epoch": 25.080981027302176,
      "grad_norm": 0.13398119807243347,
      "learning_rate": 0.04983803794539565,
      "loss": 0.0001,
      "step": 54200
    },
    {
      "epoch": 25.085608514576585,
      "grad_norm": 0.047587137669324875,
      "learning_rate": 0.049828782970846835,
      "loss": 0.0001,
      "step": 54210
    },
    {
      "epoch": 25.090236001850997,
      "grad_norm": 0.005781013518571854,
      "learning_rate": 0.04981952799629801,
      "loss": 0.0002,
      "step": 54220
    },
    {
      "epoch": 25.094863489125405,
      "grad_norm": 0.0026375651359558105,
      "learning_rate": 0.049810273021749195,
      "loss": 0.0,
      "step": 54230
    },
    {
      "epoch": 25.099490976399814,
      "grad_norm": 0.005690865684300661,
      "learning_rate": 0.04980101804720037,
      "loss": 0.0003,
      "step": 54240
    },
    {
      "epoch": 25.104118463674226,
      "grad_norm": 0.0023230218794196844,
      "learning_rate": 0.049791763072651554,
      "loss": 0.0001,
      "step": 54250
    },
    {
      "epoch": 25.108745950948634,
      "grad_norm": 0.002688405103981495,
      "learning_rate": 0.04978250809810273,
      "loss": 0.0001,
      "step": 54260
    },
    {
      "epoch": 25.113373438223046,
      "grad_norm": 0.9546005129814148,
      "learning_rate": 0.049773253123553914,
      "loss": 0.0004,
      "step": 54270
    },
    {
      "epoch": 25.118000925497455,
      "grad_norm": 0.0014295566361397505,
      "learning_rate": 0.04976399814900509,
      "loss": 0.0016,
      "step": 54280
    },
    {
      "epoch": 25.122628412771864,
      "grad_norm": 0.005180926527827978,
      "learning_rate": 0.04975474317445627,
      "loss": 0.0001,
      "step": 54290
    },
    {
      "epoch": 25.127255900046276,
      "grad_norm": 0.00229906034655869,
      "learning_rate": 0.049745488199907456,
      "loss": 0.0002,
      "step": 54300
    },
    {
      "epoch": 25.131883387320684,
      "grad_norm": 0.016165971755981445,
      "learning_rate": 0.04973623322535863,
      "loss": 0.0001,
      "step": 54310
    },
    {
      "epoch": 25.136510874595096,
      "grad_norm": 0.05797915905714035,
      "learning_rate": 0.049726978250809815,
      "loss": 0.0001,
      "step": 54320
    },
    {
      "epoch": 25.141138361869505,
      "grad_norm": 0.005889226216822863,
      "learning_rate": 0.04971772327626099,
      "loss": 0.0003,
      "step": 54330
    },
    {
      "epoch": 25.145765849143913,
      "grad_norm": 0.08908738940954208,
      "learning_rate": 0.049708468301712175,
      "loss": 0.0001,
      "step": 54340
    },
    {
      "epoch": 25.150393336418325,
      "grad_norm": 0.001646105432882905,
      "learning_rate": 0.04969921332716335,
      "loss": 0.0001,
      "step": 54350
    },
    {
      "epoch": 25.155020823692734,
      "grad_norm": 0.06454366445541382,
      "learning_rate": 0.049689958352614534,
      "loss": 0.0001,
      "step": 54360
    },
    {
      "epoch": 25.159648310967146,
      "grad_norm": 0.007452179677784443,
      "learning_rate": 0.04968070337806571,
      "loss": 0.0001,
      "step": 54370
    },
    {
      "epoch": 25.164275798241555,
      "grad_norm": 0.0033914705272763968,
      "learning_rate": 0.049671448403516893,
      "loss": 0.0,
      "step": 54380
    },
    {
      "epoch": 25.168903285515963,
      "grad_norm": 0.321818470954895,
      "learning_rate": 0.04966219342896808,
      "loss": 0.0002,
      "step": 54390
    },
    {
      "epoch": 25.173530772790375,
      "grad_norm": 0.003245076397433877,
      "learning_rate": 0.04965293845441925,
      "loss": 0.0,
      "step": 54400
    },
    {
      "epoch": 25.178158260064784,
      "grad_norm": 0.15997831523418427,
      "learning_rate": 0.049643683479870436,
      "loss": 0.0001,
      "step": 54410
    },
    {
      "epoch": 25.182785747339196,
      "grad_norm": 0.0015570104587823153,
      "learning_rate": 0.04963442850532161,
      "loss": 0.0,
      "step": 54420
    },
    {
      "epoch": 25.187413234613604,
      "grad_norm": 0.027811994776129723,
      "learning_rate": 0.049625173530772795,
      "loss": 0.0001,
      "step": 54430
    },
    {
      "epoch": 25.192040721888016,
      "grad_norm": 0.004952703602612019,
      "learning_rate": 0.04961591855622397,
      "loss": 0.0002,
      "step": 54440
    },
    {
      "epoch": 25.196668209162425,
      "grad_norm": 0.019929587841033936,
      "learning_rate": 0.04960666358167515,
      "loss": 0.0001,
      "step": 54450
    },
    {
      "epoch": 25.201295696436834,
      "grad_norm": 0.007754512596875429,
      "learning_rate": 0.04959740860712633,
      "loss": 0.0001,
      "step": 54460
    },
    {
      "epoch": 25.205923183711246,
      "grad_norm": 0.0066678402945399284,
      "learning_rate": 0.049588153632577514,
      "loss": 0.0002,
      "step": 54470
    },
    {
      "epoch": 25.210550670985654,
      "grad_norm": 0.1662529557943344,
      "learning_rate": 0.0495788986580287,
      "loss": 0.0001,
      "step": 54480
    },
    {
      "epoch": 25.215178158260066,
      "grad_norm": 0.07067175954580307,
      "learning_rate": 0.04956964368347987,
      "loss": 0.0001,
      "step": 54490
    },
    {
      "epoch": 25.219805645534475,
      "grad_norm": 0.03125504031777382,
      "learning_rate": 0.04956038870893106,
      "loss": 0.0001,
      "step": 54500
    },
    {
      "epoch": 25.224433132808883,
      "grad_norm": 3.2753915786743164,
      "learning_rate": 0.04955113373438223,
      "loss": 0.0007,
      "step": 54510
    },
    {
      "epoch": 25.229060620083295,
      "grad_norm": 0.04464320093393326,
      "learning_rate": 0.049541878759833416,
      "loss": 0.0001,
      "step": 54520
    },
    {
      "epoch": 25.233688107357704,
      "grad_norm": 0.01659810170531273,
      "learning_rate": 0.04953262378528459,
      "loss": 0.0001,
      "step": 54530
    },
    {
      "epoch": 25.238315594632116,
      "grad_norm": 0.005611544009298086,
      "learning_rate": 0.04952336881073577,
      "loss": 0.0001,
      "step": 54540
    },
    {
      "epoch": 25.242943081906525,
      "grad_norm": 0.02462618052959442,
      "learning_rate": 0.04951411383618695,
      "loss": 0.0004,
      "step": 54550
    },
    {
      "epoch": 25.247570569180933,
      "grad_norm": 0.008911366574466228,
      "learning_rate": 0.049504858861638135,
      "loss": 0.0001,
      "step": 54560
    },
    {
      "epoch": 25.252198056455345,
      "grad_norm": 0.002343568718060851,
      "learning_rate": 0.04949560388708932,
      "loss": 0.0002,
      "step": 54570
    },
    {
      "epoch": 25.256825543729754,
      "grad_norm": 0.0006826852331869304,
      "learning_rate": 0.049486348912540494,
      "loss": 0.0001,
      "step": 54580
    },
    {
      "epoch": 25.261453031004166,
      "grad_norm": 0.022090986371040344,
      "learning_rate": 0.04947709393799168,
      "loss": 0.0001,
      "step": 54590
    },
    {
      "epoch": 25.266080518278574,
      "grad_norm": 0.0051146759651601315,
      "learning_rate": 0.04946783896344285,
      "loss": 0.0003,
      "step": 54600
    },
    {
      "epoch": 25.270708005552986,
      "grad_norm": 0.0006983118364587426,
      "learning_rate": 0.04945858398889403,
      "loss": 0.0001,
      "step": 54610
    },
    {
      "epoch": 25.275335492827395,
      "grad_norm": 0.0018418679246678948,
      "learning_rate": 0.04944932901434521,
      "loss": 0.0002,
      "step": 54620
    },
    {
      "epoch": 25.279962980101804,
      "grad_norm": 0.0017887387657538056,
      "learning_rate": 0.04944007403979639,
      "loss": 0.0003,
      "step": 54630
    },
    {
      "epoch": 25.284590467376216,
      "grad_norm": 0.030454624444246292,
      "learning_rate": 0.04943081906524757,
      "loss": 0.0001,
      "step": 54640
    },
    {
      "epoch": 25.289217954650624,
      "grad_norm": 0.002506373217329383,
      "learning_rate": 0.049421564090698755,
      "loss": 0.001,
      "step": 54650
    },
    {
      "epoch": 25.293845441925036,
      "grad_norm": 0.008272480219602585,
      "learning_rate": 0.04941230911614994,
      "loss": 0.0002,
      "step": 54660
    },
    {
      "epoch": 25.298472929199445,
      "grad_norm": 0.010067085735499859,
      "learning_rate": 0.049403054141601115,
      "loss": 0.0012,
      "step": 54670
    },
    {
      "epoch": 25.303100416473853,
      "grad_norm": 0.0006065444904379547,
      "learning_rate": 0.04939379916705229,
      "loss": 0.0001,
      "step": 54680
    },
    {
      "epoch": 25.307727903748265,
      "grad_norm": 0.0060241674073040485,
      "learning_rate": 0.049384544192503474,
      "loss": 0.0001,
      "step": 54690
    },
    {
      "epoch": 25.312355391022674,
      "grad_norm": 0.0010058137122541666,
      "learning_rate": 0.04937528921795465,
      "loss": 0.0002,
      "step": 54700
    },
    {
      "epoch": 25.316982878297086,
      "grad_norm": 0.001352492137812078,
      "learning_rate": 0.04936603424340583,
      "loss": 0.0001,
      "step": 54710
    },
    {
      "epoch": 25.321610365571495,
      "grad_norm": 0.01644788309931755,
      "learning_rate": 0.04935677926885701,
      "loss": 0.0002,
      "step": 54720
    },
    {
      "epoch": 25.326237852845903,
      "grad_norm": 0.02242264896631241,
      "learning_rate": 0.04934752429430819,
      "loss": 0.0059,
      "step": 54730
    },
    {
      "epoch": 25.330865340120315,
      "grad_norm": 0.030204329639673233,
      "learning_rate": 0.049338269319759376,
      "loss": 0.0001,
      "step": 54740
    },
    {
      "epoch": 25.335492827394724,
      "grad_norm": 0.11122920364141464,
      "learning_rate": 0.04932901434521056,
      "loss": 0.0001,
      "step": 54750
    },
    {
      "epoch": 25.340120314669136,
      "grad_norm": 0.009905952028930187,
      "learning_rate": 0.049319759370661735,
      "loss": 0.0006,
      "step": 54760
    },
    {
      "epoch": 25.344747801943544,
      "grad_norm": 0.018462738022208214,
      "learning_rate": 0.04931050439611291,
      "loss": 0.0002,
      "step": 54770
    },
    {
      "epoch": 25.349375289217953,
      "grad_norm": 0.010702131316065788,
      "learning_rate": 0.049301249421564095,
      "loss": 0.0,
      "step": 54780
    },
    {
      "epoch": 25.354002776492365,
      "grad_norm": 0.19178210198879242,
      "learning_rate": 0.04929199444701527,
      "loss": 0.0003,
      "step": 54790
    },
    {
      "epoch": 25.358630263766774,
      "grad_norm": 0.017927102744579315,
      "learning_rate": 0.049282739472466454,
      "loss": 0.0002,
      "step": 54800
    },
    {
      "epoch": 25.363257751041186,
      "grad_norm": 0.0015323907136917114,
      "learning_rate": 0.04927348449791763,
      "loss": 0.0001,
      "step": 54810
    },
    {
      "epoch": 25.367885238315594,
      "grad_norm": 0.0313476100564003,
      "learning_rate": 0.04926422952336881,
      "loss": 0.0009,
      "step": 54820
    },
    {
      "epoch": 25.372512725590006,
      "grad_norm": 0.0361056849360466,
      "learning_rate": 0.049254974548819996,
      "loss": 0.0001,
      "step": 54830
    },
    {
      "epoch": 25.377140212864415,
      "grad_norm": 0.0032578648533672094,
      "learning_rate": 0.04924571957427117,
      "loss": 0.0008,
      "step": 54840
    },
    {
      "epoch": 25.381767700138823,
      "grad_norm": 0.00896486733108759,
      "learning_rate": 0.049236464599722356,
      "loss": 0.0001,
      "step": 54850
    },
    {
      "epoch": 25.386395187413235,
      "grad_norm": 0.0263469647616148,
      "learning_rate": 0.04922720962517353,
      "loss": 0.0001,
      "step": 54860
    },
    {
      "epoch": 25.391022674687644,
      "grad_norm": 0.0037201179657131433,
      "learning_rate": 0.049217954650624715,
      "loss": 0.0001,
      "step": 54870
    },
    {
      "epoch": 25.395650161962056,
      "grad_norm": 0.002126954961568117,
      "learning_rate": 0.04920869967607589,
      "loss": 0.0,
      "step": 54880
    },
    {
      "epoch": 25.400277649236465,
      "grad_norm": 0.03818339854478836,
      "learning_rate": 0.049199444701527074,
      "loss": 0.0002,
      "step": 54890
    },
    {
      "epoch": 25.404905136510873,
      "grad_norm": 0.015116088092327118,
      "learning_rate": 0.04919018972697825,
      "loss": 0.0001,
      "step": 54900
    },
    {
      "epoch": 25.409532623785285,
      "grad_norm": 0.011393699795007706,
      "learning_rate": 0.049180934752429434,
      "loss": 0.0014,
      "step": 54910
    },
    {
      "epoch": 25.414160111059694,
      "grad_norm": 0.01188806351274252,
      "learning_rate": 0.04917167977788062,
      "loss": 0.0001,
      "step": 54920
    },
    {
      "epoch": 25.418787598334106,
      "grad_norm": 0.00391659839078784,
      "learning_rate": 0.04916242480333179,
      "loss": 0.0,
      "step": 54930
    },
    {
      "epoch": 25.423415085608514,
      "grad_norm": 0.01045957114547491,
      "learning_rate": 0.049153169828782976,
      "loss": 0.0001,
      "step": 54940
    },
    {
      "epoch": 25.428042572882923,
      "grad_norm": 0.012821818701922894,
      "learning_rate": 0.04914391485423415,
      "loss": 0.0005,
      "step": 54950
    },
    {
      "epoch": 25.432670060157335,
      "grad_norm": 0.012211927212774754,
      "learning_rate": 0.049134659879685336,
      "loss": 0.0012,
      "step": 54960
    },
    {
      "epoch": 25.437297547431744,
      "grad_norm": 0.019894186407327652,
      "learning_rate": 0.04912540490513651,
      "loss": 0.0001,
      "step": 54970
    },
    {
      "epoch": 25.441925034706156,
      "grad_norm": 0.002269608899950981,
      "learning_rate": 0.049116149930587695,
      "loss": 0.0001,
      "step": 54980
    },
    {
      "epoch": 25.446552521980564,
      "grad_norm": 0.0007406597724184394,
      "learning_rate": 0.04910689495603887,
      "loss": 0.0001,
      "step": 54990
    },
    {
      "epoch": 25.451180009254976,
      "grad_norm": 0.00644280482083559,
      "learning_rate": 0.049097639981490054,
      "loss": 0.0003,
      "step": 55000
    },
    {
      "epoch": 25.455807496529385,
      "grad_norm": 12.276782035827637,
      "learning_rate": 0.04908838500694124,
      "loss": 0.0025,
      "step": 55010
    },
    {
      "epoch": 25.460434983803793,
      "grad_norm": 0.0054197064600884914,
      "learning_rate": 0.049079130032392414,
      "loss": 0.0004,
      "step": 55020
    },
    {
      "epoch": 25.465062471078205,
      "grad_norm": 0.007237977813929319,
      "learning_rate": 0.0490698750578436,
      "loss": 0.0005,
      "step": 55030
    },
    {
      "epoch": 25.469689958352614,
      "grad_norm": 0.008814195170998573,
      "learning_rate": 0.04906062008329477,
      "loss": 0.0,
      "step": 55040
    },
    {
      "epoch": 25.474317445627026,
      "grad_norm": 0.004700271878391504,
      "learning_rate": 0.049051365108745956,
      "loss": 0.0002,
      "step": 55050
    },
    {
      "epoch": 25.478944932901435,
      "grad_norm": 0.02666790783405304,
      "learning_rate": 0.04904211013419713,
      "loss": 0.0001,
      "step": 55060
    },
    {
      "epoch": 25.483572420175843,
      "grad_norm": 0.024532413110136986,
      "learning_rate": 0.04903285515964831,
      "loss": 0.0001,
      "step": 55070
    },
    {
      "epoch": 25.488199907450255,
      "grad_norm": 0.11574091762304306,
      "learning_rate": 0.04902360018509949,
      "loss": 0.0001,
      "step": 55080
    },
    {
      "epoch": 25.492827394724664,
      "grad_norm": 0.05903441831469536,
      "learning_rate": 0.049014345210550675,
      "loss": 0.0001,
      "step": 55090
    },
    {
      "epoch": 25.497454881999076,
      "grad_norm": 0.02451724000275135,
      "learning_rate": 0.04900509023600186,
      "loss": 0.0001,
      "step": 55100
    },
    {
      "epoch": 25.502082369273484,
      "grad_norm": 0.034482166171073914,
      "learning_rate": 0.048995835261453034,
      "loss": 0.0001,
      "step": 55110
    },
    {
      "epoch": 25.506709856547893,
      "grad_norm": 0.03169211372733116,
      "learning_rate": 0.04898658028690422,
      "loss": 0.0001,
      "step": 55120
    },
    {
      "epoch": 25.511337343822305,
      "grad_norm": 0.07237497717142105,
      "learning_rate": 0.048977325312355394,
      "loss": 0.0001,
      "step": 55130
    },
    {
      "epoch": 25.515964831096714,
      "grad_norm": 0.0007079213974066079,
      "learning_rate": 0.04896807033780657,
      "loss": 0.0002,
      "step": 55140
    },
    {
      "epoch": 25.520592318371126,
      "grad_norm": 0.0007330332882702351,
      "learning_rate": 0.04895881536325775,
      "loss": 0.0001,
      "step": 55150
    },
    {
      "epoch": 25.525219805645534,
      "grad_norm": 0.30023470520973206,
      "learning_rate": 0.04894956038870893,
      "loss": 0.0002,
      "step": 55160
    },
    {
      "epoch": 25.529847292919946,
      "grad_norm": 0.011991889216005802,
      "learning_rate": 0.04894030541416011,
      "loss": 0.0001,
      "step": 55170
    },
    {
      "epoch": 25.534474780194355,
      "grad_norm": 0.005922368261963129,
      "learning_rate": 0.048931050439611296,
      "loss": 0.0002,
      "step": 55180
    },
    {
      "epoch": 25.539102267468763,
      "grad_norm": 0.00260838377289474,
      "learning_rate": 0.04892179546506248,
      "loss": 0.0001,
      "step": 55190
    },
    {
      "epoch": 25.543729754743175,
      "grad_norm": 0.006617181468755007,
      "learning_rate": 0.048912540490513655,
      "loss": 0.0,
      "step": 55200
    },
    {
      "epoch": 25.548357242017584,
      "grad_norm": 0.0019290823256596923,
      "learning_rate": 0.04890328551596484,
      "loss": 0.0,
      "step": 55210
    },
    {
      "epoch": 25.552984729291996,
      "grad_norm": 0.0019142107339575887,
      "learning_rate": 0.048894030541416014,
      "loss": 0.0001,
      "step": 55220
    },
    {
      "epoch": 25.557612216566405,
      "grad_norm": 0.001267465646378696,
      "learning_rate": 0.04888477556686719,
      "loss": 0.0,
      "step": 55230
    },
    {
      "epoch": 25.562239703840813,
      "grad_norm": 0.002438128925859928,
      "learning_rate": 0.048875520592318374,
      "loss": 0.0002,
      "step": 55240
    },
    {
      "epoch": 25.566867191115225,
      "grad_norm": 0.00039920976269058883,
      "learning_rate": 0.04886626561776955,
      "loss": 0.0001,
      "step": 55250
    },
    {
      "epoch": 25.571494678389634,
      "grad_norm": 0.04604914039373398,
      "learning_rate": 0.04885701064322073,
      "loss": 0.0001,
      "step": 55260
    },
    {
      "epoch": 25.576122165664046,
      "grad_norm": 0.0018524290062487125,
      "learning_rate": 0.048847755668671916,
      "loss": 0.0004,
      "step": 55270
    },
    {
      "epoch": 25.580749652938454,
      "grad_norm": 0.000765678589232266,
      "learning_rate": 0.0488385006941231,
      "loss": 0.0001,
      "step": 55280
    },
    {
      "epoch": 25.585377140212863,
      "grad_norm": 0.007168645039200783,
      "learning_rate": 0.048829245719574275,
      "loss": 0.0003,
      "step": 55290
    },
    {
      "epoch": 25.590004627487275,
      "grad_norm": 0.022734034806489944,
      "learning_rate": 0.04881999074502545,
      "loss": 0.0006,
      "step": 55300
    },
    {
      "epoch": 25.594632114761684,
      "grad_norm": 0.0027839145623147488,
      "learning_rate": 0.048810735770476635,
      "loss": 0.0001,
      "step": 55310
    },
    {
      "epoch": 25.599259602036096,
      "grad_norm": 0.008125430904328823,
      "learning_rate": 0.04880148079592781,
      "loss": 0.0001,
      "step": 55320
    },
    {
      "epoch": 25.603887089310504,
      "grad_norm": 0.4780520498752594,
      "learning_rate": 0.048792225821378994,
      "loss": 0.0004,
      "step": 55330
    },
    {
      "epoch": 25.608514576584913,
      "grad_norm": 0.07383431494235992,
      "learning_rate": 0.04878297084683017,
      "loss": 0.0002,
      "step": 55340
    },
    {
      "epoch": 25.613142063859325,
      "grad_norm": 0.11537130177021027,
      "learning_rate": 0.048773715872281354,
      "loss": 0.0001,
      "step": 55350
    },
    {
      "epoch": 25.617769551133733,
      "grad_norm": 0.005513255950063467,
      "learning_rate": 0.04876446089773254,
      "loss": 0.0,
      "step": 55360
    },
    {
      "epoch": 25.622397038408145,
      "grad_norm": 0.6860339641571045,
      "learning_rate": 0.04875520592318371,
      "loss": 0.0002,
      "step": 55370
    },
    {
      "epoch": 25.627024525682554,
      "grad_norm": 0.0012434395030140877,
      "learning_rate": 0.048745950948634896,
      "loss": 0.0002,
      "step": 55380
    },
    {
      "epoch": 25.631652012956966,
      "grad_norm": 0.07512002438306808,
      "learning_rate": 0.04873669597408607,
      "loss": 0.0003,
      "step": 55390
    },
    {
      "epoch": 25.636279500231375,
      "grad_norm": 0.035867467522621155,
      "learning_rate": 0.048727440999537255,
      "loss": 0.0002,
      "step": 55400
    },
    {
      "epoch": 25.640906987505783,
      "grad_norm": 0.004684046842157841,
      "learning_rate": 0.04871818602498843,
      "loss": 0.0,
      "step": 55410
    },
    {
      "epoch": 25.645534474780195,
      "grad_norm": 0.010646243579685688,
      "learning_rate": 0.048708931050439615,
      "loss": 0.0001,
      "step": 55420
    },
    {
      "epoch": 25.650161962054604,
      "grad_norm": 0.001300273695960641,
      "learning_rate": 0.04869967607589079,
      "loss": 0.0002,
      "step": 55430
    },
    {
      "epoch": 25.654789449329016,
      "grad_norm": 0.0024555353447794914,
      "learning_rate": 0.048690421101341974,
      "loss": 0.0002,
      "step": 55440
    },
    {
      "epoch": 25.659416936603424,
      "grad_norm": 1.36701238155365,
      "learning_rate": 0.04868116612679316,
      "loss": 0.0007,
      "step": 55450
    },
    {
      "epoch": 25.664044423877833,
      "grad_norm": 0.02417496219277382,
      "learning_rate": 0.04867191115224433,
      "loss": 0.0003,
      "step": 55460
    },
    {
      "epoch": 25.668671911152245,
      "grad_norm": 0.006968808826059103,
      "learning_rate": 0.04866265617769552,
      "loss": 0.0003,
      "step": 55470
    },
    {
      "epoch": 25.673299398426654,
      "grad_norm": 0.00037316177622415125,
      "learning_rate": 0.04865340120314669,
      "loss": 0.0001,
      "step": 55480
    },
    {
      "epoch": 25.677926885701066,
      "grad_norm": 0.004270319826900959,
      "learning_rate": 0.048644146228597876,
      "loss": 0.0,
      "step": 55490
    },
    {
      "epoch": 25.682554372975474,
      "grad_norm": 0.0036824762355536222,
      "learning_rate": 0.04863489125404905,
      "loss": 0.0018,
      "step": 55500
    },
    {
      "epoch": 25.687181860249883,
      "grad_norm": 0.01010366715490818,
      "learning_rate": 0.048625636279500235,
      "loss": 0.0001,
      "step": 55510
    },
    {
      "epoch": 25.691809347524295,
      "grad_norm": 0.0006524956552311778,
      "learning_rate": 0.04861638130495141,
      "loss": 0.0002,
      "step": 55520
    },
    {
      "epoch": 25.696436834798703,
      "grad_norm": 0.0035986152943223715,
      "learning_rate": 0.048607126330402595,
      "loss": 0.0001,
      "step": 55530
    },
    {
      "epoch": 25.701064322073115,
      "grad_norm": 0.01020639669150114,
      "learning_rate": 0.04859787135585378,
      "loss": 0.0,
      "step": 55540
    },
    {
      "epoch": 25.705691809347524,
      "grad_norm": 0.016120068728923798,
      "learning_rate": 0.048588616381304954,
      "loss": 0.0001,
      "step": 55550
    },
    {
      "epoch": 25.710319296621932,
      "grad_norm": 0.035714566707611084,
      "learning_rate": 0.04857936140675614,
      "loss": 0.0001,
      "step": 55560
    },
    {
      "epoch": 25.714946783896345,
      "grad_norm": 0.0019797738641500473,
      "learning_rate": 0.04857010643220731,
      "loss": 0.0002,
      "step": 55570
    },
    {
      "epoch": 25.719574271170753,
      "grad_norm": 0.017064018175005913,
      "learning_rate": 0.0485608514576585,
      "loss": 0.0001,
      "step": 55580
    },
    {
      "epoch": 25.724201758445165,
      "grad_norm": 0.0007929325802251697,
      "learning_rate": 0.04855159648310967,
      "loss": 0.0027,
      "step": 55590
    },
    {
      "epoch": 25.728829245719574,
      "grad_norm": 0.007174663711339235,
      "learning_rate": 0.04854234150856085,
      "loss": 0.0002,
      "step": 55600
    },
    {
      "epoch": 25.733456732993986,
      "grad_norm": 0.023630406707525253,
      "learning_rate": 0.04853308653401203,
      "loss": 0.0001,
      "step": 55610
    },
    {
      "epoch": 25.738084220268394,
      "grad_norm": 0.0013082873774692416,
      "learning_rate": 0.048523831559463215,
      "loss": 0.0006,
      "step": 55620
    },
    {
      "epoch": 25.742711707542803,
      "grad_norm": 0.0019729991909116507,
      "learning_rate": 0.0485145765849144,
      "loss": 0.0002,
      "step": 55630
    },
    {
      "epoch": 25.747339194817215,
      "grad_norm": 0.014085741713643074,
      "learning_rate": 0.048505321610365575,
      "loss": 0.0002,
      "step": 55640
    },
    {
      "epoch": 25.751966682091624,
      "grad_norm": 0.020763663575053215,
      "learning_rate": 0.04849606663581676,
      "loss": 0.0003,
      "step": 55650
    },
    {
      "epoch": 25.756594169366036,
      "grad_norm": 0.015136875212192535,
      "learning_rate": 0.048486811661267934,
      "loss": 0.0002,
      "step": 55660
    },
    {
      "epoch": 25.761221656640444,
      "grad_norm": 0.0005650338134728372,
      "learning_rate": 0.04847755668671911,
      "loss": 0.0001,
      "step": 55670
    },
    {
      "epoch": 25.765849143914853,
      "grad_norm": 0.000644099316559732,
      "learning_rate": 0.04846830171217029,
      "loss": 0.0016,
      "step": 55680
    },
    {
      "epoch": 25.770476631189265,
      "grad_norm": 0.008798166178166866,
      "learning_rate": 0.04845904673762147,
      "loss": 0.0,
      "step": 55690
    },
    {
      "epoch": 25.775104118463673,
      "grad_norm": 0.08856949955224991,
      "learning_rate": 0.04844979176307265,
      "loss": 0.0002,
      "step": 55700
    },
    {
      "epoch": 25.779731605738085,
      "grad_norm": 0.01858970709145069,
      "learning_rate": 0.048440536788523836,
      "loss": 0.0004,
      "step": 55710
    },
    {
      "epoch": 25.784359093012494,
      "grad_norm": 0.024851085618138313,
      "learning_rate": 0.04843128181397502,
      "loss": 0.0003,
      "step": 55720
    },
    {
      "epoch": 25.788986580286902,
      "grad_norm": 0.000844358466565609,
      "learning_rate": 0.048422026839426195,
      "loss": 0.0001,
      "step": 55730
    },
    {
      "epoch": 25.793614067561315,
      "grad_norm": 0.009013754315674305,
      "learning_rate": 0.04841277186487738,
      "loss": 0.0001,
      "step": 55740
    },
    {
      "epoch": 25.798241554835723,
      "grad_norm": 0.0021210175473243,
      "learning_rate": 0.048403516890328555,
      "loss": 0.0,
      "step": 55750
    },
    {
      "epoch": 25.802869042110135,
      "grad_norm": 0.0016769047360867262,
      "learning_rate": 0.04839426191577973,
      "loss": 0.0001,
      "step": 55760
    },
    {
      "epoch": 25.807496529384544,
      "grad_norm": 0.001643390511162579,
      "learning_rate": 0.048385006941230914,
      "loss": 0.0001,
      "step": 55770
    },
    {
      "epoch": 25.812124016658956,
      "grad_norm": 0.0011295491131022573,
      "learning_rate": 0.04837575196668209,
      "loss": 0.0003,
      "step": 55780
    },
    {
      "epoch": 25.816751503933364,
      "grad_norm": 0.23192358016967773,
      "learning_rate": 0.04836649699213327,
      "loss": 0.0001,
      "step": 55790
    },
    {
      "epoch": 25.821378991207773,
      "grad_norm": 0.003767544636502862,
      "learning_rate": 0.048357242017584456,
      "loss": 0.0001,
      "step": 55800
    },
    {
      "epoch": 25.826006478482185,
      "grad_norm": 0.016997309401631355,
      "learning_rate": 0.04834798704303564,
      "loss": 0.0008,
      "step": 55810
    },
    {
      "epoch": 25.830633965756594,
      "grad_norm": 0.012960107065737247,
      "learning_rate": 0.048338732068486816,
      "loss": 0.0001,
      "step": 55820
    },
    {
      "epoch": 25.835261453031006,
      "grad_norm": 0.002558651380240917,
      "learning_rate": 0.04832947709393799,
      "loss": 0.0002,
      "step": 55830
    },
    {
      "epoch": 25.839888940305414,
      "grad_norm": 0.0019458412425592542,
      "learning_rate": 0.048320222119389175,
      "loss": 0.0,
      "step": 55840
    },
    {
      "epoch": 25.844516427579823,
      "grad_norm": 0.0209797453135252,
      "learning_rate": 0.04831096714484035,
      "loss": 0.0001,
      "step": 55850
    },
    {
      "epoch": 25.849143914854235,
      "grad_norm": 0.031189564615488052,
      "learning_rate": 0.048301712170291534,
      "loss": 0.0002,
      "step": 55860
    },
    {
      "epoch": 25.853771402128643,
      "grad_norm": 0.0015977841103449464,
      "learning_rate": 0.04829245719574271,
      "loss": 0.0001,
      "step": 55870
    },
    {
      "epoch": 25.858398889403055,
      "grad_norm": 0.002850328106433153,
      "learning_rate": 0.048283202221193894,
      "loss": 0.0009,
      "step": 55880
    },
    {
      "epoch": 25.863026376677464,
      "grad_norm": 0.008321788161993027,
      "learning_rate": 0.04827394724664508,
      "loss": 0.0001,
      "step": 55890
    },
    {
      "epoch": 25.867653863951872,
      "grad_norm": 0.0428820475935936,
      "learning_rate": 0.04826469227209625,
      "loss": 0.0001,
      "step": 55900
    },
    {
      "epoch": 25.872281351226285,
      "grad_norm": 0.0019360786536708474,
      "learning_rate": 0.048255437297547436,
      "loss": 0.0001,
      "step": 55910
    },
    {
      "epoch": 25.876908838500693,
      "grad_norm": 0.018482495099306107,
      "learning_rate": 0.04824618232299861,
      "loss": 0.0001,
      "step": 55920
    },
    {
      "epoch": 25.881536325775105,
      "grad_norm": 11.70186710357666,
      "learning_rate": 0.048236927348449796,
      "loss": 0.0033,
      "step": 55930
    },
    {
      "epoch": 25.886163813049514,
      "grad_norm": 0.05501595139503479,
      "learning_rate": 0.04822767237390097,
      "loss": 0.0001,
      "step": 55940
    },
    {
      "epoch": 25.890791300323926,
      "grad_norm": 0.0029269785154610872,
      "learning_rate": 0.048218417399352155,
      "loss": 0.0,
      "step": 55950
    },
    {
      "epoch": 25.895418787598334,
      "grad_norm": 0.012496151961386204,
      "learning_rate": 0.04820916242480333,
      "loss": 0.0001,
      "step": 55960
    },
    {
      "epoch": 25.900046274872743,
      "grad_norm": 0.0003224463143851608,
      "learning_rate": 0.048199907450254514,
      "loss": 0.0003,
      "step": 55970
    },
    {
      "epoch": 25.904673762147155,
      "grad_norm": 0.008728846907615662,
      "learning_rate": 0.0481906524757057,
      "loss": 0.0001,
      "step": 55980
    },
    {
      "epoch": 25.909301249421564,
      "grad_norm": 0.0038089537993073463,
      "learning_rate": 0.048181397501156874,
      "loss": 0.0001,
      "step": 55990
    },
    {
      "epoch": 25.913928736695976,
      "grad_norm": 0.05299220606684685,
      "learning_rate": 0.04817214252660806,
      "loss": 0.0002,
      "step": 56000
    },
    {
      "epoch": 25.918556223970384,
      "grad_norm": 0.003608705010265112,
      "learning_rate": 0.04816288755205923,
      "loss": 0.0001,
      "step": 56010
    },
    {
      "epoch": 25.923183711244793,
      "grad_norm": 0.06354185938835144,
      "learning_rate": 0.048153632577510416,
      "loss": 0.0002,
      "step": 56020
    },
    {
      "epoch": 25.927811198519205,
      "grad_norm": 0.07337955385446548,
      "learning_rate": 0.04814437760296159,
      "loss": 0.0003,
      "step": 56030
    },
    {
      "epoch": 25.932438685793613,
      "grad_norm": 0.005823603831231594,
      "learning_rate": 0.048135122628412776,
      "loss": 0.0001,
      "step": 56040
    },
    {
      "epoch": 25.937066173068025,
      "grad_norm": 0.0017692189430817962,
      "learning_rate": 0.04812586765386395,
      "loss": 0.0001,
      "step": 56050
    },
    {
      "epoch": 25.941693660342434,
      "grad_norm": 0.015006576664745808,
      "learning_rate": 0.048116612679315135,
      "loss": 0.0001,
      "step": 56060
    },
    {
      "epoch": 25.946321147616842,
      "grad_norm": 0.001975518651306629,
      "learning_rate": 0.04810735770476632,
      "loss": 0.0001,
      "step": 56070
    },
    {
      "epoch": 25.950948634891255,
      "grad_norm": 0.0010559206129983068,
      "learning_rate": 0.048098102730217494,
      "loss": 0.0,
      "step": 56080
    },
    {
      "epoch": 25.955576122165663,
      "grad_norm": 0.007051886525005102,
      "learning_rate": 0.04808884775566868,
      "loss": 0.0001,
      "step": 56090
    },
    {
      "epoch": 25.960203609440075,
      "grad_norm": 0.0015947228530421853,
      "learning_rate": 0.048079592781119854,
      "loss": 0.0,
      "step": 56100
    },
    {
      "epoch": 25.964831096714484,
      "grad_norm": 0.002551836194470525,
      "learning_rate": 0.04807033780657104,
      "loss": 0.0001,
      "step": 56110
    },
    {
      "epoch": 25.969458583988896,
      "grad_norm": 0.005143308080732822,
      "learning_rate": 0.04806108283202221,
      "loss": 0.0,
      "step": 56120
    },
    {
      "epoch": 25.974086071263304,
      "grad_norm": 0.010709419846534729,
      "learning_rate": 0.04805182785747339,
      "loss": 0.0,
      "step": 56130
    },
    {
      "epoch": 25.978713558537713,
      "grad_norm": 0.020816745236516,
      "learning_rate": 0.04804257288292457,
      "loss": 0.0002,
      "step": 56140
    },
    {
      "epoch": 25.983341045812125,
      "grad_norm": 0.0028352963272482157,
      "learning_rate": 0.048033317908375756,
      "loss": 0.0003,
      "step": 56150
    },
    {
      "epoch": 25.987968533086534,
      "grad_norm": 0.013268286362290382,
      "learning_rate": 0.04802406293382694,
      "loss": 0.0003,
      "step": 56160
    },
    {
      "epoch": 25.992596020360946,
      "grad_norm": 0.001959258457645774,
      "learning_rate": 0.048014807959278115,
      "loss": 0.0003,
      "step": 56170
    },
    {
      "epoch": 25.997223507635354,
      "grad_norm": 0.00039501412538811564,
      "learning_rate": 0.0480055529847293,
      "loss": 0.0,
      "step": 56180
    },
    {
      "epoch": 26.0,
      "eval_accuracy_branch1": 0.9909104914496996,
      "eval_accuracy_branch2": 0.5008473270682483,
      "eval_f1_branch1": 0.9914462994517844,
      "eval_f1_branch2": 0.4997572861258812,
      "eval_loss": 0.015895824879407883,
      "eval_precision_branch1": 0.9917032542305154,
      "eval_precision_branch2": 0.500854777390473,
      "eval_recall_branch1": 0.9912707787173655,
      "eval_recall_branch2": 0.5008473270682483,
      "eval_runtime": 29.1618,
      "eval_samples_per_second": 445.171,
      "eval_steps_per_second": 55.655,
      "step": 56186
    },
    {
      "epoch": 26.001850994909763,
      "grad_norm": 0.0013857899466529489,
      "learning_rate": 0.047996298010180474,
      "loss": 0.1395,
      "step": 56190
    },
    {
      "epoch": 26.006478482184175,
      "grad_norm": 0.0033894621301442385,
      "learning_rate": 0.04798704303563166,
      "loss": 0.0002,
      "step": 56200
    },
    {
      "epoch": 26.011105969458583,
      "grad_norm": 0.0038580759428441525,
      "learning_rate": 0.047977788061082834,
      "loss": 0.0001,
      "step": 56210
    },
    {
      "epoch": 26.015733456732995,
      "grad_norm": 0.7260753512382507,
      "learning_rate": 0.04796853308653401,
      "loss": 0.0006,
      "step": 56220
    },
    {
      "epoch": 26.020360944007404,
      "grad_norm": 0.008112240582704544,
      "learning_rate": 0.04795927811198519,
      "loss": 0.0004,
      "step": 56230
    },
    {
      "epoch": 26.024988431281812,
      "grad_norm": 0.21462805569171906,
      "learning_rate": 0.047950023137436376,
      "loss": 0.0005,
      "step": 56240
    },
    {
      "epoch": 26.029615918556225,
      "grad_norm": 0.005423476453870535,
      "learning_rate": 0.04794076816288756,
      "loss": 0.0007,
      "step": 56250
    },
    {
      "epoch": 26.034243405830633,
      "grad_norm": 0.037989746779203415,
      "learning_rate": 0.047931513188338735,
      "loss": 0.0015,
      "step": 56260
    },
    {
      "epoch": 26.038870893105045,
      "grad_norm": 0.0007968184654600918,
      "learning_rate": 0.04792225821378992,
      "loss": 0.0002,
      "step": 56270
    },
    {
      "epoch": 26.043498380379454,
      "grad_norm": 0.12040963023900986,
      "learning_rate": 0.047913003239241095,
      "loss": 0.0001,
      "step": 56280
    },
    {
      "epoch": 26.048125867653862,
      "grad_norm": 0.0037515764124691486,
      "learning_rate": 0.04790374826469227,
      "loss": 0.0002,
      "step": 56290
    },
    {
      "epoch": 26.052753354928274,
      "grad_norm": 0.0013609875459223986,
      "learning_rate": 0.047894493290143454,
      "loss": 0.0001,
      "step": 56300
    },
    {
      "epoch": 26.057380842202683,
      "grad_norm": 0.009188435971736908,
      "learning_rate": 0.04788523831559463,
      "loss": 0.0001,
      "step": 56310
    },
    {
      "epoch": 26.062008329477095,
      "grad_norm": 0.07485487312078476,
      "learning_rate": 0.047875983341045814,
      "loss": 0.0002,
      "step": 56320
    },
    {
      "epoch": 26.066635816751504,
      "grad_norm": 0.021904636174440384,
      "learning_rate": 0.047866728366497,
      "loss": 0.0002,
      "step": 56330
    },
    {
      "epoch": 26.071263304025916,
      "grad_norm": 0.0032617582473903894,
      "learning_rate": 0.04785747339194818,
      "loss": 0.0003,
      "step": 56340
    },
    {
      "epoch": 26.075890791300324,
      "grad_norm": 0.0038859557826071978,
      "learning_rate": 0.047848218417399356,
      "loss": 0.0001,
      "step": 56350
    },
    {
      "epoch": 26.080518278574733,
      "grad_norm": 0.0027087724301964045,
      "learning_rate": 0.04783896344285053,
      "loss": 0.0002,
      "step": 56360
    },
    {
      "epoch": 26.085145765849145,
      "grad_norm": 0.012298033572733402,
      "learning_rate": 0.047829708468301715,
      "loss": 0.0,
      "step": 56370
    },
    {
      "epoch": 26.089773253123553,
      "grad_norm": 0.17895154654979706,
      "learning_rate": 0.04782045349375289,
      "loss": 0.0011,
      "step": 56380
    },
    {
      "epoch": 26.094400740397965,
      "grad_norm": 0.0029125367291271687,
      "learning_rate": 0.047811198519204075,
      "loss": 0.0001,
      "step": 56390
    },
    {
      "epoch": 26.099028227672374,
      "grad_norm": 0.053012922406196594,
      "learning_rate": 0.04780194354465525,
      "loss": 0.0002,
      "step": 56400
    },
    {
      "epoch": 26.103655714946782,
      "grad_norm": 0.005498701706528664,
      "learning_rate": 0.047792688570106434,
      "loss": 0.0001,
      "step": 56410
    },
    {
      "epoch": 26.108283202221195,
      "grad_norm": 0.00944350752979517,
      "learning_rate": 0.04778343359555762,
      "loss": 0.0001,
      "step": 56420
    },
    {
      "epoch": 26.112910689495603,
      "grad_norm": 0.0038771191611886024,
      "learning_rate": 0.0477741786210088,
      "loss": 0.0,
      "step": 56430
    },
    {
      "epoch": 26.117538176770015,
      "grad_norm": 0.02886919304728508,
      "learning_rate": 0.04776492364645998,
      "loss": 0.0001,
      "step": 56440
    },
    {
      "epoch": 26.122165664044424,
      "grad_norm": 0.12975803017616272,
      "learning_rate": 0.04775566867191115,
      "loss": 0.0001,
      "step": 56450
    },
    {
      "epoch": 26.126793151318832,
      "grad_norm": 0.013753301464021206,
      "learning_rate": 0.047746413697362336,
      "loss": 0.0003,
      "step": 56460
    },
    {
      "epoch": 26.131420638593244,
      "grad_norm": 0.004137679003179073,
      "learning_rate": 0.04773715872281351,
      "loss": 0.0001,
      "step": 56470
    },
    {
      "epoch": 26.136048125867653,
      "grad_norm": 0.020258009433746338,
      "learning_rate": 0.047727903748264695,
      "loss": 0.0001,
      "step": 56480
    },
    {
      "epoch": 26.140675613142065,
      "grad_norm": 0.005240808706730604,
      "learning_rate": 0.04771864877371587,
      "loss": 0.0029,
      "step": 56490
    },
    {
      "epoch": 26.145303100416474,
      "grad_norm": 0.0011529253097251058,
      "learning_rate": 0.047709393799167055,
      "loss": 0.0002,
      "step": 56500
    },
    {
      "epoch": 26.149930587690882,
      "grad_norm": 0.008029432035982609,
      "learning_rate": 0.04770013882461824,
      "loss": 0.0001,
      "step": 56510
    },
    {
      "epoch": 26.154558074965294,
      "grad_norm": 0.002674821298569441,
      "learning_rate": 0.047690883850069414,
      "loss": 0.0002,
      "step": 56520
    },
    {
      "epoch": 26.159185562239703,
      "grad_norm": 0.40725934505462646,
      "learning_rate": 0.0476816288755206,
      "loss": 0.0006,
      "step": 56530
    },
    {
      "epoch": 26.163813049514115,
      "grad_norm": 0.0654330626130104,
      "learning_rate": 0.04767237390097177,
      "loss": 0.0001,
      "step": 56540
    },
    {
      "epoch": 26.168440536788523,
      "grad_norm": 0.0245968010276556,
      "learning_rate": 0.04766311892642296,
      "loss": 0.0001,
      "step": 56550
    },
    {
      "epoch": 26.173068024062935,
      "grad_norm": 0.012752028182148933,
      "learning_rate": 0.04765386395187413,
      "loss": 0.0001,
      "step": 56560
    },
    {
      "epoch": 26.177695511337344,
      "grad_norm": 0.003382903290912509,
      "learning_rate": 0.047644608977325316,
      "loss": 0.0001,
      "step": 56570
    },
    {
      "epoch": 26.182322998611752,
      "grad_norm": 0.0004913868615403771,
      "learning_rate": 0.04763535400277649,
      "loss": 0.0001,
      "step": 56580
    },
    {
      "epoch": 26.186950485886165,
      "grad_norm": 0.0015229714335873723,
      "learning_rate": 0.047626099028227675,
      "loss": 0.0,
      "step": 56590
    },
    {
      "epoch": 26.191577973160573,
      "grad_norm": 0.014567329548299313,
      "learning_rate": 0.04761684405367886,
      "loss": 0.0001,
      "step": 56600
    },
    {
      "epoch": 26.196205460434985,
      "grad_norm": 0.003511691465973854,
      "learning_rate": 0.047607589079130035,
      "loss": 0.0001,
      "step": 56610
    },
    {
      "epoch": 26.200832947709394,
      "grad_norm": 0.005023316014558077,
      "learning_rate": 0.04759833410458122,
      "loss": 0.0002,
      "step": 56620
    },
    {
      "epoch": 26.205460434983802,
      "grad_norm": 0.01095532812178135,
      "learning_rate": 0.047589079130032394,
      "loss": 0.0,
      "step": 56630
    },
    {
      "epoch": 26.210087922258214,
      "grad_norm": 0.035838570445775986,
      "learning_rate": 0.04757982415548358,
      "loss": 0.0001,
      "step": 56640
    },
    {
      "epoch": 26.214715409532623,
      "grad_norm": 0.016686366870999336,
      "learning_rate": 0.04757056918093475,
      "loss": 0.0001,
      "step": 56650
    },
    {
      "epoch": 26.219342896807035,
      "grad_norm": 0.16882196068763733,
      "learning_rate": 0.047561314206385936,
      "loss": 0.0001,
      "step": 56660
    },
    {
      "epoch": 26.223970384081444,
      "grad_norm": 0.007850385271012783,
      "learning_rate": 0.04755205923183711,
      "loss": 0.0001,
      "step": 56670
    },
    {
      "epoch": 26.228597871355852,
      "grad_norm": 0.04384702071547508,
      "learning_rate": 0.047542804257288296,
      "loss": 0.0005,
      "step": 56680
    },
    {
      "epoch": 26.233225358630264,
      "grad_norm": 0.017100295051932335,
      "learning_rate": 0.04753354928273948,
      "loss": 0.0001,
      "step": 56690
    },
    {
      "epoch": 26.237852845904673,
      "grad_norm": 0.027413545176386833,
      "learning_rate": 0.047524294308190655,
      "loss": 0.0002,
      "step": 56700
    },
    {
      "epoch": 26.242480333179085,
      "grad_norm": 0.009514535777270794,
      "learning_rate": 0.04751503933364184,
      "loss": 0.0002,
      "step": 56710
    },
    {
      "epoch": 26.247107820453493,
      "grad_norm": 0.003126046620309353,
      "learning_rate": 0.047505784359093015,
      "loss": 0.0001,
      "step": 56720
    },
    {
      "epoch": 26.251735307727905,
      "grad_norm": 0.0035825083032250404,
      "learning_rate": 0.0474965293845442,
      "loss": 0.0001,
      "step": 56730
    },
    {
      "epoch": 26.256362795002314,
      "grad_norm": 0.006986851803958416,
      "learning_rate": 0.047487274409995374,
      "loss": 0.0,
      "step": 56740
    },
    {
      "epoch": 26.260990282276722,
      "grad_norm": 0.0190963763743639,
      "learning_rate": 0.04747801943544655,
      "loss": 0.0001,
      "step": 56750
    },
    {
      "epoch": 26.265617769551135,
      "grad_norm": 0.056986138224601746,
      "learning_rate": 0.04746876446089773,
      "loss": 0.0002,
      "step": 56760
    },
    {
      "epoch": 26.270245256825543,
      "grad_norm": 0.002397753531113267,
      "learning_rate": 0.047459509486348916,
      "loss": 0.0,
      "step": 56770
    },
    {
      "epoch": 26.274872744099955,
      "grad_norm": 0.06829462200403214,
      "learning_rate": 0.0474502545118001,
      "loss": 0.0001,
      "step": 56780
    },
    {
      "epoch": 26.279500231374364,
      "grad_norm": 0.00265425699763,
      "learning_rate": 0.047440999537251276,
      "loss": 0.0,
      "step": 56790
    },
    {
      "epoch": 26.284127718648772,
      "grad_norm": 0.005013480316847563,
      "learning_rate": 0.04743174456270246,
      "loss": 0.0001,
      "step": 56800
    },
    {
      "epoch": 26.288755205923184,
      "grad_norm": 0.007051182445138693,
      "learning_rate": 0.047422489588153635,
      "loss": 0.0,
      "step": 56810
    },
    {
      "epoch": 26.293382693197593,
      "grad_norm": 0.005464273504912853,
      "learning_rate": 0.04741323461360481,
      "loss": 0.0001,
      "step": 56820
    },
    {
      "epoch": 26.298010180472005,
      "grad_norm": 0.0015560992760583758,
      "learning_rate": 0.047403979639055994,
      "loss": 0.0,
      "step": 56830
    },
    {
      "epoch": 26.302637667746414,
      "grad_norm": 0.0013610322494059801,
      "learning_rate": 0.04739472466450717,
      "loss": 0.0001,
      "step": 56840
    },
    {
      "epoch": 26.307265155020822,
      "grad_norm": 0.0012403836008161306,
      "learning_rate": 0.047385469689958354,
      "loss": 0.0001,
      "step": 56850
    },
    {
      "epoch": 26.311892642295234,
      "grad_norm": 0.009565910324454308,
      "learning_rate": 0.04737621471540954,
      "loss": 0.0001,
      "step": 56860
    },
    {
      "epoch": 26.316520129569643,
      "grad_norm": 0.0415780171751976,
      "learning_rate": 0.04736695974086072,
      "loss": 0.0002,
      "step": 56870
    },
    {
      "epoch": 26.321147616844055,
      "grad_norm": 0.0038992289919406176,
      "learning_rate": 0.047357704766311896,
      "loss": 0.0001,
      "step": 56880
    },
    {
      "epoch": 26.325775104118463,
      "grad_norm": 0.001958846813067794,
      "learning_rate": 0.04734844979176308,
      "loss": 0.0001,
      "step": 56890
    },
    {
      "epoch": 26.330402591392875,
      "grad_norm": 0.025392115116119385,
      "learning_rate": 0.047339194817214256,
      "loss": 0.0019,
      "step": 56900
    },
    {
      "epoch": 26.335030078667284,
      "grad_norm": 0.3142682909965515,
      "learning_rate": 0.04732993984266543,
      "loss": 0.0002,
      "step": 56910
    },
    {
      "epoch": 26.339657565941692,
      "grad_norm": 0.010851629078388214,
      "learning_rate": 0.047320684868116615,
      "loss": 0.0001,
      "step": 56920
    },
    {
      "epoch": 26.344285053216105,
      "grad_norm": 0.003908966202288866,
      "learning_rate": 0.04731142989356779,
      "loss": 0.0004,
      "step": 56930
    },
    {
      "epoch": 26.348912540490513,
      "grad_norm": 0.047347307205200195,
      "learning_rate": 0.047302174919018974,
      "loss": 0.0001,
      "step": 56940
    },
    {
      "epoch": 26.353540027764925,
      "grad_norm": 0.001938255038112402,
      "learning_rate": 0.04729291994447016,
      "loss": 0.0002,
      "step": 56950
    },
    {
      "epoch": 26.358167515039334,
      "grad_norm": 0.030283909291028976,
      "learning_rate": 0.04728366496992134,
      "loss": 0.0002,
      "step": 56960
    },
    {
      "epoch": 26.362795002313742,
      "grad_norm": 0.0029889086727052927,
      "learning_rate": 0.04727440999537252,
      "loss": 0.0001,
      "step": 56970
    },
    {
      "epoch": 26.367422489588154,
      "grad_norm": 0.019062964245676994,
      "learning_rate": 0.04726515502082369,
      "loss": 0.0003,
      "step": 56980
    },
    {
      "epoch": 26.372049976862563,
      "grad_norm": 0.00152900954708457,
      "learning_rate": 0.047255900046274876,
      "loss": 0.0001,
      "step": 56990
    },
    {
      "epoch": 26.376677464136975,
      "grad_norm": 0.0031861404422670603,
      "learning_rate": 0.04724664507172605,
      "loss": 0.0001,
      "step": 57000
    },
    {
      "epoch": 26.381304951411384,
      "grad_norm": 0.0014225848717615008,
      "learning_rate": 0.047237390097177236,
      "loss": 0.0004,
      "step": 57010
    },
    {
      "epoch": 26.385932438685792,
      "grad_norm": 0.0026389427948743105,
      "learning_rate": 0.04722813512262841,
      "loss": 0.0002,
      "step": 57020
    },
    {
      "epoch": 26.390559925960204,
      "grad_norm": 0.0034115056041628122,
      "learning_rate": 0.047218880148079595,
      "loss": 0.0001,
      "step": 57030
    },
    {
      "epoch": 26.395187413234613,
      "grad_norm": 0.0005201701424084604,
      "learning_rate": 0.04720962517353078,
      "loss": 0.0,
      "step": 57040
    },
    {
      "epoch": 26.399814900509025,
      "grad_norm": 0.0030026636086404324,
      "learning_rate": 0.047200370198981954,
      "loss": 0.0002,
      "step": 57050
    },
    {
      "epoch": 26.404442387783433,
      "grad_norm": 0.007497682236135006,
      "learning_rate": 0.04719111522443314,
      "loss": 0.0001,
      "step": 57060
    },
    {
      "epoch": 26.409069875057842,
      "grad_norm": 0.002741433447226882,
      "learning_rate": 0.047181860249884314,
      "loss": 0.0,
      "step": 57070
    },
    {
      "epoch": 26.413697362332254,
      "grad_norm": 0.0022177540231496096,
      "learning_rate": 0.0471726052753355,
      "loss": 0.0001,
      "step": 57080
    },
    {
      "epoch": 26.418324849606662,
      "grad_norm": 0.11591386795043945,
      "learning_rate": 0.04716335030078667,
      "loss": 0.0001,
      "step": 57090
    },
    {
      "epoch": 26.422952336881075,
      "grad_norm": 0.002270960481837392,
      "learning_rate": 0.047154095326237856,
      "loss": 0.0003,
      "step": 57100
    },
    {
      "epoch": 26.427579824155483,
      "grad_norm": 0.05660506337881088,
      "learning_rate": 0.04714484035168903,
      "loss": 0.0001,
      "step": 57110
    },
    {
      "epoch": 26.432207311429895,
      "grad_norm": 0.18094506859779358,
      "learning_rate": 0.047135585377140216,
      "loss": 0.0005,
      "step": 57120
    },
    {
      "epoch": 26.436834798704304,
      "grad_norm": 0.003664051415398717,
      "learning_rate": 0.0471263304025914,
      "loss": 0.0001,
      "step": 57130
    },
    {
      "epoch": 26.441462285978712,
      "grad_norm": 0.001056625391356647,
      "learning_rate": 0.047117075428042575,
      "loss": 0.0001,
      "step": 57140
    },
    {
      "epoch": 26.446089773253124,
      "grad_norm": 0.027405237779021263,
      "learning_rate": 0.04710782045349376,
      "loss": 0.0001,
      "step": 57150
    },
    {
      "epoch": 26.450717260527533,
      "grad_norm": 0.08450459688901901,
      "learning_rate": 0.047098565478944934,
      "loss": 0.0001,
      "step": 57160
    },
    {
      "epoch": 26.455344747801945,
      "grad_norm": 0.0016052222345024347,
      "learning_rate": 0.04708931050439612,
      "loss": 0.0001,
      "step": 57170
    },
    {
      "epoch": 26.459972235076354,
      "grad_norm": 0.0013910389970988035,
      "learning_rate": 0.047080055529847294,
      "loss": 0.0003,
      "step": 57180
    },
    {
      "epoch": 26.464599722350762,
      "grad_norm": 0.04174087941646576,
      "learning_rate": 0.04707080055529848,
      "loss": 0.0002,
      "step": 57190
    },
    {
      "epoch": 26.469227209625174,
      "grad_norm": 0.0023293846752494574,
      "learning_rate": 0.04706154558074965,
      "loss": 0.0,
      "step": 57200
    },
    {
      "epoch": 26.473854696899583,
      "grad_norm": 0.01396816223859787,
      "learning_rate": 0.047052290606200836,
      "loss": 0.0002,
      "step": 57210
    },
    {
      "epoch": 26.478482184173995,
      "grad_norm": 0.06517553329467773,
      "learning_rate": 0.04704303563165202,
      "loss": 0.0001,
      "step": 57220
    },
    {
      "epoch": 26.483109671448403,
      "grad_norm": 0.04464389383792877,
      "learning_rate": 0.047033780657103195,
      "loss": 0.0001,
      "step": 57230
    },
    {
      "epoch": 26.487737158722812,
      "grad_norm": 0.008297895081341267,
      "learning_rate": 0.04702452568255438,
      "loss": 0.0009,
      "step": 57240
    },
    {
      "epoch": 26.492364645997224,
      "grad_norm": 0.0017023351974785328,
      "learning_rate": 0.047015270708005555,
      "loss": 0.0002,
      "step": 57250
    },
    {
      "epoch": 26.496992133271632,
      "grad_norm": 0.07828871160745621,
      "learning_rate": 0.04700601573345674,
      "loss": 0.0001,
      "step": 57260
    },
    {
      "epoch": 26.501619620546045,
      "grad_norm": 0.04475370794534683,
      "learning_rate": 0.046996760758907914,
      "loss": 0.0002,
      "step": 57270
    },
    {
      "epoch": 26.506247107820453,
      "grad_norm": 0.01812715455889702,
      "learning_rate": 0.04698750578435909,
      "loss": 0.0,
      "step": 57280
    },
    {
      "epoch": 26.51087459509486,
      "grad_norm": 0.0021250126883387566,
      "learning_rate": 0.046978250809810274,
      "loss": 0.0001,
      "step": 57290
    },
    {
      "epoch": 26.515502082369274,
      "grad_norm": 0.0004973470931872725,
      "learning_rate": 0.04696899583526146,
      "loss": 0.0001,
      "step": 57300
    },
    {
      "epoch": 26.520129569643682,
      "grad_norm": 0.00285789230838418,
      "learning_rate": 0.04695974086071264,
      "loss": 0.0002,
      "step": 57310
    },
    {
      "epoch": 26.524757056918094,
      "grad_norm": 0.4541771709918976,
      "learning_rate": 0.046950485886163816,
      "loss": 0.0003,
      "step": 57320
    },
    {
      "epoch": 26.529384544192503,
      "grad_norm": 0.002321661217138171,
      "learning_rate": 0.046941230911615,
      "loss": 0.0004,
      "step": 57330
    },
    {
      "epoch": 26.534012031466915,
      "grad_norm": 0.004192817490547895,
      "learning_rate": 0.046931975937066175,
      "loss": 0.0001,
      "step": 57340
    },
    {
      "epoch": 26.538639518741324,
      "grad_norm": 0.0029703346081078053,
      "learning_rate": 0.04692272096251736,
      "loss": 0.0001,
      "step": 57350
    },
    {
      "epoch": 26.543267006015732,
      "grad_norm": 0.12378652393817902,
      "learning_rate": 0.046913465987968535,
      "loss": 0.0001,
      "step": 57360
    },
    {
      "epoch": 26.547894493290144,
      "grad_norm": 0.0015156894223764539,
      "learning_rate": 0.04690421101341971,
      "loss": 0.0023,
      "step": 57370
    },
    {
      "epoch": 26.552521980564553,
      "grad_norm": 0.003993936348706484,
      "learning_rate": 0.046894956038870894,
      "loss": 0.0002,
      "step": 57380
    },
    {
      "epoch": 26.557149467838965,
      "grad_norm": 0.4739285707473755,
      "learning_rate": 0.04688570106432208,
      "loss": 0.0002,
      "step": 57390
    },
    {
      "epoch": 26.561776955113373,
      "grad_norm": 0.0028217576909810305,
      "learning_rate": 0.04687644608977326,
      "loss": 0.0001,
      "step": 57400
    },
    {
      "epoch": 26.566404442387782,
      "grad_norm": 0.015960117802023888,
      "learning_rate": 0.04686719111522444,
      "loss": 0.0001,
      "step": 57410
    },
    {
      "epoch": 26.571031929662194,
      "grad_norm": 0.018013691529631615,
      "learning_rate": 0.04685793614067562,
      "loss": 0.0001,
      "step": 57420
    },
    {
      "epoch": 26.575659416936602,
      "grad_norm": 0.01730194315314293,
      "learning_rate": 0.046848681166126796,
      "loss": 0.0002,
      "step": 57430
    },
    {
      "epoch": 26.580286904211015,
      "grad_norm": 0.0016337739070877433,
      "learning_rate": 0.04683942619157797,
      "loss": 0.0002,
      "step": 57440
    },
    {
      "epoch": 26.584914391485423,
      "grad_norm": 0.008313453756272793,
      "learning_rate": 0.046830171217029155,
      "loss": 0.0001,
      "step": 57450
    },
    {
      "epoch": 26.58954187875983,
      "grad_norm": 0.007097710855305195,
      "learning_rate": 0.04682091624248033,
      "loss": 0.0001,
      "step": 57460
    },
    {
      "epoch": 26.594169366034244,
      "grad_norm": 0.09469132125377655,
      "learning_rate": 0.046811661267931515,
      "loss": 0.0001,
      "step": 57470
    },
    {
      "epoch": 26.598796853308652,
      "grad_norm": 0.0006328912568278611,
      "learning_rate": 0.0468024062933827,
      "loss": 0.0001,
      "step": 57480
    },
    {
      "epoch": 26.603424340583064,
      "grad_norm": 0.020833367481827736,
      "learning_rate": 0.04679315131883388,
      "loss": 0.0,
      "step": 57490
    },
    {
      "epoch": 26.608051827857473,
      "grad_norm": 0.0018984368070960045,
      "learning_rate": 0.04678389634428506,
      "loss": 0.0002,
      "step": 57500
    },
    {
      "epoch": 26.612679315131885,
      "grad_norm": 0.0946066826581955,
      "learning_rate": 0.04677464136973623,
      "loss": 0.0002,
      "step": 57510
    },
    {
      "epoch": 26.617306802406294,
      "grad_norm": 0.09522462636232376,
      "learning_rate": 0.04676538639518742,
      "loss": 0.0001,
      "step": 57520
    },
    {
      "epoch": 26.621934289680702,
      "grad_norm": 0.002583290683105588,
      "learning_rate": 0.04675613142063859,
      "loss": 0.0035,
      "step": 57530
    },
    {
      "epoch": 26.626561776955114,
      "grad_norm": 0.005234547425061464,
      "learning_rate": 0.046746876446089776,
      "loss": 0.0001,
      "step": 57540
    },
    {
      "epoch": 26.631189264229523,
      "grad_norm": 0.0014213231625035405,
      "learning_rate": 0.04673762147154095,
      "loss": 0.0001,
      "step": 57550
    },
    {
      "epoch": 26.635816751503935,
      "grad_norm": 0.013906884007155895,
      "learning_rate": 0.046728366496992135,
      "loss": 0.0003,
      "step": 57560
    },
    {
      "epoch": 26.640444238778343,
      "grad_norm": 0.00973063986748457,
      "learning_rate": 0.04671911152244332,
      "loss": 0.0001,
      "step": 57570
    },
    {
      "epoch": 26.645071726052752,
      "grad_norm": 0.0032004998065531254,
      "learning_rate": 0.0467098565478945,
      "loss": 0.0001,
      "step": 57580
    },
    {
      "epoch": 26.649699213327164,
      "grad_norm": 0.009449322707951069,
      "learning_rate": 0.04670060157334568,
      "loss": 0.0001,
      "step": 57590
    },
    {
      "epoch": 26.654326700601572,
      "grad_norm": 0.006604780443012714,
      "learning_rate": 0.046691346598796854,
      "loss": 0.0001,
      "step": 57600
    },
    {
      "epoch": 26.658954187875985,
      "grad_norm": 0.0008592656813561916,
      "learning_rate": 0.04668209162424804,
      "loss": 0.0001,
      "step": 57610
    },
    {
      "epoch": 26.663581675150393,
      "grad_norm": 0.09259572625160217,
      "learning_rate": 0.04667283664969921,
      "loss": 0.0003,
      "step": 57620
    },
    {
      "epoch": 26.6682091624248,
      "grad_norm": 0.000641210819594562,
      "learning_rate": 0.046663581675150397,
      "loss": 0.0001,
      "step": 57630
    },
    {
      "epoch": 26.672836649699214,
      "grad_norm": 0.0005428082076832652,
      "learning_rate": 0.04665432670060157,
      "loss": 0.0,
      "step": 57640
    },
    {
      "epoch": 26.677464136973622,
      "grad_norm": 0.010869127698242664,
      "learning_rate": 0.046645071726052756,
      "loss": 0.0001,
      "step": 57650
    },
    {
      "epoch": 26.682091624248034,
      "grad_norm": 0.011744191870093346,
      "learning_rate": 0.04663581675150394,
      "loss": 0.0002,
      "step": 57660
    },
    {
      "epoch": 26.686719111522443,
      "grad_norm": 0.0055339946411550045,
      "learning_rate": 0.046626561776955115,
      "loss": 0.0003,
      "step": 57670
    },
    {
      "epoch": 26.691346598796855,
      "grad_norm": 0.008032104931771755,
      "learning_rate": 0.0466173068024063,
      "loss": 0.0001,
      "step": 57680
    },
    {
      "epoch": 26.695974086071264,
      "grad_norm": 0.00289903674274683,
      "learning_rate": 0.046608051827857475,
      "loss": 0.0005,
      "step": 57690
    },
    {
      "epoch": 26.700601573345672,
      "grad_norm": 0.017568841576576233,
      "learning_rate": 0.04659879685330866,
      "loss": 0.0001,
      "step": 57700
    },
    {
      "epoch": 26.705229060620084,
      "grad_norm": 0.0019035539589822292,
      "learning_rate": 0.046589541878759834,
      "loss": 0.0001,
      "step": 57710
    },
    {
      "epoch": 26.709856547894493,
      "grad_norm": 0.001571623608469963,
      "learning_rate": 0.04658028690421102,
      "loss": 0.0,
      "step": 57720
    },
    {
      "epoch": 26.714484035168905,
      "grad_norm": 0.0026433991733938456,
      "learning_rate": 0.04657103192966219,
      "loss": 0.0,
      "step": 57730
    },
    {
      "epoch": 26.719111522443313,
      "grad_norm": 0.009552756324410439,
      "learning_rate": 0.046561776955113376,
      "loss": 0.0001,
      "step": 57740
    },
    {
      "epoch": 26.723739009717722,
      "grad_norm": 0.005232097115367651,
      "learning_rate": 0.04655252198056456,
      "loss": 0.0,
      "step": 57750
    },
    {
      "epoch": 26.728366496992134,
      "grad_norm": 0.001199320424348116,
      "learning_rate": 0.046543267006015736,
      "loss": 0.0,
      "step": 57760
    },
    {
      "epoch": 26.732993984266542,
      "grad_norm": 0.0048224469646811485,
      "learning_rate": 0.04653401203146692,
      "loss": 0.0001,
      "step": 57770
    },
    {
      "epoch": 26.737621471540955,
      "grad_norm": 0.0028665955178439617,
      "learning_rate": 0.046524757056918095,
      "loss": 0.0001,
      "step": 57780
    },
    {
      "epoch": 26.742248958815363,
      "grad_norm": 0.14349129796028137,
      "learning_rate": 0.04651550208236928,
      "loss": 0.0003,
      "step": 57790
    },
    {
      "epoch": 26.74687644608977,
      "grad_norm": 0.009511617943644524,
      "learning_rate": 0.046506247107820455,
      "loss": 0.0004,
      "step": 57800
    },
    {
      "epoch": 26.751503933364184,
      "grad_norm": 0.016957178711891174,
      "learning_rate": 0.04649699213327164,
      "loss": 0.0004,
      "step": 57810
    },
    {
      "epoch": 26.756131420638592,
      "grad_norm": 0.0014482424594461918,
      "learning_rate": 0.046487737158722814,
      "loss": 0.0001,
      "step": 57820
    },
    {
      "epoch": 26.760758907913004,
      "grad_norm": 0.0004474239831324667,
      "learning_rate": 0.046478482184174,
      "loss": 0.0001,
      "step": 57830
    },
    {
      "epoch": 26.765386395187413,
      "grad_norm": 0.0288846455514431,
      "learning_rate": 0.04646922720962518,
      "loss": 0.0001,
      "step": 57840
    },
    {
      "epoch": 26.770013882461825,
      "grad_norm": 0.03359709680080414,
      "learning_rate": 0.046459972235076356,
      "loss": 0.0001,
      "step": 57850
    },
    {
      "epoch": 26.774641369736234,
      "grad_norm": 0.007692151702940464,
      "learning_rate": 0.04645071726052754,
      "loss": 0.0004,
      "step": 57860
    },
    {
      "epoch": 26.779268857010642,
      "grad_norm": 0.0929950401186943,
      "learning_rate": 0.046441462285978716,
      "loss": 0.0001,
      "step": 57870
    },
    {
      "epoch": 26.783896344285054,
      "grad_norm": 0.08051689714193344,
      "learning_rate": 0.0464322073114299,
      "loss": 0.0001,
      "step": 57880
    },
    {
      "epoch": 26.788523831559463,
      "grad_norm": 0.339631050825119,
      "learning_rate": 0.046422952336881075,
      "loss": 0.0002,
      "step": 57890
    },
    {
      "epoch": 26.793151318833875,
      "grad_norm": 0.02842683158814907,
      "learning_rate": 0.04641369736233225,
      "loss": 0.0001,
      "step": 57900
    },
    {
      "epoch": 26.797778806108283,
      "grad_norm": 0.00107638631016016,
      "learning_rate": 0.046404442387783434,
      "loss": 0.0001,
      "step": 57910
    },
    {
      "epoch": 26.802406293382692,
      "grad_norm": 0.023379167541861534,
      "learning_rate": 0.04639518741323462,
      "loss": 0.0,
      "step": 57920
    },
    {
      "epoch": 26.807033780657104,
      "grad_norm": 0.005500888917595148,
      "learning_rate": 0.0463859324386858,
      "loss": 0.0005,
      "step": 57930
    },
    {
      "epoch": 26.811661267931512,
      "grad_norm": 0.25871801376342773,
      "learning_rate": 0.04637667746413698,
      "loss": 0.0002,
      "step": 57940
    },
    {
      "epoch": 26.816288755205925,
      "grad_norm": 0.022674182429909706,
      "learning_rate": 0.04636742248958816,
      "loss": 0.0004,
      "step": 57950
    },
    {
      "epoch": 26.820916242480333,
      "grad_norm": 0.008469565771520138,
      "learning_rate": 0.046358167515039336,
      "loss": 0.0001,
      "step": 57960
    },
    {
      "epoch": 26.82554372975474,
      "grad_norm": 0.04306968301534653,
      "learning_rate": 0.04634891254049051,
      "loss": 0.0002,
      "step": 57970
    },
    {
      "epoch": 26.830171217029154,
      "grad_norm": 0.02598329447209835,
      "learning_rate": 0.046339657565941696,
      "loss": 0.0001,
      "step": 57980
    },
    {
      "epoch": 26.834798704303562,
      "grad_norm": 0.0028520331252366304,
      "learning_rate": 0.04633040259139287,
      "loss": 0.0004,
      "step": 57990
    },
    {
      "epoch": 26.839426191577974,
      "grad_norm": 0.0006854503299109638,
      "learning_rate": 0.046321147616844055,
      "loss": 0.0,
      "step": 58000
    },
    {
      "epoch": 26.844053678852383,
      "grad_norm": 0.026435526087880135,
      "learning_rate": 0.04631189264229524,
      "loss": 0.0001,
      "step": 58010
    },
    {
      "epoch": 26.84868116612679,
      "grad_norm": 0.0011827866546809673,
      "learning_rate": 0.04630263766774642,
      "loss": 0.0001,
      "step": 58020
    },
    {
      "epoch": 26.853308653401204,
      "grad_norm": 0.007177581544965506,
      "learning_rate": 0.0462933826931976,
      "loss": 0.0001,
      "step": 58030
    },
    {
      "epoch": 26.857936140675612,
      "grad_norm": 0.008876335807144642,
      "learning_rate": 0.046284127718648774,
      "loss": 0.0002,
      "step": 58040
    },
    {
      "epoch": 26.862563627950024,
      "grad_norm": 0.008639911189675331,
      "learning_rate": 0.04627487274409996,
      "loss": 0.0001,
      "step": 58050
    },
    {
      "epoch": 26.867191115224433,
      "grad_norm": 0.12363485246896744,
      "learning_rate": 0.04626561776955113,
      "loss": 0.0001,
      "step": 58060
    },
    {
      "epoch": 26.871818602498845,
      "grad_norm": 0.017471399158239365,
      "learning_rate": 0.046256362795002316,
      "loss": 0.0,
      "step": 58070
    },
    {
      "epoch": 26.876446089773253,
      "grad_norm": 0.019406428560614586,
      "learning_rate": 0.04624710782045349,
      "loss": 0.0,
      "step": 58080
    },
    {
      "epoch": 26.881073577047662,
      "grad_norm": 0.0010226729791611433,
      "learning_rate": 0.046237852845904676,
      "loss": 0.0,
      "step": 58090
    },
    {
      "epoch": 26.885701064322074,
      "grad_norm": 0.021639730781316757,
      "learning_rate": 0.04622859787135586,
      "loss": 0.0001,
      "step": 58100
    },
    {
      "epoch": 26.890328551596482,
      "grad_norm": 0.003989325370639563,
      "learning_rate": 0.04621934289680704,
      "loss": 0.0005,
      "step": 58110
    },
    {
      "epoch": 26.894956038870895,
      "grad_norm": 0.021684134379029274,
      "learning_rate": 0.04621008792225822,
      "loss": 0.0001,
      "step": 58120
    },
    {
      "epoch": 26.899583526145303,
      "grad_norm": 0.010277505964040756,
      "learning_rate": 0.046200832947709394,
      "loss": 0.0001,
      "step": 58130
    },
    {
      "epoch": 26.90421101341971,
      "grad_norm": 0.001193637028336525,
      "learning_rate": 0.04619157797316058,
      "loss": 0.0,
      "step": 58140
    },
    {
      "epoch": 26.908838500694124,
      "grad_norm": 0.021394677460193634,
      "learning_rate": 0.046182322998611754,
      "loss": 0.0001,
      "step": 58150
    },
    {
      "epoch": 26.913465987968532,
      "grad_norm": 1.6844267845153809,
      "learning_rate": 0.04617306802406294,
      "loss": 0.0008,
      "step": 58160
    },
    {
      "epoch": 26.918093475242944,
      "grad_norm": 0.010700359009206295,
      "learning_rate": 0.04616381304951411,
      "loss": 0.0031,
      "step": 58170
    },
    {
      "epoch": 26.922720962517353,
      "grad_norm": 0.0017332382267341018,
      "learning_rate": 0.046154558074965296,
      "loss": 0.0,
      "step": 58180
    },
    {
      "epoch": 26.92734844979176,
      "grad_norm": 0.008668449707329273,
      "learning_rate": 0.04614530310041648,
      "loss": 0.003,
      "step": 58190
    },
    {
      "epoch": 26.931975937066174,
      "grad_norm": 0.20291124284267426,
      "learning_rate": 0.046136048125867656,
      "loss": 0.0002,
      "step": 58200
    },
    {
      "epoch": 26.936603424340582,
      "grad_norm": 0.1567588448524475,
      "learning_rate": 0.04612679315131884,
      "loss": 0.0002,
      "step": 58210
    },
    {
      "epoch": 26.941230911614994,
      "grad_norm": 0.014531425200402737,
      "learning_rate": 0.046117538176770015,
      "loss": 0.0001,
      "step": 58220
    },
    {
      "epoch": 26.945858398889403,
      "grad_norm": 0.042832449078559875,
      "learning_rate": 0.0461082832022212,
      "loss": 0.0001,
      "step": 58230
    },
    {
      "epoch": 26.95048588616381,
      "grad_norm": 0.0031213625334203243,
      "learning_rate": 0.046099028227672374,
      "loss": 0.0003,
      "step": 58240
    },
    {
      "epoch": 26.955113373438223,
      "grad_norm": 0.0033727919217199087,
      "learning_rate": 0.04608977325312356,
      "loss": 0.0001,
      "step": 58250
    },
    {
      "epoch": 26.959740860712632,
      "grad_norm": 0.018398812040686607,
      "learning_rate": 0.046080518278574734,
      "loss": 0.0001,
      "step": 58260
    },
    {
      "epoch": 26.964368347987044,
      "grad_norm": 0.005647449754178524,
      "learning_rate": 0.04607126330402592,
      "loss": 0.0001,
      "step": 58270
    },
    {
      "epoch": 26.968995835261452,
      "grad_norm": 0.00066237966530025,
      "learning_rate": 0.0460620083294771,
      "loss": 0.0001,
      "step": 58280
    },
    {
      "epoch": 26.973623322535865,
      "grad_norm": 0.014743940904736519,
      "learning_rate": 0.046052753354928276,
      "loss": 0.0,
      "step": 58290
    },
    {
      "epoch": 26.978250809810273,
      "grad_norm": 0.3229607045650482,
      "learning_rate": 0.04604349838037946,
      "loss": 0.0002,
      "step": 58300
    },
    {
      "epoch": 26.98287829708468,
      "grad_norm": 0.006897919811308384,
      "learning_rate": 0.046034243405830635,
      "loss": 0.0001,
      "step": 58310
    },
    {
      "epoch": 26.987505784359094,
      "grad_norm": 0.0072575518861413,
      "learning_rate": 0.04602498843128182,
      "loss": 0.0001,
      "step": 58320
    },
    {
      "epoch": 26.992133271633502,
      "grad_norm": 0.0013771668309345841,
      "learning_rate": 0.046015733456732995,
      "loss": 0.0001,
      "step": 58330
    },
    {
      "epoch": 26.996760758907914,
      "grad_norm": 0.0008152122609317303,
      "learning_rate": 0.04600647848218418,
      "loss": 0.0015,
      "step": 58340
    },
    {
      "epoch": 27.0,
      "eval_accuracy_branch1": 0.9906023725157911,
      "eval_accuracy_branch2": 0.4999229702665229,
      "eval_f1_branch1": 0.9915896364879615,
      "eval_f1_branch2": 0.49956749081398943,
      "eval_loss": 0.020301731303334236,
      "eval_precision_branch1": 0.9919220831134328,
      "eval_precision_branch2": 0.499922750772284,
      "eval_recall_branch1": 0.9913823429322638,
      "eval_recall_branch2": 0.4999229702665229,
      "eval_runtime": 28.8407,
      "eval_samples_per_second": 450.128,
      "eval_steps_per_second": 56.275,
      "step": 58347
    },
    {
      "epoch": 27.001388246182323,
      "grad_norm": 0.002866145223379135,
      "learning_rate": 0.045997223507635354,
      "loss": 0.0003,
      "step": 58350
    },
    {
      "epoch": 27.00601573345673,
      "grad_norm": 0.054604314267635345,
      "learning_rate": 0.04598796853308654,
      "loss": 0.0001,
      "step": 58360
    },
    {
      "epoch": 27.010643220731144,
      "grad_norm": 0.03232698515057564,
      "learning_rate": 0.04597871355853772,
      "loss": 0.0002,
      "step": 58370
    },
    {
      "epoch": 27.015270708005552,
      "grad_norm": 0.0078056007623672485,
      "learning_rate": 0.0459694585839889,
      "loss": 0.0003,
      "step": 58380
    },
    {
      "epoch": 27.019898195279964,
      "grad_norm": 0.00767433550208807,
      "learning_rate": 0.04596020360944008,
      "loss": 0.0,
      "step": 58390
    },
    {
      "epoch": 27.024525682554373,
      "grad_norm": 0.005258616525679827,
      "learning_rate": 0.045950948634891256,
      "loss": 0.0009,
      "step": 58400
    },
    {
      "epoch": 27.02915316982878,
      "grad_norm": 0.06033745035529137,
      "learning_rate": 0.04594169366034244,
      "loss": 0.0002,
      "step": 58410
    },
    {
      "epoch": 27.033780657103193,
      "grad_norm": 0.001930546248331666,
      "learning_rate": 0.045932438685793615,
      "loss": 0.0004,
      "step": 58420
    },
    {
      "epoch": 27.038408144377602,
      "grad_norm": 0.0005898952367715538,
      "learning_rate": 0.04592318371124479,
      "loss": 0.0002,
      "step": 58430
    },
    {
      "epoch": 27.043035631652014,
      "grad_norm": 0.007721686270087957,
      "learning_rate": 0.045913928736695975,
      "loss": 0.0004,
      "step": 58440
    },
    {
      "epoch": 27.047663118926422,
      "grad_norm": 0.002121501602232456,
      "learning_rate": 0.04590467376214716,
      "loss": 0.0002,
      "step": 58450
    },
    {
      "epoch": 27.052290606200835,
      "grad_norm": 0.0028173269238322973,
      "learning_rate": 0.04589541878759834,
      "loss": 0.0001,
      "step": 58460
    },
    {
      "epoch": 27.056918093475243,
      "grad_norm": 0.0010479696793481708,
      "learning_rate": 0.04588616381304952,
      "loss": 0.0,
      "step": 58470
    },
    {
      "epoch": 27.06154558074965,
      "grad_norm": 0.06974785029888153,
      "learning_rate": 0.0458769088385007,
      "loss": 0.0004,
      "step": 58480
    },
    {
      "epoch": 27.066173068024064,
      "grad_norm": 0.008491596207022667,
      "learning_rate": 0.04586765386395188,
      "loss": 0.0001,
      "step": 58490
    },
    {
      "epoch": 27.070800555298472,
      "grad_norm": 0.010202785022556782,
      "learning_rate": 0.04585839888940305,
      "loss": 0.0,
      "step": 58500
    },
    {
      "epoch": 27.075428042572884,
      "grad_norm": 0.001000854535959661,
      "learning_rate": 0.045849143914854236,
      "loss": 0.0,
      "step": 58510
    },
    {
      "epoch": 27.080055529847293,
      "grad_norm": 0.005505385342985392,
      "learning_rate": 0.04583988894030541,
      "loss": 0.0001,
      "step": 58520
    },
    {
      "epoch": 27.0846830171217,
      "grad_norm": 0.012705603614449501,
      "learning_rate": 0.045830633965756595,
      "loss": 0.0001,
      "step": 58530
    },
    {
      "epoch": 27.089310504396114,
      "grad_norm": 0.1863386034965515,
      "learning_rate": 0.04582137899120778,
      "loss": 0.0002,
      "step": 58540
    },
    {
      "epoch": 27.093937991670522,
      "grad_norm": 0.054003287106752396,
      "learning_rate": 0.04581212401665896,
      "loss": 0.0001,
      "step": 58550
    },
    {
      "epoch": 27.098565478944934,
      "grad_norm": 0.0033626537770032883,
      "learning_rate": 0.04580286904211014,
      "loss": 0.0002,
      "step": 58560
    },
    {
      "epoch": 27.103192966219343,
      "grad_norm": 0.02185329981148243,
      "learning_rate": 0.04579361406756132,
      "loss": 0.0001,
      "step": 58570
    },
    {
      "epoch": 27.10782045349375,
      "grad_norm": 0.0016623145202174783,
      "learning_rate": 0.0457843590930125,
      "loss": 0.0002,
      "step": 58580
    },
    {
      "epoch": 27.112447940768163,
      "grad_norm": 0.01257807295769453,
      "learning_rate": 0.04577510411846367,
      "loss": 0.0002,
      "step": 58590
    },
    {
      "epoch": 27.117075428042572,
      "grad_norm": 0.04234681278467178,
      "learning_rate": 0.04576584914391486,
      "loss": 0.0001,
      "step": 58600
    },
    {
      "epoch": 27.121702915316984,
      "grad_norm": 0.018907058984041214,
      "learning_rate": 0.04575659416936603,
      "loss": 0.0005,
      "step": 58610
    },
    {
      "epoch": 27.126330402591392,
      "grad_norm": 0.01229187473654747,
      "learning_rate": 0.045747339194817216,
      "loss": 0.0001,
      "step": 58620
    },
    {
      "epoch": 27.130957889865805,
      "grad_norm": 0.0292134340852499,
      "learning_rate": 0.0457380842202684,
      "loss": 0.0001,
      "step": 58630
    },
    {
      "epoch": 27.135585377140213,
      "grad_norm": 0.00280069001019001,
      "learning_rate": 0.04572882924571958,
      "loss": 0.0001,
      "step": 58640
    },
    {
      "epoch": 27.14021286441462,
      "grad_norm": 0.06892846524715424,
      "learning_rate": 0.04571957427117076,
      "loss": 0.0003,
      "step": 58650
    },
    {
      "epoch": 27.144840351689034,
      "grad_norm": 0.0010573662584647536,
      "learning_rate": 0.045710319296621935,
      "loss": 0.0001,
      "step": 58660
    },
    {
      "epoch": 27.149467838963442,
      "grad_norm": 0.014336767606437206,
      "learning_rate": 0.04570106432207312,
      "loss": 0.0002,
      "step": 58670
    },
    {
      "epoch": 27.154095326237854,
      "grad_norm": 0.0027395596262067556,
      "learning_rate": 0.045691809347524294,
      "loss": 0.0003,
      "step": 58680
    },
    {
      "epoch": 27.158722813512263,
      "grad_norm": 0.0013100466458126903,
      "learning_rate": 0.04568255437297548,
      "loss": 0.0001,
      "step": 58690
    },
    {
      "epoch": 27.16335030078667,
      "grad_norm": 0.0019346893532201648,
      "learning_rate": 0.04567329939842665,
      "loss": 0.0001,
      "step": 58700
    },
    {
      "epoch": 27.167977788061084,
      "grad_norm": 0.005182528868317604,
      "learning_rate": 0.045664044423877836,
      "loss": 0.0046,
      "step": 58710
    },
    {
      "epoch": 27.172605275335492,
      "grad_norm": 0.002670625690370798,
      "learning_rate": 0.04565478944932902,
      "loss": 0.0004,
      "step": 58720
    },
    {
      "epoch": 27.177232762609904,
      "grad_norm": 0.0008721529156900942,
      "learning_rate": 0.045645534474780196,
      "loss": 0.0001,
      "step": 58730
    },
    {
      "epoch": 27.181860249884313,
      "grad_norm": 0.6364610195159912,
      "learning_rate": 0.04563627950023138,
      "loss": 0.0001,
      "step": 58740
    },
    {
      "epoch": 27.18648773715872,
      "grad_norm": 0.012269226834177971,
      "learning_rate": 0.045627024525682555,
      "loss": 0.0001,
      "step": 58750
    },
    {
      "epoch": 27.191115224433133,
      "grad_norm": 0.06845340877771378,
      "learning_rate": 0.04561776955113374,
      "loss": 0.0006,
      "step": 58760
    },
    {
      "epoch": 27.195742711707542,
      "grad_norm": 0.017195770516991615,
      "learning_rate": 0.045608514576584915,
      "loss": 0.0001,
      "step": 58770
    },
    {
      "epoch": 27.200370198981954,
      "grad_norm": 0.5551750063896179,
      "learning_rate": 0.0455992596020361,
      "loss": 0.0003,
      "step": 58780
    },
    {
      "epoch": 27.204997686256362,
      "grad_norm": 0.003035369561985135,
      "learning_rate": 0.045590004627487274,
      "loss": 0.0001,
      "step": 58790
    },
    {
      "epoch": 27.20962517353077,
      "grad_norm": 0.006876141764223576,
      "learning_rate": 0.04558074965293846,
      "loss": 0.0002,
      "step": 58800
    },
    {
      "epoch": 27.214252660805183,
      "grad_norm": 0.015317285433411598,
      "learning_rate": 0.04557149467838964,
      "loss": 0.0004,
      "step": 58810
    },
    {
      "epoch": 27.21888014807959,
      "grad_norm": 0.0035897190682590008,
      "learning_rate": 0.045562239703840816,
      "loss": 0.0001,
      "step": 58820
    },
    {
      "epoch": 27.223507635354004,
      "grad_norm": 0.07144960016012192,
      "learning_rate": 0.045552984729292,
      "loss": 0.0001,
      "step": 58830
    },
    {
      "epoch": 27.228135122628412,
      "grad_norm": 0.15877307951450348,
      "learning_rate": 0.045543729754743176,
      "loss": 0.0001,
      "step": 58840
    },
    {
      "epoch": 27.232762609902824,
      "grad_norm": 0.002167793922126293,
      "learning_rate": 0.04553447478019436,
      "loss": 0.0001,
      "step": 58850
    },
    {
      "epoch": 27.237390097177233,
      "grad_norm": 0.0010720927966758609,
      "learning_rate": 0.045525219805645535,
      "loss": 0.0001,
      "step": 58860
    },
    {
      "epoch": 27.24201758445164,
      "grad_norm": 0.0004831017286051065,
      "learning_rate": 0.04551596483109672,
      "loss": 0.0001,
      "step": 58870
    },
    {
      "epoch": 27.246645071726054,
      "grad_norm": 0.043037980794906616,
      "learning_rate": 0.045506709856547894,
      "loss": 0.0001,
      "step": 58880
    },
    {
      "epoch": 27.251272559000462,
      "grad_norm": 0.012596731074154377,
      "learning_rate": 0.04549745488199908,
      "loss": 0.0019,
      "step": 58890
    },
    {
      "epoch": 27.255900046274874,
      "grad_norm": 0.004184149671345949,
      "learning_rate": 0.04548819990745026,
      "loss": 0.0008,
      "step": 58900
    },
    {
      "epoch": 27.260527533549283,
      "grad_norm": 0.004295257851481438,
      "learning_rate": 0.04547894493290144,
      "loss": 0.0002,
      "step": 58910
    },
    {
      "epoch": 27.26515502082369,
      "grad_norm": 0.0046858531422913074,
      "learning_rate": 0.04546968995835262,
      "loss": 0.0001,
      "step": 58920
    },
    {
      "epoch": 27.269782508098103,
      "grad_norm": 0.0029510273598134518,
      "learning_rate": 0.045460434983803796,
      "loss": 0.0001,
      "step": 58930
    },
    {
      "epoch": 27.274409995372512,
      "grad_norm": 0.21998471021652222,
      "learning_rate": 0.04545118000925498,
      "loss": 0.0001,
      "step": 58940
    },
    {
      "epoch": 27.279037482646924,
      "grad_norm": 0.0031914799474179745,
      "learning_rate": 0.045441925034706156,
      "loss": 0.0001,
      "step": 58950
    },
    {
      "epoch": 27.283664969921332,
      "grad_norm": 0.03402130678296089,
      "learning_rate": 0.04543267006015733,
      "loss": 0.0001,
      "step": 58960
    },
    {
      "epoch": 27.28829245719574,
      "grad_norm": 0.0892973467707634,
      "learning_rate": 0.045423415085608515,
      "loss": 0.0001,
      "step": 58970
    },
    {
      "epoch": 27.292919944470153,
      "grad_norm": 0.021828336641192436,
      "learning_rate": 0.0454141601110597,
      "loss": 0.0,
      "step": 58980
    },
    {
      "epoch": 27.29754743174456,
      "grad_norm": 0.0033506136387586594,
      "learning_rate": 0.04540490513651088,
      "loss": 0.0001,
      "step": 58990
    },
    {
      "epoch": 27.302174919018974,
      "grad_norm": 0.005657563917338848,
      "learning_rate": 0.04539565016196206,
      "loss": 0.0011,
      "step": 59000
    },
    {
      "epoch": 27.306802406293382,
      "grad_norm": 0.006726275663822889,
      "learning_rate": 0.04538639518741324,
      "loss": 0.0002,
      "step": 59010
    },
    {
      "epoch": 27.311429893567794,
      "grad_norm": 0.0019195422064512968,
      "learning_rate": 0.04537714021286442,
      "loss": 0.0001,
      "step": 59020
    },
    {
      "epoch": 27.316057380842203,
      "grad_norm": 0.0009507560171186924,
      "learning_rate": 0.0453678852383156,
      "loss": 0.0001,
      "step": 59030
    },
    {
      "epoch": 27.32068486811661,
      "grad_norm": 0.020684948191046715,
      "learning_rate": 0.045358630263766776,
      "loss": 0.0001,
      "step": 59040
    },
    {
      "epoch": 27.325312355391024,
      "grad_norm": 0.00523305032402277,
      "learning_rate": 0.04534937528921795,
      "loss": 0.0001,
      "step": 59050
    },
    {
      "epoch": 27.329939842665432,
      "grad_norm": 0.00463262340053916,
      "learning_rate": 0.045340120314669136,
      "loss": 0.0,
      "step": 59060
    },
    {
      "epoch": 27.334567329939844,
      "grad_norm": 0.0038675833493471146,
      "learning_rate": 0.04533086534012032,
      "loss": 0.0001,
      "step": 59070
    },
    {
      "epoch": 27.339194817214253,
      "grad_norm": 0.033333536237478256,
      "learning_rate": 0.0453216103655715,
      "loss": 0.0001,
      "step": 59080
    },
    {
      "epoch": 27.34382230448866,
      "grad_norm": 0.009890655055642128,
      "learning_rate": 0.04531235539102268,
      "loss": 0.0002,
      "step": 59090
    },
    {
      "epoch": 27.348449791763073,
      "grad_norm": 0.02520183101296425,
      "learning_rate": 0.04530310041647386,
      "loss": 0.0001,
      "step": 59100
    },
    {
      "epoch": 27.353077279037482,
      "grad_norm": 0.011958383955061436,
      "learning_rate": 0.04529384544192504,
      "loss": 0.0002,
      "step": 59110
    },
    {
      "epoch": 27.357704766311894,
      "grad_norm": 0.002171139232814312,
      "learning_rate": 0.045284590467376214,
      "loss": 0.0003,
      "step": 59120
    },
    {
      "epoch": 27.362332253586302,
      "grad_norm": 0.002455596113577485,
      "learning_rate": 0.0452753354928274,
      "loss": 0.0002,
      "step": 59130
    },
    {
      "epoch": 27.36695974086071,
      "grad_norm": 0.004635117948055267,
      "learning_rate": 0.04526608051827857,
      "loss": 0.0001,
      "step": 59140
    },
    {
      "epoch": 27.371587228135123,
      "grad_norm": 0.01599983498454094,
      "learning_rate": 0.045256825543729756,
      "loss": 0.0001,
      "step": 59150
    },
    {
      "epoch": 27.37621471540953,
      "grad_norm": 0.0052327439188957214,
      "learning_rate": 0.04524757056918094,
      "loss": 0.0001,
      "step": 59160
    },
    {
      "epoch": 27.380842202683944,
      "grad_norm": 0.0022525645326822996,
      "learning_rate": 0.04523831559463212,
      "loss": 0.0001,
      "step": 59170
    },
    {
      "epoch": 27.385469689958352,
      "grad_norm": 0.009145430289208889,
      "learning_rate": 0.0452290606200833,
      "loss": 0.0,
      "step": 59180
    },
    {
      "epoch": 27.39009717723276,
      "grad_norm": 0.0011206899071112275,
      "learning_rate": 0.045219805645534475,
      "loss": 0.0002,
      "step": 59190
    },
    {
      "epoch": 27.394724664507173,
      "grad_norm": 0.0032474317122250795,
      "learning_rate": 0.04521055067098566,
      "loss": 0.0,
      "step": 59200
    },
    {
      "epoch": 27.39935215178158,
      "grad_norm": 0.016335077583789825,
      "learning_rate": 0.045201295696436834,
      "loss": 0.0006,
      "step": 59210
    },
    {
      "epoch": 27.403979639055994,
      "grad_norm": 0.004092542454600334,
      "learning_rate": 0.04519204072188802,
      "loss": 0.0001,
      "step": 59220
    },
    {
      "epoch": 27.408607126330402,
      "grad_norm": 0.0033067017793655396,
      "learning_rate": 0.045182785747339194,
      "loss": 0.0001,
      "step": 59230
    },
    {
      "epoch": 27.413234613604814,
      "grad_norm": 0.003444066969677806,
      "learning_rate": 0.04517353077279038,
      "loss": 0.0084,
      "step": 59240
    },
    {
      "epoch": 27.417862100879223,
      "grad_norm": 0.002738015027716756,
      "learning_rate": 0.04516427579824156,
      "loss": 0.0016,
      "step": 59250
    },
    {
      "epoch": 27.42248958815363,
      "grad_norm": 0.022148115560412407,
      "learning_rate": 0.04515502082369274,
      "loss": 0.0001,
      "step": 59260
    },
    {
      "epoch": 27.427117075428043,
      "grad_norm": 0.005547651089727879,
      "learning_rate": 0.04514576584914392,
      "loss": 0.0001,
      "step": 59270
    },
    {
      "epoch": 27.431744562702452,
      "grad_norm": 0.004706184379756451,
      "learning_rate": 0.045136510874595095,
      "loss": 0.001,
      "step": 59280
    },
    {
      "epoch": 27.436372049976864,
      "grad_norm": 0.001378332730382681,
      "learning_rate": 0.04512725590004628,
      "loss": 0.0001,
      "step": 59290
    },
    {
      "epoch": 27.440999537251272,
      "grad_norm": 0.007464243099093437,
      "learning_rate": 0.045118000925497455,
      "loss": 0.0001,
      "step": 59300
    },
    {
      "epoch": 27.44562702452568,
      "grad_norm": 0.012672176584601402,
      "learning_rate": 0.04510874595094864,
      "loss": 0.0,
      "step": 59310
    },
    {
      "epoch": 27.450254511800093,
      "grad_norm": 0.005361027549952269,
      "learning_rate": 0.045099490976399814,
      "loss": 0.0002,
      "step": 59320
    },
    {
      "epoch": 27.4548819990745,
      "grad_norm": 0.0032693445682525635,
      "learning_rate": 0.045090236001851,
      "loss": 0.0,
      "step": 59330
    },
    {
      "epoch": 27.459509486348914,
      "grad_norm": 0.007649644277989864,
      "learning_rate": 0.04508098102730218,
      "loss": 0.0001,
      "step": 59340
    },
    {
      "epoch": 27.464136973623322,
      "grad_norm": 0.007623414508998394,
      "learning_rate": 0.04507172605275336,
      "loss": 0.0001,
      "step": 59350
    },
    {
      "epoch": 27.46876446089773,
      "grad_norm": 0.022340741008520126,
      "learning_rate": 0.04506247107820454,
      "loss": 0.0001,
      "step": 59360
    },
    {
      "epoch": 27.473391948172143,
      "grad_norm": 0.11751426756381989,
      "learning_rate": 0.045053216103655716,
      "loss": 0.0004,
      "step": 59370
    },
    {
      "epoch": 27.47801943544655,
      "grad_norm": 0.001795684453099966,
      "learning_rate": 0.0450439611291069,
      "loss": 0.0001,
      "step": 59380
    },
    {
      "epoch": 27.482646922720964,
      "grad_norm": 0.0006581998895853758,
      "learning_rate": 0.045034706154558075,
      "loss": 0.0002,
      "step": 59390
    },
    {
      "epoch": 27.487274409995372,
      "grad_norm": 0.000785080308560282,
      "learning_rate": 0.04502545118000926,
      "loss": 0.0001,
      "step": 59400
    },
    {
      "epoch": 27.491901897269784,
      "grad_norm": 0.00639938423410058,
      "learning_rate": 0.045016196205460435,
      "loss": 0.0002,
      "step": 59410
    },
    {
      "epoch": 27.496529384544193,
      "grad_norm": 0.6587805151939392,
      "learning_rate": 0.04500694123091162,
      "loss": 0.0002,
      "step": 59420
    },
    {
      "epoch": 27.5011568718186,
      "grad_norm": 0.009259266778826714,
      "learning_rate": 0.0449976862563628,
      "loss": 0.0001,
      "step": 59430
    },
    {
      "epoch": 27.505784359093013,
      "grad_norm": 0.11326201260089874,
      "learning_rate": 0.04498843128181398,
      "loss": 0.0001,
      "step": 59440
    },
    {
      "epoch": 27.510411846367422,
      "grad_norm": 0.003627962665632367,
      "learning_rate": 0.04497917630726516,
      "loss": 0.0001,
      "step": 59450
    },
    {
      "epoch": 27.515039333641834,
      "grad_norm": 0.016775211319327354,
      "learning_rate": 0.04496992133271634,
      "loss": 0.0003,
      "step": 59460
    },
    {
      "epoch": 27.519666820916243,
      "grad_norm": 0.012518388219177723,
      "learning_rate": 0.04496066635816752,
      "loss": 0.0014,
      "step": 59470
    },
    {
      "epoch": 27.52429430819065,
      "grad_norm": 0.010247880592942238,
      "learning_rate": 0.044951411383618696,
      "loss": 0.0002,
      "step": 59480
    },
    {
      "epoch": 27.528921795465063,
      "grad_norm": 0.017254916951060295,
      "learning_rate": 0.04494215640906988,
      "loss": 0.0001,
      "step": 59490
    },
    {
      "epoch": 27.53354928273947,
      "grad_norm": 0.0371694453060627,
      "learning_rate": 0.044932901434521055,
      "loss": 0.0001,
      "step": 59500
    },
    {
      "epoch": 27.538176770013884,
      "grad_norm": 0.0020846030674874783,
      "learning_rate": 0.04492364645997224,
      "loss": 0.0001,
      "step": 59510
    },
    {
      "epoch": 27.542804257288292,
      "grad_norm": 0.033925674855709076,
      "learning_rate": 0.04491439148542342,
      "loss": 0.0003,
      "step": 59520
    },
    {
      "epoch": 27.5474317445627,
      "grad_norm": 0.0163261741399765,
      "learning_rate": 0.0449051365108746,
      "loss": 0.0008,
      "step": 59530
    },
    {
      "epoch": 27.552059231837113,
      "grad_norm": 0.042744796723127365,
      "learning_rate": 0.04489588153632578,
      "loss": 0.0002,
      "step": 59540
    },
    {
      "epoch": 27.55668671911152,
      "grad_norm": 0.006454485468566418,
      "learning_rate": 0.04488662656177696,
      "loss": 0.0001,
      "step": 59550
    },
    {
      "epoch": 27.561314206385934,
      "grad_norm": 0.01571742445230484,
      "learning_rate": 0.04487737158722814,
      "loss": 0.0001,
      "step": 59560
    },
    {
      "epoch": 27.565941693660342,
      "grad_norm": 0.0025135097093880177,
      "learning_rate": 0.04486811661267932,
      "loss": 0.0001,
      "step": 59570
    },
    {
      "epoch": 27.570569180934754,
      "grad_norm": 0.020243898034095764,
      "learning_rate": 0.04485886163813049,
      "loss": 0.0001,
      "step": 59580
    },
    {
      "epoch": 27.575196668209163,
      "grad_norm": 0.01140803936868906,
      "learning_rate": 0.044849606663581676,
      "loss": 0.0,
      "step": 59590
    },
    {
      "epoch": 27.57982415548357,
      "grad_norm": 0.011520706117153168,
      "learning_rate": 0.04484035168903286,
      "loss": 0.0001,
      "step": 59600
    },
    {
      "epoch": 27.584451642757983,
      "grad_norm": 0.040195077657699585,
      "learning_rate": 0.04483109671448404,
      "loss": 0.0007,
      "step": 59610
    },
    {
      "epoch": 27.589079130032392,
      "grad_norm": 0.13087579607963562,
      "learning_rate": 0.04482184173993522,
      "loss": 0.0001,
      "step": 59620
    },
    {
      "epoch": 27.593706617306804,
      "grad_norm": 0.004614079836755991,
      "learning_rate": 0.0448125867653864,
      "loss": 0.0001,
      "step": 59630
    },
    {
      "epoch": 27.598334104581213,
      "grad_norm": 0.003954730462282896,
      "learning_rate": 0.04480333179083758,
      "loss": 0.0,
      "step": 59640
    },
    {
      "epoch": 27.60296159185562,
      "grad_norm": 0.022921672090888023,
      "learning_rate": 0.044794076816288754,
      "loss": 0.0001,
      "step": 59650
    },
    {
      "epoch": 27.607589079130033,
      "grad_norm": 0.10030420869588852,
      "learning_rate": 0.04478482184173994,
      "loss": 0.0002,
      "step": 59660
    },
    {
      "epoch": 27.61221656640444,
      "grad_norm": 0.00021367482258938253,
      "learning_rate": 0.04477556686719111,
      "loss": 0.0002,
      "step": 59670
    },
    {
      "epoch": 27.616844053678854,
      "grad_norm": 0.003916410729289055,
      "learning_rate": 0.044766311892642296,
      "loss": 0.0002,
      "step": 59680
    },
    {
      "epoch": 27.621471540953262,
      "grad_norm": 0.0035014781169593334,
      "learning_rate": 0.04475705691809348,
      "loss": 0.0001,
      "step": 59690
    },
    {
      "epoch": 27.62609902822767,
      "grad_norm": 0.037283092737197876,
      "learning_rate": 0.04474780194354466,
      "loss": 0.0002,
      "step": 59700
    },
    {
      "epoch": 27.630726515502083,
      "grad_norm": 0.048738520592451096,
      "learning_rate": 0.04473854696899584,
      "loss": 0.0002,
      "step": 59710
    },
    {
      "epoch": 27.63535400277649,
      "grad_norm": 0.0021559854503721,
      "learning_rate": 0.04472929199444702,
      "loss": 0.0001,
      "step": 59720
    },
    {
      "epoch": 27.639981490050904,
      "grad_norm": 0.07336205989122391,
      "learning_rate": 0.0447200370198982,
      "loss": 0.0001,
      "step": 59730
    },
    {
      "epoch": 27.644608977325312,
      "grad_norm": 0.12600907683372498,
      "learning_rate": 0.044710782045349375,
      "loss": 0.0007,
      "step": 59740
    },
    {
      "epoch": 27.649236464599724,
      "grad_norm": 0.10498341172933578,
      "learning_rate": 0.04470152707080056,
      "loss": 0.0001,
      "step": 59750
    },
    {
      "epoch": 27.653863951874133,
      "grad_norm": 0.015271092765033245,
      "learning_rate": 0.044692272096251734,
      "loss": 0.0001,
      "step": 59760
    },
    {
      "epoch": 27.65849143914854,
      "grad_norm": 0.0010804713238030672,
      "learning_rate": 0.04468301712170292,
      "loss": 0.0001,
      "step": 59770
    },
    {
      "epoch": 27.663118926422953,
      "grad_norm": 0.013988526538014412,
      "learning_rate": 0.0446737621471541,
      "loss": 0.0001,
      "step": 59780
    },
    {
      "epoch": 27.667746413697362,
      "grad_norm": 0.011837471276521683,
      "learning_rate": 0.04466450717260528,
      "loss": 0.0,
      "step": 59790
    },
    {
      "epoch": 27.672373900971774,
      "grad_norm": 0.001894871355034411,
      "learning_rate": 0.04465525219805646,
      "loss": 0.0007,
      "step": 59800
    },
    {
      "epoch": 27.677001388246183,
      "grad_norm": 0.011452948674559593,
      "learning_rate": 0.044645997223507636,
      "loss": 0.0004,
      "step": 59810
    },
    {
      "epoch": 27.68162887552059,
      "grad_norm": 0.022175287827849388,
      "learning_rate": 0.04463674224895882,
      "loss": 0.0001,
      "step": 59820
    },
    {
      "epoch": 27.686256362795003,
      "grad_norm": 0.058313529938459396,
      "learning_rate": 0.044627487274409995,
      "loss": 0.0001,
      "step": 59830
    },
    {
      "epoch": 27.69088385006941,
      "grad_norm": 0.04995395243167877,
      "learning_rate": 0.04461823229986118,
      "loss": 0.0026,
      "step": 59840
    },
    {
      "epoch": 27.695511337343824,
      "grad_norm": 0.04304925352334976,
      "learning_rate": 0.044608977325312354,
      "loss": 0.0001,
      "step": 59850
    },
    {
      "epoch": 27.700138824618232,
      "grad_norm": 0.003055248875170946,
      "learning_rate": 0.04459972235076354,
      "loss": 0.0001,
      "step": 59860
    },
    {
      "epoch": 27.70476631189264,
      "grad_norm": 0.006106835789978504,
      "learning_rate": 0.04459046737621472,
      "loss": 0.001,
      "step": 59870
    },
    {
      "epoch": 27.709393799167053,
      "grad_norm": 0.08660255372524261,
      "learning_rate": 0.0445812124016659,
      "loss": 0.0001,
      "step": 59880
    },
    {
      "epoch": 27.71402128644146,
      "grad_norm": 0.0566459484398365,
      "learning_rate": 0.04457195742711708,
      "loss": 0.0002,
      "step": 59890
    },
    {
      "epoch": 27.718648773715874,
      "grad_norm": 0.0008430543239228427,
      "learning_rate": 0.044562702452568256,
      "loss": 0.0,
      "step": 59900
    },
    {
      "epoch": 27.723276260990282,
      "grad_norm": 0.01699262671172619,
      "learning_rate": 0.04455344747801944,
      "loss": 0.0001,
      "step": 59910
    },
    {
      "epoch": 27.72790374826469,
      "grad_norm": 0.0005273149581626058,
      "learning_rate": 0.044544192503470616,
      "loss": 0.0001,
      "step": 59920
    },
    {
      "epoch": 27.732531235539103,
      "grad_norm": 0.012503503821790218,
      "learning_rate": 0.0445349375289218,
      "loss": 0.0002,
      "step": 59930
    },
    {
      "epoch": 27.73715872281351,
      "grad_norm": 0.0014494570204988122,
      "learning_rate": 0.044525682554372975,
      "loss": 0.0001,
      "step": 59940
    },
    {
      "epoch": 27.741786210087923,
      "grad_norm": 0.0009963084012269974,
      "learning_rate": 0.04451642757982416,
      "loss": 0.0001,
      "step": 59950
    },
    {
      "epoch": 27.746413697362332,
      "grad_norm": 0.022243682295084,
      "learning_rate": 0.04450717260527534,
      "loss": 0.0003,
      "step": 59960
    },
    {
      "epoch": 27.751041184636744,
      "grad_norm": 0.0014424880500882864,
      "learning_rate": 0.04449791763072652,
      "loss": 0.0,
      "step": 59970
    },
    {
      "epoch": 27.755668671911153,
      "grad_norm": 0.004433020018041134,
      "learning_rate": 0.0444886626561777,
      "loss": 0.0001,
      "step": 59980
    },
    {
      "epoch": 27.76029615918556,
      "grad_norm": 0.009579828009009361,
      "learning_rate": 0.04447940768162888,
      "loss": 0.0001,
      "step": 59990
    },
    {
      "epoch": 27.764923646459973,
      "grad_norm": 0.005191042087972164,
      "learning_rate": 0.04447015270708006,
      "loss": 0.0002,
      "step": 60000
    },
    {
      "epoch": 27.76955113373438,
      "grad_norm": 0.0038880163338035345,
      "learning_rate": 0.044460897732531236,
      "loss": 0.0001,
      "step": 60010
    },
    {
      "epoch": 27.774178621008794,
      "grad_norm": 0.01619456149637699,
      "learning_rate": 0.04445164275798242,
      "loss": 0.0001,
      "step": 60020
    },
    {
      "epoch": 27.778806108283202,
      "grad_norm": 0.003214197000488639,
      "learning_rate": 0.044442387783433596,
      "loss": 0.0001,
      "step": 60030
    },
    {
      "epoch": 27.78343359555761,
      "grad_norm": 0.0033247543033212423,
      "learning_rate": 0.04443313280888478,
      "loss": 0.0,
      "step": 60040
    },
    {
      "epoch": 27.788061082832023,
      "grad_norm": 0.006917484104633331,
      "learning_rate": 0.04442387783433596,
      "loss": 0.0002,
      "step": 60050
    },
    {
      "epoch": 27.79268857010643,
      "grad_norm": 0.018128156661987305,
      "learning_rate": 0.04441462285978714,
      "loss": 0.0001,
      "step": 60060
    },
    {
      "epoch": 27.797316057380844,
      "grad_norm": 0.05967290699481964,
      "learning_rate": 0.04440536788523832,
      "loss": 0.0001,
      "step": 60070
    },
    {
      "epoch": 27.801943544655252,
      "grad_norm": 0.0240572988986969,
      "learning_rate": 0.0443961129106895,
      "loss": 0.0001,
      "step": 60080
    },
    {
      "epoch": 27.80657103192966,
      "grad_norm": 0.0025552581064403057,
      "learning_rate": 0.04438685793614068,
      "loss": 0.0001,
      "step": 60090
    },
    {
      "epoch": 27.811198519204073,
      "grad_norm": 0.0024018739350140095,
      "learning_rate": 0.04437760296159186,
      "loss": 0.0,
      "step": 60100
    },
    {
      "epoch": 27.81582600647848,
      "grad_norm": 0.024030301719903946,
      "learning_rate": 0.04436834798704303,
      "loss": 0.0003,
      "step": 60110
    },
    {
      "epoch": 27.820453493752893,
      "grad_norm": 0.0022507801186293364,
      "learning_rate": 0.044359093012494216,
      "loss": 0.0001,
      "step": 60120
    },
    {
      "epoch": 27.825080981027302,
      "grad_norm": 0.007623344659805298,
      "learning_rate": 0.0443498380379454,
      "loss": 0.0001,
      "step": 60130
    },
    {
      "epoch": 27.82970846830171,
      "grad_norm": 0.0020990718621760607,
      "learning_rate": 0.04434058306339658,
      "loss": 0.0001,
      "step": 60140
    },
    {
      "epoch": 27.834335955576123,
      "grad_norm": 0.011545277200639248,
      "learning_rate": 0.04433132808884776,
      "loss": 0.0001,
      "step": 60150
    },
    {
      "epoch": 27.83896344285053,
      "grad_norm": 0.02546232007443905,
      "learning_rate": 0.04432207311429894,
      "loss": 0.0001,
      "step": 60160
    },
    {
      "epoch": 27.843590930124943,
      "grad_norm": 0.003579705487936735,
      "learning_rate": 0.04431281813975012,
      "loss": 0.0002,
      "step": 60170
    },
    {
      "epoch": 27.84821841739935,
      "grad_norm": 0.008615491911768913,
      "learning_rate": 0.0443035631652013,
      "loss": 0.0001,
      "step": 60180
    },
    {
      "epoch": 27.852845904673764,
      "grad_norm": 0.17912156879901886,
      "learning_rate": 0.04429430819065248,
      "loss": 0.0002,
      "step": 60190
    },
    {
      "epoch": 27.857473391948172,
      "grad_norm": 1.6555118560791016,
      "learning_rate": 0.044285053216103654,
      "loss": 0.0003,
      "step": 60200
    },
    {
      "epoch": 27.86210087922258,
      "grad_norm": 0.00367934862151742,
      "learning_rate": 0.04427579824155484,
      "loss": 0.0001,
      "step": 60210
    },
    {
      "epoch": 27.866728366496993,
      "grad_norm": 0.001694550970569253,
      "learning_rate": 0.04426654326700602,
      "loss": 0.0001,
      "step": 60220
    },
    {
      "epoch": 27.8713558537714,
      "grad_norm": 0.0016751360381022096,
      "learning_rate": 0.0442572882924572,
      "loss": 0.0001,
      "step": 60230
    },
    {
      "epoch": 27.875983341045814,
      "grad_norm": 0.002174348570406437,
      "learning_rate": 0.04424803331790838,
      "loss": 0.0001,
      "step": 60240
    },
    {
      "epoch": 27.880610828320222,
      "grad_norm": 0.1806669980287552,
      "learning_rate": 0.04423877834335956,
      "loss": 0.0002,
      "step": 60250
    },
    {
      "epoch": 27.88523831559463,
      "grad_norm": 0.001240945653989911,
      "learning_rate": 0.04422952336881074,
      "loss": 0.0001,
      "step": 60260
    },
    {
      "epoch": 27.889865802869043,
      "grad_norm": 0.01997457444667816,
      "learning_rate": 0.044220268394261915,
      "loss": 0.0004,
      "step": 60270
    },
    {
      "epoch": 27.89449329014345,
      "grad_norm": 0.002701452700421214,
      "learning_rate": 0.0442110134197131,
      "loss": 0.0001,
      "step": 60280
    },
    {
      "epoch": 27.899120777417863,
      "grad_norm": 0.03448398411273956,
      "learning_rate": 0.044201758445164274,
      "loss": 0.0001,
      "step": 60290
    },
    {
      "epoch": 27.903748264692272,
      "grad_norm": 0.0033215435687452555,
      "learning_rate": 0.04419250347061546,
      "loss": 0.0,
      "step": 60300
    },
    {
      "epoch": 27.90837575196668,
      "grad_norm": 0.01260092668235302,
      "learning_rate": 0.04418324849606664,
      "loss": 0.0001,
      "step": 60310
    },
    {
      "epoch": 27.913003239241093,
      "grad_norm": 0.01701657474040985,
      "learning_rate": 0.044173993521517824,
      "loss": 0.0005,
      "step": 60320
    },
    {
      "epoch": 27.9176307265155,
      "grad_norm": 0.004557702224701643,
      "learning_rate": 0.044164738546969,
      "loss": 0.0002,
      "step": 60330
    },
    {
      "epoch": 27.922258213789913,
      "grad_norm": 0.005107611883431673,
      "learning_rate": 0.044155483572420176,
      "loss": 0.0001,
      "step": 60340
    },
    {
      "epoch": 27.92688570106432,
      "grad_norm": 0.003311499487608671,
      "learning_rate": 0.04414622859787136,
      "loss": 0.0001,
      "step": 60350
    },
    {
      "epoch": 27.931513188338734,
      "grad_norm": 0.005984126124531031,
      "learning_rate": 0.044136973623322535,
      "loss": 0.001,
      "step": 60360
    },
    {
      "epoch": 27.936140675613142,
      "grad_norm": 0.0033092289231717587,
      "learning_rate": 0.04412771864877372,
      "loss": 0.0002,
      "step": 60370
    },
    {
      "epoch": 27.94076816288755,
      "grad_norm": 0.01286518108099699,
      "learning_rate": 0.044118463674224895,
      "loss": 0.0001,
      "step": 60380
    },
    {
      "epoch": 27.945395650161963,
      "grad_norm": 0.0016174250049516559,
      "learning_rate": 0.04410920869967608,
      "loss": 0.0001,
      "step": 60390
    },
    {
      "epoch": 27.95002313743637,
      "grad_norm": 0.008434059098362923,
      "learning_rate": 0.04409995372512726,
      "loss": 0.0001,
      "step": 60400
    },
    {
      "epoch": 27.954650624710784,
      "grad_norm": 0.011247841641306877,
      "learning_rate": 0.044090698750578444,
      "loss": 0.0001,
      "step": 60410
    },
    {
      "epoch": 27.959278111985192,
      "grad_norm": 0.010216638445854187,
      "learning_rate": 0.04408144377602962,
      "loss": 0.0001,
      "step": 60420
    },
    {
      "epoch": 27.9639055992596,
      "grad_norm": 0.0017701073084026575,
      "learning_rate": 0.0440721888014808,
      "loss": 0.0001,
      "step": 60430
    },
    {
      "epoch": 27.968533086534013,
      "grad_norm": 0.0018940847367048264,
      "learning_rate": 0.04406293382693198,
      "loss": 0.0001,
      "step": 60440
    },
    {
      "epoch": 27.97316057380842,
      "grad_norm": 0.004014692734926939,
      "learning_rate": 0.044053678852383156,
      "loss": 0.0005,
      "step": 60450
    },
    {
      "epoch": 27.977788061082833,
      "grad_norm": 0.042373254895210266,
      "learning_rate": 0.04404442387783434,
      "loss": 0.0003,
      "step": 60460
    },
    {
      "epoch": 27.982415548357242,
      "grad_norm": 0.0008859238005243242,
      "learning_rate": 0.044035168903285515,
      "loss": 0.0001,
      "step": 60470
    },
    {
      "epoch": 27.98704303563165,
      "grad_norm": 0.024321500211954117,
      "learning_rate": 0.0440259139287367,
      "loss": 0.0001,
      "step": 60480
    },
    {
      "epoch": 27.991670522906063,
      "grad_norm": 0.004906046204268932,
      "learning_rate": 0.04401665895418788,
      "loss": 0.0001,
      "step": 60490
    },
    {
      "epoch": 27.99629801018047,
      "grad_norm": 0.002950025489553809,
      "learning_rate": 0.04400740397963906,
      "loss": 0.0002,
      "step": 60500
    },
    {
      "epoch": 28.0,
      "eval_accuracy_branch1": 0.9906794022492682,
      "eval_accuracy_branch2": 0.49922970266522876,
      "eval_f1_branch1": 0.9911134847637044,
      "eval_f1_branch2": 0.4990512832023501,
      "eval_loss": 0.017781727015972137,
      "eval_precision_branch1": 0.9915398685219969,
      "eval_precision_branch2": 0.4992286036935373,
      "eval_recall_branch1": 0.9907957889950428,
      "eval_recall_branch2": 0.4992297026652288,
      "eval_runtime": 28.8936,
      "eval_samples_per_second": 449.304,
      "eval_steps_per_second": 56.172,
      "step": 60508
    },
    {
      "epoch": 28.000925497454883,
      "grad_norm": 0.005317118484526873,
      "learning_rate": 0.04399814900509024,
      "loss": 0.0438,
      "step": 60510
    },
    {
      "epoch": 28.00555298472929,
      "grad_norm": 0.0017495775828137994,
      "learning_rate": 0.04398889403054142,
      "loss": 0.0,
      "step": 60520
    },
    {
      "epoch": 28.010180472003704,
      "grad_norm": 0.003715528640896082,
      "learning_rate": 0.0439796390559926,
      "loss": 0.0001,
      "step": 60530
    },
    {
      "epoch": 28.014807959278112,
      "grad_norm": 0.0008090270566754043,
      "learning_rate": 0.04397038408144378,
      "loss": 0.0001,
      "step": 60540
    },
    {
      "epoch": 28.01943544655252,
      "grad_norm": 0.11440783739089966,
      "learning_rate": 0.04396112910689496,
      "loss": 0.0001,
      "step": 60550
    },
    {
      "epoch": 28.024062933826933,
      "grad_norm": 0.06253018975257874,
      "learning_rate": 0.043951874132346136,
      "loss": 0.0001,
      "step": 60560
    },
    {
      "epoch": 28.02869042110134,
      "grad_norm": 0.045332808047533035,
      "learning_rate": 0.04394261915779732,
      "loss": 0.0002,
      "step": 60570
    },
    {
      "epoch": 28.033317908375754,
      "grad_norm": 0.0024721724912524223,
      "learning_rate": 0.0439333641832485,
      "loss": 0.0001,
      "step": 60580
    },
    {
      "epoch": 28.037945395650162,
      "grad_norm": 0.007612313609570265,
      "learning_rate": 0.04392410920869968,
      "loss": 0.0002,
      "step": 60590
    },
    {
      "epoch": 28.04257288292457,
      "grad_norm": 0.0030725717078894377,
      "learning_rate": 0.04391485423415086,
      "loss": 0.0001,
      "step": 60600
    },
    {
      "epoch": 28.047200370198983,
      "grad_norm": 0.010537260212004185,
      "learning_rate": 0.04390559925960204,
      "loss": 0.0001,
      "step": 60610
    },
    {
      "epoch": 28.05182785747339,
      "grad_norm": 0.007251732051372528,
      "learning_rate": 0.04389634428505322,
      "loss": 0.0001,
      "step": 60620
    },
    {
      "epoch": 28.056455344747803,
      "grad_norm": 0.003757846774533391,
      "learning_rate": 0.0438870893105044,
      "loss": 0.0006,
      "step": 60630
    },
    {
      "epoch": 28.061082832022212,
      "grad_norm": 0.02484823390841484,
      "learning_rate": 0.04387783433595557,
      "loss": 0.0001,
      "step": 60640
    },
    {
      "epoch": 28.06571031929662,
      "grad_norm": 0.04419275000691414,
      "learning_rate": 0.043868579361406757,
      "loss": 0.0002,
      "step": 60650
    },
    {
      "epoch": 28.070337806571033,
      "grad_norm": 0.003928325604647398,
      "learning_rate": 0.04385932438685794,
      "loss": 0.0001,
      "step": 60660
    },
    {
      "epoch": 28.07496529384544,
      "grad_norm": 0.021740898489952087,
      "learning_rate": 0.04385006941230912,
      "loss": 0.0001,
      "step": 60670
    },
    {
      "epoch": 28.079592781119853,
      "grad_norm": 0.003096817759796977,
      "learning_rate": 0.0438408144377603,
      "loss": 0.0001,
      "step": 60680
    },
    {
      "epoch": 28.08422026839426,
      "grad_norm": 0.002961953403428197,
      "learning_rate": 0.04383155946321148,
      "loss": 0.0002,
      "step": 60690
    },
    {
      "epoch": 28.08884775566867,
      "grad_norm": 0.0005276970332488418,
      "learning_rate": 0.04382230448866266,
      "loss": 0.0,
      "step": 60700
    },
    {
      "epoch": 28.093475242943082,
      "grad_norm": 0.008108745329082012,
      "learning_rate": 0.04381304951411384,
      "loss": 0.0001,
      "step": 60710
    },
    {
      "epoch": 28.09810273021749,
      "grad_norm": 0.18674439191818237,
      "learning_rate": 0.04380379453956502,
      "loss": 0.0001,
      "step": 60720
    },
    {
      "epoch": 28.102730217491903,
      "grad_norm": 0.005261540412902832,
      "learning_rate": 0.043794539565016194,
      "loss": 0.0012,
      "step": 60730
    },
    {
      "epoch": 28.10735770476631,
      "grad_norm": 0.03740823268890381,
      "learning_rate": 0.04378528459046738,
      "loss": 0.0001,
      "step": 60740
    },
    {
      "epoch": 28.111985192040724,
      "grad_norm": 0.004520755261182785,
      "learning_rate": 0.04377602961591856,
      "loss": 0.0001,
      "step": 60750
    },
    {
      "epoch": 28.116612679315132,
      "grad_norm": 0.0007719589048065245,
      "learning_rate": 0.04376677464136974,
      "loss": 0.0001,
      "step": 60760
    },
    {
      "epoch": 28.12124016658954,
      "grad_norm": 0.0012837706599384546,
      "learning_rate": 0.04375751966682092,
      "loss": 0.0001,
      "step": 60770
    },
    {
      "epoch": 28.125867653863953,
      "grad_norm": 0.18540576100349426,
      "learning_rate": 0.0437482646922721,
      "loss": 0.0004,
      "step": 60780
    },
    {
      "epoch": 28.13049514113836,
      "grad_norm": 0.013106340542435646,
      "learning_rate": 0.04373900971772328,
      "loss": 0.0003,
      "step": 60790
    },
    {
      "epoch": 28.135122628412773,
      "grad_norm": 0.010285351425409317,
      "learning_rate": 0.043729754743174455,
      "loss": 0.0,
      "step": 60800
    },
    {
      "epoch": 28.139750115687182,
      "grad_norm": 0.0025086007080972195,
      "learning_rate": 0.04372049976862564,
      "loss": 0.0001,
      "step": 60810
    },
    {
      "epoch": 28.14437760296159,
      "grad_norm": 0.0011186727788299322,
      "learning_rate": 0.043711244794076815,
      "loss": 0.0001,
      "step": 60820
    },
    {
      "epoch": 28.149005090236003,
      "grad_norm": 0.010865424759685993,
      "learning_rate": 0.043701989819528,
      "loss": 0.0006,
      "step": 60830
    },
    {
      "epoch": 28.15363257751041,
      "grad_norm": 0.003206186927855015,
      "learning_rate": 0.04369273484497918,
      "loss": 0.0,
      "step": 60840
    },
    {
      "epoch": 28.158260064784823,
      "grad_norm": 0.016431616619229317,
      "learning_rate": 0.043683479870430364,
      "loss": 0.0003,
      "step": 60850
    },
    {
      "epoch": 28.16288755205923,
      "grad_norm": 0.03942122682929039,
      "learning_rate": 0.04367422489588154,
      "loss": 0.0001,
      "step": 60860
    },
    {
      "epoch": 28.16751503933364,
      "grad_norm": 0.012692106887698174,
      "learning_rate": 0.043664969921332716,
      "loss": 0.0007,
      "step": 60870
    },
    {
      "epoch": 28.172142526608052,
      "grad_norm": 0.0007331531960517168,
      "learning_rate": 0.0436557149467839,
      "loss": 0.0004,
      "step": 60880
    },
    {
      "epoch": 28.17677001388246,
      "grad_norm": 0.11506278067827225,
      "learning_rate": 0.043646459972235076,
      "loss": 0.0006,
      "step": 60890
    },
    {
      "epoch": 28.181397501156873,
      "grad_norm": 0.003839425975456834,
      "learning_rate": 0.04363720499768626,
      "loss": 0.0001,
      "step": 60900
    },
    {
      "epoch": 28.18602498843128,
      "grad_norm": 0.005228409543633461,
      "learning_rate": 0.043627950023137435,
      "loss": 0.0001,
      "step": 60910
    },
    {
      "epoch": 28.190652475705694,
      "grad_norm": 0.3662905991077423,
      "learning_rate": 0.04361869504858862,
      "loss": 0.0004,
      "step": 60920
    },
    {
      "epoch": 28.195279962980102,
      "grad_norm": 0.007871576584875584,
      "learning_rate": 0.0436094400740398,
      "loss": 0.0002,
      "step": 60930
    },
    {
      "epoch": 28.19990745025451,
      "grad_norm": 0.003910167142748833,
      "learning_rate": 0.043600185099490985,
      "loss": 0.0002,
      "step": 60940
    },
    {
      "epoch": 28.204534937528923,
      "grad_norm": 0.01975381001830101,
      "learning_rate": 0.04359093012494216,
      "loss": 0.0001,
      "step": 60950
    },
    {
      "epoch": 28.20916242480333,
      "grad_norm": 0.029571829363703728,
      "learning_rate": 0.04358167515039334,
      "loss": 0.0001,
      "step": 60960
    },
    {
      "epoch": 28.213789912077743,
      "grad_norm": 0.012820817530155182,
      "learning_rate": 0.04357242017584452,
      "loss": 0.0001,
      "step": 60970
    },
    {
      "epoch": 28.218417399352152,
      "grad_norm": 0.003528429428115487,
      "learning_rate": 0.043563165201295696,
      "loss": 0.0,
      "step": 60980
    },
    {
      "epoch": 28.22304488662656,
      "grad_norm": 0.017515994608402252,
      "learning_rate": 0.04355391022674688,
      "loss": 0.0002,
      "step": 60990
    },
    {
      "epoch": 28.227672373900973,
      "grad_norm": 0.0018330732127651572,
      "learning_rate": 0.043544655252198056,
      "loss": 0.0001,
      "step": 61000
    },
    {
      "epoch": 28.23229986117538,
      "grad_norm": 0.0015707139391452074,
      "learning_rate": 0.04353540027764924,
      "loss": 0.0001,
      "step": 61010
    },
    {
      "epoch": 28.236927348449793,
      "grad_norm": 0.007944385521113873,
      "learning_rate": 0.04352614530310042,
      "loss": 0.0003,
      "step": 61020
    },
    {
      "epoch": 28.2415548357242,
      "grad_norm": 0.011468288488686085,
      "learning_rate": 0.0435168903285516,
      "loss": 0.0004,
      "step": 61030
    },
    {
      "epoch": 28.24618232299861,
      "grad_norm": 0.0017386385006830096,
      "learning_rate": 0.04350763535400278,
      "loss": 0.0001,
      "step": 61040
    },
    {
      "epoch": 28.250809810273022,
      "grad_norm": 0.004954648669809103,
      "learning_rate": 0.04349838037945396,
      "loss": 0.0002,
      "step": 61050
    },
    {
      "epoch": 28.25543729754743,
      "grad_norm": 0.004815870430320501,
      "learning_rate": 0.04348912540490514,
      "loss": 0.0001,
      "step": 61060
    },
    {
      "epoch": 28.260064784821843,
      "grad_norm": 0.03680713102221489,
      "learning_rate": 0.04347987043035632,
      "loss": 0.0003,
      "step": 61070
    },
    {
      "epoch": 28.26469227209625,
      "grad_norm": 0.0026810287963598967,
      "learning_rate": 0.0434706154558075,
      "loss": 0.0011,
      "step": 61080
    },
    {
      "epoch": 28.26931975937066,
      "grad_norm": 0.0037870605010539293,
      "learning_rate": 0.043461360481258676,
      "loss": 0.0039,
      "step": 61090
    },
    {
      "epoch": 28.273947246645072,
      "grad_norm": 0.0011280509643256664,
      "learning_rate": 0.04345210550670986,
      "loss": 0.0,
      "step": 61100
    },
    {
      "epoch": 28.27857473391948,
      "grad_norm": 0.008322268724441528,
      "learning_rate": 0.04344285053216104,
      "loss": 0.0,
      "step": 61110
    },
    {
      "epoch": 28.283202221193893,
      "grad_norm": 0.0004976207856088877,
      "learning_rate": 0.04343359555761222,
      "loss": 0.0001,
      "step": 61120
    },
    {
      "epoch": 28.2878297084683,
      "grad_norm": 0.00029481673846021295,
      "learning_rate": 0.0434243405830634,
      "loss": 0.0001,
      "step": 61130
    },
    {
      "epoch": 28.292457195742713,
      "grad_norm": 0.0019227203447371721,
      "learning_rate": 0.04341508560851458,
      "loss": 0.0002,
      "step": 61140
    },
    {
      "epoch": 28.297084683017122,
      "grad_norm": 0.0013767469208687544,
      "learning_rate": 0.04340583063396576,
      "loss": 0.0,
      "step": 61150
    },
    {
      "epoch": 28.30171217029153,
      "grad_norm": 0.07298490405082703,
      "learning_rate": 0.04339657565941694,
      "loss": 0.0001,
      "step": 61160
    },
    {
      "epoch": 28.306339657565943,
      "grad_norm": 0.10309106111526489,
      "learning_rate": 0.04338732068486812,
      "loss": 0.0001,
      "step": 61170
    },
    {
      "epoch": 28.31096714484035,
      "grad_norm": 0.003498645266517997,
      "learning_rate": 0.0433780657103193,
      "loss": 0.0001,
      "step": 61180
    },
    {
      "epoch": 28.315594632114763,
      "grad_norm": 0.013774797320365906,
      "learning_rate": 0.04336881073577048,
      "loss": 0.0001,
      "step": 61190
    },
    {
      "epoch": 28.32022211938917,
      "grad_norm": 0.0023838861379772425,
      "learning_rate": 0.04335955576122166,
      "loss": 0.0,
      "step": 61200
    },
    {
      "epoch": 28.32484960666358,
      "grad_norm": 0.19733260571956635,
      "learning_rate": 0.04335030078667284,
      "loss": 0.0004,
      "step": 61210
    },
    {
      "epoch": 28.329477093937992,
      "grad_norm": 0.002619106089696288,
      "learning_rate": 0.04334104581212402,
      "loss": 0.0,
      "step": 61220
    },
    {
      "epoch": 28.3341045812124,
      "grad_norm": 0.03550546616315842,
      "learning_rate": 0.0433317908375752,
      "loss": 0.0001,
      "step": 61230
    },
    {
      "epoch": 28.338732068486813,
      "grad_norm": 0.001645079581066966,
      "learning_rate": 0.04332253586302638,
      "loss": 0.0001,
      "step": 61240
    },
    {
      "epoch": 28.34335955576122,
      "grad_norm": 0.0011877515353262424,
      "learning_rate": 0.04331328088847756,
      "loss": 0.0,
      "step": 61250
    },
    {
      "epoch": 28.34798704303563,
      "grad_norm": 0.048060525208711624,
      "learning_rate": 0.043304025913928734,
      "loss": 0.0002,
      "step": 61260
    },
    {
      "epoch": 28.352614530310042,
      "grad_norm": 0.001989760436117649,
      "learning_rate": 0.04329477093937992,
      "loss": 0.0,
      "step": 61270
    },
    {
      "epoch": 28.35724201758445,
      "grad_norm": 0.02184678614139557,
      "learning_rate": 0.0432855159648311,
      "loss": 0.0001,
      "step": 61280
    },
    {
      "epoch": 28.361869504858863,
      "grad_norm": 0.001863335375674069,
      "learning_rate": 0.043276260990282284,
      "loss": 0.0001,
      "step": 61290
    },
    {
      "epoch": 28.36649699213327,
      "grad_norm": 0.005021199584007263,
      "learning_rate": 0.04326700601573346,
      "loss": 0.0001,
      "step": 61300
    },
    {
      "epoch": 28.371124479407683,
      "grad_norm": 0.0023484653793275356,
      "learning_rate": 0.04325775104118464,
      "loss": 0.0,
      "step": 61310
    },
    {
      "epoch": 28.375751966682092,
      "grad_norm": 0.0037244807463139296,
      "learning_rate": 0.04324849606663582,
      "loss": 0.0001,
      "step": 61320
    },
    {
      "epoch": 28.3803794539565,
      "grad_norm": 0.005581498146057129,
      "learning_rate": 0.043239241092086995,
      "loss": 0.0,
      "step": 61330
    },
    {
      "epoch": 28.385006941230913,
      "grad_norm": 0.0042219567112624645,
      "learning_rate": 0.04322998611753818,
      "loss": 0.0002,
      "step": 61340
    },
    {
      "epoch": 28.38963442850532,
      "grad_norm": 0.002654885407537222,
      "learning_rate": 0.043220731142989355,
      "loss": 0.0001,
      "step": 61350
    },
    {
      "epoch": 28.394261915779733,
      "grad_norm": 0.0015538770239800215,
      "learning_rate": 0.04321147616844054,
      "loss": 0.0001,
      "step": 61360
    },
    {
      "epoch": 28.39888940305414,
      "grad_norm": 0.20543910562992096,
      "learning_rate": 0.04320222119389172,
      "loss": 0.0002,
      "step": 61370
    },
    {
      "epoch": 28.40351689032855,
      "grad_norm": 0.003219134407117963,
      "learning_rate": 0.043192966219342904,
      "loss": 0.0001,
      "step": 61380
    },
    {
      "epoch": 28.408144377602962,
      "grad_norm": 0.006600317545235157,
      "learning_rate": 0.04318371124479408,
      "loss": 0.0001,
      "step": 61390
    },
    {
      "epoch": 28.41277186487737,
      "grad_norm": 0.00804150477051735,
      "learning_rate": 0.043174456270245264,
      "loss": 0.0002,
      "step": 61400
    },
    {
      "epoch": 28.417399352151783,
      "grad_norm": 0.006753815338015556,
      "learning_rate": 0.04316520129569644,
      "loss": 0.0001,
      "step": 61410
    },
    {
      "epoch": 28.42202683942619,
      "grad_norm": 0.005176067817956209,
      "learning_rate": 0.043155946321147616,
      "loss": 0.0001,
      "step": 61420
    },
    {
      "epoch": 28.4266543267006,
      "grad_norm": 0.0012473698006942868,
      "learning_rate": 0.0431466913465988,
      "loss": 0.0003,
      "step": 61430
    },
    {
      "epoch": 28.431281813975012,
      "grad_norm": 0.008639626204967499,
      "learning_rate": 0.043137436372049975,
      "loss": 0.0002,
      "step": 61440
    },
    {
      "epoch": 28.43590930124942,
      "grad_norm": 0.003282248741015792,
      "learning_rate": 0.04312818139750116,
      "loss": 0.0001,
      "step": 61450
    },
    {
      "epoch": 28.440536788523833,
      "grad_norm": 0.0035709841176867485,
      "learning_rate": 0.04311892642295234,
      "loss": 0.0022,
      "step": 61460
    },
    {
      "epoch": 28.44516427579824,
      "grad_norm": 0.003793522482737899,
      "learning_rate": 0.043109671448403525,
      "loss": 0.0,
      "step": 61470
    },
    {
      "epoch": 28.449791763072653,
      "grad_norm": 0.011763321235775948,
      "learning_rate": 0.0431004164738547,
      "loss": 0.0,
      "step": 61480
    },
    {
      "epoch": 28.454419250347062,
      "grad_norm": 0.001383266062475741,
      "learning_rate": 0.04309116149930588,
      "loss": 0.0001,
      "step": 61490
    },
    {
      "epoch": 28.45904673762147,
      "grad_norm": 0.02677026204764843,
      "learning_rate": 0.04308190652475706,
      "loss": 0.0001,
      "step": 61500
    },
    {
      "epoch": 28.463674224895883,
      "grad_norm": 0.011250351555645466,
      "learning_rate": 0.04307265155020824,
      "loss": 0.0001,
      "step": 61510
    },
    {
      "epoch": 28.46830171217029,
      "grad_norm": 0.006885362323373556,
      "learning_rate": 0.04306339657565942,
      "loss": 0.0001,
      "step": 61520
    },
    {
      "epoch": 28.472929199444703,
      "grad_norm": 0.46918606758117676,
      "learning_rate": 0.043054141601110596,
      "loss": 0.0001,
      "step": 61530
    },
    {
      "epoch": 28.47755668671911,
      "grad_norm": 0.004410027991980314,
      "learning_rate": 0.04304488662656178,
      "loss": 0.0001,
      "step": 61540
    },
    {
      "epoch": 28.48218417399352,
      "grad_norm": 0.0025263428688049316,
      "learning_rate": 0.04303563165201296,
      "loss": 0.0001,
      "step": 61550
    },
    {
      "epoch": 28.486811661267932,
      "grad_norm": 0.015495997853577137,
      "learning_rate": 0.04302637667746414,
      "loss": 0.0001,
      "step": 61560
    },
    {
      "epoch": 28.49143914854234,
      "grad_norm": 0.0010277009569108486,
      "learning_rate": 0.04301712170291532,
      "loss": 0.0019,
      "step": 61570
    },
    {
      "epoch": 28.496066635816753,
      "grad_norm": 0.0022397865541279316,
      "learning_rate": 0.0430078667283665,
      "loss": 0.0001,
      "step": 61580
    },
    {
      "epoch": 28.50069412309116,
      "grad_norm": 0.005878170486539602,
      "learning_rate": 0.04299861175381768,
      "loss": 0.0,
      "step": 61590
    },
    {
      "epoch": 28.50532161036557,
      "grad_norm": 0.0024169092066586018,
      "learning_rate": 0.04298935677926886,
      "loss": 0.0008,
      "step": 61600
    },
    {
      "epoch": 28.509949097639982,
      "grad_norm": 0.0103618074208498,
      "learning_rate": 0.04298010180472004,
      "loss": 0.0001,
      "step": 61610
    },
    {
      "epoch": 28.51457658491439,
      "grad_norm": 0.0038769294042140245,
      "learning_rate": 0.04297084683017122,
      "loss": 0.0004,
      "step": 61620
    },
    {
      "epoch": 28.519204072188803,
      "grad_norm": 0.009413430467247963,
      "learning_rate": 0.0429615918556224,
      "loss": 0.0001,
      "step": 61630
    },
    {
      "epoch": 28.52383155946321,
      "grad_norm": 0.0031831387896090746,
      "learning_rate": 0.04295233688107358,
      "loss": 0.0009,
      "step": 61640
    },
    {
      "epoch": 28.52845904673762,
      "grad_norm": 0.004873491823673248,
      "learning_rate": 0.04294308190652476,
      "loss": 0.0001,
      "step": 61650
    },
    {
      "epoch": 28.533086534012032,
      "grad_norm": 0.00344530469737947,
      "learning_rate": 0.04293382693197594,
      "loss": 0.0001,
      "step": 61660
    },
    {
      "epoch": 28.53771402128644,
      "grad_norm": 0.002577998675405979,
      "learning_rate": 0.04292457195742712,
      "loss": 0.0001,
      "step": 61670
    },
    {
      "epoch": 28.542341508560853,
      "grad_norm": 0.020595235750079155,
      "learning_rate": 0.0429153169828783,
      "loss": 0.0003,
      "step": 61680
    },
    {
      "epoch": 28.54696899583526,
      "grad_norm": 0.006498133763670921,
      "learning_rate": 0.04290606200832948,
      "loss": 0.0001,
      "step": 61690
    },
    {
      "epoch": 28.551596483109673,
      "grad_norm": 0.0032188312616199255,
      "learning_rate": 0.04289680703378066,
      "loss": 0.0,
      "step": 61700
    },
    {
      "epoch": 28.55622397038408,
      "grad_norm": 0.0033279117196798325,
      "learning_rate": 0.04288755205923184,
      "loss": 0.0002,
      "step": 61710
    },
    {
      "epoch": 28.56085145765849,
      "grad_norm": 0.026480764150619507,
      "learning_rate": 0.04287829708468302,
      "loss": 0.0001,
      "step": 61720
    },
    {
      "epoch": 28.565478944932902,
      "grad_norm": 0.011315234936773777,
      "learning_rate": 0.0428690421101342,
      "loss": 0.0001,
      "step": 61730
    },
    {
      "epoch": 28.57010643220731,
      "grad_norm": 0.031565193086862564,
      "learning_rate": 0.04285978713558538,
      "loss": 0.0001,
      "step": 61740
    },
    {
      "epoch": 28.574733919481723,
      "grad_norm": 0.013834145851433277,
      "learning_rate": 0.04285053216103656,
      "loss": 0.0001,
      "step": 61750
    },
    {
      "epoch": 28.57936140675613,
      "grad_norm": 0.007398910820484161,
      "learning_rate": 0.04284127718648774,
      "loss": 0.0001,
      "step": 61760
    },
    {
      "epoch": 28.58398889403054,
      "grad_norm": 0.001582056749612093,
      "learning_rate": 0.04283202221193892,
      "loss": 0.0001,
      "step": 61770
    },
    {
      "epoch": 28.588616381304952,
      "grad_norm": 0.004354784730821848,
      "learning_rate": 0.0428227672373901,
      "loss": 0.0001,
      "step": 61780
    },
    {
      "epoch": 28.59324386857936,
      "grad_norm": 0.0021931673400104046,
      "learning_rate": 0.042813512262841275,
      "loss": 0.0002,
      "step": 61790
    },
    {
      "epoch": 28.597871355853773,
      "grad_norm": 0.02741595357656479,
      "learning_rate": 0.04280425728829246,
      "loss": 0.0002,
      "step": 61800
    },
    {
      "epoch": 28.60249884312818,
      "grad_norm": 0.08465072512626648,
      "learning_rate": 0.04279500231374364,
      "loss": 0.0001,
      "step": 61810
    },
    {
      "epoch": 28.60712633040259,
      "grad_norm": 0.0026891843881458044,
      "learning_rate": 0.042785747339194824,
      "loss": 0.0,
      "step": 61820
    },
    {
      "epoch": 28.611753817677002,
      "grad_norm": 0.0423990823328495,
      "learning_rate": 0.042776492364646,
      "loss": 0.0001,
      "step": 61830
    },
    {
      "epoch": 28.61638130495141,
      "grad_norm": 0.0057468232698738575,
      "learning_rate": 0.04276723739009718,
      "loss": 0.0,
      "step": 61840
    },
    {
      "epoch": 28.621008792225823,
      "grad_norm": 0.008369015529751778,
      "learning_rate": 0.04275798241554836,
      "loss": 0.0001,
      "step": 61850
    },
    {
      "epoch": 28.62563627950023,
      "grad_norm": 0.0029303780756890774,
      "learning_rate": 0.04274872744099954,
      "loss": 0.0001,
      "step": 61860
    },
    {
      "epoch": 28.63026376677464,
      "grad_norm": 0.021313900128006935,
      "learning_rate": 0.04273947246645072,
      "loss": 0.0003,
      "step": 61870
    },
    {
      "epoch": 28.63489125404905,
      "grad_norm": 0.0032813281286507845,
      "learning_rate": 0.042730217491901895,
      "loss": 0.0001,
      "step": 61880
    },
    {
      "epoch": 28.63951874132346,
      "grad_norm": 0.0018415035447105765,
      "learning_rate": 0.04272096251735308,
      "loss": 0.0001,
      "step": 61890
    },
    {
      "epoch": 28.644146228597872,
      "grad_norm": 0.004130009561777115,
      "learning_rate": 0.04271170754280426,
      "loss": 0.0001,
      "step": 61900
    },
    {
      "epoch": 28.64877371587228,
      "grad_norm": 0.001895476714707911,
      "learning_rate": 0.042702452568255445,
      "loss": 0.0002,
      "step": 61910
    },
    {
      "epoch": 28.653401203146693,
      "grad_norm": 0.011964847333729267,
      "learning_rate": 0.04269319759370662,
      "loss": 0.0001,
      "step": 61920
    },
    {
      "epoch": 28.6580286904211,
      "grad_norm": 0.015897953882813454,
      "learning_rate": 0.042683942619157804,
      "loss": 0.0001,
      "step": 61930
    },
    {
      "epoch": 28.66265617769551,
      "grad_norm": 0.001615115674212575,
      "learning_rate": 0.04267468764460898,
      "loss": 0.0025,
      "step": 61940
    },
    {
      "epoch": 28.667283664969922,
      "grad_norm": 0.5347362160682678,
      "learning_rate": 0.042665432670060156,
      "loss": 0.0002,
      "step": 61950
    },
    {
      "epoch": 28.67191115224433,
      "grad_norm": 0.058222364634275436,
      "learning_rate": 0.04265617769551134,
      "loss": 0.0001,
      "step": 61960
    },
    {
      "epoch": 28.676538639518743,
      "grad_norm": 0.0009339976822957397,
      "learning_rate": 0.042646922720962516,
      "loss": 0.0001,
      "step": 61970
    },
    {
      "epoch": 28.68116612679315,
      "grad_norm": 0.025585521012544632,
      "learning_rate": 0.0426376677464137,
      "loss": 0.0001,
      "step": 61980
    },
    {
      "epoch": 28.68579361406756,
      "grad_norm": 0.00981223676353693,
      "learning_rate": 0.04262841277186488,
      "loss": 0.0001,
      "step": 61990
    },
    {
      "epoch": 28.690421101341972,
      "grad_norm": 0.0694635659456253,
      "learning_rate": 0.042619157797316065,
      "loss": 0.0001,
      "step": 62000
    },
    {
      "epoch": 28.69504858861638,
      "grad_norm": 0.0014813512098044157,
      "learning_rate": 0.04260990282276724,
      "loss": 0.0002,
      "step": 62010
    },
    {
      "epoch": 28.699676075890793,
      "grad_norm": 0.0017757301684468985,
      "learning_rate": 0.04260064784821842,
      "loss": 0.0001,
      "step": 62020
    },
    {
      "epoch": 28.7043035631652,
      "grad_norm": 0.019761020317673683,
      "learning_rate": 0.0425913928736696,
      "loss": 0.0001,
      "step": 62030
    },
    {
      "epoch": 28.70893105043961,
      "grad_norm": 0.0022506332024931908,
      "learning_rate": 0.04258213789912078,
      "loss": 0.0001,
      "step": 62040
    },
    {
      "epoch": 28.71355853771402,
      "grad_norm": 0.06367875635623932,
      "learning_rate": 0.04257288292457196,
      "loss": 0.0003,
      "step": 62050
    },
    {
      "epoch": 28.71818602498843,
      "grad_norm": 0.18123996257781982,
      "learning_rate": 0.042563627950023136,
      "loss": 0.0001,
      "step": 62060
    },
    {
      "epoch": 28.722813512262842,
      "grad_norm": 0.012470709159970284,
      "learning_rate": 0.04255437297547432,
      "loss": 0.0008,
      "step": 62070
    },
    {
      "epoch": 28.72744099953725,
      "grad_norm": 0.005444478243589401,
      "learning_rate": 0.0425451180009255,
      "loss": 0.0001,
      "step": 62080
    },
    {
      "epoch": 28.732068486811663,
      "grad_norm": 0.01954868994653225,
      "learning_rate": 0.042535863026376686,
      "loss": 0.0051,
      "step": 62090
    },
    {
      "epoch": 28.73669597408607,
      "grad_norm": 0.0048048244789242744,
      "learning_rate": 0.04252660805182786,
      "loss": 0.0002,
      "step": 62100
    },
    {
      "epoch": 28.74132346136048,
      "grad_norm": 0.0026037374045699835,
      "learning_rate": 0.04251735307727904,
      "loss": 0.0001,
      "step": 62110
    },
    {
      "epoch": 28.745950948634892,
      "grad_norm": 0.006393864285200834,
      "learning_rate": 0.04250809810273022,
      "loss": 0.0001,
      "step": 62120
    },
    {
      "epoch": 28.7505784359093,
      "grad_norm": 0.0022760452702641487,
      "learning_rate": 0.0424988431281814,
      "loss": 0.0001,
      "step": 62130
    },
    {
      "epoch": 28.755205923183713,
      "grad_norm": 0.003565866267308593,
      "learning_rate": 0.04248958815363258,
      "loss": 0.0001,
      "step": 62140
    },
    {
      "epoch": 28.75983341045812,
      "grad_norm": 0.0030285536777228117,
      "learning_rate": 0.04248033317908376,
      "loss": 0.0001,
      "step": 62150
    },
    {
      "epoch": 28.76446089773253,
      "grad_norm": 0.01571538671851158,
      "learning_rate": 0.04247107820453494,
      "loss": 0.0001,
      "step": 62160
    },
    {
      "epoch": 28.769088385006942,
      "grad_norm": 0.03716910257935524,
      "learning_rate": 0.04246182322998612,
      "loss": 0.0002,
      "step": 62170
    },
    {
      "epoch": 28.77371587228135,
      "grad_norm": 0.0017652653623372316,
      "learning_rate": 0.0424525682554373,
      "loss": 0.0001,
      "step": 62180
    },
    {
      "epoch": 28.778343359555763,
      "grad_norm": 0.015433748252689838,
      "learning_rate": 0.04244331328088848,
      "loss": 0.0001,
      "step": 62190
    },
    {
      "epoch": 28.78297084683017,
      "grad_norm": 0.00119274971075356,
      "learning_rate": 0.04243405830633966,
      "loss": 0.0001,
      "step": 62200
    },
    {
      "epoch": 28.78759833410458,
      "grad_norm": 0.03652723506093025,
      "learning_rate": 0.04242480333179084,
      "loss": 0.0001,
      "step": 62210
    },
    {
      "epoch": 28.79222582137899,
      "grad_norm": 0.008150031790137291,
      "learning_rate": 0.04241554835724202,
      "loss": 0.0002,
      "step": 62220
    },
    {
      "epoch": 28.7968533086534,
      "grad_norm": 0.014171015471220016,
      "learning_rate": 0.0424062933826932,
      "loss": 0.0002,
      "step": 62230
    },
    {
      "epoch": 28.801480795927812,
      "grad_norm": 0.00540150236338377,
      "learning_rate": 0.04239703840814438,
      "loss": 0.0001,
      "step": 62240
    },
    {
      "epoch": 28.80610828320222,
      "grad_norm": 0.0024478379637002945,
      "learning_rate": 0.04238778343359556,
      "loss": 0.0,
      "step": 62250
    },
    {
      "epoch": 28.810735770476633,
      "grad_norm": 0.01767497882246971,
      "learning_rate": 0.042378528459046744,
      "loss": 0.0002,
      "step": 62260
    },
    {
      "epoch": 28.81536325775104,
      "grad_norm": 0.000547978444956243,
      "learning_rate": 0.04236927348449792,
      "loss": 0.0,
      "step": 62270
    },
    {
      "epoch": 28.81999074502545,
      "grad_norm": 0.012229012325406075,
      "learning_rate": 0.0423600185099491,
      "loss": 0.0001,
      "step": 62280
    },
    {
      "epoch": 28.824618232299862,
      "grad_norm": 0.0012753409100696445,
      "learning_rate": 0.04235076353540028,
      "loss": 0.0002,
      "step": 62290
    },
    {
      "epoch": 28.82924571957427,
      "grad_norm": 0.0022721351124346256,
      "learning_rate": 0.04234150856085146,
      "loss": 0.0001,
      "step": 62300
    },
    {
      "epoch": 28.833873206848683,
      "grad_norm": 0.0011537428945302963,
      "learning_rate": 0.04233225358630264,
      "loss": 0.0004,
      "step": 62310
    },
    {
      "epoch": 28.83850069412309,
      "grad_norm": 0.009387665428221226,
      "learning_rate": 0.04232299861175382,
      "loss": 0.0001,
      "step": 62320
    },
    {
      "epoch": 28.8431281813975,
      "grad_norm": 0.6215639114379883,
      "learning_rate": 0.042313743637205,
      "loss": 0.0002,
      "step": 62330
    },
    {
      "epoch": 28.847755668671912,
      "grad_norm": 0.19534198939800262,
      "learning_rate": 0.04230448866265618,
      "loss": 0.0003,
      "step": 62340
    },
    {
      "epoch": 28.85238315594632,
      "grad_norm": 0.034553054720163345,
      "learning_rate": 0.042295233688107364,
      "loss": 0.0001,
      "step": 62350
    },
    {
      "epoch": 28.857010643220733,
      "grad_norm": 0.0048783873207867146,
      "learning_rate": 0.04228597871355854,
      "loss": 0.0002,
      "step": 62360
    },
    {
      "epoch": 28.86163813049514,
      "grad_norm": 0.006241409108042717,
      "learning_rate": 0.042276723739009724,
      "loss": 0.0001,
      "step": 62370
    },
    {
      "epoch": 28.86626561776955,
      "grad_norm": 0.005345430690795183,
      "learning_rate": 0.0422674687644609,
      "loss": 0.0001,
      "step": 62380
    },
    {
      "epoch": 28.87089310504396,
      "grad_norm": 0.03472967818379402,
      "learning_rate": 0.04225821378991208,
      "loss": 0.0002,
      "step": 62390
    },
    {
      "epoch": 28.87552059231837,
      "grad_norm": 0.006918708793818951,
      "learning_rate": 0.04224895881536326,
      "loss": 0.0001,
      "step": 62400
    },
    {
      "epoch": 28.880148079592782,
      "grad_norm": 0.007485950831323862,
      "learning_rate": 0.042239703840814435,
      "loss": 0.0002,
      "step": 62410
    },
    {
      "epoch": 28.88477556686719,
      "grad_norm": 0.007576850708574057,
      "learning_rate": 0.04223044886626562,
      "loss": 0.0001,
      "step": 62420
    },
    {
      "epoch": 28.889403054141603,
      "grad_norm": 0.014793560840189457,
      "learning_rate": 0.0422211938917168,
      "loss": 0.0001,
      "step": 62430
    },
    {
      "epoch": 28.89403054141601,
      "grad_norm": 0.07131674140691757,
      "learning_rate": 0.042211938917167985,
      "loss": 0.0001,
      "step": 62440
    },
    {
      "epoch": 28.89865802869042,
      "grad_norm": 0.004737282171845436,
      "learning_rate": 0.04220268394261916,
      "loss": 0.0001,
      "step": 62450
    },
    {
      "epoch": 28.903285515964832,
      "grad_norm": 0.012704562395811081,
      "learning_rate": 0.042193428968070344,
      "loss": 0.0001,
      "step": 62460
    },
    {
      "epoch": 28.90791300323924,
      "grad_norm": 0.009148736484348774,
      "learning_rate": 0.04218417399352152,
      "loss": 0.0001,
      "step": 62470
    },
    {
      "epoch": 28.912540490513653,
      "grad_norm": 0.010425200685858727,
      "learning_rate": 0.0421749190189727,
      "loss": 0.0001,
      "step": 62480
    },
    {
      "epoch": 28.91716797778806,
      "grad_norm": 0.02815440483391285,
      "learning_rate": 0.04216566404442388,
      "loss": 0.0001,
      "step": 62490
    },
    {
      "epoch": 28.92179546506247,
      "grad_norm": 0.0030690713319927454,
      "learning_rate": 0.042156409069875056,
      "loss": 0.0,
      "step": 62500
    },
    {
      "epoch": 28.926422952336882,
      "grad_norm": 0.05244079604744911,
      "learning_rate": 0.04214715409532624,
      "loss": 0.0002,
      "step": 62510
    },
    {
      "epoch": 28.93105043961129,
      "grad_norm": 0.003375868545845151,
      "learning_rate": 0.04213789912077742,
      "loss": 0.0,
      "step": 62520
    },
    {
      "epoch": 28.935677926885703,
      "grad_norm": 0.00562052708119154,
      "learning_rate": 0.042128644146228605,
      "loss": 0.0,
      "step": 62530
    },
    {
      "epoch": 28.94030541416011,
      "grad_norm": 0.0013443173374980688,
      "learning_rate": 0.04211938917167978,
      "loss": 0.0001,
      "step": 62540
    },
    {
      "epoch": 28.94493290143452,
      "grad_norm": 0.008895071223378181,
      "learning_rate": 0.042110134197130965,
      "loss": 0.0,
      "step": 62550
    },
    {
      "epoch": 28.94956038870893,
      "grad_norm": 0.0399104543030262,
      "learning_rate": 0.04210087922258214,
      "loss": 0.0001,
      "step": 62560
    },
    {
      "epoch": 28.95418787598334,
      "grad_norm": 0.17329034209251404,
      "learning_rate": 0.04209162424803332,
      "loss": 0.0002,
      "step": 62570
    },
    {
      "epoch": 28.958815363257752,
      "grad_norm": 0.010102024301886559,
      "learning_rate": 0.0420823692734845,
      "loss": 0.0001,
      "step": 62580
    },
    {
      "epoch": 28.96344285053216,
      "grad_norm": 0.001100863446481526,
      "learning_rate": 0.04207311429893568,
      "loss": 0.0035,
      "step": 62590
    },
    {
      "epoch": 28.96807033780657,
      "grad_norm": 0.019636498764157295,
      "learning_rate": 0.04206385932438686,
      "loss": 0.0002,
      "step": 62600
    },
    {
      "epoch": 28.97269782508098,
      "grad_norm": 0.00308223906904459,
      "learning_rate": 0.04205460434983804,
      "loss": 0.0001,
      "step": 62610
    },
    {
      "epoch": 28.97732531235539,
      "grad_norm": 0.23596224188804626,
      "learning_rate": 0.042045349375289226,
      "loss": 0.0001,
      "step": 62620
    },
    {
      "epoch": 28.981952799629802,
      "grad_norm": 0.0406009815633297,
      "learning_rate": 0.0420360944007404,
      "loss": 0.0001,
      "step": 62630
    },
    {
      "epoch": 28.98658028690421,
      "grad_norm": 0.0034986978862434626,
      "learning_rate": 0.04202683942619158,
      "loss": 0.0002,
      "step": 62640
    },
    {
      "epoch": 28.991207774178623,
      "grad_norm": 0.0028396588750183582,
      "learning_rate": 0.04201758445164276,
      "loss": 0.0001,
      "step": 62650
    },
    {
      "epoch": 28.99583526145303,
      "grad_norm": 0.0011107196332886815,
      "learning_rate": 0.04200832947709394,
      "loss": 0.0002,
      "step": 62660
    },
    {
      "epoch": 29.0,
      "eval_accuracy_branch1": 0.9897550454475428,
      "eval_accuracy_branch2": 0.5000770297334771,
      "eval_f1_branch1": 0.990745328989671,
      "eval_f1_branch2": 0.5000648793387894,
      "eval_loss": 0.019639212638139725,
      "eval_precision_branch1": 0.9909321714976325,
      "eval_precision_branch2": 0.5000770372227101,
      "eval_recall_branch1": 0.9906745523350149,
      "eval_recall_branch2": 0.5000770297334771,
      "eval_runtime": 29.9992,
      "eval_samples_per_second": 432.744,
      "eval_steps_per_second": 54.101,
      "step": 62669
    },
    {
      "epoch": 29.00046274872744,
      "grad_norm": 0.0071734958328306675,
      "learning_rate": 0.04199907450254512,
      "loss": 0.0341,
      "step": 62670
    },
    {
      "epoch": 29.005090236001852,
      "grad_norm": 0.0079388078302145,
      "learning_rate": 0.0419898195279963,
      "loss": 0.0001,
      "step": 62680
    },
    {
      "epoch": 29.00971772327626,
      "grad_norm": 0.01539663877338171,
      "learning_rate": 0.04198056455344748,
      "loss": 0.0002,
      "step": 62690
    },
    {
      "epoch": 29.014345210550673,
      "grad_norm": 0.005748532246798277,
      "learning_rate": 0.04197130957889866,
      "loss": 0.0001,
      "step": 62700
    },
    {
      "epoch": 29.01897269782508,
      "grad_norm": 0.002181397285312414,
      "learning_rate": 0.04196205460434984,
      "loss": 0.0001,
      "step": 62710
    },
    {
      "epoch": 29.02360018509949,
      "grad_norm": 0.027009883895516396,
      "learning_rate": 0.04195279962980102,
      "loss": 0.0003,
      "step": 62720
    },
    {
      "epoch": 29.0282276723739,
      "grad_norm": 0.014294454827904701,
      "learning_rate": 0.0419435446552522,
      "loss": 0.0007,
      "step": 62730
    },
    {
      "epoch": 29.03285515964831,
      "grad_norm": 0.02526603452861309,
      "learning_rate": 0.04193428968070338,
      "loss": 0.0001,
      "step": 62740
    },
    {
      "epoch": 29.037482646922722,
      "grad_norm": 0.002134831389412284,
      "learning_rate": 0.04192503470615456,
      "loss": 0.0001,
      "step": 62750
    },
    {
      "epoch": 29.04211013419713,
      "grad_norm": 0.21442297101020813,
      "learning_rate": 0.04191577973160574,
      "loss": 0.0001,
      "step": 62760
    },
    {
      "epoch": 29.04673762147154,
      "grad_norm": 0.004424788989126682,
      "learning_rate": 0.04190652475705692,
      "loss": 0.0001,
      "step": 62770
    },
    {
      "epoch": 29.05136510874595,
      "grad_norm": 0.007307858671993017,
      "learning_rate": 0.0418972697825081,
      "loss": 0.0004,
      "step": 62780
    },
    {
      "epoch": 29.05599259602036,
      "grad_norm": 0.005531288683414459,
      "learning_rate": 0.041888014807959284,
      "loss": 0.0,
      "step": 62790
    },
    {
      "epoch": 29.060620083294772,
      "grad_norm": 0.012458435259759426,
      "learning_rate": 0.04187875983341046,
      "loss": 0.0001,
      "step": 62800
    },
    {
      "epoch": 29.06524757056918,
      "grad_norm": 0.014388184063136578,
      "learning_rate": 0.04186950485886164,
      "loss": 0.0001,
      "step": 62810
    },
    {
      "epoch": 29.06987505784359,
      "grad_norm": 0.02160760387778282,
      "learning_rate": 0.04186024988431282,
      "loss": 0.0002,
      "step": 62820
    },
    {
      "epoch": 29.074502545118,
      "grad_norm": 0.22352392971515656,
      "learning_rate": 0.041850994909764,
      "loss": 0.0002,
      "step": 62830
    },
    {
      "epoch": 29.07913003239241,
      "grad_norm": 0.0223381444811821,
      "learning_rate": 0.04184173993521518,
      "loss": 0.0001,
      "step": 62840
    },
    {
      "epoch": 29.083757519666822,
      "grad_norm": 0.03172443434596062,
      "learning_rate": 0.04183248496066636,
      "loss": 0.0001,
      "step": 62850
    },
    {
      "epoch": 29.08838500694123,
      "grad_norm": 0.0027767487335950136,
      "learning_rate": 0.04182322998611754,
      "loss": 0.0011,
      "step": 62860
    },
    {
      "epoch": 29.093012494215643,
      "grad_norm": 0.0016524712555110455,
      "learning_rate": 0.04181397501156872,
      "loss": 0.0001,
      "step": 62870
    },
    {
      "epoch": 29.09763998149005,
      "grad_norm": 0.025353360921144485,
      "learning_rate": 0.041804720037019905,
      "loss": 0.0002,
      "step": 62880
    },
    {
      "epoch": 29.10226746876446,
      "grad_norm": 0.0070015303790569305,
      "learning_rate": 0.04179546506247108,
      "loss": 0.0001,
      "step": 62890
    },
    {
      "epoch": 29.10689495603887,
      "grad_norm": 0.017171494662761688,
      "learning_rate": 0.041786210087922264,
      "loss": 0.0001,
      "step": 62900
    },
    {
      "epoch": 29.11152244331328,
      "grad_norm": 0.0020945679862052202,
      "learning_rate": 0.04177695511337344,
      "loss": 0.0001,
      "step": 62910
    },
    {
      "epoch": 29.116149930587692,
      "grad_norm": 0.0026295825373381376,
      "learning_rate": 0.04176770013882462,
      "loss": 0.0001,
      "step": 62920
    },
    {
      "epoch": 29.1207774178621,
      "grad_norm": 0.00283639645203948,
      "learning_rate": 0.0417584451642758,
      "loss": 0.0001,
      "step": 62930
    },
    {
      "epoch": 29.12540490513651,
      "grad_norm": 0.0057380362413823605,
      "learning_rate": 0.041749190189726976,
      "loss": 0.0001,
      "step": 62940
    },
    {
      "epoch": 29.13003239241092,
      "grad_norm": 0.0017114403890445828,
      "learning_rate": 0.04173993521517816,
      "loss": 0.0001,
      "step": 62950
    },
    {
      "epoch": 29.13465987968533,
      "grad_norm": 0.0014837959315627813,
      "learning_rate": 0.04173068024062934,
      "loss": 0.0,
      "step": 62960
    },
    {
      "epoch": 29.139287366959742,
      "grad_norm": 0.0021742372773587704,
      "learning_rate": 0.041721425266080525,
      "loss": 0.0001,
      "step": 62970
    },
    {
      "epoch": 29.14391485423415,
      "grad_norm": 0.0026407092809677124,
      "learning_rate": 0.0417121702915317,
      "loss": 0.0001,
      "step": 62980
    },
    {
      "epoch": 29.14854234150856,
      "grad_norm": 0.023257657885551453,
      "learning_rate": 0.041702915316982885,
      "loss": 0.0002,
      "step": 62990
    },
    {
      "epoch": 29.15316982878297,
      "grad_norm": 0.0019037412712350488,
      "learning_rate": 0.04169366034243406,
      "loss": 0.0001,
      "step": 63000
    },
    {
      "epoch": 29.15779731605738,
      "grad_norm": 0.006757744122296572,
      "learning_rate": 0.041684405367885244,
      "loss": 0.0001,
      "step": 63010
    },
    {
      "epoch": 29.162424803331792,
      "grad_norm": 0.07709760963916779,
      "learning_rate": 0.04167515039333642,
      "loss": 0.0001,
      "step": 63020
    },
    {
      "epoch": 29.1670522906062,
      "grad_norm": 0.005543012171983719,
      "learning_rate": 0.041665895418787596,
      "loss": 0.0001,
      "step": 63030
    },
    {
      "epoch": 29.171679777880613,
      "grad_norm": 0.007414433639496565,
      "learning_rate": 0.04165664044423878,
      "loss": 0.0001,
      "step": 63040
    },
    {
      "epoch": 29.17630726515502,
      "grad_norm": 0.09203966706991196,
      "learning_rate": 0.04164738546968996,
      "loss": 0.0001,
      "step": 63050
    },
    {
      "epoch": 29.18093475242943,
      "grad_norm": 0.19372662901878357,
      "learning_rate": 0.041638130495141146,
      "loss": 0.0001,
      "step": 63060
    },
    {
      "epoch": 29.18556223970384,
      "grad_norm": 0.0010859883623197675,
      "learning_rate": 0.04162887552059232,
      "loss": 0.0001,
      "step": 63070
    },
    {
      "epoch": 29.19018972697825,
      "grad_norm": 0.0008059096871875226,
      "learning_rate": 0.041619620546043505,
      "loss": 0.0,
      "step": 63080
    },
    {
      "epoch": 29.194817214252662,
      "grad_norm": 0.02509840950369835,
      "learning_rate": 0.04161036557149468,
      "loss": 0.0001,
      "step": 63090
    },
    {
      "epoch": 29.19944470152707,
      "grad_norm": 0.03398408368229866,
      "learning_rate": 0.04160111059694586,
      "loss": 0.0002,
      "step": 63100
    },
    {
      "epoch": 29.20407218880148,
      "grad_norm": 0.006155961658805609,
      "learning_rate": 0.04159185562239704,
      "loss": 0.0001,
      "step": 63110
    },
    {
      "epoch": 29.20869967607589,
      "grad_norm": 0.003914855420589447,
      "learning_rate": 0.04158260064784822,
      "loss": 0.0,
      "step": 63120
    },
    {
      "epoch": 29.2133271633503,
      "grad_norm": 0.0008343126391991973,
      "learning_rate": 0.0415733456732994,
      "loss": 0.0001,
      "step": 63130
    },
    {
      "epoch": 29.217954650624712,
      "grad_norm": 0.0013924509985372424,
      "learning_rate": 0.04156409069875058,
      "loss": 0.0,
      "step": 63140
    },
    {
      "epoch": 29.22258213789912,
      "grad_norm": 0.049391672015190125,
      "learning_rate": 0.041554835724201766,
      "loss": 0.0004,
      "step": 63150
    },
    {
      "epoch": 29.22720962517353,
      "grad_norm": 0.008580178022384644,
      "learning_rate": 0.04154558074965294,
      "loss": 0.0001,
      "step": 63160
    },
    {
      "epoch": 29.23183711244794,
      "grad_norm": 0.0019028863171115518,
      "learning_rate": 0.04153632577510412,
      "loss": 0.0001,
      "step": 63170
    },
    {
      "epoch": 29.23646459972235,
      "grad_norm": 0.0008014088380150497,
      "learning_rate": 0.0415270708005553,
      "loss": 0.0001,
      "step": 63180
    },
    {
      "epoch": 29.241092086996762,
      "grad_norm": 1.0302320718765259,
      "learning_rate": 0.04151781582600648,
      "loss": 0.0003,
      "step": 63190
    },
    {
      "epoch": 29.24571957427117,
      "grad_norm": 0.004611740354448557,
      "learning_rate": 0.04150856085145766,
      "loss": 0.0001,
      "step": 63200
    },
    {
      "epoch": 29.250347061545583,
      "grad_norm": 0.03151065856218338,
      "learning_rate": 0.04149930587690884,
      "loss": 0.0001,
      "step": 63210
    },
    {
      "epoch": 29.25497454881999,
      "grad_norm": 0.004736671224236488,
      "learning_rate": 0.04149005090236002,
      "loss": 0.0001,
      "step": 63220
    },
    {
      "epoch": 29.2596020360944,
      "grad_norm": 0.0015631651040166616,
      "learning_rate": 0.041480795927811204,
      "loss": 0.0003,
      "step": 63230
    },
    {
      "epoch": 29.26422952336881,
      "grad_norm": 0.011960513889789581,
      "learning_rate": 0.04147154095326238,
      "loss": 0.0001,
      "step": 63240
    },
    {
      "epoch": 29.26885701064322,
      "grad_norm": 0.014882195740938187,
      "learning_rate": 0.04146228597871356,
      "loss": 0.0001,
      "step": 63250
    },
    {
      "epoch": 29.273484497917632,
      "grad_norm": 0.0028897144366055727,
      "learning_rate": 0.04145303100416474,
      "loss": 0.0002,
      "step": 63260
    },
    {
      "epoch": 29.27811198519204,
      "grad_norm": 0.0093103451654315,
      "learning_rate": 0.04144377602961592,
      "loss": 0.0,
      "step": 63270
    },
    {
      "epoch": 29.28273947246645,
      "grad_norm": 0.00302143138833344,
      "learning_rate": 0.0414345210550671,
      "loss": 0.0001,
      "step": 63280
    },
    {
      "epoch": 29.28736695974086,
      "grad_norm": 0.013205723837018013,
      "learning_rate": 0.04142526608051828,
      "loss": 0.0001,
      "step": 63290
    },
    {
      "epoch": 29.29199444701527,
      "grad_norm": 0.011309354566037655,
      "learning_rate": 0.04141601110596946,
      "loss": 0.0001,
      "step": 63300
    },
    {
      "epoch": 29.296621934289682,
      "grad_norm": 0.003359694266691804,
      "learning_rate": 0.04140675613142064,
      "loss": 0.0001,
      "step": 63310
    },
    {
      "epoch": 29.30124942156409,
      "grad_norm": 0.009683318436145782,
      "learning_rate": 0.041397501156871824,
      "loss": 0.0,
      "step": 63320
    },
    {
      "epoch": 29.3058769088385,
      "grad_norm": 0.0006030579679645598,
      "learning_rate": 0.041388246182323,
      "loss": 0.0001,
      "step": 63330
    },
    {
      "epoch": 29.31050439611291,
      "grad_norm": 0.0027647451497614384,
      "learning_rate": 0.041378991207774184,
      "loss": 0.0001,
      "step": 63340
    },
    {
      "epoch": 29.31513188338732,
      "grad_norm": 0.041634101420640945,
      "learning_rate": 0.04136973623322536,
      "loss": 0.0001,
      "step": 63350
    },
    {
      "epoch": 29.319759370661732,
      "grad_norm": 0.043937381356954575,
      "learning_rate": 0.04136048125867654,
      "loss": 0.0031,
      "step": 63360
    },
    {
      "epoch": 29.32438685793614,
      "grad_norm": 0.0024737988132983446,
      "learning_rate": 0.04135122628412772,
      "loss": 0.0001,
      "step": 63370
    },
    {
      "epoch": 29.32901434521055,
      "grad_norm": 0.0018056936096400023,
      "learning_rate": 0.0413419713095789,
      "loss": 0.0001,
      "step": 63380
    },
    {
      "epoch": 29.33364183248496,
      "grad_norm": 0.005132492631673813,
      "learning_rate": 0.04133271633503008,
      "loss": 0.0001,
      "step": 63390
    },
    {
      "epoch": 29.33826931975937,
      "grad_norm": 0.003479131031781435,
      "learning_rate": 0.04132346136048126,
      "loss": 0.0001,
      "step": 63400
    },
    {
      "epoch": 29.34289680703378,
      "grad_norm": 0.0016696397215127945,
      "learning_rate": 0.041314206385932445,
      "loss": 0.0001,
      "step": 63410
    },
    {
      "epoch": 29.34752429430819,
      "grad_norm": 0.0050190649926662445,
      "learning_rate": 0.04130495141138362,
      "loss": 0.0001,
      "step": 63420
    },
    {
      "epoch": 29.352151781582602,
      "grad_norm": 0.0489145927131176,
      "learning_rate": 0.041295696436834804,
      "loss": 0.0001,
      "step": 63430
    },
    {
      "epoch": 29.35677926885701,
      "grad_norm": 0.0019466300727799535,
      "learning_rate": 0.04128644146228598,
      "loss": 0.0,
      "step": 63440
    },
    {
      "epoch": 29.36140675613142,
      "grad_norm": 0.0008413252653554082,
      "learning_rate": 0.041277186487737164,
      "loss": 0.0002,
      "step": 63450
    },
    {
      "epoch": 29.36603424340583,
      "grad_norm": 0.006439201068133116,
      "learning_rate": 0.04126793151318834,
      "loss": 0.0001,
      "step": 63460
    },
    {
      "epoch": 29.37066173068024,
      "grad_norm": 0.01154926884919405,
      "learning_rate": 0.041258676538639516,
      "loss": 0.0001,
      "step": 63470
    },
    {
      "epoch": 29.375289217954652,
      "grad_norm": 0.004147716797888279,
      "learning_rate": 0.0412494215640907,
      "loss": 0.0002,
      "step": 63480
    },
    {
      "epoch": 29.37991670522906,
      "grad_norm": 0.009205161593854427,
      "learning_rate": 0.04124016658954188,
      "loss": 0.0009,
      "step": 63490
    },
    {
      "epoch": 29.38454419250347,
      "grad_norm": 0.09049353748559952,
      "learning_rate": 0.041230911614993065,
      "loss": 0.0001,
      "step": 63500
    },
    {
      "epoch": 29.38917167977788,
      "grad_norm": 0.020848380401730537,
      "learning_rate": 0.04122165664044424,
      "loss": 0.0001,
      "step": 63510
    },
    {
      "epoch": 29.39379916705229,
      "grad_norm": 0.02301928773522377,
      "learning_rate": 0.041212401665895425,
      "loss": 0.0001,
      "step": 63520
    },
    {
      "epoch": 29.398426654326702,
      "grad_norm": 0.019170481711626053,
      "learning_rate": 0.0412031466913466,
      "loss": 0.0001,
      "step": 63530
    },
    {
      "epoch": 29.40305414160111,
      "grad_norm": 0.007206491194665432,
      "learning_rate": 0.041193891716797784,
      "loss": 0.0001,
      "step": 63540
    },
    {
      "epoch": 29.40768162887552,
      "grad_norm": 0.0058900401927530766,
      "learning_rate": 0.04118463674224896,
      "loss": 0.0002,
      "step": 63550
    },
    {
      "epoch": 29.41230911614993,
      "grad_norm": 0.0011413710890337825,
      "learning_rate": 0.04117538176770014,
      "loss": 0.0001,
      "step": 63560
    },
    {
      "epoch": 29.41693660342434,
      "grad_norm": 0.13079619407653809,
      "learning_rate": 0.04116612679315132,
      "loss": 0.0002,
      "step": 63570
    },
    {
      "epoch": 29.42156409069875,
      "grad_norm": 0.011748995631933212,
      "learning_rate": 0.0411568718186025,
      "loss": 0.0001,
      "step": 63580
    },
    {
      "epoch": 29.42619157797316,
      "grad_norm": 0.0026008309796452522,
      "learning_rate": 0.041147616844053686,
      "loss": 0.0001,
      "step": 63590
    },
    {
      "epoch": 29.430819065247572,
      "grad_norm": 0.0010667559690773487,
      "learning_rate": 0.04113836186950486,
      "loss": 0.0001,
      "step": 63600
    },
    {
      "epoch": 29.43544655252198,
      "grad_norm": 0.3620847463607788,
      "learning_rate": 0.041129106894956045,
      "loss": 0.0001,
      "step": 63610
    },
    {
      "epoch": 29.44007403979639,
      "grad_norm": 0.0037751332856714725,
      "learning_rate": 0.04111985192040722,
      "loss": 0.0,
      "step": 63620
    },
    {
      "epoch": 29.4447015270708,
      "grad_norm": 0.008523193188011646,
      "learning_rate": 0.0411105969458584,
      "loss": 0.0001,
      "step": 63630
    },
    {
      "epoch": 29.44932901434521,
      "grad_norm": 0.0010206950828433037,
      "learning_rate": 0.04110134197130958,
      "loss": 0.0001,
      "step": 63640
    },
    {
      "epoch": 29.453956501619622,
      "grad_norm": 0.007157136220484972,
      "learning_rate": 0.04109208699676076,
      "loss": 0.0,
      "step": 63650
    },
    {
      "epoch": 29.45858398889403,
      "grad_norm": 0.017139192670583725,
      "learning_rate": 0.04108283202221194,
      "loss": 0.0001,
      "step": 63660
    },
    {
      "epoch": 29.46321147616844,
      "grad_norm": 0.0017174645327031612,
      "learning_rate": 0.04107357704766312,
      "loss": 0.0,
      "step": 63670
    },
    {
      "epoch": 29.46783896344285,
      "grad_norm": 0.0008789991261437535,
      "learning_rate": 0.04106432207311431,
      "loss": 0.0002,
      "step": 63680
    },
    {
      "epoch": 29.47246645071726,
      "grad_norm": 0.11333201080560684,
      "learning_rate": 0.04105506709856548,
      "loss": 0.0001,
      "step": 63690
    },
    {
      "epoch": 29.477093937991672,
      "grad_norm": 0.015673642978072166,
      "learning_rate": 0.04104581212401666,
      "loss": 0.0001,
      "step": 63700
    },
    {
      "epoch": 29.48172142526608,
      "grad_norm": 0.02732207253575325,
      "learning_rate": 0.04103655714946784,
      "loss": 0.0001,
      "step": 63710
    },
    {
      "epoch": 29.48634891254049,
      "grad_norm": 0.007393243722617626,
      "learning_rate": 0.04102730217491902,
      "loss": 0.0,
      "step": 63720
    },
    {
      "epoch": 29.4909763998149,
      "grad_norm": 0.047221001237630844,
      "learning_rate": 0.0410180472003702,
      "loss": 0.0005,
      "step": 63730
    },
    {
      "epoch": 29.49560388708931,
      "grad_norm": 0.024307595565915108,
      "learning_rate": 0.04100879222582138,
      "loss": 0.0001,
      "step": 63740
    },
    {
      "epoch": 29.50023137436372,
      "grad_norm": 0.006575307808816433,
      "learning_rate": 0.04099953725127256,
      "loss": 0.0011,
      "step": 63750
    },
    {
      "epoch": 29.50485886163813,
      "grad_norm": 0.004871254786849022,
      "learning_rate": 0.040990282276723744,
      "loss": 0.0002,
      "step": 63760
    },
    {
      "epoch": 29.50948634891254,
      "grad_norm": 0.014485735446214676,
      "learning_rate": 0.04098102730217493,
      "loss": 0.0001,
      "step": 63770
    },
    {
      "epoch": 29.51411383618695,
      "grad_norm": 0.1886589527130127,
      "learning_rate": 0.0409717723276261,
      "loss": 0.0013,
      "step": 63780
    },
    {
      "epoch": 29.51874132346136,
      "grad_norm": 0.02701672911643982,
      "learning_rate": 0.04096251735307728,
      "loss": 0.0002,
      "step": 63790
    },
    {
      "epoch": 29.52336881073577,
      "grad_norm": 0.000821415102109313,
      "learning_rate": 0.04095326237852846,
      "loss": 0.0003,
      "step": 63800
    },
    {
      "epoch": 29.52799629801018,
      "grad_norm": 0.002700045006349683,
      "learning_rate": 0.04094400740397964,
      "loss": 0.0001,
      "step": 63810
    },
    {
      "epoch": 29.532623785284592,
      "grad_norm": 0.0038539390079677105,
      "learning_rate": 0.04093475242943082,
      "loss": 0.0001,
      "step": 63820
    },
    {
      "epoch": 29.537251272559,
      "grad_norm": 0.0176326185464859,
      "learning_rate": 0.040925497454882,
      "loss": 0.0001,
      "step": 63830
    },
    {
      "epoch": 29.54187875983341,
      "grad_norm": 0.010073726996779442,
      "learning_rate": 0.04091624248033318,
      "loss": 0.0001,
      "step": 63840
    },
    {
      "epoch": 29.54650624710782,
      "grad_norm": 0.03384561091661453,
      "learning_rate": 0.040906987505784365,
      "loss": 0.0002,
      "step": 63850
    },
    {
      "epoch": 29.55113373438223,
      "grad_norm": 0.00797985028475523,
      "learning_rate": 0.04089773253123554,
      "loss": 0.0003,
      "step": 63860
    },
    {
      "epoch": 29.555761221656642,
      "grad_norm": 0.11093877255916595,
      "learning_rate": 0.040888477556686724,
      "loss": 0.0001,
      "step": 63870
    },
    {
      "epoch": 29.56038870893105,
      "grad_norm": 0.0004012471763417125,
      "learning_rate": 0.0408792225821379,
      "loss": 0.0001,
      "step": 63880
    },
    {
      "epoch": 29.56501619620546,
      "grad_norm": 0.03295696899294853,
      "learning_rate": 0.04086996760758908,
      "loss": 0.0001,
      "step": 63890
    },
    {
      "epoch": 29.56964368347987,
      "grad_norm": 0.009491456672549248,
      "learning_rate": 0.04086071263304026,
      "loss": 0.0002,
      "step": 63900
    },
    {
      "epoch": 29.57427117075428,
      "grad_norm": 0.0006218464113771915,
      "learning_rate": 0.04085145765849144,
      "loss": 0.0,
      "step": 63910
    },
    {
      "epoch": 29.57889865802869,
      "grad_norm": 0.013272124342620373,
      "learning_rate": 0.04084220268394262,
      "loss": 0.0001,
      "step": 63920
    },
    {
      "epoch": 29.5835261453031,
      "grad_norm": 0.004497624933719635,
      "learning_rate": 0.0408329477093938,
      "loss": 0.0001,
      "step": 63930
    },
    {
      "epoch": 29.58815363257751,
      "grad_norm": 0.018173132091760635,
      "learning_rate": 0.040823692734844985,
      "loss": 0.0001,
      "step": 63940
    },
    {
      "epoch": 29.59278111985192,
      "grad_norm": 0.00994686409831047,
      "learning_rate": 0.04081443776029616,
      "loss": 0.0015,
      "step": 63950
    },
    {
      "epoch": 29.59740860712633,
      "grad_norm": 0.003595495130866766,
      "learning_rate": 0.040805182785747345,
      "loss": 0.0001,
      "step": 63960
    },
    {
      "epoch": 29.60203609440074,
      "grad_norm": 0.007059944793581963,
      "learning_rate": 0.04079592781119852,
      "loss": 0.0001,
      "step": 63970
    },
    {
      "epoch": 29.60666358167515,
      "grad_norm": 0.024772202596068382,
      "learning_rate": 0.040786672836649704,
      "loss": 0.0002,
      "step": 63980
    },
    {
      "epoch": 29.611291068949562,
      "grad_norm": 0.03149101883172989,
      "learning_rate": 0.04077741786210088,
      "loss": 0.0008,
      "step": 63990
    },
    {
      "epoch": 29.61591855622397,
      "grad_norm": 0.015583513304591179,
      "learning_rate": 0.04076816288755206,
      "loss": 0.0001,
      "step": 64000
    },
    {
      "epoch": 29.62054604349838,
      "grad_norm": 0.00817148108035326,
      "learning_rate": 0.04075890791300324,
      "loss": 0.0002,
      "step": 64010
    },
    {
      "epoch": 29.62517353077279,
      "grad_norm": 0.025108182802796364,
      "learning_rate": 0.04074965293845442,
      "loss": 0.0001,
      "step": 64020
    },
    {
      "epoch": 29.6298010180472,
      "grad_norm": 0.005873078014701605,
      "learning_rate": 0.040740397963905606,
      "loss": 0.0001,
      "step": 64030
    },
    {
      "epoch": 29.634428505321612,
      "grad_norm": 0.06387365609407425,
      "learning_rate": 0.04073114298935678,
      "loss": 0.0,
      "step": 64040
    },
    {
      "epoch": 29.63905599259602,
      "grad_norm": 0.008696313947439194,
      "learning_rate": 0.040721888014807965,
      "loss": 0.0001,
      "step": 64050
    },
    {
      "epoch": 29.64368347987043,
      "grad_norm": 0.011777173727750778,
      "learning_rate": 0.04071263304025914,
      "loss": 0.0001,
      "step": 64060
    },
    {
      "epoch": 29.64831096714484,
      "grad_norm": 0.010593554936349392,
      "learning_rate": 0.040703378065710324,
      "loss": 0.0001,
      "step": 64070
    },
    {
      "epoch": 29.65293845441925,
      "grad_norm": 0.005761829204857349,
      "learning_rate": 0.0406941230911615,
      "loss": 0.0001,
      "step": 64080
    },
    {
      "epoch": 29.65756594169366,
      "grad_norm": 0.030384203419089317,
      "learning_rate": 0.04068486811661268,
      "loss": 0.0001,
      "step": 64090
    },
    {
      "epoch": 29.66219342896807,
      "grad_norm": 0.0034806306939572096,
      "learning_rate": 0.04067561314206386,
      "loss": 0.0,
      "step": 64100
    },
    {
      "epoch": 29.66682091624248,
      "grad_norm": 0.029190881177783012,
      "learning_rate": 0.04066635816751504,
      "loss": 0.0002,
      "step": 64110
    },
    {
      "epoch": 29.67144840351689,
      "grad_norm": 0.002032355172559619,
      "learning_rate": 0.040657103192966226,
      "loss": 0.0006,
      "step": 64120
    },
    {
      "epoch": 29.6760758907913,
      "grad_norm": 0.016547393053770065,
      "learning_rate": 0.0406478482184174,
      "loss": 0.0001,
      "step": 64130
    },
    {
      "epoch": 29.68070337806571,
      "grad_norm": 0.011492695659399033,
      "learning_rate": 0.040638593243868586,
      "loss": 0.0001,
      "step": 64140
    },
    {
      "epoch": 29.68533086534012,
      "grad_norm": 0.002105528023093939,
      "learning_rate": 0.04062933826931976,
      "loss": 0.0,
      "step": 64150
    },
    {
      "epoch": 29.689958352614532,
      "grad_norm": 0.00641401344910264,
      "learning_rate": 0.04062008329477094,
      "loss": 0.0001,
      "step": 64160
    },
    {
      "epoch": 29.69458583988894,
      "grad_norm": 0.013968013226985931,
      "learning_rate": 0.04061082832022212,
      "loss": 0.0001,
      "step": 64170
    },
    {
      "epoch": 29.69921332716335,
      "grad_norm": 0.01063973642885685,
      "learning_rate": 0.0406015733456733,
      "loss": 0.0001,
      "step": 64180
    },
    {
      "epoch": 29.70384081443776,
      "grad_norm": 0.010675988160073757,
      "learning_rate": 0.04059231837112448,
      "loss": 0.0001,
      "step": 64190
    },
    {
      "epoch": 29.70846830171217,
      "grad_norm": 0.017381370067596436,
      "learning_rate": 0.040583063396575664,
      "loss": 0.0,
      "step": 64200
    },
    {
      "epoch": 29.713095788986582,
      "grad_norm": 0.005633586086332798,
      "learning_rate": 0.04057380842202685,
      "loss": 0.0001,
      "step": 64210
    },
    {
      "epoch": 29.71772327626099,
      "grad_norm": 0.0019356408156454563,
      "learning_rate": 0.04056455344747802,
      "loss": 0.0001,
      "step": 64220
    },
    {
      "epoch": 29.7223507635354,
      "grad_norm": 0.0013735268730670214,
      "learning_rate": 0.040555298472929206,
      "loss": 0.0001,
      "step": 64230
    },
    {
      "epoch": 29.72697825080981,
      "grad_norm": 0.01452222466468811,
      "learning_rate": 0.04054604349838038,
      "loss": 0.0001,
      "step": 64240
    },
    {
      "epoch": 29.73160573808422,
      "grad_norm": 0.0024814526550471783,
      "learning_rate": 0.04053678852383156,
      "loss": 0.0002,
      "step": 64250
    },
    {
      "epoch": 29.73623322535863,
      "grad_norm": 0.0016752318479120731,
      "learning_rate": 0.04052753354928274,
      "loss": 0.0001,
      "step": 64260
    },
    {
      "epoch": 29.74086071263304,
      "grad_norm": 0.0009020931902341545,
      "learning_rate": 0.04051827857473392,
      "loss": 0.0,
      "step": 64270
    },
    {
      "epoch": 29.74548819990745,
      "grad_norm": 0.012342250905930996,
      "learning_rate": 0.0405090236001851,
      "loss": 0.0,
      "step": 64280
    },
    {
      "epoch": 29.75011568718186,
      "grad_norm": 0.014356530271470547,
      "learning_rate": 0.040499768625636284,
      "loss": 0.0002,
      "step": 64290
    },
    {
      "epoch": 29.75474317445627,
      "grad_norm": 0.2915824055671692,
      "learning_rate": 0.04049051365108747,
      "loss": 0.0001,
      "step": 64300
    },
    {
      "epoch": 29.75937066173068,
      "grad_norm": 0.0013843104243278503,
      "learning_rate": 0.040481258676538644,
      "loss": 0.0009,
      "step": 64310
    },
    {
      "epoch": 29.76399814900509,
      "grad_norm": 0.0030013835057616234,
      "learning_rate": 0.04047200370198982,
      "loss": 0.0002,
      "step": 64320
    },
    {
      "epoch": 29.7686256362795,
      "grad_norm": 0.0017094514332711697,
      "learning_rate": 0.040462748727441,
      "loss": 0.0001,
      "step": 64330
    },
    {
      "epoch": 29.77325312355391,
      "grad_norm": 0.0025161488447338343,
      "learning_rate": 0.04045349375289218,
      "loss": 0.0004,
      "step": 64340
    },
    {
      "epoch": 29.77788061082832,
      "grad_norm": 0.004355540033429861,
      "learning_rate": 0.04044423877834336,
      "loss": 0.0001,
      "step": 64350
    },
    {
      "epoch": 29.78250809810273,
      "grad_norm": 0.0034318934194743633,
      "learning_rate": 0.04043498380379454,
      "loss": 0.0001,
      "step": 64360
    },
    {
      "epoch": 29.78713558537714,
      "grad_norm": 0.0023043062537908554,
      "learning_rate": 0.04042572882924572,
      "loss": 0.0,
      "step": 64370
    },
    {
      "epoch": 29.791763072651552,
      "grad_norm": 0.1159193366765976,
      "learning_rate": 0.040416473854696905,
      "loss": 0.0001,
      "step": 64380
    },
    {
      "epoch": 29.79639055992596,
      "grad_norm": 0.06184680014848709,
      "learning_rate": 0.04040721888014808,
      "loss": 0.0001,
      "step": 64390
    },
    {
      "epoch": 29.80101804720037,
      "grad_norm": 0.003329667029902339,
      "learning_rate": 0.040397963905599264,
      "loss": 0.0001,
      "step": 64400
    },
    {
      "epoch": 29.80564553447478,
      "grad_norm": 0.002439626259729266,
      "learning_rate": 0.04038870893105044,
      "loss": 0.0001,
      "step": 64410
    },
    {
      "epoch": 29.81027302174919,
      "grad_norm": 0.022430989891290665,
      "learning_rate": 0.040379453956501624,
      "loss": 0.0002,
      "step": 64420
    },
    {
      "epoch": 29.8149005090236,
      "grad_norm": 0.007338812109082937,
      "learning_rate": 0.0403701989819528,
      "loss": 0.0002,
      "step": 64430
    },
    {
      "epoch": 29.81952799629801,
      "grad_norm": 0.003082580165937543,
      "learning_rate": 0.04036094400740398,
      "loss": 0.0001,
      "step": 64440
    },
    {
      "epoch": 29.82415548357242,
      "grad_norm": 0.03967311233282089,
      "learning_rate": 0.04035168903285516,
      "loss": 0.0002,
      "step": 64450
    },
    {
      "epoch": 29.82878297084683,
      "grad_norm": 0.4133765399456024,
      "learning_rate": 0.04034243405830634,
      "loss": 0.0002,
      "step": 64460
    },
    {
      "epoch": 29.83341045812124,
      "grad_norm": 0.0015594048891216516,
      "learning_rate": 0.040333179083757525,
      "loss": 0.0002,
      "step": 64470
    },
    {
      "epoch": 29.83803794539565,
      "grad_norm": 0.0018656406318768859,
      "learning_rate": 0.0403239241092087,
      "loss": 0.0,
      "step": 64480
    },
    {
      "epoch": 29.84266543267006,
      "grad_norm": 0.0045406571589410305,
      "learning_rate": 0.040314669134659885,
      "loss": 0.0001,
      "step": 64490
    },
    {
      "epoch": 29.84729291994447,
      "grad_norm": 0.023628080263733864,
      "learning_rate": 0.04030541416011106,
      "loss": 0.0,
      "step": 64500
    },
    {
      "epoch": 29.85192040721888,
      "grad_norm": 0.0018985774368047714,
      "learning_rate": 0.040296159185562244,
      "loss": 0.0001,
      "step": 64510
    },
    {
      "epoch": 29.85654789449329,
      "grad_norm": 1.6071683168411255,
      "learning_rate": 0.04028690421101342,
      "loss": 0.0003,
      "step": 64520
    },
    {
      "epoch": 29.8611753817677,
      "grad_norm": 0.015860887244343758,
      "learning_rate": 0.040277649236464604,
      "loss": 0.0001,
      "step": 64530
    },
    {
      "epoch": 29.86580286904211,
      "grad_norm": 0.04269033670425415,
      "learning_rate": 0.04026839426191578,
      "loss": 0.0003,
      "step": 64540
    },
    {
      "epoch": 29.87043035631652,
      "grad_norm": 0.010307162068784237,
      "learning_rate": 0.04025913928736696,
      "loss": 0.0018,
      "step": 64550
    },
    {
      "epoch": 29.87505784359093,
      "grad_norm": 0.17993509769439697,
      "learning_rate": 0.040249884312818146,
      "loss": 0.0002,
      "step": 64560
    },
    {
      "epoch": 29.87968533086534,
      "grad_norm": 0.0805496945977211,
      "learning_rate": 0.04024062933826932,
      "loss": 0.0001,
      "step": 64570
    },
    {
      "epoch": 29.88431281813975,
      "grad_norm": 0.0013701946008950472,
      "learning_rate": 0.040231374363720505,
      "loss": 0.0001,
      "step": 64580
    },
    {
      "epoch": 29.88894030541416,
      "grad_norm": 0.0015618152683600783,
      "learning_rate": 0.04022211938917168,
      "loss": 0.0,
      "step": 64590
    },
    {
      "epoch": 29.89356779268857,
      "grad_norm": 0.0023740294855087996,
      "learning_rate": 0.040212864414622865,
      "loss": 0.0001,
      "step": 64600
    },
    {
      "epoch": 29.89819527996298,
      "grad_norm": 0.01804128661751747,
      "learning_rate": 0.04020360944007404,
      "loss": 0.0001,
      "step": 64610
    },
    {
      "epoch": 29.90282276723739,
      "grad_norm": 0.010343358851969242,
      "learning_rate": 0.04019435446552522,
      "loss": 0.0,
      "step": 64620
    },
    {
      "epoch": 29.9074502545118,
      "grad_norm": 0.16528473794460297,
      "learning_rate": 0.0401850994909764,
      "loss": 0.0001,
      "step": 64630
    },
    {
      "epoch": 29.91207774178621,
      "grad_norm": 0.026362501084804535,
      "learning_rate": 0.040175844516427583,
      "loss": 0.0001,
      "step": 64640
    },
    {
      "epoch": 29.91670522906062,
      "grad_norm": 0.09840033948421478,
      "learning_rate": 0.04016658954187877,
      "loss": 0.0001,
      "step": 64650
    },
    {
      "epoch": 29.92133271633503,
      "grad_norm": 0.011575736105442047,
      "learning_rate": 0.04015733456732994,
      "loss": 0.0001,
      "step": 64660
    },
    {
      "epoch": 29.92596020360944,
      "grad_norm": 0.007321634795516729,
      "learning_rate": 0.040148079592781126,
      "loss": 0.0001,
      "step": 64670
    },
    {
      "epoch": 29.93058769088385,
      "grad_norm": 0.013217685744166374,
      "learning_rate": 0.0401388246182323,
      "loss": 0.0004,
      "step": 64680
    },
    {
      "epoch": 29.93521517815826,
      "grad_norm": 0.024232730269432068,
      "learning_rate": 0.040129569643683485,
      "loss": 0.0001,
      "step": 64690
    },
    {
      "epoch": 29.93984266543267,
      "grad_norm": 0.008262716233730316,
      "learning_rate": 0.04012031466913466,
      "loss": 0.0003,
      "step": 64700
    },
    {
      "epoch": 29.94447015270708,
      "grad_norm": 0.010866492986679077,
      "learning_rate": 0.04011105969458584,
      "loss": 0.0,
      "step": 64710
    },
    {
      "epoch": 29.94909763998149,
      "grad_norm": 0.0027694939635694027,
      "learning_rate": 0.04010180472003702,
      "loss": 0.0001,
      "step": 64720
    },
    {
      "epoch": 29.9537251272559,
      "grad_norm": 0.5656855702400208,
      "learning_rate": 0.040092549745488204,
      "loss": 0.0001,
      "step": 64730
    },
    {
      "epoch": 29.95835261453031,
      "grad_norm": 0.017028551548719406,
      "learning_rate": 0.04008329477093939,
      "loss": 0.0001,
      "step": 64740
    },
    {
      "epoch": 29.96298010180472,
      "grad_norm": 0.00815846212208271,
      "learning_rate": 0.04007403979639056,
      "loss": 0.0,
      "step": 64750
    },
    {
      "epoch": 29.96760758907913,
      "grad_norm": 0.01690344326198101,
      "learning_rate": 0.04006478482184175,
      "loss": 0.0001,
      "step": 64760
    },
    {
      "epoch": 29.97223507635354,
      "grad_norm": 0.01624077558517456,
      "learning_rate": 0.04005552984729292,
      "loss": 0.0001,
      "step": 64770
    },
    {
      "epoch": 29.97686256362795,
      "grad_norm": 0.005317364819347858,
      "learning_rate": 0.0400462748727441,
      "loss": 0.0001,
      "step": 64780
    },
    {
      "epoch": 29.98149005090236,
      "grad_norm": 0.007487187627702951,
      "learning_rate": 0.04003701989819528,
      "loss": 0.0,
      "step": 64790
    },
    {
      "epoch": 29.98611753817677,
      "grad_norm": 0.015714913606643677,
      "learning_rate": 0.04002776492364646,
      "loss": 0.0004,
      "step": 64800
    },
    {
      "epoch": 29.99074502545118,
      "grad_norm": 0.014894207008183002,
      "learning_rate": 0.04001850994909764,
      "loss": 0.0001,
      "step": 64810
    },
    {
      "epoch": 29.99537251272559,
      "grad_norm": 0.011476235464215279,
      "learning_rate": 0.040009254974548825,
      "loss": 0.0,
      "step": 64820
    },
    {
      "epoch": 30.0,
      "grad_norm": 111.95327758789062,
      "learning_rate": 0.04000000000000001,
      "loss": 0.2031,
      "step": 64830
    },
    {
      "epoch": 30.0,
      "eval_accuracy_branch1": 0.9913726698505623,
      "eval_accuracy_branch2": 0.5005392081343398,
      "eval_f1_branch1": 0.992125557787952,
      "eval_f1_branch2": 0.5004452920772289,
      "eval_loss": 0.016375327482819557,
      "eval_precision_branch1": 0.9923436232495149,
      "eval_precision_branch2": 0.5005396139230249,
      "eval_recall_branch1": 0.9919857772079734,
      "eval_recall_branch2": 0.5005392081343398,
      "eval_runtime": 28.8109,
      "eval_samples_per_second": 450.593,
      "eval_steps_per_second": 56.333,
      "step": 64830
    },
    {
      "epoch": 30.00462748727441,
      "grad_norm": 0.007129178382456303,
      "learning_rate": 0.039990745025451184,
      "loss": 0.0,
      "step": 64840
    },
    {
      "epoch": 30.00925497454882,
      "grad_norm": 0.006169248837977648,
      "learning_rate": 0.03998149005090236,
      "loss": 0.0001,
      "step": 64850
    },
    {
      "epoch": 30.01388246182323,
      "grad_norm": 0.01658805087208748,
      "learning_rate": 0.03997223507635354,
      "loss": 0.0002,
      "step": 64860
    },
    {
      "epoch": 30.01850994909764,
      "grad_norm": 0.014931393787264824,
      "learning_rate": 0.03996298010180472,
      "loss": 0.0001,
      "step": 64870
    },
    {
      "epoch": 30.02313743637205,
      "grad_norm": 0.004842579364776611,
      "learning_rate": 0.0399537251272559,
      "loss": 0.0001,
      "step": 64880
    },
    {
      "epoch": 30.02776492364646,
      "grad_norm": 0.046808306127786636,
      "learning_rate": 0.03994447015270708,
      "loss": 0.0,
      "step": 64890
    },
    {
      "epoch": 30.03239241092087,
      "grad_norm": 0.0028372318483889103,
      "learning_rate": 0.03993521517815826,
      "loss": 0.0009,
      "step": 64900
    },
    {
      "epoch": 30.03701989819528,
      "grad_norm": 0.0018685349496081471,
      "learning_rate": 0.039925960203609445,
      "loss": 0.0004,
      "step": 64910
    },
    {
      "epoch": 30.04164738546969,
      "grad_norm": 0.002387541113421321,
      "learning_rate": 0.03991670522906063,
      "loss": 0.0001,
      "step": 64920
    },
    {
      "epoch": 30.0462748727441,
      "grad_norm": 0.08062063157558441,
      "learning_rate": 0.039907450254511805,
      "loss": 0.0002,
      "step": 64930
    },
    {
      "epoch": 30.05090236001851,
      "grad_norm": 0.07105225324630737,
      "learning_rate": 0.03989819527996298,
      "loss": 0.0001,
      "step": 64940
    },
    {
      "epoch": 30.05552984729292,
      "grad_norm": 0.002775913570076227,
      "learning_rate": 0.039888940305414164,
      "loss": 0.0,
      "step": 64950
    },
    {
      "epoch": 30.06015733456733,
      "grad_norm": 0.008063910529017448,
      "learning_rate": 0.03987968533086534,
      "loss": 0.0001,
      "step": 64960
    },
    {
      "epoch": 30.06478482184174,
      "grad_norm": 0.00963475089520216,
      "learning_rate": 0.03987043035631652,
      "loss": 0.0001,
      "step": 64970
    },
    {
      "epoch": 30.06941230911615,
      "grad_norm": 0.001689340453594923,
      "learning_rate": 0.0398611753817677,
      "loss": 0.0003,
      "step": 64980
    },
    {
      "epoch": 30.07403979639056,
      "grad_norm": 0.5195124745368958,
      "learning_rate": 0.03985192040721888,
      "loss": 0.0012,
      "step": 64990
    },
    {
      "epoch": 30.07866728366497,
      "grad_norm": 0.27259939908981323,
      "learning_rate": 0.039842665432670066,
      "loss": 0.0002,
      "step": 65000
    },
    {
      "epoch": 30.08329477093938,
      "grad_norm": 0.0059335664846003056,
      "learning_rate": 0.03983341045812124,
      "loss": 0.0002,
      "step": 65010
    },
    {
      "epoch": 30.08792225821379,
      "grad_norm": 0.043620765209198,
      "learning_rate": 0.039824155483572425,
      "loss": 0.0004,
      "step": 65020
    },
    {
      "epoch": 30.0925497454882,
      "grad_norm": 0.003823396982625127,
      "learning_rate": 0.0398149005090236,
      "loss": 0.0001,
      "step": 65030
    },
    {
      "epoch": 30.09717723276261,
      "grad_norm": 0.0121572595089674,
      "learning_rate": 0.039805645534474784,
      "loss": 0.0001,
      "step": 65040
    },
    {
      "epoch": 30.10180472003702,
      "grad_norm": 0.0006337545346468687,
      "learning_rate": 0.03979639055992596,
      "loss": 0.0001,
      "step": 65050
    },
    {
      "epoch": 30.10643220731143,
      "grad_norm": 0.0049817911349236965,
      "learning_rate": 0.039787135585377144,
      "loss": 0.0001,
      "step": 65060
    },
    {
      "epoch": 30.11105969458584,
      "grad_norm": 0.001835449249483645,
      "learning_rate": 0.03977788061082832,
      "loss": 0.0002,
      "step": 65070
    },
    {
      "epoch": 30.11568718186025,
      "grad_norm": 0.1317848563194275,
      "learning_rate": 0.0397686256362795,
      "loss": 0.0002,
      "step": 65080
    },
    {
      "epoch": 30.12031466913466,
      "grad_norm": 0.0008305590599775314,
      "learning_rate": 0.039759370661730686,
      "loss": 0.0,
      "step": 65090
    },
    {
      "epoch": 30.12494215640907,
      "grad_norm": 0.006342841312289238,
      "learning_rate": 0.03975011568718186,
      "loss": 0.0012,
      "step": 65100
    },
    {
      "epoch": 30.129569643683478,
      "grad_norm": 0.0050599523819983006,
      "learning_rate": 0.039740860712633046,
      "loss": 0.0004,
      "step": 65110
    },
    {
      "epoch": 30.13419713095789,
      "grad_norm": 0.004979746416211128,
      "learning_rate": 0.03973160573808422,
      "loss": 0.0001,
      "step": 65120
    },
    {
      "epoch": 30.1388246182323,
      "grad_norm": 0.0028690285980701447,
      "learning_rate": 0.039722350763535405,
      "loss": 0.0001,
      "step": 65130
    },
    {
      "epoch": 30.14345210550671,
      "grad_norm": 0.002080982318148017,
      "learning_rate": 0.03971309578898658,
      "loss": 0.0004,
      "step": 65140
    },
    {
      "epoch": 30.14807959278112,
      "grad_norm": 0.008059494197368622,
      "learning_rate": 0.039703840814437764,
      "loss": 0.0002,
      "step": 65150
    },
    {
      "epoch": 30.15270708005553,
      "grad_norm": 0.3512118458747864,
      "learning_rate": 0.03969458583988894,
      "loss": 0.0001,
      "step": 65160
    },
    {
      "epoch": 30.15733456732994,
      "grad_norm": 0.0026093199849128723,
      "learning_rate": 0.039685330865340124,
      "loss": 0.0001,
      "step": 65170
    },
    {
      "epoch": 30.16196205460435,
      "grad_norm": 0.01801689714193344,
      "learning_rate": 0.03967607589079131,
      "loss": 0.0001,
      "step": 65180
    },
    {
      "epoch": 30.16658954187876,
      "grad_norm": 0.014420215040445328,
      "learning_rate": 0.03966682091624248,
      "loss": 0.0001,
      "step": 65190
    },
    {
      "epoch": 30.17121702915317,
      "grad_norm": 0.006546463817358017,
      "learning_rate": 0.039657565941693666,
      "loss": 0.0002,
      "step": 65200
    },
    {
      "epoch": 30.17584451642758,
      "grad_norm": 0.0008802704978734255,
      "learning_rate": 0.03964831096714484,
      "loss": 0.0001,
      "step": 65210
    },
    {
      "epoch": 30.18047200370199,
      "grad_norm": 0.0013841448817402124,
      "learning_rate": 0.039639055992596026,
      "loss": 0.0001,
      "step": 65220
    },
    {
      "epoch": 30.1850994909764,
      "grad_norm": 0.005397662054747343,
      "learning_rate": 0.0396298010180472,
      "loss": 0.0001,
      "step": 65230
    },
    {
      "epoch": 30.18972697825081,
      "grad_norm": 0.004221297334879637,
      "learning_rate": 0.03962054604349838,
      "loss": 0.0001,
      "step": 65240
    },
    {
      "epoch": 30.19435446552522,
      "grad_norm": 0.01038713101297617,
      "learning_rate": 0.03961129106894956,
      "loss": 0.0001,
      "step": 65250
    },
    {
      "epoch": 30.19898195279963,
      "grad_norm": 0.02754932828247547,
      "learning_rate": 0.039602036094400744,
      "loss": 0.0001,
      "step": 65260
    },
    {
      "epoch": 30.20360944007404,
      "grad_norm": 0.0540795736014843,
      "learning_rate": 0.03959278111985193,
      "loss": 0.0004,
      "step": 65270
    },
    {
      "epoch": 30.208236927348448,
      "grad_norm": 0.0050889053381979465,
      "learning_rate": 0.039583526145303104,
      "loss": 0.0002,
      "step": 65280
    },
    {
      "epoch": 30.21286441462286,
      "grad_norm": 0.006794888526201248,
      "learning_rate": 0.03957427117075429,
      "loss": 0.0001,
      "step": 65290
    },
    {
      "epoch": 30.21749190189727,
      "grad_norm": 0.007509684190154076,
      "learning_rate": 0.03956501619620546,
      "loss": 0.0003,
      "step": 65300
    },
    {
      "epoch": 30.22211938917168,
      "grad_norm": 0.007223229389637709,
      "learning_rate": 0.03955576122165664,
      "loss": 0.0001,
      "step": 65310
    },
    {
      "epoch": 30.22674687644609,
      "grad_norm": 0.0021077035926282406,
      "learning_rate": 0.03954650624710782,
      "loss": 0.0002,
      "step": 65320
    },
    {
      "epoch": 30.2313743637205,
      "grad_norm": 0.005461352877318859,
      "learning_rate": 0.039537251272559,
      "loss": 0.0001,
      "step": 65330
    },
    {
      "epoch": 30.23600185099491,
      "grad_norm": 0.013376637361943722,
      "learning_rate": 0.03952799629801018,
      "loss": 0.0001,
      "step": 65340
    },
    {
      "epoch": 30.24062933826932,
      "grad_norm": 0.06541650742292404,
      "learning_rate": 0.039518741323461365,
      "loss": 0.0001,
      "step": 65350
    },
    {
      "epoch": 30.24525682554373,
      "grad_norm": 0.06486112624406815,
      "learning_rate": 0.03950948634891255,
      "loss": 0.0001,
      "step": 65360
    },
    {
      "epoch": 30.24988431281814,
      "grad_norm": 0.010529544204473495,
      "learning_rate": 0.039500231374363724,
      "loss": 0.0002,
      "step": 65370
    },
    {
      "epoch": 30.25451180009255,
      "grad_norm": 0.012010687962174416,
      "learning_rate": 0.03949097639981491,
      "loss": 0.0001,
      "step": 65380
    },
    {
      "epoch": 30.25913928736696,
      "grad_norm": 0.020491909235715866,
      "learning_rate": 0.039481721425266084,
      "loss": 0.0001,
      "step": 65390
    },
    {
      "epoch": 30.26376677464137,
      "grad_norm": 0.0014500481775030494,
      "learning_rate": 0.03947246645071726,
      "loss": 0.0,
      "step": 65400
    },
    {
      "epoch": 30.26839426191578,
      "grad_norm": 0.0011056889779865742,
      "learning_rate": 0.03946321147616844,
      "loss": 0.0002,
      "step": 65410
    },
    {
      "epoch": 30.27302174919019,
      "grad_norm": 0.005057998467236757,
      "learning_rate": 0.03945395650161962,
      "loss": 0.001,
      "step": 65420
    },
    {
      "epoch": 30.2776492364646,
      "grad_norm": 0.00581516046077013,
      "learning_rate": 0.0394447015270708,
      "loss": 0.0001,
      "step": 65430
    },
    {
      "epoch": 30.28227672373901,
      "grad_norm": 0.00032116484362632036,
      "learning_rate": 0.039435446552521985,
      "loss": 0.0001,
      "step": 65440
    },
    {
      "epoch": 30.286904211013418,
      "grad_norm": 0.0012654020683839917,
      "learning_rate": 0.03942619157797317,
      "loss": 0.0,
      "step": 65450
    },
    {
      "epoch": 30.29153169828783,
      "grad_norm": 0.015885571017861366,
      "learning_rate": 0.039416936603424345,
      "loss": 0.0001,
      "step": 65460
    },
    {
      "epoch": 30.29615918556224,
      "grad_norm": 0.009483152069151402,
      "learning_rate": 0.03940768162887552,
      "loss": 0.0001,
      "step": 65470
    },
    {
      "epoch": 30.30078667283665,
      "grad_norm": 0.0073531256057322025,
      "learning_rate": 0.039398426654326704,
      "loss": 0.0001,
      "step": 65480
    },
    {
      "epoch": 30.30541416011106,
      "grad_norm": 0.002588713075965643,
      "learning_rate": 0.03938917167977788,
      "loss": 0.0001,
      "step": 65490
    },
    {
      "epoch": 30.310041647385468,
      "grad_norm": 0.8338563442230225,
      "learning_rate": 0.039379916705229064,
      "loss": 0.0003,
      "step": 65500
    },
    {
      "epoch": 30.31466913465988,
      "grad_norm": 0.028399495407938957,
      "learning_rate": 0.03937066173068024,
      "loss": 0.0001,
      "step": 65510
    },
    {
      "epoch": 30.31929662193429,
      "grad_norm": 0.014749187044799328,
      "learning_rate": 0.03936140675613142,
      "loss": 0.0001,
      "step": 65520
    },
    {
      "epoch": 30.3239241092087,
      "grad_norm": 0.006474366877228022,
      "learning_rate": 0.039352151781582606,
      "loss": 0.0001,
      "step": 65530
    },
    {
      "epoch": 30.32855159648311,
      "grad_norm": 0.041706349700689316,
      "learning_rate": 0.03934289680703378,
      "loss": 0.0002,
      "step": 65540
    },
    {
      "epoch": 30.33317908375752,
      "grad_norm": 0.003444518893957138,
      "learning_rate": 0.039333641832484965,
      "loss": 0.0001,
      "step": 65550
    },
    {
      "epoch": 30.33780657103193,
      "grad_norm": 0.001741966581903398,
      "learning_rate": 0.03932438685793614,
      "loss": 0.0,
      "step": 65560
    },
    {
      "epoch": 30.34243405830634,
      "grad_norm": 0.00920784566551447,
      "learning_rate": 0.039315131883387325,
      "loss": 0.0001,
      "step": 65570
    },
    {
      "epoch": 30.34706154558075,
      "grad_norm": 0.010937760584056377,
      "learning_rate": 0.0393058769088385,
      "loss": 0.0,
      "step": 65580
    },
    {
      "epoch": 30.35168903285516,
      "grad_norm": 0.2595672309398651,
      "learning_rate": 0.039296621934289684,
      "loss": 0.0001,
      "step": 65590
    },
    {
      "epoch": 30.35631652012957,
      "grad_norm": 0.0018872969085350633,
      "learning_rate": 0.03928736695974086,
      "loss": 0.0,
      "step": 65600
    },
    {
      "epoch": 30.36094400740398,
      "grad_norm": 0.00035279191797599196,
      "learning_rate": 0.039278111985192043,
      "loss": 0.0001,
      "step": 65610
    },
    {
      "epoch": 30.365571494678388,
      "grad_norm": 0.02426433376967907,
      "learning_rate": 0.03926885701064323,
      "loss": 0.0001,
      "step": 65620
    },
    {
      "epoch": 30.3701989819528,
      "grad_norm": 0.00180398253723979,
      "learning_rate": 0.0392596020360944,
      "loss": 0.0001,
      "step": 65630
    },
    {
      "epoch": 30.37482646922721,
      "grad_norm": 0.0037950617261230946,
      "learning_rate": 0.039250347061545586,
      "loss": 0.0001,
      "step": 65640
    },
    {
      "epoch": 30.37945395650162,
      "grad_norm": 0.0017377814510837197,
      "learning_rate": 0.03924109208699676,
      "loss": 0.0001,
      "step": 65650
    },
    {
      "epoch": 30.38408144377603,
      "grad_norm": 0.008268527686595917,
      "learning_rate": 0.039231837112447945,
      "loss": 0.0002,
      "step": 65660
    },
    {
      "epoch": 30.388708931050438,
      "grad_norm": 0.01870853640139103,
      "learning_rate": 0.03922258213789912,
      "loss": 0.0001,
      "step": 65670
    },
    {
      "epoch": 30.39333641832485,
      "grad_norm": 0.136244535446167,
      "learning_rate": 0.039213327163350305,
      "loss": 0.0001,
      "step": 65680
    },
    {
      "epoch": 30.39796390559926,
      "grad_norm": 0.0017711055697873235,
      "learning_rate": 0.03920407218880148,
      "loss": 0.0001,
      "step": 65690
    },
    {
      "epoch": 30.40259139287367,
      "grad_norm": 0.0022629171144217253,
      "learning_rate": 0.039194817214252664,
      "loss": 0.0001,
      "step": 65700
    },
    {
      "epoch": 30.40721888014808,
      "grad_norm": 0.002081614453345537,
      "learning_rate": 0.03918556223970385,
      "loss": 0.0001,
      "step": 65710
    },
    {
      "epoch": 30.41184636742249,
      "grad_norm": 0.004364584572613239,
      "learning_rate": 0.03917630726515502,
      "loss": 0.0001,
      "step": 65720
    },
    {
      "epoch": 30.4164738546969,
      "grad_norm": 0.07824312895536423,
      "learning_rate": 0.03916705229060621,
      "loss": 0.0001,
      "step": 65730
    },
    {
      "epoch": 30.42110134197131,
      "grad_norm": 0.15724854171276093,
      "learning_rate": 0.03915779731605738,
      "loss": 0.0002,
      "step": 65740
    },
    {
      "epoch": 30.42572882924572,
      "grad_norm": 0.009481638669967651,
      "learning_rate": 0.039148542341508566,
      "loss": 0.0001,
      "step": 65750
    },
    {
      "epoch": 30.43035631652013,
      "grad_norm": 0.4125168025493622,
      "learning_rate": 0.03913928736695974,
      "loss": 0.0002,
      "step": 65760
    },
    {
      "epoch": 30.43498380379454,
      "grad_norm": 0.010029756464064121,
      "learning_rate": 0.03913003239241092,
      "loss": 0.0001,
      "step": 65770
    },
    {
      "epoch": 30.43961129106895,
      "grad_norm": 0.04074016585946083,
      "learning_rate": 0.0391207774178621,
      "loss": 0.0001,
      "step": 65780
    },
    {
      "epoch": 30.444238778343358,
      "grad_norm": 0.0044020190834999084,
      "learning_rate": 0.039111522443313285,
      "loss": 0.0001,
      "step": 65790
    },
    {
      "epoch": 30.44886626561777,
      "grad_norm": 0.0013556608464568853,
      "learning_rate": 0.03910226746876447,
      "loss": 0.0001,
      "step": 65800
    },
    {
      "epoch": 30.45349375289218,
      "grad_norm": 0.030499279499053955,
      "learning_rate": 0.039093012494215644,
      "loss": 0.0001,
      "step": 65810
    },
    {
      "epoch": 30.45812124016659,
      "grad_norm": 0.006214603781700134,
      "learning_rate": 0.03908375751966683,
      "loss": 0.0,
      "step": 65820
    },
    {
      "epoch": 30.462748727441,
      "grad_norm": 0.18814681470394135,
      "learning_rate": 0.039074502545118,
      "loss": 0.0002,
      "step": 65830
    },
    {
      "epoch": 30.467376214715408,
      "grad_norm": 0.02419351600110531,
      "learning_rate": 0.03906524757056918,
      "loss": 0.0001,
      "step": 65840
    },
    {
      "epoch": 30.47200370198982,
      "grad_norm": 0.003033008426427841,
      "learning_rate": 0.03905599259602036,
      "loss": 0.0001,
      "step": 65850
    },
    {
      "epoch": 30.47663118926423,
      "grad_norm": 0.013236084952950478,
      "learning_rate": 0.03904673762147154,
      "loss": 0.0001,
      "step": 65860
    },
    {
      "epoch": 30.48125867653864,
      "grad_norm": 0.0008535523083992302,
      "learning_rate": 0.03903748264692272,
      "loss": 0.0001,
      "step": 65870
    },
    {
      "epoch": 30.48588616381305,
      "grad_norm": 0.0005067003075964749,
      "learning_rate": 0.039028227672373905,
      "loss": 0.0001,
      "step": 65880
    },
    {
      "epoch": 30.49051365108746,
      "grad_norm": 0.005617899354547262,
      "learning_rate": 0.03901897269782509,
      "loss": 0.0001,
      "step": 65890
    },
    {
      "epoch": 30.49514113836187,
      "grad_norm": 0.07958664745092392,
      "learning_rate": 0.039009717723276265,
      "loss": 0.0001,
      "step": 65900
    },
    {
      "epoch": 30.49976862563628,
      "grad_norm": 0.007282838691025972,
      "learning_rate": 0.03900046274872745,
      "loss": 0.0002,
      "step": 65910
    },
    {
      "epoch": 30.50439611291069,
      "grad_norm": 0.003073448548093438,
      "learning_rate": 0.038991207774178624,
      "loss": 0.0003,
      "step": 65920
    },
    {
      "epoch": 30.5090236001851,
      "grad_norm": 0.002518521388992667,
      "learning_rate": 0.0389819527996298,
      "loss": 0.0,
      "step": 65930
    },
    {
      "epoch": 30.51365108745951,
      "grad_norm": 0.008468241430819035,
      "learning_rate": 0.03897269782508098,
      "loss": 0.0001,
      "step": 65940
    },
    {
      "epoch": 30.51827857473392,
      "grad_norm": 0.009373622946441174,
      "learning_rate": 0.03896344285053216,
      "loss": 0.0001,
      "step": 65950
    },
    {
      "epoch": 30.522906062008328,
      "grad_norm": 0.0017772105056792498,
      "learning_rate": 0.03895418787598334,
      "loss": 0.0,
      "step": 65960
    },
    {
      "epoch": 30.52753354928274,
      "grad_norm": 0.031665049493312836,
      "learning_rate": 0.038944932901434526,
      "loss": 0.0004,
      "step": 65970
    },
    {
      "epoch": 30.53216103655715,
      "grad_norm": 0.006061997264623642,
      "learning_rate": 0.03893567792688571,
      "loss": 0.0002,
      "step": 65980
    },
    {
      "epoch": 30.53678852383156,
      "grad_norm": 0.014881420880556107,
      "learning_rate": 0.038926422952336885,
      "loss": 0.0001,
      "step": 65990
    },
    {
      "epoch": 30.54141601110597,
      "grad_norm": 0.030064811930060387,
      "learning_rate": 0.03891716797778806,
      "loss": 0.0057,
      "step": 66000
    },
    {
      "epoch": 30.546043498380378,
      "grad_norm": 0.007403446361422539,
      "learning_rate": 0.038907913003239245,
      "loss": 0.0001,
      "step": 66010
    },
    {
      "epoch": 30.55067098565479,
      "grad_norm": 0.0006138414028100669,
      "learning_rate": 0.03889865802869042,
      "loss": 0.0003,
      "step": 66020
    },
    {
      "epoch": 30.5552984729292,
      "grad_norm": 0.02421032078564167,
      "learning_rate": 0.038889403054141604,
      "loss": 0.0001,
      "step": 66030
    },
    {
      "epoch": 30.55992596020361,
      "grad_norm": 0.02159692719578743,
      "learning_rate": 0.03888014807959278,
      "loss": 0.0001,
      "step": 66040
    },
    {
      "epoch": 30.56455344747802,
      "grad_norm": 0.014484194107353687,
      "learning_rate": 0.03887089310504396,
      "loss": 0.0003,
      "step": 66050
    },
    {
      "epoch": 30.56918093475243,
      "grad_norm": 0.0006971284165047109,
      "learning_rate": 0.038861638130495146,
      "loss": 0.0001,
      "step": 66060
    },
    {
      "epoch": 30.57380842202684,
      "grad_norm": 0.008002697490155697,
      "learning_rate": 0.03885238315594632,
      "loss": 0.0002,
      "step": 66070
    },
    {
      "epoch": 30.57843590930125,
      "grad_norm": 0.01240147091448307,
      "learning_rate": 0.038843128181397506,
      "loss": 0.0001,
      "step": 66080
    },
    {
      "epoch": 30.58306339657566,
      "grad_norm": 0.07254789769649506,
      "learning_rate": 0.03883387320684868,
      "loss": 0.0001,
      "step": 66090
    },
    {
      "epoch": 30.58769088385007,
      "grad_norm": 0.011064723134040833,
      "learning_rate": 0.038824618232299865,
      "loss": 0.0,
      "step": 66100
    },
    {
      "epoch": 30.59231837112448,
      "grad_norm": 0.0038283527828752995,
      "learning_rate": 0.03881536325775104,
      "loss": 0.0,
      "step": 66110
    },
    {
      "epoch": 30.59694585839889,
      "grad_norm": 0.003664358053356409,
      "learning_rate": 0.038806108283202224,
      "loss": 0.0001,
      "step": 66120
    },
    {
      "epoch": 30.601573345673298,
      "grad_norm": 0.0036777642089873552,
      "learning_rate": 0.0387968533086534,
      "loss": 0.0001,
      "step": 66130
    },
    {
      "epoch": 30.60620083294771,
      "grad_norm": 0.05506424978375435,
      "learning_rate": 0.038787598334104584,
      "loss": 0.0004,
      "step": 66140
    },
    {
      "epoch": 30.61082832022212,
      "grad_norm": 0.004302330780774355,
      "learning_rate": 0.03877834335955577,
      "loss": 0.0001,
      "step": 66150
    },
    {
      "epoch": 30.61545580749653,
      "grad_norm": 0.00402823556214571,
      "learning_rate": 0.03876908838500694,
      "loss": 0.0001,
      "step": 66160
    },
    {
      "epoch": 30.62008329477094,
      "grad_norm": 0.0008069068426266313,
      "learning_rate": 0.038759833410458126,
      "loss": 0.0001,
      "step": 66170
    },
    {
      "epoch": 30.624710782045348,
      "grad_norm": 0.02706793136894703,
      "learning_rate": 0.0387505784359093,
      "loss": 0.0003,
      "step": 66180
    },
    {
      "epoch": 30.62933826931976,
      "grad_norm": 0.01710754819214344,
      "learning_rate": 0.038741323461360486,
      "loss": 0.0,
      "step": 66190
    },
    {
      "epoch": 30.63396575659417,
      "grad_norm": 0.009097103960812092,
      "learning_rate": 0.03873206848681166,
      "loss": 0.0003,
      "step": 66200
    },
    {
      "epoch": 30.63859324386858,
      "grad_norm": 0.002779030241072178,
      "learning_rate": 0.038722813512262845,
      "loss": 0.0,
      "step": 66210
    },
    {
      "epoch": 30.64322073114299,
      "grad_norm": 0.025779683142900467,
      "learning_rate": 0.03871355853771402,
      "loss": 0.0007,
      "step": 66220
    },
    {
      "epoch": 30.647848218417398,
      "grad_norm": 0.02320447750389576,
      "learning_rate": 0.038704303563165204,
      "loss": 0.0001,
      "step": 66230
    },
    {
      "epoch": 30.65247570569181,
      "grad_norm": 0.001748815644532442,
      "learning_rate": 0.03869504858861639,
      "loss": 0.0001,
      "step": 66240
    },
    {
      "epoch": 30.65710319296622,
      "grad_norm": 0.005494455341249704,
      "learning_rate": 0.038685793614067564,
      "loss": 0.0001,
      "step": 66250
    },
    {
      "epoch": 30.66173068024063,
      "grad_norm": 0.027674023061990738,
      "learning_rate": 0.03867653863951875,
      "loss": 0.0001,
      "step": 66260
    },
    {
      "epoch": 30.66635816751504,
      "grad_norm": 0.00707132276147604,
      "learning_rate": 0.03866728366496992,
      "loss": 0.0003,
      "step": 66270
    },
    {
      "epoch": 30.67098565478945,
      "grad_norm": 0.01667100377380848,
      "learning_rate": 0.038658028690421106,
      "loss": 0.0,
      "step": 66280
    },
    {
      "epoch": 30.67561314206386,
      "grad_norm": 0.018225833773612976,
      "learning_rate": 0.03864877371587228,
      "loss": 0.0,
      "step": 66290
    },
    {
      "epoch": 30.680240629338268,
      "grad_norm": 0.0005166181363165379,
      "learning_rate": 0.03863951874132346,
      "loss": 0.0001,
      "step": 66300
    },
    {
      "epoch": 30.68486811661268,
      "grad_norm": 0.0005602291785180569,
      "learning_rate": 0.03863026376677464,
      "loss": 0.0001,
      "step": 66310
    },
    {
      "epoch": 30.68949560388709,
      "grad_norm": 0.0016479797195643187,
      "learning_rate": 0.038621008792225825,
      "loss": 0.0001,
      "step": 66320
    },
    {
      "epoch": 30.6941230911615,
      "grad_norm": 0.0036813579499721527,
      "learning_rate": 0.03861175381767701,
      "loss": 0.0001,
      "step": 66330
    },
    {
      "epoch": 30.69875057843591,
      "grad_norm": 0.006454049609601498,
      "learning_rate": 0.038602498843128184,
      "loss": 0.0,
      "step": 66340
    },
    {
      "epoch": 30.703378065710318,
      "grad_norm": 0.007630581501871347,
      "learning_rate": 0.03859324386857937,
      "loss": 0.0002,
      "step": 66350
    },
    {
      "epoch": 30.70800555298473,
      "grad_norm": 0.004028411116451025,
      "learning_rate": 0.038583988894030544,
      "loss": 0.0001,
      "step": 66360
    },
    {
      "epoch": 30.71263304025914,
      "grad_norm": 0.020754341036081314,
      "learning_rate": 0.03857473391948173,
      "loss": 0.0001,
      "step": 66370
    },
    {
      "epoch": 30.71726052753355,
      "grad_norm": 0.00135797041002661,
      "learning_rate": 0.0385654789449329,
      "loss": 0.0001,
      "step": 66380
    },
    {
      "epoch": 30.72188801480796,
      "grad_norm": 0.0014470346504822373,
      "learning_rate": 0.03855622397038408,
      "loss": 0.0,
      "step": 66390
    },
    {
      "epoch": 30.726515502082368,
      "grad_norm": 0.0054532908834517,
      "learning_rate": 0.03854696899583526,
      "loss": 0.0001,
      "step": 66400
    },
    {
      "epoch": 30.73114298935678,
      "grad_norm": 0.21382874250411987,
      "learning_rate": 0.038537714021286446,
      "loss": 0.0002,
      "step": 66410
    },
    {
      "epoch": 30.73577047663119,
      "grad_norm": 0.0009252806776203215,
      "learning_rate": 0.03852845904673763,
      "loss": 0.0005,
      "step": 66420
    },
    {
      "epoch": 30.7403979639056,
      "grad_norm": 0.007596049923449755,
      "learning_rate": 0.038519204072188805,
      "loss": 0.0001,
      "step": 66430
    },
    {
      "epoch": 30.74502545118001,
      "grad_norm": 0.10241152346134186,
      "learning_rate": 0.03850994909763999,
      "loss": 0.0001,
      "step": 66440
    },
    {
      "epoch": 30.749652938454417,
      "grad_norm": 0.005174539517611265,
      "learning_rate": 0.038500694123091164,
      "loss": 0.0001,
      "step": 66450
    },
    {
      "epoch": 30.75428042572883,
      "grad_norm": 0.00977017730474472,
      "learning_rate": 0.03849143914854234,
      "loss": 0.0001,
      "step": 66460
    },
    {
      "epoch": 30.758907913003238,
      "grad_norm": 0.005151466466486454,
      "learning_rate": 0.038482184173993524,
      "loss": 0.0002,
      "step": 66470
    },
    {
      "epoch": 30.76353540027765,
      "grad_norm": 0.0019756448455154896,
      "learning_rate": 0.0384729291994447,
      "loss": 0.0001,
      "step": 66480
    },
    {
      "epoch": 30.76816288755206,
      "grad_norm": 0.0069318413734436035,
      "learning_rate": 0.03846367422489588,
      "loss": 0.0001,
      "step": 66490
    },
    {
      "epoch": 30.77279037482647,
      "grad_norm": 0.03360552340745926,
      "learning_rate": 0.038454419250347066,
      "loss": 0.0,
      "step": 66500
    },
    {
      "epoch": 30.77741786210088,
      "grad_norm": 0.0006153890863060951,
      "learning_rate": 0.03844516427579825,
      "loss": 0.0002,
      "step": 66510
    },
    {
      "epoch": 30.782045349375288,
      "grad_norm": 0.31642207503318787,
      "learning_rate": 0.038435909301249425,
      "loss": 0.0005,
      "step": 66520
    },
    {
      "epoch": 30.7866728366497,
      "grad_norm": 0.001211237395182252,
      "learning_rate": 0.0384266543267006,
      "loss": 0.0001,
      "step": 66530
    },
    {
      "epoch": 30.79130032392411,
      "grad_norm": 0.0011323849903419614,
      "learning_rate": 0.038417399352151785,
      "loss": 0.0003,
      "step": 66540
    },
    {
      "epoch": 30.79592781119852,
      "grad_norm": 0.004664181731641293,
      "learning_rate": 0.03840814437760296,
      "loss": 0.0001,
      "step": 66550
    },
    {
      "epoch": 30.80055529847293,
      "grad_norm": 0.0347425639629364,
      "learning_rate": 0.038398889403054144,
      "loss": 0.0001,
      "step": 66560
    },
    {
      "epoch": 30.805182785747338,
      "grad_norm": 0.11096404492855072,
      "learning_rate": 0.03838963442850532,
      "loss": 0.0001,
      "step": 66570
    },
    {
      "epoch": 30.80981027302175,
      "grad_norm": 0.00852713268250227,
      "learning_rate": 0.038380379453956504,
      "loss": 0.0001,
      "step": 66580
    },
    {
      "epoch": 30.81443776029616,
      "grad_norm": 0.0035332886036485434,
      "learning_rate": 0.03837112447940769,
      "loss": 0.0001,
      "step": 66590
    },
    {
      "epoch": 30.81906524757057,
      "grad_norm": 0.002402312122285366,
      "learning_rate": 0.03836186950485887,
      "loss": 0.0001,
      "step": 66600
    },
    {
      "epoch": 30.82369273484498,
      "grad_norm": 0.02240724116563797,
      "learning_rate": 0.038352614530310046,
      "loss": 0.0001,
      "step": 66610
    },
    {
      "epoch": 30.828320222119387,
      "grad_norm": 0.011609741486608982,
      "learning_rate": 0.03834335955576122,
      "loss": 0.0001,
      "step": 66620
    },
    {
      "epoch": 30.8329477093938,
      "grad_norm": 0.0013904788065701723,
      "learning_rate": 0.038334104581212405,
      "loss": 0.0002,
      "step": 66630
    },
    {
      "epoch": 30.837575196668208,
      "grad_norm": 0.14087142050266266,
      "learning_rate": 0.03832484960666358,
      "loss": 0.0001,
      "step": 66640
    },
    {
      "epoch": 30.84220268394262,
      "grad_norm": 0.0028886538930237293,
      "learning_rate": 0.038315594632114765,
      "loss": 0.0001,
      "step": 66650
    },
    {
      "epoch": 30.84683017121703,
      "grad_norm": 0.15966235101222992,
      "learning_rate": 0.03830633965756594,
      "loss": 0.0001,
      "step": 66660
    },
    {
      "epoch": 30.85145765849144,
      "grad_norm": 0.008923850022256374,
      "learning_rate": 0.038297084683017124,
      "loss": 0.0001,
      "step": 66670
    },
    {
      "epoch": 30.85608514576585,
      "grad_norm": 0.001707912073470652,
      "learning_rate": 0.03828782970846831,
      "loss": 0.0,
      "step": 66680
    },
    {
      "epoch": 30.860712633040258,
      "grad_norm": 0.005146917887032032,
      "learning_rate": 0.03827857473391948,
      "loss": 0.0001,
      "step": 66690
    },
    {
      "epoch": 30.86534012031467,
      "grad_norm": 0.025923658162355423,
      "learning_rate": 0.03826931975937067,
      "loss": 0.0001,
      "step": 66700
    },
    {
      "epoch": 30.86996760758908,
      "grad_norm": 0.0005665987846441567,
      "learning_rate": 0.03826006478482184,
      "loss": 0.0,
      "step": 66710
    },
    {
      "epoch": 30.87459509486349,
      "grad_norm": 0.03559664264321327,
      "learning_rate": 0.038250809810273026,
      "loss": 0.0003,
      "step": 66720
    },
    {
      "epoch": 30.8792225821379,
      "grad_norm": 0.005087754689157009,
      "learning_rate": 0.0382415548357242,
      "loss": 0.0001,
      "step": 66730
    },
    {
      "epoch": 30.883850069412308,
      "grad_norm": 0.005331107880920172,
      "learning_rate": 0.038232299861175385,
      "loss": 0.0,
      "step": 66740
    },
    {
      "epoch": 30.88847755668672,
      "grad_norm": 0.024282259866595268,
      "learning_rate": 0.03822304488662656,
      "loss": 0.0001,
      "step": 66750
    },
    {
      "epoch": 30.89310504396113,
      "grad_norm": 0.056797973811626434,
      "learning_rate": 0.038213789912077745,
      "loss": 0.0001,
      "step": 66760
    },
    {
      "epoch": 30.89773253123554,
      "grad_norm": 0.12909731268882751,
      "learning_rate": 0.03820453493752893,
      "loss": 0.0001,
      "step": 66770
    },
    {
      "epoch": 30.90236001850995,
      "grad_norm": 0.0053177024237811565,
      "learning_rate": 0.038195279962980104,
      "loss": 0.0,
      "step": 66780
    },
    {
      "epoch": 30.906987505784357,
      "grad_norm": 0.00110026472248137,
      "learning_rate": 0.03818602498843129,
      "loss": 0.0001,
      "step": 66790
    },
    {
      "epoch": 30.91161499305877,
      "grad_norm": 0.06107044592499733,
      "learning_rate": 0.03817677001388246,
      "loss": 0.0001,
      "step": 66800
    },
    {
      "epoch": 30.916242480333178,
      "grad_norm": 0.0023902463726699352,
      "learning_rate": 0.03816751503933365,
      "loss": 0.0001,
      "step": 66810
    },
    {
      "epoch": 30.92086996760759,
      "grad_norm": 0.00413794070482254,
      "learning_rate": 0.03815826006478482,
      "loss": 0.0002,
      "step": 66820
    },
    {
      "epoch": 30.925497454882,
      "grad_norm": 0.01559504959732294,
      "learning_rate": 0.038149005090236006,
      "loss": 0.0,
      "step": 66830
    },
    {
      "epoch": 30.93012494215641,
      "grad_norm": 0.009718010202050209,
      "learning_rate": 0.03813975011568718,
      "loss": 0.0001,
      "step": 66840
    },
    {
      "epoch": 30.93475242943082,
      "grad_norm": 0.0005994715029373765,
      "learning_rate": 0.038130495141138365,
      "loss": 0.0007,
      "step": 66850
    },
    {
      "epoch": 30.939379916705228,
      "grad_norm": 0.002511114114895463,
      "learning_rate": 0.03812124016658955,
      "loss": 0.0001,
      "step": 66860
    },
    {
      "epoch": 30.94400740397964,
      "grad_norm": 0.019996078684926033,
      "learning_rate": 0.038111985192040725,
      "loss": 0.0001,
      "step": 66870
    },
    {
      "epoch": 30.94863489125405,
      "grad_norm": 0.016329634934663773,
      "learning_rate": 0.03810273021749191,
      "loss": 0.0001,
      "step": 66880
    },
    {
      "epoch": 30.95326237852846,
      "grad_norm": 0.0012870478676632047,
      "learning_rate": 0.038093475242943084,
      "loss": 0.0002,
      "step": 66890
    },
    {
      "epoch": 30.95788986580287,
      "grad_norm": 0.0127845648676157,
      "learning_rate": 0.03808422026839427,
      "loss": 0.0005,
      "step": 66900
    },
    {
      "epoch": 30.962517353077278,
      "grad_norm": 0.019315646961331367,
      "learning_rate": 0.03807496529384544,
      "loss": 0.0,
      "step": 66910
    },
    {
      "epoch": 30.96714484035169,
      "grad_norm": 0.025031043216586113,
      "learning_rate": 0.03806571031929662,
      "loss": 0.0001,
      "step": 66920
    },
    {
      "epoch": 30.9717723276261,
      "grad_norm": 0.002387585351243615,
      "learning_rate": 0.0380564553447478,
      "loss": 0.0001,
      "step": 66930
    },
    {
      "epoch": 30.97639981490051,
      "grad_norm": 0.004380432888865471,
      "learning_rate": 0.038047200370198986,
      "loss": 0.0001,
      "step": 66940
    },
    {
      "epoch": 30.98102730217492,
      "grad_norm": 0.0126551054418087,
      "learning_rate": 0.03803794539565017,
      "loss": 0.0001,
      "step": 66950
    },
    {
      "epoch": 30.985654789449327,
      "grad_norm": 0.005474439822137356,
      "learning_rate": 0.038028690421101345,
      "loss": 0.0,
      "step": 66960
    },
    {
      "epoch": 30.99028227672374,
      "grad_norm": 0.007190786302089691,
      "learning_rate": 0.03801943544655253,
      "loss": 0.0001,
      "step": 66970
    },
    {
      "epoch": 30.994909763998148,
      "grad_norm": 0.007138117216527462,
      "learning_rate": 0.038010180472003705,
      "loss": 0.0001,
      "step": 66980
    },
    {
      "epoch": 30.99953725127256,
      "grad_norm": 0.004968354478478432,
      "learning_rate": 0.03800092549745488,
      "loss": 0.0,
      "step": 66990
    },
    {
      "epoch": 31.0,
      "eval_accuracy_branch1": 0.9896780157140657,
      "eval_accuracy_branch2": 0.4996918810660915,
      "eval_f1_branch1": 0.9904690704004746,
      "eval_f1_branch2": 0.49952225313537685,
      "eval_loss": 0.021344631910324097,
      "eval_precision_branch1": 0.9904843933507878,
      "eval_precision_branch2": 0.4996914627735172,
      "eval_recall_branch1": 0.9906127833831398,
      "eval_recall_branch2": 0.4996918810660915,
      "eval_runtime": 28.8539,
      "eval_samples_per_second": 449.922,
      "eval_steps_per_second": 56.249,
      "step": 66991
    },
    {
      "epoch": 31.00416473854697,
      "grad_norm": 0.03321416303515434,
      "learning_rate": 0.037991670522906064,
      "loss": 0.0001,
      "step": 67000
    },
    {
      "epoch": 31.008792225821377,
      "grad_norm": 0.012651876546442509,
      "learning_rate": 0.03798241554835724,
      "loss": 0.0001,
      "step": 67010
    },
    {
      "epoch": 31.01341971309579,
      "grad_norm": 0.0033436252269893885,
      "learning_rate": 0.03797316057380842,
      "loss": 0.0003,
      "step": 67020
    },
    {
      "epoch": 31.018047200370198,
      "grad_norm": 0.033388614654541016,
      "learning_rate": 0.037963905599259606,
      "loss": 0.0001,
      "step": 67030
    },
    {
      "epoch": 31.02267468764461,
      "grad_norm": 0.002041538944467902,
      "learning_rate": 0.03795465062471079,
      "loss": 0.0001,
      "step": 67040
    },
    {
      "epoch": 31.02730217491902,
      "grad_norm": 0.000977755757048726,
      "learning_rate": 0.037945395650161966,
      "loss": 0.0001,
      "step": 67050
    },
    {
      "epoch": 31.03192966219343,
      "grad_norm": 0.002693607471883297,
      "learning_rate": 0.03793614067561315,
      "loss": 0.0,
      "step": 67060
    },
    {
      "epoch": 31.03655714946784,
      "grad_norm": 0.0013989359140396118,
      "learning_rate": 0.037926885701064325,
      "loss": 0.0,
      "step": 67070
    },
    {
      "epoch": 31.041184636742248,
      "grad_norm": 0.001471092225983739,
      "learning_rate": 0.0379176307265155,
      "loss": 0.0003,
      "step": 67080
    },
    {
      "epoch": 31.04581212401666,
      "grad_norm": 0.003995469771325588,
      "learning_rate": 0.037908375751966684,
      "loss": 0.0001,
      "step": 67090
    },
    {
      "epoch": 31.05043961129107,
      "grad_norm": 0.08429691940546036,
      "learning_rate": 0.03789912077741786,
      "loss": 0.0001,
      "step": 67100
    },
    {
      "epoch": 31.05506709856548,
      "grad_norm": 0.012361854314804077,
      "learning_rate": 0.037889865802869044,
      "loss": 0.0001,
      "step": 67110
    },
    {
      "epoch": 31.05969458583989,
      "grad_norm": 0.002202828647568822,
      "learning_rate": 0.03788061082832023,
      "loss": 0.0001,
      "step": 67120
    },
    {
      "epoch": 31.064322073114297,
      "grad_norm": 0.015880785882472992,
      "learning_rate": 0.03787135585377141,
      "loss": 0.0001,
      "step": 67130
    },
    {
      "epoch": 31.06894956038871,
      "grad_norm": 0.007198209874331951,
      "learning_rate": 0.037862100879222586,
      "loss": 0.0002,
      "step": 67140
    },
    {
      "epoch": 31.073577047663118,
      "grad_norm": 0.003937684465199709,
      "learning_rate": 0.03785284590467376,
      "loss": 0.0001,
      "step": 67150
    },
    {
      "epoch": 31.07820453493753,
      "grad_norm": 0.000965787679888308,
      "learning_rate": 0.037843590930124946,
      "loss": 0.0001,
      "step": 67160
    },
    {
      "epoch": 31.08283202221194,
      "grad_norm": 0.0026792024727910757,
      "learning_rate": 0.03783433595557612,
      "loss": 0.0001,
      "step": 67170
    },
    {
      "epoch": 31.087459509486347,
      "grad_norm": 0.0007137709762901068,
      "learning_rate": 0.037825080981027305,
      "loss": 0.0,
      "step": 67180
    },
    {
      "epoch": 31.09208699676076,
      "grad_norm": 0.02168087847530842,
      "learning_rate": 0.03781582600647848,
      "loss": 0.0001,
      "step": 67190
    },
    {
      "epoch": 31.096714484035168,
      "grad_norm": 0.0036390216555446386,
      "learning_rate": 0.037806571031929664,
      "loss": 0.0001,
      "step": 67200
    },
    {
      "epoch": 31.10134197130958,
      "grad_norm": 0.05659995600581169,
      "learning_rate": 0.03779731605738085,
      "loss": 0.0001,
      "step": 67210
    },
    {
      "epoch": 31.10596945858399,
      "grad_norm": 0.023414405062794685,
      "learning_rate": 0.037788061082832024,
      "loss": 0.0001,
      "step": 67220
    },
    {
      "epoch": 31.1105969458584,
      "grad_norm": 0.019414881244301796,
      "learning_rate": 0.03777880610828321,
      "loss": 0.0001,
      "step": 67230
    },
    {
      "epoch": 31.11522443313281,
      "grad_norm": 13.234206199645996,
      "learning_rate": 0.03776955113373438,
      "loss": 0.0043,
      "step": 67240
    },
    {
      "epoch": 31.119851920407218,
      "grad_norm": 0.0018957393476739526,
      "learning_rate": 0.037760296159185566,
      "loss": 0.0002,
      "step": 67250
    },
    {
      "epoch": 31.12447940768163,
      "grad_norm": 0.012931833043694496,
      "learning_rate": 0.03775104118463674,
      "loss": 0.0,
      "step": 67260
    },
    {
      "epoch": 31.12910689495604,
      "grad_norm": 0.0033377977088093758,
      "learning_rate": 0.037741786210087926,
      "loss": 0.0,
      "step": 67270
    },
    {
      "epoch": 31.13373438223045,
      "grad_norm": 0.0038915735203772783,
      "learning_rate": 0.0377325312355391,
      "loss": 0.0001,
      "step": 67280
    },
    {
      "epoch": 31.13836186950486,
      "grad_norm": 0.0009307013242505491,
      "learning_rate": 0.037723276260990285,
      "loss": 0.0002,
      "step": 67290
    },
    {
      "epoch": 31.142989356779267,
      "grad_norm": 0.008021577261388302,
      "learning_rate": 0.03771402128644147,
      "loss": 0.0002,
      "step": 67300
    },
    {
      "epoch": 31.14761684405368,
      "grad_norm": 0.024678736925125122,
      "learning_rate": 0.037704766311892644,
      "loss": 0.0001,
      "step": 67310
    },
    {
      "epoch": 31.152244331328088,
      "grad_norm": 0.006339519750326872,
      "learning_rate": 0.03769551133734383,
      "loss": 0.0005,
      "step": 67320
    },
    {
      "epoch": 31.1568718186025,
      "grad_norm": 0.0021527819335460663,
      "learning_rate": 0.037686256362795004,
      "loss": 0.0001,
      "step": 67330
    },
    {
      "epoch": 31.16149930587691,
      "grad_norm": 0.00211719679646194,
      "learning_rate": 0.03767700138824619,
      "loss": 0.0001,
      "step": 67340
    },
    {
      "epoch": 31.166126793151317,
      "grad_norm": 0.004138791933655739,
      "learning_rate": 0.03766774641369736,
      "loss": 0.0001,
      "step": 67350
    },
    {
      "epoch": 31.17075428042573,
      "grad_norm": 0.0019869329407811165,
      "learning_rate": 0.037658491439148546,
      "loss": 0.0,
      "step": 67360
    },
    {
      "epoch": 31.175381767700138,
      "grad_norm": 0.0027894952800124884,
      "learning_rate": 0.03764923646459972,
      "loss": 0.0001,
      "step": 67370
    },
    {
      "epoch": 31.18000925497455,
      "grad_norm": 0.004881297238171101,
      "learning_rate": 0.037639981490050906,
      "loss": 0.0002,
      "step": 67380
    },
    {
      "epoch": 31.18463674224896,
      "grad_norm": 0.048454731702804565,
      "learning_rate": 0.03763072651550209,
      "loss": 0.0002,
      "step": 67390
    },
    {
      "epoch": 31.189264229523367,
      "grad_norm": 0.001226093154400587,
      "learning_rate": 0.037621471540953265,
      "loss": 0.0001,
      "step": 67400
    },
    {
      "epoch": 31.19389171679778,
      "grad_norm": 0.0022202255204319954,
      "learning_rate": 0.03761221656640445,
      "loss": 0.0001,
      "step": 67410
    },
    {
      "epoch": 31.198519204072188,
      "grad_norm": 0.0014633920509368181,
      "learning_rate": 0.037602961591855624,
      "loss": 0.0001,
      "step": 67420
    },
    {
      "epoch": 31.2031466913466,
      "grad_norm": 0.05929774045944214,
      "learning_rate": 0.03759370661730681,
      "loss": 0.0,
      "step": 67430
    },
    {
      "epoch": 31.20777417862101,
      "grad_norm": 0.00863108690828085,
      "learning_rate": 0.037584451642757984,
      "loss": 0.0001,
      "step": 67440
    },
    {
      "epoch": 31.21240166589542,
      "grad_norm": 0.0010300235589966178,
      "learning_rate": 0.03757519666820916,
      "loss": 0.0001,
      "step": 67450
    },
    {
      "epoch": 31.21702915316983,
      "grad_norm": 0.00476905656978488,
      "learning_rate": 0.03756594169366034,
      "loss": 0.0,
      "step": 67460
    },
    {
      "epoch": 31.221656640444237,
      "grad_norm": 0.08523836731910706,
      "learning_rate": 0.037556686719111526,
      "loss": 0.0001,
      "step": 67470
    },
    {
      "epoch": 31.22628412771865,
      "grad_norm": 0.10000889003276825,
      "learning_rate": 0.03754743174456271,
      "loss": 0.0001,
      "step": 67480
    },
    {
      "epoch": 31.230911614993058,
      "grad_norm": 0.0017964221769943833,
      "learning_rate": 0.037538176770013885,
      "loss": 0.0,
      "step": 67490
    },
    {
      "epoch": 31.23553910226747,
      "grad_norm": 0.009320318698883057,
      "learning_rate": 0.03752892179546507,
      "loss": 0.0023,
      "step": 67500
    },
    {
      "epoch": 31.24016658954188,
      "grad_norm": 0.04009821638464928,
      "learning_rate": 0.037519666820916245,
      "loss": 0.0002,
      "step": 67510
    },
    {
      "epoch": 31.244794076816287,
      "grad_norm": 0.11775673925876617,
      "learning_rate": 0.03751041184636743,
      "loss": 0.0001,
      "step": 67520
    },
    {
      "epoch": 31.2494215640907,
      "grad_norm": 0.016188886016607285,
      "learning_rate": 0.037501156871818604,
      "loss": 0.0001,
      "step": 67530
    },
    {
      "epoch": 31.254049051365108,
      "grad_norm": 0.03111480548977852,
      "learning_rate": 0.03749190189726978,
      "loss": 0.0,
      "step": 67540
    },
    {
      "epoch": 31.25867653863952,
      "grad_norm": 0.001934004481881857,
      "learning_rate": 0.037482646922720964,
      "loss": 0.0001,
      "step": 67550
    },
    {
      "epoch": 31.26330402591393,
      "grad_norm": 0.0055647967383265495,
      "learning_rate": 0.03747339194817214,
      "loss": 0.0001,
      "step": 67560
    },
    {
      "epoch": 31.267931513188337,
      "grad_norm": 0.0007521674851886928,
      "learning_rate": 0.03746413697362332,
      "loss": 0.0001,
      "step": 67570
    },
    {
      "epoch": 31.27255900046275,
      "grad_norm": 0.034348536282777786,
      "learning_rate": 0.037454881999074506,
      "loss": 0.0001,
      "step": 67580
    },
    {
      "epoch": 31.277186487737158,
      "grad_norm": 0.0008982910076156259,
      "learning_rate": 0.03744562702452569,
      "loss": 0.0001,
      "step": 67590
    },
    {
      "epoch": 31.28181397501157,
      "grad_norm": 0.0009049542713910341,
      "learning_rate": 0.037436372049976865,
      "loss": 0.0001,
      "step": 67600
    },
    {
      "epoch": 31.28644146228598,
      "grad_norm": 0.010053587146103382,
      "learning_rate": 0.03742711707542804,
      "loss": 0.0003,
      "step": 67610
    },
    {
      "epoch": 31.29106894956039,
      "grad_norm": 0.0009970535757020116,
      "learning_rate": 0.037417862100879225,
      "loss": 0.0001,
      "step": 67620
    },
    {
      "epoch": 31.2956964368348,
      "grad_norm": 0.012551452964544296,
      "learning_rate": 0.0374086071263304,
      "loss": 0.0,
      "step": 67630
    },
    {
      "epoch": 31.300323924109207,
      "grad_norm": 0.016439776867628098,
      "learning_rate": 0.037399352151781584,
      "loss": 0.0001,
      "step": 67640
    },
    {
      "epoch": 31.30495141138362,
      "grad_norm": 0.00865477230399847,
      "learning_rate": 0.03739009717723276,
      "loss": 0.0005,
      "step": 67650
    },
    {
      "epoch": 31.309578898658028,
      "grad_norm": 0.010399676859378815,
      "learning_rate": 0.037380842202683943,
      "loss": 0.0001,
      "step": 67660
    },
    {
      "epoch": 31.31420638593244,
      "grad_norm": 0.002817742759361863,
      "learning_rate": 0.03737158722813513,
      "loss": 0.0001,
      "step": 67670
    },
    {
      "epoch": 31.31883387320685,
      "grad_norm": 0.013059536926448345,
      "learning_rate": 0.0373623322535863,
      "loss": 0.0001,
      "step": 67680
    },
    {
      "epoch": 31.323461360481257,
      "grad_norm": 0.005446962080895901,
      "learning_rate": 0.037353077279037486,
      "loss": 0.0001,
      "step": 67690
    },
    {
      "epoch": 31.32808884775567,
      "grad_norm": 0.034924544394016266,
      "learning_rate": 0.03734382230448866,
      "loss": 0.0001,
      "step": 67700
    },
    {
      "epoch": 31.332716335030078,
      "grad_norm": 0.12090488523244858,
      "learning_rate": 0.037334567329939845,
      "loss": 0.0001,
      "step": 67710
    },
    {
      "epoch": 31.33734382230449,
      "grad_norm": 0.002726086648181081,
      "learning_rate": 0.03732531235539102,
      "loss": 0.0001,
      "step": 67720
    },
    {
      "epoch": 31.3419713095789,
      "grad_norm": 0.003670928068459034,
      "learning_rate": 0.037316057380842205,
      "loss": 0.0002,
      "step": 67730
    },
    {
      "epoch": 31.346598796853307,
      "grad_norm": 0.016094636172056198,
      "learning_rate": 0.03730680240629338,
      "loss": 0.0001,
      "step": 67740
    },
    {
      "epoch": 31.35122628412772,
      "grad_norm": 0.0013151338789612055,
      "learning_rate": 0.037297547431744564,
      "loss": 0.0001,
      "step": 67750
    },
    {
      "epoch": 31.355853771402128,
      "grad_norm": 0.0009569201502017677,
      "learning_rate": 0.03728829245719575,
      "loss": 0.0,
      "step": 67760
    },
    {
      "epoch": 31.36048125867654,
      "grad_norm": 0.005351077765226364,
      "learning_rate": 0.03727903748264692,
      "loss": 0.0001,
      "step": 67770
    },
    {
      "epoch": 31.36510874595095,
      "grad_norm": 0.001509529654867947,
      "learning_rate": 0.03726978250809811,
      "loss": 0.0001,
      "step": 67780
    },
    {
      "epoch": 31.36973623322536,
      "grad_norm": 0.002684600418433547,
      "learning_rate": 0.03726052753354928,
      "loss": 0.0,
      "step": 67790
    },
    {
      "epoch": 31.37436372049977,
      "grad_norm": 0.0008731854031793773,
      "learning_rate": 0.037251272559000466,
      "loss": 0.0,
      "step": 67800
    },
    {
      "epoch": 31.378991207774177,
      "grad_norm": 0.0022083004005253315,
      "learning_rate": 0.03724201758445164,
      "loss": 0.0038,
      "step": 67810
    },
    {
      "epoch": 31.38361869504859,
      "grad_norm": 0.025114839896559715,
      "learning_rate": 0.037232762609902825,
      "loss": 0.0003,
      "step": 67820
    },
    {
      "epoch": 31.388246182322998,
      "grad_norm": 0.02132013998925686,
      "learning_rate": 0.037223507635354,
      "loss": 0.0001,
      "step": 67830
    },
    {
      "epoch": 31.39287366959741,
      "grad_norm": 0.009092515334486961,
      "learning_rate": 0.037214252660805185,
      "loss": 0.0001,
      "step": 67840
    },
    {
      "epoch": 31.39750115687182,
      "grad_norm": 0.016094237565994263,
      "learning_rate": 0.03720499768625637,
      "loss": 0.0001,
      "step": 67850
    },
    {
      "epoch": 31.402128644146227,
      "grad_norm": 0.0047194394282996655,
      "learning_rate": 0.037195742711707544,
      "loss": 0.0,
      "step": 67860
    },
    {
      "epoch": 31.40675613142064,
      "grad_norm": 0.0029734375420957804,
      "learning_rate": 0.03718648773715873,
      "loss": 0.0,
      "step": 67870
    },
    {
      "epoch": 31.411383618695048,
      "grad_norm": 0.002323734574019909,
      "learning_rate": 0.0371772327626099,
      "loss": 0.0004,
      "step": 67880
    },
    {
      "epoch": 31.41601110596946,
      "grad_norm": 0.023233389481902122,
      "learning_rate": 0.037167977788061086,
      "loss": 0.0001,
      "step": 67890
    },
    {
      "epoch": 31.42063859324387,
      "grad_norm": 0.0037768816109746695,
      "learning_rate": 0.03715872281351226,
      "loss": 0.0001,
      "step": 67900
    },
    {
      "epoch": 31.425266080518277,
      "grad_norm": 0.00567241944372654,
      "learning_rate": 0.03714946783896344,
      "loss": 0.0,
      "step": 67910
    },
    {
      "epoch": 31.42989356779269,
      "grad_norm": 0.11098504811525345,
      "learning_rate": 0.03714021286441462,
      "loss": 0.0001,
      "step": 67920
    },
    {
      "epoch": 31.434521055067098,
      "grad_norm": 0.13282275199890137,
      "learning_rate": 0.037130957889865805,
      "loss": 0.0001,
      "step": 67930
    },
    {
      "epoch": 31.43914854234151,
      "grad_norm": 0.007123151794075966,
      "learning_rate": 0.03712170291531699,
      "loss": 0.0003,
      "step": 67940
    },
    {
      "epoch": 31.44377602961592,
      "grad_norm": 0.03567884862422943,
      "learning_rate": 0.037112447940768165,
      "loss": 0.0001,
      "step": 67950
    },
    {
      "epoch": 31.448403516890327,
      "grad_norm": 0.003257310250774026,
      "learning_rate": 0.03710319296621935,
      "loss": 0.0002,
      "step": 67960
    },
    {
      "epoch": 31.45303100416474,
      "grad_norm": 0.002666427753865719,
      "learning_rate": 0.037093937991670524,
      "loss": 0.0001,
      "step": 67970
    },
    {
      "epoch": 31.457658491439147,
      "grad_norm": 0.08904480934143066,
      "learning_rate": 0.03708468301712171,
      "loss": 0.0001,
      "step": 67980
    },
    {
      "epoch": 31.46228597871356,
      "grad_norm": 0.0018756245262920856,
      "learning_rate": 0.03707542804257288,
      "loss": 0.0,
      "step": 67990
    },
    {
      "epoch": 31.466913465987968,
      "grad_norm": 0.018947379663586617,
      "learning_rate": 0.03706617306802406,
      "loss": 0.0,
      "step": 68000
    },
    {
      "epoch": 31.47154095326238,
      "grad_norm": 0.004397968295961618,
      "learning_rate": 0.03705691809347524,
      "loss": 0.0,
      "step": 68010
    },
    {
      "epoch": 31.47616844053679,
      "grad_norm": 0.020543502643704414,
      "learning_rate": 0.037047663118926426,
      "loss": 0.0001,
      "step": 68020
    },
    {
      "epoch": 31.480795927811197,
      "grad_norm": 0.0024971545208245516,
      "learning_rate": 0.03703840814437761,
      "loss": 0.0001,
      "step": 68030
    },
    {
      "epoch": 31.48542341508561,
      "grad_norm": 0.047843486070632935,
      "learning_rate": 0.037029153169828785,
      "loss": 0.0006,
      "step": 68040
    },
    {
      "epoch": 31.490050902360018,
      "grad_norm": 0.016235537827014923,
      "learning_rate": 0.03701989819527997,
      "loss": 0.0003,
      "step": 68050
    },
    {
      "epoch": 31.49467838963443,
      "grad_norm": 0.12157922238111496,
      "learning_rate": 0.037010643220731144,
      "loss": 0.0003,
      "step": 68060
    },
    {
      "epoch": 31.49930587690884,
      "grad_norm": 0.1393100619316101,
      "learning_rate": 0.03700138824618232,
      "loss": 0.0002,
      "step": 68070
    },
    {
      "epoch": 31.503933364183247,
      "grad_norm": 0.016149628907442093,
      "learning_rate": 0.036992133271633504,
      "loss": 0.0002,
      "step": 68080
    },
    {
      "epoch": 31.50856085145766,
      "grad_norm": 0.002031726995483041,
      "learning_rate": 0.03698287829708468,
      "loss": 0.0001,
      "step": 68090
    },
    {
      "epoch": 31.513188338732068,
      "grad_norm": 0.0031248643063008785,
      "learning_rate": 0.03697362332253586,
      "loss": 0.0001,
      "step": 68100
    },
    {
      "epoch": 31.51781582600648,
      "grad_norm": 0.05982380732893944,
      "learning_rate": 0.036964368347987046,
      "loss": 0.0004,
      "step": 68110
    },
    {
      "epoch": 31.52244331328089,
      "grad_norm": 0.006462742108851671,
      "learning_rate": 0.03695511337343823,
      "loss": 0.0,
      "step": 68120
    },
    {
      "epoch": 31.527070800555297,
      "grad_norm": 0.02296919748187065,
      "learning_rate": 0.036945858398889406,
      "loss": 0.0003,
      "step": 68130
    },
    {
      "epoch": 31.53169828782971,
      "grad_norm": 0.01869288459420204,
      "learning_rate": 0.03693660342434058,
      "loss": 0.0001,
      "step": 68140
    },
    {
      "epoch": 31.536325775104117,
      "grad_norm": 0.0015739387599751353,
      "learning_rate": 0.036927348449791765,
      "loss": 0.0001,
      "step": 68150
    },
    {
      "epoch": 31.54095326237853,
      "grad_norm": 0.0018709900323301554,
      "learning_rate": 0.03691809347524294,
      "loss": 0.0,
      "step": 68160
    },
    {
      "epoch": 31.545580749652938,
      "grad_norm": 0.016671214252710342,
      "learning_rate": 0.036908838500694124,
      "loss": 0.0002,
      "step": 68170
    },
    {
      "epoch": 31.550208236927347,
      "grad_norm": 0.0023440723307430744,
      "learning_rate": 0.0368995835261453,
      "loss": 0.0,
      "step": 68180
    },
    {
      "epoch": 31.55483572420176,
      "grad_norm": 0.003132933983579278,
      "learning_rate": 0.036890328551596484,
      "loss": 0.0001,
      "step": 68190
    },
    {
      "epoch": 31.559463211476167,
      "grad_norm": 0.0667685866355896,
      "learning_rate": 0.03688107357704767,
      "loss": 0.0002,
      "step": 68200
    },
    {
      "epoch": 31.56409069875058,
      "grad_norm": 0.0009662763332016766,
      "learning_rate": 0.03687181860249884,
      "loss": 0.0002,
      "step": 68210
    },
    {
      "epoch": 31.568718186024988,
      "grad_norm": 0.0035569423343986273,
      "learning_rate": 0.036862563627950026,
      "loss": 0.0,
      "step": 68220
    },
    {
      "epoch": 31.5733456732994,
      "grad_norm": 0.001119328080676496,
      "learning_rate": 0.0368533086534012,
      "loss": 0.0002,
      "step": 68230
    },
    {
      "epoch": 31.57797316057381,
      "grad_norm": 0.018991868942975998,
      "learning_rate": 0.036844053678852386,
      "loss": 0.0001,
      "step": 68240
    },
    {
      "epoch": 31.582600647848217,
      "grad_norm": 0.0006264428375288844,
      "learning_rate": 0.03683479870430356,
      "loss": 0.0001,
      "step": 68250
    },
    {
      "epoch": 31.58722813512263,
      "grad_norm": 0.02996799908578396,
      "learning_rate": 0.036825543729754745,
      "loss": 0.0,
      "step": 68260
    },
    {
      "epoch": 31.591855622397038,
      "grad_norm": 0.07100317627191544,
      "learning_rate": 0.03681628875520592,
      "loss": 0.0001,
      "step": 68270
    },
    {
      "epoch": 31.59648310967145,
      "grad_norm": 0.003565763356164098,
      "learning_rate": 0.036807033780657104,
      "loss": 0.0004,
      "step": 68280
    },
    {
      "epoch": 31.60111059694586,
      "grad_norm": 0.0019513665465638041,
      "learning_rate": 0.03679777880610829,
      "loss": 0.0001,
      "step": 68290
    },
    {
      "epoch": 31.605738084220267,
      "grad_norm": 0.9642031788825989,
      "learning_rate": 0.036788523831559464,
      "loss": 0.0003,
      "step": 68300
    },
    {
      "epoch": 31.61036557149468,
      "grad_norm": 0.03397442400455475,
      "learning_rate": 0.03677926885701065,
      "loss": 0.0001,
      "step": 68310
    },
    {
      "epoch": 31.614993058769087,
      "grad_norm": 0.0028103457298129797,
      "learning_rate": 0.03677001388246182,
      "loss": 0.0001,
      "step": 68320
    },
    {
      "epoch": 31.6196205460435,
      "grad_norm": 0.010631394572556019,
      "learning_rate": 0.036760758907913006,
      "loss": 0.0005,
      "step": 68330
    },
    {
      "epoch": 31.624248033317908,
      "grad_norm": 0.0471838116645813,
      "learning_rate": 0.03675150393336418,
      "loss": 0.0001,
      "step": 68340
    },
    {
      "epoch": 31.628875520592317,
      "grad_norm": 0.005213185213506222,
      "learning_rate": 0.036742248958815366,
      "loss": 0.0001,
      "step": 68350
    },
    {
      "epoch": 31.63350300786673,
      "grad_norm": 0.004074042662978172,
      "learning_rate": 0.03673299398426654,
      "loss": 0.0001,
      "step": 68360
    },
    {
      "epoch": 31.638130495141137,
      "grad_norm": 0.0070800380781292915,
      "learning_rate": 0.036723739009717725,
      "loss": 0.0002,
      "step": 68370
    },
    {
      "epoch": 31.64275798241555,
      "grad_norm": 0.00257659750059247,
      "learning_rate": 0.03671448403516891,
      "loss": 0.0004,
      "step": 68380
    },
    {
      "epoch": 31.647385469689958,
      "grad_norm": 0.04794757440686226,
      "learning_rate": 0.036705229060620084,
      "loss": 0.0001,
      "step": 68390
    },
    {
      "epoch": 31.65201295696437,
      "grad_norm": 0.0012254309840500355,
      "learning_rate": 0.03669597408607127,
      "loss": 0.0,
      "step": 68400
    },
    {
      "epoch": 31.65664044423878,
      "grad_norm": 0.0033330190926790237,
      "learning_rate": 0.036686719111522444,
      "loss": 0.0001,
      "step": 68410
    },
    {
      "epoch": 31.661267931513187,
      "grad_norm": 0.037668388336896896,
      "learning_rate": 0.03667746413697363,
      "loss": 0.0,
      "step": 68420
    },
    {
      "epoch": 31.6658954187876,
      "grad_norm": 0.006783338729292154,
      "learning_rate": 0.0366682091624248,
      "loss": 0.0003,
      "step": 68430
    },
    {
      "epoch": 31.670522906062008,
      "grad_norm": 0.006976547185331583,
      "learning_rate": 0.03665895418787598,
      "loss": 0.0001,
      "step": 68440
    },
    {
      "epoch": 31.67515039333642,
      "grad_norm": 0.005538363009691238,
      "learning_rate": 0.03664969921332716,
      "loss": 0.0001,
      "step": 68450
    },
    {
      "epoch": 31.67977788061083,
      "grad_norm": 0.006028294563293457,
      "learning_rate": 0.036640444238778345,
      "loss": 0.0001,
      "step": 68460
    },
    {
      "epoch": 31.684405367885237,
      "grad_norm": 0.005538391415029764,
      "learning_rate": 0.03663118926422953,
      "loss": 0.0001,
      "step": 68470
    },
    {
      "epoch": 31.68903285515965,
      "grad_norm": 0.017964554950594902,
      "learning_rate": 0.036621934289680705,
      "loss": 0.0001,
      "step": 68480
    },
    {
      "epoch": 31.693660342434057,
      "grad_norm": 0.018748177215456963,
      "learning_rate": 0.03661267931513189,
      "loss": 0.0001,
      "step": 68490
    },
    {
      "epoch": 31.69828782970847,
      "grad_norm": 0.0014371828874573112,
      "learning_rate": 0.036603424340583064,
      "loss": 0.0001,
      "step": 68500
    },
    {
      "epoch": 31.702915316982878,
      "grad_norm": 0.004586258437484503,
      "learning_rate": 0.03659416936603425,
      "loss": 0.0002,
      "step": 68510
    },
    {
      "epoch": 31.707542804257287,
      "grad_norm": 0.0033054908271878958,
      "learning_rate": 0.036584914391485424,
      "loss": 0.0001,
      "step": 68520
    },
    {
      "epoch": 31.7121702915317,
      "grad_norm": 0.0022841107565909624,
      "learning_rate": 0.0365756594169366,
      "loss": 0.0001,
      "step": 68530
    },
    {
      "epoch": 31.716797778806107,
      "grad_norm": 0.001768649322912097,
      "learning_rate": 0.03656640444238778,
      "loss": 0.0,
      "step": 68540
    },
    {
      "epoch": 31.72142526608052,
      "grad_norm": 0.01083404291421175,
      "learning_rate": 0.036557149467838966,
      "loss": 0.0,
      "step": 68550
    },
    {
      "epoch": 31.726052753354928,
      "grad_norm": 0.007107262033969164,
      "learning_rate": 0.03654789449329015,
      "loss": 0.0008,
      "step": 68560
    },
    {
      "epoch": 31.73068024062934,
      "grad_norm": 0.06555391848087311,
      "learning_rate": 0.036538639518741325,
      "loss": 0.0001,
      "step": 68570
    },
    {
      "epoch": 31.73530772790375,
      "grad_norm": 0.0016101701185107231,
      "learning_rate": 0.03652938454419251,
      "loss": 0.0004,
      "step": 68580
    },
    {
      "epoch": 31.739935215178157,
      "grad_norm": 0.016127686947584152,
      "learning_rate": 0.036520129569643685,
      "loss": 0.0001,
      "step": 68590
    },
    {
      "epoch": 31.74456270245257,
      "grad_norm": 0.006558289285749197,
      "learning_rate": 0.03651087459509486,
      "loss": 0.0001,
      "step": 68600
    },
    {
      "epoch": 31.749190189726978,
      "grad_norm": 0.0005588865606114268,
      "learning_rate": 0.036501619620546044,
      "loss": 0.0,
      "step": 68610
    },
    {
      "epoch": 31.75381767700139,
      "grad_norm": 0.9291432499885559,
      "learning_rate": 0.03649236464599722,
      "loss": 0.0003,
      "step": 68620
    },
    {
      "epoch": 31.7584451642758,
      "grad_norm": 0.009401592426002026,
      "learning_rate": 0.036483109671448403,
      "loss": 0.0001,
      "step": 68630
    },
    {
      "epoch": 31.763072651550207,
      "grad_norm": 0.0026735623832792044,
      "learning_rate": 0.03647385469689959,
      "loss": 0.0001,
      "step": 68640
    },
    {
      "epoch": 31.76770013882462,
      "grad_norm": 0.0073648965917527676,
      "learning_rate": 0.03646459972235077,
      "loss": 0.0001,
      "step": 68650
    },
    {
      "epoch": 31.772327626099027,
      "grad_norm": 0.003652611281722784,
      "learning_rate": 0.036455344747801946,
      "loss": 0.0001,
      "step": 68660
    },
    {
      "epoch": 31.77695511337344,
      "grad_norm": 0.08576736599206924,
      "learning_rate": 0.03644608977325312,
      "loss": 0.0001,
      "step": 68670
    },
    {
      "epoch": 31.781582600647848,
      "grad_norm": 0.005481556989252567,
      "learning_rate": 0.036436834798704305,
      "loss": 0.0001,
      "step": 68680
    },
    {
      "epoch": 31.786210087922257,
      "grad_norm": 0.05810592696070671,
      "learning_rate": 0.03642757982415548,
      "loss": 0.0001,
      "step": 68690
    },
    {
      "epoch": 31.79083757519667,
      "grad_norm": 0.005505037028342485,
      "learning_rate": 0.036418324849606665,
      "loss": 0.0001,
      "step": 68700
    },
    {
      "epoch": 31.795465062471077,
      "grad_norm": 0.7959579229354858,
      "learning_rate": 0.03640906987505784,
      "loss": 0.0004,
      "step": 68710
    },
    {
      "epoch": 31.80009254974549,
      "grad_norm": 0.0022278118412941694,
      "learning_rate": 0.036399814900509024,
      "loss": 0.0001,
      "step": 68720
    },
    {
      "epoch": 31.804720037019898,
      "grad_norm": 0.00427048048004508,
      "learning_rate": 0.03639055992596021,
      "loss": 0.0001,
      "step": 68730
    },
    {
      "epoch": 31.80934752429431,
      "grad_norm": 0.007803482469171286,
      "learning_rate": 0.03638130495141139,
      "loss": 0.0001,
      "step": 68740
    },
    {
      "epoch": 31.81397501156872,
      "grad_norm": 0.017945827916264534,
      "learning_rate": 0.03637204997686257,
      "loss": 0.0001,
      "step": 68750
    },
    {
      "epoch": 31.818602498843127,
      "grad_norm": 0.001470403280109167,
      "learning_rate": 0.03636279500231374,
      "loss": 0.0003,
      "step": 68760
    },
    {
      "epoch": 31.82322998611754,
      "grad_norm": 0.0058755516074597836,
      "learning_rate": 0.036353540027764926,
      "loss": 0.0003,
      "step": 68770
    },
    {
      "epoch": 31.827857473391948,
      "grad_norm": 0.008241904899477959,
      "learning_rate": 0.0363442850532161,
      "loss": 0.0001,
      "step": 68780
    },
    {
      "epoch": 31.83248496066636,
      "grad_norm": 0.0019873827695846558,
      "learning_rate": 0.036335030078667285,
      "loss": 0.0001,
      "step": 68790
    },
    {
      "epoch": 31.83711244794077,
      "grad_norm": 0.0007646703743375838,
      "learning_rate": 0.03632577510411846,
      "loss": 0.0001,
      "step": 68800
    },
    {
      "epoch": 31.841739935215177,
      "grad_norm": 0.01871204376220703,
      "learning_rate": 0.036316520129569645,
      "loss": 0.0001,
      "step": 68810
    },
    {
      "epoch": 31.84636742248959,
      "grad_norm": 0.00471421517431736,
      "learning_rate": 0.03630726515502083,
      "loss": 0.0001,
      "step": 68820
    },
    {
      "epoch": 31.850994909763997,
      "grad_norm": 0.0015253027668222785,
      "learning_rate": 0.036298010180472004,
      "loss": 0.0,
      "step": 68830
    },
    {
      "epoch": 31.85562239703841,
      "grad_norm": 0.006256356835365295,
      "learning_rate": 0.03628875520592319,
      "loss": 0.0001,
      "step": 68840
    },
    {
      "epoch": 31.860249884312818,
      "grad_norm": 0.0036784044932574034,
      "learning_rate": 0.03627950023137436,
      "loss": 0.0,
      "step": 68850
    },
    {
      "epoch": 31.864877371587227,
      "grad_norm": 0.2154780775308609,
      "learning_rate": 0.036270245256825547,
      "loss": 0.0002,
      "step": 68860
    },
    {
      "epoch": 31.86950485886164,
      "grad_norm": 0.0077373068779706955,
      "learning_rate": 0.03626099028227672,
      "loss": 0.0001,
      "step": 68870
    },
    {
      "epoch": 31.874132346136047,
      "grad_norm": 0.04188154637813568,
      "learning_rate": 0.036251735307727906,
      "loss": 0.0001,
      "step": 68880
    },
    {
      "epoch": 31.87875983341046,
      "grad_norm": 0.006375350523740053,
      "learning_rate": 0.03624248033317908,
      "loss": 0.0,
      "step": 68890
    },
    {
      "epoch": 31.883387320684868,
      "grad_norm": 0.014379788190126419,
      "learning_rate": 0.036233225358630265,
      "loss": 0.0001,
      "step": 68900
    },
    {
      "epoch": 31.888014807959276,
      "grad_norm": 0.09636757522821426,
      "learning_rate": 0.03622397038408145,
      "loss": 0.0001,
      "step": 68910
    },
    {
      "epoch": 31.89264229523369,
      "grad_norm": 0.0077073341235518456,
      "learning_rate": 0.036214715409532625,
      "loss": 0.0001,
      "step": 68920
    },
    {
      "epoch": 31.897269782508097,
      "grad_norm": 0.0022734084632247686,
      "learning_rate": 0.03620546043498381,
      "loss": 0.0001,
      "step": 68930
    },
    {
      "epoch": 31.90189726978251,
      "grad_norm": 0.0036613994743674994,
      "learning_rate": 0.036196205460434984,
      "loss": 0.0001,
      "step": 68940
    },
    {
      "epoch": 31.906524757056918,
      "grad_norm": 0.01663082093000412,
      "learning_rate": 0.03618695048588617,
      "loss": 0.0001,
      "step": 68950
    },
    {
      "epoch": 31.91115224433133,
      "grad_norm": 0.004582855850458145,
      "learning_rate": 0.03617769551133734,
      "loss": 0.0001,
      "step": 68960
    },
    {
      "epoch": 31.91577973160574,
      "grad_norm": 0.001528889057226479,
      "learning_rate": 0.036168440536788526,
      "loss": 0.0001,
      "step": 68970
    },
    {
      "epoch": 31.920407218880147,
      "grad_norm": 0.01161954179406166,
      "learning_rate": 0.0361591855622397,
      "loss": 0.0001,
      "step": 68980
    },
    {
      "epoch": 31.92503470615456,
      "grad_norm": 0.005702754016965628,
      "learning_rate": 0.036149930587690886,
      "loss": 0.0,
      "step": 68990
    },
    {
      "epoch": 31.929662193428967,
      "grad_norm": 0.009431391954421997,
      "learning_rate": 0.03614067561314207,
      "loss": 0.0001,
      "step": 69000
    },
    {
      "epoch": 31.93428968070338,
      "grad_norm": 0.11066587269306183,
      "learning_rate": 0.036131420638593245,
      "loss": 0.0001,
      "step": 69010
    },
    {
      "epoch": 31.938917167977788,
      "grad_norm": 0.0025478394236415625,
      "learning_rate": 0.03612216566404443,
      "loss": 0.0001,
      "step": 69020
    },
    {
      "epoch": 31.943544655252197,
      "grad_norm": 0.02186156064271927,
      "learning_rate": 0.036112910689495605,
      "loss": 0.0001,
      "step": 69030
    },
    {
      "epoch": 31.94817214252661,
      "grad_norm": 0.00520062493160367,
      "learning_rate": 0.03610365571494679,
      "loss": 0.0001,
      "step": 69040
    },
    {
      "epoch": 31.952799629801017,
      "grad_norm": 0.004103514365851879,
      "learning_rate": 0.036094400740397964,
      "loss": 0.0001,
      "step": 69050
    },
    {
      "epoch": 31.95742711707543,
      "grad_norm": 0.006263966206461191,
      "learning_rate": 0.03608514576584914,
      "loss": 0.0006,
      "step": 69060
    },
    {
      "epoch": 31.962054604349838,
      "grad_norm": 0.0032072020694613457,
      "learning_rate": 0.03607589079130032,
      "loss": 0.0001,
      "step": 69070
    },
    {
      "epoch": 31.966682091624246,
      "grad_norm": 0.006100452039390802,
      "learning_rate": 0.036066635816751506,
      "loss": 0.0,
      "step": 69080
    },
    {
      "epoch": 31.97130957889866,
      "grad_norm": 0.008503555320203304,
      "learning_rate": 0.03605738084220269,
      "loss": 0.0001,
      "step": 69090
    },
    {
      "epoch": 31.975937066173067,
      "grad_norm": 0.0015013678930699825,
      "learning_rate": 0.036048125867653866,
      "loss": 0.0001,
      "step": 69100
    },
    {
      "epoch": 31.98056455344748,
      "grad_norm": 0.008537808433175087,
      "learning_rate": 0.03603887089310505,
      "loss": 0.0002,
      "step": 69110
    },
    {
      "epoch": 31.985192040721888,
      "grad_norm": 0.006152103189378977,
      "learning_rate": 0.036029615918556225,
      "loss": 0.0001,
      "step": 69120
    },
    {
      "epoch": 31.989819527996296,
      "grad_norm": 0.0005724224029108882,
      "learning_rate": 0.0360203609440074,
      "loss": 0.0001,
      "step": 69130
    },
    {
      "epoch": 31.99444701527071,
      "grad_norm": 0.007396458648145199,
      "learning_rate": 0.036011105969458584,
      "loss": 0.0001,
      "step": 69140
    },
    {
      "epoch": 31.999074502545117,
      "grad_norm": 3.1943132877349854,
      "learning_rate": 0.03600185099490976,
      "loss": 0.0009,
      "step": 69150
    },
    {
      "epoch": 32.0,
      "eval_accuracy_branch1": 0.9913726698505623,
      "eval_accuracy_branch2": 0.49946079186566017,
      "eval_f1_branch1": 0.9918807975057509,
      "eval_f1_branch2": 0.4993466707413541,
      "eval_loss": 0.016789816319942474,
      "eval_precision_branch1": 0.9917814067941922,
      "eval_precision_branch2": 0.49946029977908124,
      "eval_recall_branch1": 0.9921027348186933,
      "eval_recall_branch2": 0.49946079186566017,
      "eval_runtime": 28.9655,
      "eval_samples_per_second": 448.188,
      "eval_steps_per_second": 56.032,
      "step": 69152
    },
    {
      "epoch": 32.003701989819525,
      "grad_norm": 0.022957833483815193,
      "learning_rate": 0.035992596020360944,
      "loss": 0.0002,
      "step": 69160
    },
    {
      "epoch": 32.00832947709394,
      "grad_norm": 0.00400114431977272,
      "learning_rate": 0.03598334104581213,
      "loss": 0.0005,
      "step": 69170
    },
    {
      "epoch": 32.01295696436835,
      "grad_norm": 0.19739732146263123,
      "learning_rate": 0.03597408607126331,
      "loss": 0.0002,
      "step": 69180
    },
    {
      "epoch": 32.017584451642755,
      "grad_norm": 0.3046009838581085,
      "learning_rate": 0.035964831096714486,
      "loss": 0.0002,
      "step": 69190
    },
    {
      "epoch": 32.02221193891717,
      "grad_norm": 0.012628091499209404,
      "learning_rate": 0.03595557612216567,
      "loss": 0.0001,
      "step": 69200
    },
    {
      "epoch": 32.02683942619158,
      "grad_norm": 1.1734294891357422,
      "learning_rate": 0.035946321147616846,
      "loss": 0.0006,
      "step": 69210
    },
    {
      "epoch": 32.03146691346599,
      "grad_norm": 0.00522772828117013,
      "learning_rate": 0.03593706617306802,
      "loss": 0.0005,
      "step": 69220
    },
    {
      "epoch": 32.036094400740396,
      "grad_norm": 0.03133409097790718,
      "learning_rate": 0.035927811198519205,
      "loss": 0.0001,
      "step": 69230
    },
    {
      "epoch": 32.04072188801481,
      "grad_norm": 0.01525968685746193,
      "learning_rate": 0.03591855622397038,
      "loss": 0.0004,
      "step": 69240
    },
    {
      "epoch": 32.04534937528922,
      "grad_norm": 0.0044267140328884125,
      "learning_rate": 0.035909301249421564,
      "loss": 0.0001,
      "step": 69250
    },
    {
      "epoch": 32.049976862563625,
      "grad_norm": 0.005072270520031452,
      "learning_rate": 0.03590004627487275,
      "loss": 0.0001,
      "step": 69260
    },
    {
      "epoch": 32.05460434983804,
      "grad_norm": 0.003618841292336583,
      "learning_rate": 0.03589079130032393,
      "loss": 0.0001,
      "step": 69270
    },
    {
      "epoch": 32.05923183711245,
      "grad_norm": 0.028714381158351898,
      "learning_rate": 0.03588153632577511,
      "loss": 0.0003,
      "step": 69280
    },
    {
      "epoch": 32.06385932438686,
      "grad_norm": 0.0007556830532848835,
      "learning_rate": 0.03587228135122628,
      "loss": 0.0002,
      "step": 69290
    },
    {
      "epoch": 32.068486811661266,
      "grad_norm": 0.0008907166193239391,
      "learning_rate": 0.035863026376677466,
      "loss": 0.0001,
      "step": 69300
    },
    {
      "epoch": 32.07311429893568,
      "grad_norm": 0.0009110462269745767,
      "learning_rate": 0.03585377140212864,
      "loss": 0.0001,
      "step": 69310
    },
    {
      "epoch": 32.07774178621009,
      "grad_norm": 0.003388437908142805,
      "learning_rate": 0.035844516427579826,
      "loss": 0.0003,
      "step": 69320
    },
    {
      "epoch": 32.082369273484495,
      "grad_norm": 0.003677849192172289,
      "learning_rate": 0.035835261453031,
      "loss": 0.0001,
      "step": 69330
    },
    {
      "epoch": 32.08699676075891,
      "grad_norm": 0.08419269323348999,
      "learning_rate": 0.035826006478482185,
      "loss": 0.0001,
      "step": 69340
    },
    {
      "epoch": 32.09162424803332,
      "grad_norm": 0.35349926352500916,
      "learning_rate": 0.03581675150393337,
      "loss": 0.0002,
      "step": 69350
    },
    {
      "epoch": 32.096251735307725,
      "grad_norm": 0.0252231415361166,
      "learning_rate": 0.035807496529384544,
      "loss": 0.0086,
      "step": 69360
    },
    {
      "epoch": 32.10087922258214,
      "grad_norm": 0.008670921437442303,
      "learning_rate": 0.03579824155483573,
      "loss": 0.0001,
      "step": 69370
    },
    {
      "epoch": 32.10550670985655,
      "grad_norm": 0.0013085290556773543,
      "learning_rate": 0.035788986580286904,
      "loss": 0.0001,
      "step": 69380
    },
    {
      "epoch": 32.11013419713096,
      "grad_norm": 0.004581692162901163,
      "learning_rate": 0.03577973160573809,
      "loss": 0.0001,
      "step": 69390
    },
    {
      "epoch": 32.114761684405366,
      "grad_norm": 0.006217746064066887,
      "learning_rate": 0.03577047663118926,
      "loss": 0.0001,
      "step": 69400
    },
    {
      "epoch": 32.11938917167978,
      "grad_norm": 0.017886899411678314,
      "learning_rate": 0.035761221656640446,
      "loss": 0.0001,
      "step": 69410
    },
    {
      "epoch": 32.12401665895419,
      "grad_norm": 0.015006420202553272,
      "learning_rate": 0.03575196668209162,
      "loss": 0.0001,
      "step": 69420
    },
    {
      "epoch": 32.128644146228595,
      "grad_norm": 0.01829974725842476,
      "learning_rate": 0.035742711707542806,
      "loss": 0.0001,
      "step": 69430
    },
    {
      "epoch": 32.13327163350301,
      "grad_norm": 0.0076504177413880825,
      "learning_rate": 0.03573345673299399,
      "loss": 0.0001,
      "step": 69440
    },
    {
      "epoch": 32.13789912077742,
      "grad_norm": 0.008787344209849834,
      "learning_rate": 0.035724201758445165,
      "loss": 0.0001,
      "step": 69450
    },
    {
      "epoch": 32.14252660805183,
      "grad_norm": 0.005598023068159819,
      "learning_rate": 0.03571494678389635,
      "loss": 0.0002,
      "step": 69460
    },
    {
      "epoch": 32.147154095326236,
      "grad_norm": 0.0068616364151239395,
      "learning_rate": 0.035705691809347524,
      "loss": 0.0002,
      "step": 69470
    },
    {
      "epoch": 32.15178158260065,
      "grad_norm": 0.009906798601150513,
      "learning_rate": 0.03569643683479871,
      "loss": 0.0001,
      "step": 69480
    },
    {
      "epoch": 32.15640906987506,
      "grad_norm": 0.011901731602847576,
      "learning_rate": 0.035687181860249884,
      "loss": 0.0002,
      "step": 69490
    },
    {
      "epoch": 32.161036557149465,
      "grad_norm": 0.012080049142241478,
      "learning_rate": 0.03567792688570107,
      "loss": 0.0,
      "step": 69500
    },
    {
      "epoch": 32.16566404442388,
      "grad_norm": 0.0036275179591029882,
      "learning_rate": 0.03566867191115224,
      "loss": 0.0004,
      "step": 69510
    },
    {
      "epoch": 32.17029153169829,
      "grad_norm": 0.1344965398311615,
      "learning_rate": 0.035659416936603426,
      "loss": 0.003,
      "step": 69520
    },
    {
      "epoch": 32.174919018972695,
      "grad_norm": 0.00318447919562459,
      "learning_rate": 0.03565016196205461,
      "loss": 0.0001,
      "step": 69530
    },
    {
      "epoch": 32.17954650624711,
      "grad_norm": 0.04158615320920944,
      "learning_rate": 0.035640906987505785,
      "loss": 0.0001,
      "step": 69540
    },
    {
      "epoch": 32.18417399352152,
      "grad_norm": 0.003942613489925861,
      "learning_rate": 0.03563165201295697,
      "loss": 0.0,
      "step": 69550
    },
    {
      "epoch": 32.18880148079593,
      "grad_norm": 0.009421761147677898,
      "learning_rate": 0.035622397038408145,
      "loss": 0.0009,
      "step": 69560
    },
    {
      "epoch": 32.193428968070336,
      "grad_norm": 0.007206990849226713,
      "learning_rate": 0.03561314206385933,
      "loss": 0.0001,
      "step": 69570
    },
    {
      "epoch": 32.19805645534475,
      "grad_norm": 0.0014797362964600325,
      "learning_rate": 0.035603887089310504,
      "loss": 0.0001,
      "step": 69580
    },
    {
      "epoch": 32.20268394261916,
      "grad_norm": 0.0018904971657320857,
      "learning_rate": 0.03559463211476168,
      "loss": 0.0001,
      "step": 69590
    },
    {
      "epoch": 32.207311429893565,
      "grad_norm": 0.050907231867313385,
      "learning_rate": 0.035585377140212864,
      "loss": 0.0001,
      "step": 69600
    },
    {
      "epoch": 32.21193891716798,
      "grad_norm": 0.0035285488702356815,
      "learning_rate": 0.03557612216566405,
      "loss": 0.0003,
      "step": 69610
    },
    {
      "epoch": 32.21656640444239,
      "grad_norm": 0.0010037421016022563,
      "learning_rate": 0.03556686719111523,
      "loss": 0.0001,
      "step": 69620
    },
    {
      "epoch": 32.2211938917168,
      "grad_norm": 0.0032311431132256985,
      "learning_rate": 0.035557612216566406,
      "loss": 0.0001,
      "step": 69630
    },
    {
      "epoch": 32.225821378991206,
      "grad_norm": 0.05263428017497063,
      "learning_rate": 0.03554835724201759,
      "loss": 0.0001,
      "step": 69640
    },
    {
      "epoch": 32.23044886626562,
      "grad_norm": 0.009639043360948563,
      "learning_rate": 0.035539102267468765,
      "loss": 0.0001,
      "step": 69650
    },
    {
      "epoch": 32.23507635354003,
      "grad_norm": 0.004499060567468405,
      "learning_rate": 0.03552984729291995,
      "loss": 0.0001,
      "step": 69660
    },
    {
      "epoch": 32.239703840814435,
      "grad_norm": 0.0020085445139557123,
      "learning_rate": 0.035520592318371125,
      "loss": 0.0001,
      "step": 69670
    },
    {
      "epoch": 32.24433132808885,
      "grad_norm": 0.0017845474649220705,
      "learning_rate": 0.0355113373438223,
      "loss": 0.0001,
      "step": 69680
    },
    {
      "epoch": 32.24895881536326,
      "grad_norm": 0.012226907536387444,
      "learning_rate": 0.035502082369273484,
      "loss": 0.0001,
      "step": 69690
    },
    {
      "epoch": 32.253586302637665,
      "grad_norm": 0.21230487525463104,
      "learning_rate": 0.03549282739472467,
      "loss": 0.0141,
      "step": 69700
    },
    {
      "epoch": 32.25821378991208,
      "grad_norm": 0.00391556927934289,
      "learning_rate": 0.03548357242017585,
      "loss": 0.0001,
      "step": 69710
    },
    {
      "epoch": 32.26284127718649,
      "grad_norm": 0.04300335422158241,
      "learning_rate": 0.03547431744562703,
      "loss": 0.0001,
      "step": 69720
    },
    {
      "epoch": 32.2674687644609,
      "grad_norm": 0.004002813715487719,
      "learning_rate": 0.03546506247107821,
      "loss": 0.0001,
      "step": 69730
    },
    {
      "epoch": 32.272096251735306,
      "grad_norm": 0.022692354395985603,
      "learning_rate": 0.035455807496529386,
      "loss": 0.0001,
      "step": 69740
    },
    {
      "epoch": 32.27672373900972,
      "grad_norm": 0.00218021753244102,
      "learning_rate": 0.03544655252198056,
      "loss": 0.0001,
      "step": 69750
    },
    {
      "epoch": 32.28135122628413,
      "grad_norm": 0.0023768411483615637,
      "learning_rate": 0.035437297547431745,
      "loss": 0.0001,
      "step": 69760
    },
    {
      "epoch": 32.285978713558535,
      "grad_norm": 0.0036111921072006226,
      "learning_rate": 0.03542804257288292,
      "loss": 0.0001,
      "step": 69770
    },
    {
      "epoch": 32.29060620083295,
      "grad_norm": 0.0023374236188828945,
      "learning_rate": 0.035418787598334105,
      "loss": 0.0,
      "step": 69780
    },
    {
      "epoch": 32.29523368810736,
      "grad_norm": 0.008604363538324833,
      "learning_rate": 0.03540953262378529,
      "loss": 0.0001,
      "step": 69790
    },
    {
      "epoch": 32.299861175381764,
      "grad_norm": 0.014028656296432018,
      "learning_rate": 0.03540027764923647,
      "loss": 0.0,
      "step": 69800
    },
    {
      "epoch": 32.304488662656176,
      "grad_norm": 1.0585383176803589,
      "learning_rate": 0.03539102267468765,
      "loss": 0.0003,
      "step": 69810
    },
    {
      "epoch": 32.30911614993059,
      "grad_norm": 0.10799537599086761,
      "learning_rate": 0.03538176770013882,
      "loss": 0.0001,
      "step": 69820
    },
    {
      "epoch": 32.313743637205,
      "grad_norm": 0.0013289505150169134,
      "learning_rate": 0.03537251272559001,
      "loss": 0.0003,
      "step": 69830
    },
    {
      "epoch": 32.318371124479405,
      "grad_norm": 0.01184007991105318,
      "learning_rate": 0.03536325775104118,
      "loss": 0.0001,
      "step": 69840
    },
    {
      "epoch": 32.32299861175382,
      "grad_norm": 0.014287690632045269,
      "learning_rate": 0.035354002776492366,
      "loss": 0.0001,
      "step": 69850
    },
    {
      "epoch": 32.32762609902823,
      "grad_norm": 0.17860198020935059,
      "learning_rate": 0.03534474780194354,
      "loss": 0.0001,
      "step": 69860
    },
    {
      "epoch": 32.332253586302635,
      "grad_norm": 0.0006679223733954132,
      "learning_rate": 0.035335492827394725,
      "loss": 0.0003,
      "step": 69870
    },
    {
      "epoch": 32.33688107357705,
      "grad_norm": 0.003616795176640153,
      "learning_rate": 0.03532623785284591,
      "loss": 0.0004,
      "step": 69880
    },
    {
      "epoch": 32.34150856085146,
      "grad_norm": 0.06299523264169693,
      "learning_rate": 0.03531698287829709,
      "loss": 0.0002,
      "step": 69890
    },
    {
      "epoch": 32.34613604812587,
      "grad_norm": 0.0013744416646659374,
      "learning_rate": 0.03530772790374827,
      "loss": 0.0,
      "step": 69900
    },
    {
      "epoch": 32.350763535400276,
      "grad_norm": 0.0037310810294002295,
      "learning_rate": 0.035298472929199444,
      "loss": 0.0,
      "step": 69910
    },
    {
      "epoch": 32.35539102267469,
      "grad_norm": 0.005111068021506071,
      "learning_rate": 0.03528921795465063,
      "loss": 0.0001,
      "step": 69920
    },
    {
      "epoch": 32.3600185099491,
      "grad_norm": 0.005854192189872265,
      "learning_rate": 0.0352799629801018,
      "loss": 0.0,
      "step": 69930
    },
    {
      "epoch": 32.364645997223505,
      "grad_norm": 0.003453464712947607,
      "learning_rate": 0.035270708005552986,
      "loss": 0.0008,
      "step": 69940
    },
    {
      "epoch": 32.36927348449792,
      "grad_norm": 0.025113947689533234,
      "learning_rate": 0.03526145303100416,
      "loss": 0.0001,
      "step": 69950
    },
    {
      "epoch": 32.37390097177233,
      "grad_norm": 0.034293677657842636,
      "learning_rate": 0.035252198056455346,
      "loss": 0.0003,
      "step": 69960
    },
    {
      "epoch": 32.378528459046734,
      "grad_norm": 0.0003769931790884584,
      "learning_rate": 0.03524294308190653,
      "loss": 0.0001,
      "step": 69970
    },
    {
      "epoch": 32.383155946321146,
      "grad_norm": 0.006423321086913347,
      "learning_rate": 0.035233688107357705,
      "loss": 0.0001,
      "step": 69980
    },
    {
      "epoch": 32.38778343359556,
      "grad_norm": 0.3316114544868469,
      "learning_rate": 0.03522443313280889,
      "loss": 0.0004,
      "step": 69990
    },
    {
      "epoch": 32.39241092086997,
      "grad_norm": 0.5596111416816711,
      "learning_rate": 0.035215178158260065,
      "loss": 0.0002,
      "step": 70000
    },
    {
      "epoch": 32.397038408144375,
      "grad_norm": 0.07421939820051193,
      "learning_rate": 0.03520592318371125,
      "loss": 0.0002,
      "step": 70010
    },
    {
      "epoch": 32.40166589541879,
      "grad_norm": 0.03849460929632187,
      "learning_rate": 0.035196668209162424,
      "loss": 0.0004,
      "step": 70020
    },
    {
      "epoch": 32.4062933826932,
      "grad_norm": 0.008312785066664219,
      "learning_rate": 0.03518741323461361,
      "loss": 0.0001,
      "step": 70030
    },
    {
      "epoch": 32.410920869967605,
      "grad_norm": 0.014298001304268837,
      "learning_rate": 0.03517815826006478,
      "loss": 0.0002,
      "step": 70040
    },
    {
      "epoch": 32.41554835724202,
      "grad_norm": 0.009660406038165092,
      "learning_rate": 0.035168903285515966,
      "loss": 0.0,
      "step": 70050
    },
    {
      "epoch": 32.42017584451643,
      "grad_norm": 0.03779922053217888,
      "learning_rate": 0.03515964831096715,
      "loss": 0.0002,
      "step": 70060
    },
    {
      "epoch": 32.42480333179084,
      "grad_norm": 0.01425747200846672,
      "learning_rate": 0.035150393336418326,
      "loss": 0.0001,
      "step": 70070
    },
    {
      "epoch": 32.429430819065246,
      "grad_norm": 0.011553490534424782,
      "learning_rate": 0.03514113836186951,
      "loss": 0.0001,
      "step": 70080
    },
    {
      "epoch": 32.43405830633966,
      "grad_norm": 0.010945692658424377,
      "learning_rate": 0.035131883387320685,
      "loss": 0.0001,
      "step": 70090
    },
    {
      "epoch": 32.43868579361407,
      "grad_norm": 0.004513590130954981,
      "learning_rate": 0.03512262841277187,
      "loss": 0.0,
      "step": 70100
    },
    {
      "epoch": 32.443313280888475,
      "grad_norm": 11.574921607971191,
      "learning_rate": 0.035113373438223044,
      "loss": 0.003,
      "step": 70110
    },
    {
      "epoch": 32.44794076816289,
      "grad_norm": 0.0003951745165977627,
      "learning_rate": 0.03510411846367423,
      "loss": 0.0001,
      "step": 70120
    },
    {
      "epoch": 32.4525682554373,
      "grad_norm": 0.007734299637377262,
      "learning_rate": 0.035094863489125404,
      "loss": 0.0002,
      "step": 70130
    },
    {
      "epoch": 32.457195742711704,
      "grad_norm": 0.07806944847106934,
      "learning_rate": 0.03508560851457659,
      "loss": 0.0002,
      "step": 70140
    },
    {
      "epoch": 32.461823229986116,
      "grad_norm": 0.0212250929325819,
      "learning_rate": 0.03507635354002777,
      "loss": 0.0001,
      "step": 70150
    },
    {
      "epoch": 32.46645071726053,
      "grad_norm": 0.006030082702636719,
      "learning_rate": 0.035067098565478946,
      "loss": 0.0001,
      "step": 70160
    },
    {
      "epoch": 32.47107820453494,
      "grad_norm": 2.4015896320343018,
      "learning_rate": 0.03505784359093013,
      "loss": 0.0012,
      "step": 70170
    },
    {
      "epoch": 32.475705691809345,
      "grad_norm": 0.09268812090158463,
      "learning_rate": 0.035048588616381306,
      "loss": 0.0001,
      "step": 70180
    },
    {
      "epoch": 32.48033317908376,
      "grad_norm": 0.0018146170768886805,
      "learning_rate": 0.03503933364183249,
      "loss": 0.0001,
      "step": 70190
    },
    {
      "epoch": 32.48496066635817,
      "grad_norm": 0.055697448551654816,
      "learning_rate": 0.035030078667283665,
      "loss": 0.0001,
      "step": 70200
    },
    {
      "epoch": 32.489588153632575,
      "grad_norm": 0.0025794811081141233,
      "learning_rate": 0.03502082369273484,
      "loss": 0.0001,
      "step": 70210
    },
    {
      "epoch": 32.49421564090699,
      "grad_norm": 0.08557706326246262,
      "learning_rate": 0.035011568718186024,
      "loss": 0.0001,
      "step": 70220
    },
    {
      "epoch": 32.4988431281814,
      "grad_norm": 0.0029216783586889505,
      "learning_rate": 0.03500231374363721,
      "loss": 0.0,
      "step": 70230
    },
    {
      "epoch": 32.50347061545581,
      "grad_norm": 0.0621052160859108,
      "learning_rate": 0.03499305876908839,
      "loss": 0.0001,
      "step": 70240
    },
    {
      "epoch": 32.508098102730216,
      "grad_norm": 0.003638965543359518,
      "learning_rate": 0.03498380379453957,
      "loss": 0.0001,
      "step": 70250
    },
    {
      "epoch": 32.51272559000463,
      "grad_norm": 0.05027693137526512,
      "learning_rate": 0.03497454881999075,
      "loss": 0.0001,
      "step": 70260
    },
    {
      "epoch": 32.51735307727904,
      "grad_norm": 0.015094145201146603,
      "learning_rate": 0.034965293845441926,
      "loss": 0.0006,
      "step": 70270
    },
    {
      "epoch": 32.521980564553445,
      "grad_norm": 0.005547666922211647,
      "learning_rate": 0.0349560388708931,
      "loss": 0.0001,
      "step": 70280
    },
    {
      "epoch": 32.52660805182786,
      "grad_norm": 0.014238161966204643,
      "learning_rate": 0.034946783896344286,
      "loss": 0.0002,
      "step": 70290
    },
    {
      "epoch": 32.53123553910227,
      "grad_norm": 0.01838221400976181,
      "learning_rate": 0.03493752892179546,
      "loss": 0.0001,
      "step": 70300
    },
    {
      "epoch": 32.535863026376674,
      "grad_norm": 0.06861443817615509,
      "learning_rate": 0.034928273947246645,
      "loss": 0.0001,
      "step": 70310
    },
    {
      "epoch": 32.540490513651086,
      "grad_norm": 0.0027077780105173588,
      "learning_rate": 0.03491901897269783,
      "loss": 0.0,
      "step": 70320
    },
    {
      "epoch": 32.5451180009255,
      "grad_norm": 0.02321467734873295,
      "learning_rate": 0.03490976399814901,
      "loss": 0.0002,
      "step": 70330
    },
    {
      "epoch": 32.54974548819991,
      "grad_norm": 0.005125737749040127,
      "learning_rate": 0.03490050902360019,
      "loss": 0.0001,
      "step": 70340
    },
    {
      "epoch": 32.554372975474315,
      "grad_norm": 0.0035323328338563442,
      "learning_rate": 0.03489125404905137,
      "loss": 0.0001,
      "step": 70350
    },
    {
      "epoch": 32.55900046274873,
      "grad_norm": 0.005944623611867428,
      "learning_rate": 0.03488199907450255,
      "loss": 0.0001,
      "step": 70360
    },
    {
      "epoch": 32.56362795002314,
      "grad_norm": 0.0027023989241570234,
      "learning_rate": 0.03487274409995372,
      "loss": 0.0002,
      "step": 70370
    },
    {
      "epoch": 32.568255437297545,
      "grad_norm": 0.07627294212579727,
      "learning_rate": 0.034863489125404906,
      "loss": 0.0001,
      "step": 70380
    },
    {
      "epoch": 32.57288292457196,
      "grad_norm": 0.0364760160446167,
      "learning_rate": 0.03485423415085608,
      "loss": 0.0001,
      "step": 70390
    },
    {
      "epoch": 32.57751041184637,
      "grad_norm": 0.001234772615134716,
      "learning_rate": 0.034844979176307266,
      "loss": 0.0001,
      "step": 70400
    },
    {
      "epoch": 32.58213789912078,
      "grad_norm": 0.010069262236356735,
      "learning_rate": 0.03483572420175845,
      "loss": 0.0001,
      "step": 70410
    },
    {
      "epoch": 32.586765386395186,
      "grad_norm": 0.0036415420472621918,
      "learning_rate": 0.03482646922720963,
      "loss": 0.0001,
      "step": 70420
    },
    {
      "epoch": 32.5913928736696,
      "grad_norm": 0.003681734437122941,
      "learning_rate": 0.03481721425266081,
      "loss": 0.0001,
      "step": 70430
    },
    {
      "epoch": 32.59602036094401,
      "grad_norm": 0.006294035818427801,
      "learning_rate": 0.034807959278111984,
      "loss": 0.0001,
      "step": 70440
    },
    {
      "epoch": 32.600647848218415,
      "grad_norm": 0.0063531105406582355,
      "learning_rate": 0.03479870430356317,
      "loss": 0.0,
      "step": 70450
    },
    {
      "epoch": 32.60527533549283,
      "grad_norm": 0.0042043025605380535,
      "learning_rate": 0.034789449329014344,
      "loss": 0.0002,
      "step": 70460
    },
    {
      "epoch": 32.60990282276724,
      "grad_norm": 0.00379451853223145,
      "learning_rate": 0.03478019435446553,
      "loss": 0.0002,
      "step": 70470
    },
    {
      "epoch": 32.614530310041644,
      "grad_norm": 0.0009974543936550617,
      "learning_rate": 0.0347709393799167,
      "loss": 0.0003,
      "step": 70480
    },
    {
      "epoch": 32.619157797316056,
      "grad_norm": 0.0008683729101903737,
      "learning_rate": 0.034761684405367886,
      "loss": 0.0,
      "step": 70490
    },
    {
      "epoch": 32.62378528459047,
      "grad_norm": 0.0007929688435979187,
      "learning_rate": 0.03475242943081907,
      "loss": 0.0,
      "step": 70500
    },
    {
      "epoch": 32.62841277186488,
      "grad_norm": 0.018433265388011932,
      "learning_rate": 0.034743174456270245,
      "loss": 0.0002,
      "step": 70510
    },
    {
      "epoch": 32.633040259139285,
      "grad_norm": 0.07963793724775314,
      "learning_rate": 0.03473391948172143,
      "loss": 0.0001,
      "step": 70520
    },
    {
      "epoch": 32.6376677464137,
      "grad_norm": 0.20106898248195648,
      "learning_rate": 0.034724664507172605,
      "loss": 0.0001,
      "step": 70530
    },
    {
      "epoch": 32.64229523368811,
      "grad_norm": 0.006207883358001709,
      "learning_rate": 0.03471540953262379,
      "loss": 0.0,
      "step": 70540
    },
    {
      "epoch": 32.646922720962515,
      "grad_norm": 0.00314126955345273,
      "learning_rate": 0.034706154558074964,
      "loss": 0.0003,
      "step": 70550
    },
    {
      "epoch": 32.65155020823693,
      "grad_norm": 0.001764620654284954,
      "learning_rate": 0.03469689958352615,
      "loss": 0.0,
      "step": 70560
    },
    {
      "epoch": 32.65617769551134,
      "grad_norm": 0.0014678037259727716,
      "learning_rate": 0.034687644608977324,
      "loss": 0.0001,
      "step": 70570
    },
    {
      "epoch": 32.66080518278575,
      "grad_norm": 0.038601018488407135,
      "learning_rate": 0.03467838963442851,
      "loss": 0.0001,
      "step": 70580
    },
    {
      "epoch": 32.665432670060156,
      "grad_norm": 0.0027964769396930933,
      "learning_rate": 0.03466913465987969,
      "loss": 0.0001,
      "step": 70590
    },
    {
      "epoch": 32.67006015733457,
      "grad_norm": 0.017165083438158035,
      "learning_rate": 0.034659879685330866,
      "loss": 0.0001,
      "step": 70600
    },
    {
      "epoch": 32.67468764460898,
      "grad_norm": 0.032471634447574615,
      "learning_rate": 0.03465062471078205,
      "loss": 0.0003,
      "step": 70610
    },
    {
      "epoch": 32.679315131883385,
      "grad_norm": 0.0011195656843483448,
      "learning_rate": 0.034641369736233225,
      "loss": 0.0001,
      "step": 70620
    },
    {
      "epoch": 32.6839426191578,
      "grad_norm": 0.14825411140918732,
      "learning_rate": 0.03463211476168441,
      "loss": 0.0003,
      "step": 70630
    },
    {
      "epoch": 32.68857010643221,
      "grad_norm": 0.0500100813806057,
      "learning_rate": 0.034622859787135585,
      "loss": 0.0001,
      "step": 70640
    },
    {
      "epoch": 32.693197593706614,
      "grad_norm": 0.08557072281837463,
      "learning_rate": 0.03461360481258677,
      "loss": 0.0002,
      "step": 70650
    },
    {
      "epoch": 32.697825080981026,
      "grad_norm": 0.00534787867218256,
      "learning_rate": 0.034604349838037944,
      "loss": 0.0002,
      "step": 70660
    },
    {
      "epoch": 32.70245256825544,
      "grad_norm": 0.73402339220047,
      "learning_rate": 0.03459509486348913,
      "loss": 0.0002,
      "step": 70670
    },
    {
      "epoch": 32.70708005552985,
      "grad_norm": 0.0030730506405234337,
      "learning_rate": 0.03458583988894031,
      "loss": 0.0001,
      "step": 70680
    },
    {
      "epoch": 32.711707542804255,
      "grad_norm": 0.004553751088678837,
      "learning_rate": 0.03457658491439149,
      "loss": 0.0001,
      "step": 70690
    },
    {
      "epoch": 32.71633503007867,
      "grad_norm": 0.02986910007894039,
      "learning_rate": 0.03456732993984267,
      "loss": 0.0001,
      "step": 70700
    },
    {
      "epoch": 32.72096251735308,
      "grad_norm": 0.004072166513651609,
      "learning_rate": 0.034558074965293846,
      "loss": 0.0001,
      "step": 70710
    },
    {
      "epoch": 32.725590004627485,
      "grad_norm": 0.005421018227934837,
      "learning_rate": 0.03454881999074503,
      "loss": 0.0001,
      "step": 70720
    },
    {
      "epoch": 32.7302174919019,
      "grad_norm": 0.020190605893731117,
      "learning_rate": 0.034539565016196205,
      "loss": 0.0001,
      "step": 70730
    },
    {
      "epoch": 32.73484497917631,
      "grad_norm": 0.0032204161398112774,
      "learning_rate": 0.03453031004164738,
      "loss": 0.0003,
      "step": 70740
    },
    {
      "epoch": 32.73947246645072,
      "grad_norm": 0.0016521528596058488,
      "learning_rate": 0.034521055067098565,
      "loss": 0.0001,
      "step": 70750
    },
    {
      "epoch": 32.744099953725126,
      "grad_norm": 0.018071556463837624,
      "learning_rate": 0.03451180009254975,
      "loss": 0.0003,
      "step": 70760
    },
    {
      "epoch": 32.74872744099954,
      "grad_norm": 0.001976423431187868,
      "learning_rate": 0.03450254511800093,
      "loss": 0.0001,
      "step": 70770
    },
    {
      "epoch": 32.75335492827395,
      "grad_norm": 0.031307097524404526,
      "learning_rate": 0.03449329014345211,
      "loss": 0.0002,
      "step": 70780
    },
    {
      "epoch": 32.757982415548355,
      "grad_norm": 0.0012705822009593248,
      "learning_rate": 0.03448403516890329,
      "loss": 0.0001,
      "step": 70790
    },
    {
      "epoch": 32.76260990282277,
      "grad_norm": 0.0017779218032956123,
      "learning_rate": 0.03447478019435447,
      "loss": 0.0001,
      "step": 70800
    },
    {
      "epoch": 32.76723739009718,
      "grad_norm": 0.0006731925532221794,
      "learning_rate": 0.03446552521980564,
      "loss": 0.0004,
      "step": 70810
    },
    {
      "epoch": 32.771864877371584,
      "grad_norm": 0.01575840637087822,
      "learning_rate": 0.034456270245256826,
      "loss": 0.0001,
      "step": 70820
    },
    {
      "epoch": 32.776492364645996,
      "grad_norm": 0.0024076940026134253,
      "learning_rate": 0.034447015270708,
      "loss": 0.0001,
      "step": 70830
    },
    {
      "epoch": 32.78111985192041,
      "grad_norm": 0.040755126625299454,
      "learning_rate": 0.034437760296159185,
      "loss": 0.0001,
      "step": 70840
    },
    {
      "epoch": 32.78574733919482,
      "grad_norm": 0.019646327942609787,
      "learning_rate": 0.03442850532161037,
      "loss": 0.0001,
      "step": 70850
    },
    {
      "epoch": 32.790374826469225,
      "grad_norm": 0.0072497776709496975,
      "learning_rate": 0.03441925034706155,
      "loss": 0.0001,
      "step": 70860
    },
    {
      "epoch": 32.79500231374364,
      "grad_norm": 0.0033340263180434704,
      "learning_rate": 0.03440999537251273,
      "loss": 0.0001,
      "step": 70870
    },
    {
      "epoch": 32.79962980101805,
      "grad_norm": 0.0019266799790784717,
      "learning_rate": 0.03440074039796391,
      "loss": 0.0001,
      "step": 70880
    },
    {
      "epoch": 32.804257288292455,
      "grad_norm": 0.009686745703220367,
      "learning_rate": 0.03439148542341509,
      "loss": 0.0002,
      "step": 70890
    },
    {
      "epoch": 32.80888477556687,
      "grad_norm": 0.00421717157587409,
      "learning_rate": 0.03438223044886626,
      "loss": 0.0001,
      "step": 70900
    },
    {
      "epoch": 32.81351226284128,
      "grad_norm": 0.0012502791360020638,
      "learning_rate": 0.034372975474317446,
      "loss": 0.0001,
      "step": 70910
    },
    {
      "epoch": 32.818139750115684,
      "grad_norm": 0.0017848695861175656,
      "learning_rate": 0.03436372049976862,
      "loss": 0.0001,
      "step": 70920
    },
    {
      "epoch": 32.822767237390096,
      "grad_norm": 0.005363628268241882,
      "learning_rate": 0.034354465525219806,
      "loss": 0.0001,
      "step": 70930
    },
    {
      "epoch": 32.82739472466451,
      "grad_norm": 0.004329780116677284,
      "learning_rate": 0.03434521055067099,
      "loss": 0.0001,
      "step": 70940
    },
    {
      "epoch": 32.83202221193892,
      "grad_norm": 0.015776734799146652,
      "learning_rate": 0.03433595557612217,
      "loss": 0.0001,
      "step": 70950
    },
    {
      "epoch": 32.836649699213325,
      "grad_norm": 0.0005203403416089714,
      "learning_rate": 0.03432670060157335,
      "loss": 0.0,
      "step": 70960
    },
    {
      "epoch": 32.84127718648774,
      "grad_norm": 0.013871523551642895,
      "learning_rate": 0.034317445627024525,
      "loss": 0.0001,
      "step": 70970
    },
    {
      "epoch": 32.84590467376215,
      "grad_norm": 0.001431011944077909,
      "learning_rate": 0.03430819065247571,
      "loss": 0.0001,
      "step": 70980
    },
    {
      "epoch": 32.850532161036554,
      "grad_norm": 0.03541790693998337,
      "learning_rate": 0.034298935677926884,
      "loss": 0.0001,
      "step": 70990
    },
    {
      "epoch": 32.855159648310966,
      "grad_norm": 0.001189281465485692,
      "learning_rate": 0.03428968070337807,
      "loss": 0.0001,
      "step": 71000
    },
    {
      "epoch": 32.85978713558538,
      "grad_norm": 0.009519083425402641,
      "learning_rate": 0.03428042572882924,
      "loss": 0.0,
      "step": 71010
    },
    {
      "epoch": 32.86441462285979,
      "grad_norm": 0.002794675761833787,
      "learning_rate": 0.034271170754280426,
      "loss": 0.0001,
      "step": 71020
    },
    {
      "epoch": 32.869042110134195,
      "grad_norm": 0.0024997806176543236,
      "learning_rate": 0.03426191577973161,
      "loss": 0.0001,
      "step": 71030
    },
    {
      "epoch": 32.87366959740861,
      "grad_norm": 0.002575938357040286,
      "learning_rate": 0.034252660805182786,
      "loss": 0.0001,
      "step": 71040
    },
    {
      "epoch": 32.87829708468302,
      "grad_norm": 0.0387638583779335,
      "learning_rate": 0.03424340583063397,
      "loss": 0.0001,
      "step": 71050
    },
    {
      "epoch": 32.882924571957425,
      "grad_norm": 0.0007583737606182694,
      "learning_rate": 0.034234150856085145,
      "loss": 0.0002,
      "step": 71060
    },
    {
      "epoch": 32.88755205923184,
      "grad_norm": 0.0050900462083518505,
      "learning_rate": 0.03422489588153633,
      "loss": 0.0001,
      "step": 71070
    },
    {
      "epoch": 32.89217954650625,
      "grad_norm": 0.003332258202135563,
      "learning_rate": 0.034215640906987504,
      "loss": 0.0002,
      "step": 71080
    },
    {
      "epoch": 32.896807033780654,
      "grad_norm": 0.008084218017756939,
      "learning_rate": 0.03420638593243869,
      "loss": 0.0004,
      "step": 71090
    },
    {
      "epoch": 32.901434521055066,
      "grad_norm": 0.0010406853398308158,
      "learning_rate": 0.034197130957889864,
      "loss": 0.0001,
      "step": 71100
    },
    {
      "epoch": 32.90606200832948,
      "grad_norm": 0.002024355111643672,
      "learning_rate": 0.03418787598334105,
      "loss": 0.0001,
      "step": 71110
    },
    {
      "epoch": 32.91068949560389,
      "grad_norm": 0.0040872409008443356,
      "learning_rate": 0.03417862100879223,
      "loss": 0.0001,
      "step": 71120
    },
    {
      "epoch": 32.915316982878295,
      "grad_norm": 0.008920456282794476,
      "learning_rate": 0.034169366034243406,
      "loss": 0.0001,
      "step": 71130
    },
    {
      "epoch": 32.91994447015271,
      "grad_norm": 0.08766056597232819,
      "learning_rate": 0.03416011105969459,
      "loss": 0.0001,
      "step": 71140
    },
    {
      "epoch": 32.92457195742712,
      "grad_norm": 0.00431795371696353,
      "learning_rate": 0.034150856085145766,
      "loss": 0.0001,
      "step": 71150
    },
    {
      "epoch": 32.929199444701524,
      "grad_norm": 0.0009889549110084772,
      "learning_rate": 0.03414160111059695,
      "loss": 0.0003,
      "step": 71160
    },
    {
      "epoch": 32.933826931975936,
      "grad_norm": 0.00055901292944327,
      "learning_rate": 0.034132346136048125,
      "loss": 0.0001,
      "step": 71170
    },
    {
      "epoch": 32.93845441925035,
      "grad_norm": 0.019562916830182076,
      "learning_rate": 0.03412309116149931,
      "loss": 0.0001,
      "step": 71180
    },
    {
      "epoch": 32.94308190652476,
      "grad_norm": 0.008723326027393341,
      "learning_rate": 0.034113836186950484,
      "loss": 0.0001,
      "step": 71190
    },
    {
      "epoch": 32.947709393799165,
      "grad_norm": 0.0019048736430704594,
      "learning_rate": 0.03410458121240167,
      "loss": 0.0001,
      "step": 71200
    },
    {
      "epoch": 32.95233688107358,
      "grad_norm": 0.0010776221752166748,
      "learning_rate": 0.03409532623785285,
      "loss": 0.0001,
      "step": 71210
    },
    {
      "epoch": 32.95696436834799,
      "grad_norm": 0.030388472601771355,
      "learning_rate": 0.03408607126330403,
      "loss": 0.0001,
      "step": 71220
    },
    {
      "epoch": 32.961591855622395,
      "grad_norm": 0.0037599969655275345,
      "learning_rate": 0.03407681628875521,
      "loss": 0.0,
      "step": 71230
    },
    {
      "epoch": 32.96621934289681,
      "grad_norm": 0.0051175751723349094,
      "learning_rate": 0.034067561314206386,
      "loss": 0.0001,
      "step": 71240
    },
    {
      "epoch": 32.97084683017122,
      "grad_norm": 0.06799250841140747,
      "learning_rate": 0.03405830633965757,
      "loss": 0.0003,
      "step": 71250
    },
    {
      "epoch": 32.975474317445624,
      "grad_norm": 0.0016771572409197688,
      "learning_rate": 0.034049051365108746,
      "loss": 0.0001,
      "step": 71260
    },
    {
      "epoch": 32.980101804720036,
      "grad_norm": 0.0024945074692368507,
      "learning_rate": 0.03403979639055992,
      "loss": 0.0015,
      "step": 71270
    },
    {
      "epoch": 32.98472929199445,
      "grad_norm": 0.02882757969200611,
      "learning_rate": 0.034030541416011105,
      "loss": 0.0001,
      "step": 71280
    },
    {
      "epoch": 32.98935677926886,
      "grad_norm": 0.001620694762095809,
      "learning_rate": 0.03402128644146229,
      "loss": 0.0003,
      "step": 71290
    },
    {
      "epoch": 32.993984266543265,
      "grad_norm": 0.004076272249221802,
      "learning_rate": 0.03401203146691347,
      "loss": 0.0001,
      "step": 71300
    },
    {
      "epoch": 32.99861175381768,
      "grad_norm": 0.002521863207221031,
      "learning_rate": 0.03400277649236465,
      "loss": 0.0003,
      "step": 71310
    },
    {
      "epoch": 33.0,
      "eval_accuracy_branch1": 0.9927592050531505,
      "eval_accuracy_branch2": 0.5000770297334771,
      "eval_f1_branch1": 0.9929753567590157,
      "eval_f1_branch2": 0.49940967626141386,
      "eval_loss": 0.014520916156470776,
      "eval_precision_branch1": 0.9932151039853988,
      "eval_precision_branch2": 0.5000774426991434,
      "eval_recall_branch1": 0.9927932838345743,
      "eval_recall_branch2": 0.5000770297334771,
      "eval_runtime": 28.8585,
      "eval_samples_per_second": 449.851,
      "eval_steps_per_second": 56.24,
      "step": 71313
    },
    {
      "epoch": 33.00323924109209,
      "grad_norm": 0.20774249732494354,
      "learning_rate": 0.03399352151781583,
      "loss": 0.0115,
      "step": 71320
    },
    {
      "epoch": 33.007866728366494,
      "grad_norm": 0.03810662031173706,
      "learning_rate": 0.03398426654326701,
      "loss": 0.0001,
      "step": 71330
    },
    {
      "epoch": 33.012494215640906,
      "grad_norm": 0.0023643006570637226,
      "learning_rate": 0.03397501156871819,
      "loss": 0.0002,
      "step": 71340
    },
    {
      "epoch": 33.01712170291532,
      "grad_norm": 0.004805086180567741,
      "learning_rate": 0.033965756594169366,
      "loss": 0.0001,
      "step": 71350
    },
    {
      "epoch": 33.02174919018973,
      "grad_norm": 0.0051364717073738575,
      "learning_rate": 0.03395650161962054,
      "loss": 0.0001,
      "step": 71360
    },
    {
      "epoch": 33.026376677464135,
      "grad_norm": 0.011443852446973324,
      "learning_rate": 0.033947246645071726,
      "loss": 0.0002,
      "step": 71370
    },
    {
      "epoch": 33.03100416473855,
      "grad_norm": 0.0077343350276350975,
      "learning_rate": 0.03393799167052291,
      "loss": 0.0001,
      "step": 71380
    },
    {
      "epoch": 33.03563165201296,
      "grad_norm": 0.002343593630939722,
      "learning_rate": 0.03392873669597409,
      "loss": 0.0002,
      "step": 71390
    },
    {
      "epoch": 33.040259139287365,
      "grad_norm": 0.0015724020777270198,
      "learning_rate": 0.03391948172142527,
      "loss": 0.0001,
      "step": 71400
    },
    {
      "epoch": 33.04488662656178,
      "grad_norm": 0.023915883153676987,
      "learning_rate": 0.03391022674687645,
      "loss": 0.0,
      "step": 71410
    },
    {
      "epoch": 33.04951411383619,
      "grad_norm": 0.01411865372210741,
      "learning_rate": 0.03390097177232763,
      "loss": 0.0001,
      "step": 71420
    },
    {
      "epoch": 33.054141601110594,
      "grad_norm": 0.016978895291686058,
      "learning_rate": 0.033891716797778804,
      "loss": 0.0001,
      "step": 71430
    },
    {
      "epoch": 33.058769088385006,
      "grad_norm": 0.0012535152491182089,
      "learning_rate": 0.03388246182322999,
      "loss": 0.0,
      "step": 71440
    },
    {
      "epoch": 33.06339657565942,
      "grad_norm": 0.7175216674804688,
      "learning_rate": 0.03387320684868116,
      "loss": 0.0003,
      "step": 71450
    },
    {
      "epoch": 33.06802406293383,
      "grad_norm": 0.030655119568109512,
      "learning_rate": 0.033863951874132346,
      "loss": 0.0001,
      "step": 71460
    },
    {
      "epoch": 33.072651550208235,
      "grad_norm": 0.003658759407699108,
      "learning_rate": 0.03385469689958353,
      "loss": 0.0001,
      "step": 71470
    },
    {
      "epoch": 33.07727903748265,
      "grad_norm": 0.005246450193226337,
      "learning_rate": 0.03384544192503471,
      "loss": 0.0003,
      "step": 71480
    },
    {
      "epoch": 33.08190652475706,
      "grad_norm": 0.001143400790169835,
      "learning_rate": 0.03383618695048589,
      "loss": 0.0001,
      "step": 71490
    },
    {
      "epoch": 33.086534012031464,
      "grad_norm": 0.006587061565369368,
      "learning_rate": 0.033826931975937065,
      "loss": 0.0001,
      "step": 71500
    },
    {
      "epoch": 33.091161499305876,
      "grad_norm": 0.005434103310108185,
      "learning_rate": 0.03381767700138825,
      "loss": 0.0005,
      "step": 71510
    },
    {
      "epoch": 33.09578898658029,
      "grad_norm": 0.0016979669453576207,
      "learning_rate": 0.033808422026839424,
      "loss": 0.0003,
      "step": 71520
    },
    {
      "epoch": 33.1004164738547,
      "grad_norm": 0.008007128722965717,
      "learning_rate": 0.03379916705229061,
      "loss": 0.0001,
      "step": 71530
    },
    {
      "epoch": 33.105043961129105,
      "grad_norm": 0.0033877117093652487,
      "learning_rate": 0.033789912077741784,
      "loss": 0.0006,
      "step": 71540
    },
    {
      "epoch": 33.10967144840352,
      "grad_norm": 0.005983442068099976,
      "learning_rate": 0.03378065710319297,
      "loss": 0.0,
      "step": 71550
    },
    {
      "epoch": 33.11429893567793,
      "grad_norm": 0.0006551764672622085,
      "learning_rate": 0.03377140212864415,
      "loss": 0.0001,
      "step": 71560
    },
    {
      "epoch": 33.118926422952335,
      "grad_norm": 0.004144621081650257,
      "learning_rate": 0.03376214715409533,
      "loss": 0.0001,
      "step": 71570
    },
    {
      "epoch": 33.12355391022675,
      "grad_norm": 0.04341774806380272,
      "learning_rate": 0.03375289217954651,
      "loss": 0.0001,
      "step": 71580
    },
    {
      "epoch": 33.12818139750116,
      "grad_norm": 0.005427211988717318,
      "learning_rate": 0.033743637204997685,
      "loss": 0.0001,
      "step": 71590
    },
    {
      "epoch": 33.132808884775564,
      "grad_norm": 0.04238871857523918,
      "learning_rate": 0.03373438223044887,
      "loss": 0.0001,
      "step": 71600
    },
    {
      "epoch": 33.137436372049976,
      "grad_norm": 0.029433682560920715,
      "learning_rate": 0.033725127255900045,
      "loss": 0.0003,
      "step": 71610
    },
    {
      "epoch": 33.14206385932439,
      "grad_norm": 0.006542075891047716,
      "learning_rate": 0.03371587228135123,
      "loss": 0.0,
      "step": 71620
    },
    {
      "epoch": 33.1466913465988,
      "grad_norm": 0.02923443727195263,
      "learning_rate": 0.033706617306802404,
      "loss": 0.0001,
      "step": 71630
    },
    {
      "epoch": 33.151318833873205,
      "grad_norm": 0.027605988085269928,
      "learning_rate": 0.03369736233225359,
      "loss": 0.0001,
      "step": 71640
    },
    {
      "epoch": 33.15594632114762,
      "grad_norm": 0.026174750179052353,
      "learning_rate": 0.03368810735770477,
      "loss": 0.0001,
      "step": 71650
    },
    {
      "epoch": 33.16057380842203,
      "grad_norm": 0.002013688674196601,
      "learning_rate": 0.03367885238315595,
      "loss": 0.0001,
      "step": 71660
    },
    {
      "epoch": 33.165201295696434,
      "grad_norm": 0.0046270256862044334,
      "learning_rate": 0.03366959740860713,
      "loss": 0.0001,
      "step": 71670
    },
    {
      "epoch": 33.169828782970846,
      "grad_norm": 0.14438359439373016,
      "learning_rate": 0.033660342434058306,
      "loss": 0.0001,
      "step": 71680
    },
    {
      "epoch": 33.17445627024526,
      "grad_norm": 0.011676245369017124,
      "learning_rate": 0.03365108745950949,
      "loss": 0.0001,
      "step": 71690
    },
    {
      "epoch": 33.17908375751966,
      "grad_norm": 0.0010297195985913277,
      "learning_rate": 0.033641832484960665,
      "loss": 0.0001,
      "step": 71700
    },
    {
      "epoch": 33.183711244794075,
      "grad_norm": 0.5943278670310974,
      "learning_rate": 0.03363257751041185,
      "loss": 0.0002,
      "step": 71710
    },
    {
      "epoch": 33.18833873206849,
      "grad_norm": 0.0691811665892601,
      "learning_rate": 0.033623322535863025,
      "loss": 0.0006,
      "step": 71720
    },
    {
      "epoch": 33.1929662193429,
      "grad_norm": 0.011622428894042969,
      "learning_rate": 0.03361406756131421,
      "loss": 0.0001,
      "step": 71730
    },
    {
      "epoch": 33.197593706617305,
      "grad_norm": 0.013095893897116184,
      "learning_rate": 0.03360481258676539,
      "loss": 0.0004,
      "step": 71740
    },
    {
      "epoch": 33.20222119389172,
      "grad_norm": 0.02970743551850319,
      "learning_rate": 0.03359555761221657,
      "loss": 0.0001,
      "step": 71750
    },
    {
      "epoch": 33.20684868116613,
      "grad_norm": 0.004678993485867977,
      "learning_rate": 0.03358630263766775,
      "loss": 0.0001,
      "step": 71760
    },
    {
      "epoch": 33.211476168440534,
      "grad_norm": 0.02033212222158909,
      "learning_rate": 0.03357704766311893,
      "loss": 0.0001,
      "step": 71770
    },
    {
      "epoch": 33.216103655714946,
      "grad_norm": 0.004896957892924547,
      "learning_rate": 0.03356779268857011,
      "loss": 0.0002,
      "step": 71780
    },
    {
      "epoch": 33.22073114298936,
      "grad_norm": 0.026513103395700455,
      "learning_rate": 0.033558537714021286,
      "loss": 0.0002,
      "step": 71790
    },
    {
      "epoch": 33.22535863026377,
      "grad_norm": 0.006465843413025141,
      "learning_rate": 0.03354928273947247,
      "loss": 0.0,
      "step": 71800
    },
    {
      "epoch": 33.229986117538175,
      "grad_norm": 0.0016635679639875889,
      "learning_rate": 0.033540027764923645,
      "loss": 0.0001,
      "step": 71810
    },
    {
      "epoch": 33.23461360481259,
      "grad_norm": 0.016675665974617004,
      "learning_rate": 0.03353077279037483,
      "loss": 0.0,
      "step": 71820
    },
    {
      "epoch": 33.239241092087,
      "grad_norm": 0.0036918804980814457,
      "learning_rate": 0.03352151781582601,
      "loss": 0.0001,
      "step": 71830
    },
    {
      "epoch": 33.243868579361404,
      "grad_norm": 0.025498563423752785,
      "learning_rate": 0.03351226284127719,
      "loss": 0.0001,
      "step": 71840
    },
    {
      "epoch": 33.248496066635816,
      "grad_norm": 0.027168521657586098,
      "learning_rate": 0.03350300786672837,
      "loss": 0.0001,
      "step": 71850
    },
    {
      "epoch": 33.25312355391023,
      "grad_norm": 0.013801549561321735,
      "learning_rate": 0.03349375289217955,
      "loss": 0.0001,
      "step": 71860
    },
    {
      "epoch": 33.25775104118463,
      "grad_norm": 0.008599618449807167,
      "learning_rate": 0.03348449791763073,
      "loss": 0.0001,
      "step": 71870
    },
    {
      "epoch": 33.262378528459045,
      "grad_norm": 0.0009086078498512506,
      "learning_rate": 0.033475242943081907,
      "loss": 0.0001,
      "step": 71880
    },
    {
      "epoch": 33.26700601573346,
      "grad_norm": 0.000928168126847595,
      "learning_rate": 0.03346598796853308,
      "loss": 0.0001,
      "step": 71890
    },
    {
      "epoch": 33.27163350300787,
      "grad_norm": 0.006259358488023281,
      "learning_rate": 0.033456732993984266,
      "loss": 0.0,
      "step": 71900
    },
    {
      "epoch": 33.276260990282275,
      "grad_norm": 0.013871045783162117,
      "learning_rate": 0.03344747801943545,
      "loss": 0.0,
      "step": 71910
    },
    {
      "epoch": 33.28088847755669,
      "grad_norm": 0.08076655119657516,
      "learning_rate": 0.03343822304488663,
      "loss": 0.0001,
      "step": 71920
    },
    {
      "epoch": 33.2855159648311,
      "grad_norm": 0.008105329237878323,
      "learning_rate": 0.03342896807033781,
      "loss": 0.0001,
      "step": 71930
    },
    {
      "epoch": 33.290143452105504,
      "grad_norm": 0.007932882755994797,
      "learning_rate": 0.03341971309578899,
      "loss": 0.0001,
      "step": 71940
    },
    {
      "epoch": 33.294770939379916,
      "grad_norm": 0.005407328251749277,
      "learning_rate": 0.03341045812124017,
      "loss": 0.0001,
      "step": 71950
    },
    {
      "epoch": 33.29939842665433,
      "grad_norm": 0.09844490885734558,
      "learning_rate": 0.033401203146691344,
      "loss": 0.0002,
      "step": 71960
    },
    {
      "epoch": 33.30402591392874,
      "grad_norm": 0.002295744838193059,
      "learning_rate": 0.03339194817214253,
      "loss": 0.0002,
      "step": 71970
    },
    {
      "epoch": 33.308653401203145,
      "grad_norm": 0.0033308216370642185,
      "learning_rate": 0.0333826931975937,
      "loss": 0.0001,
      "step": 71980
    },
    {
      "epoch": 33.31328088847756,
      "grad_norm": 0.10049190372228622,
      "learning_rate": 0.033373438223044886,
      "loss": 0.0001,
      "step": 71990
    },
    {
      "epoch": 33.31790837575197,
      "grad_norm": 0.00891809444874525,
      "learning_rate": 0.03336418324849607,
      "loss": 0.0001,
      "step": 72000
    },
    {
      "epoch": 33.322535863026374,
      "grad_norm": 0.030158238485455513,
      "learning_rate": 0.03335492827394725,
      "loss": 0.0002,
      "step": 72010
    },
    {
      "epoch": 33.327163350300786,
      "grad_norm": 0.0011060286778956652,
      "learning_rate": 0.03334567329939843,
      "loss": 0.0001,
      "step": 72020
    },
    {
      "epoch": 33.3317908375752,
      "grad_norm": 0.003969367127865553,
      "learning_rate": 0.03333641832484961,
      "loss": 0.0001,
      "step": 72030
    },
    {
      "epoch": 33.3364183248496,
      "grad_norm": 0.007164423819631338,
      "learning_rate": 0.03332716335030079,
      "loss": 0.0,
      "step": 72040
    },
    {
      "epoch": 33.341045812124015,
      "grad_norm": 0.010630450211465359,
      "learning_rate": 0.033317908375751965,
      "loss": 0.0001,
      "step": 72050
    },
    {
      "epoch": 33.34567329939843,
      "grad_norm": 0.003564136801287532,
      "learning_rate": 0.03330865340120315,
      "loss": 0.0001,
      "step": 72060
    },
    {
      "epoch": 33.35030078667284,
      "grad_norm": 0.004011653829365969,
      "learning_rate": 0.033299398426654324,
      "loss": 0.0002,
      "step": 72070
    },
    {
      "epoch": 33.354928273947245,
      "grad_norm": 0.020729251205921173,
      "learning_rate": 0.03329014345210551,
      "loss": 0.0,
      "step": 72080
    },
    {
      "epoch": 33.35955576122166,
      "grad_norm": 0.0007251754286698997,
      "learning_rate": 0.03328088847755669,
      "loss": 0.0011,
      "step": 72090
    },
    {
      "epoch": 33.36418324849607,
      "grad_norm": 0.054047103971242905,
      "learning_rate": 0.03327163350300787,
      "loss": 0.0001,
      "step": 72100
    },
    {
      "epoch": 33.368810735770474,
      "grad_norm": 0.03896838799118996,
      "learning_rate": 0.03326237852845905,
      "loss": 0.0001,
      "step": 72110
    },
    {
      "epoch": 33.373438223044886,
      "grad_norm": 0.4430444538593292,
      "learning_rate": 0.033253123553910226,
      "loss": 0.0002,
      "step": 72120
    },
    {
      "epoch": 33.3780657103193,
      "grad_norm": 0.0047043063677847385,
      "learning_rate": 0.03324386857936141,
      "loss": 0.0002,
      "step": 72130
    },
    {
      "epoch": 33.38269319759371,
      "grad_norm": 0.004344415385276079,
      "learning_rate": 0.033234613604812585,
      "loss": 0.0001,
      "step": 72140
    },
    {
      "epoch": 33.387320684868115,
      "grad_norm": 0.017665665596723557,
      "learning_rate": 0.03322535863026377,
      "loss": 0.0024,
      "step": 72150
    },
    {
      "epoch": 33.39194817214253,
      "grad_norm": 0.0025114738382399082,
      "learning_rate": 0.033216103655714944,
      "loss": 0.0001,
      "step": 72160
    },
    {
      "epoch": 33.39657565941694,
      "grad_norm": 0.004616401623934507,
      "learning_rate": 0.03320684868116613,
      "loss": 0.0,
      "step": 72170
    },
    {
      "epoch": 33.401203146691344,
      "grad_norm": 0.05213790759444237,
      "learning_rate": 0.03319759370661731,
      "loss": 0.0,
      "step": 72180
    },
    {
      "epoch": 33.405830633965756,
      "grad_norm": 0.11744724214076996,
      "learning_rate": 0.03318833873206849,
      "loss": 0.0003,
      "step": 72190
    },
    {
      "epoch": 33.41045812124017,
      "grad_norm": 0.020916331559419632,
      "learning_rate": 0.03317908375751967,
      "loss": 0.0001,
      "step": 72200
    },
    {
      "epoch": 33.41508560851457,
      "grad_norm": 0.001032134285196662,
      "learning_rate": 0.033169828782970846,
      "loss": 0.0004,
      "step": 72210
    },
    {
      "epoch": 33.419713095788985,
      "grad_norm": 0.013520289212465286,
      "learning_rate": 0.03316057380842203,
      "loss": 0.0008,
      "step": 72220
    },
    {
      "epoch": 33.4243405830634,
      "grad_norm": 0.001879204879514873,
      "learning_rate": 0.033151318833873206,
      "loss": 0.0001,
      "step": 72230
    },
    {
      "epoch": 33.42896807033781,
      "grad_norm": 0.009334356524050236,
      "learning_rate": 0.03314206385932439,
      "loss": 0.0005,
      "step": 72240
    },
    {
      "epoch": 33.433595557612215,
      "grad_norm": 0.004394708666950464,
      "learning_rate": 0.033132808884775565,
      "loss": 0.0001,
      "step": 72250
    },
    {
      "epoch": 33.43822304488663,
      "grad_norm": 0.3309376835823059,
      "learning_rate": 0.03312355391022675,
      "loss": 0.0002,
      "step": 72260
    },
    {
      "epoch": 33.44285053216104,
      "grad_norm": 0.005307713523507118,
      "learning_rate": 0.03311429893567793,
      "loss": 0.0001,
      "step": 72270
    },
    {
      "epoch": 33.447478019435444,
      "grad_norm": 0.007910008542239666,
      "learning_rate": 0.03310504396112911,
      "loss": 0.0001,
      "step": 72280
    },
    {
      "epoch": 33.452105506709856,
      "grad_norm": 0.001826551859267056,
      "learning_rate": 0.03309578898658029,
      "loss": 0.0002,
      "step": 72290
    },
    {
      "epoch": 33.45673299398427,
      "grad_norm": 0.0013376254355534911,
      "learning_rate": 0.03308653401203147,
      "loss": 0.0001,
      "step": 72300
    },
    {
      "epoch": 33.46136048125868,
      "grad_norm": 0.008683226071298122,
      "learning_rate": 0.03307727903748265,
      "loss": 0.0003,
      "step": 72310
    },
    {
      "epoch": 33.465987968533085,
      "grad_norm": 0.003983700647950172,
      "learning_rate": 0.033068024062933826,
      "loss": 0.0001,
      "step": 72320
    },
    {
      "epoch": 33.4706154558075,
      "grad_norm": 0.21840693056583405,
      "learning_rate": 0.03305876908838501,
      "loss": 0.0002,
      "step": 72330
    },
    {
      "epoch": 33.47524294308191,
      "grad_norm": 0.011841780506074429,
      "learning_rate": 0.033049514113836186,
      "loss": 0.0001,
      "step": 72340
    },
    {
      "epoch": 33.479870430356314,
      "grad_norm": 0.0024962949100881815,
      "learning_rate": 0.03304025913928737,
      "loss": 0.0001,
      "step": 72350
    },
    {
      "epoch": 33.484497917630726,
      "grad_norm": 0.003813365241512656,
      "learning_rate": 0.03303100416473855,
      "loss": 0.0,
      "step": 72360
    },
    {
      "epoch": 33.48912540490514,
      "grad_norm": 0.016700124368071556,
      "learning_rate": 0.03302174919018973,
      "loss": 0.0001,
      "step": 72370
    },
    {
      "epoch": 33.49375289217954,
      "grad_norm": 0.017368189990520477,
      "learning_rate": 0.03301249421564091,
      "loss": 0.0001,
      "step": 72380
    },
    {
      "epoch": 33.498380379453955,
      "grad_norm": 0.006268918048590422,
      "learning_rate": 0.03300323924109209,
      "loss": 0.0001,
      "step": 72390
    },
    {
      "epoch": 33.50300786672837,
      "grad_norm": 0.006732446141541004,
      "learning_rate": 0.03299398426654327,
      "loss": 0.0001,
      "step": 72400
    },
    {
      "epoch": 33.50763535400278,
      "grad_norm": 0.0016558427596464753,
      "learning_rate": 0.03298472929199445,
      "loss": 0.0002,
      "step": 72410
    },
    {
      "epoch": 33.512262841277185,
      "grad_norm": 0.033478353172540665,
      "learning_rate": 0.03297547431744562,
      "loss": 0.0,
      "step": 72420
    },
    {
      "epoch": 33.5168903285516,
      "grad_norm": 0.007776901591569185,
      "learning_rate": 0.032966219342896806,
      "loss": 0.0001,
      "step": 72430
    },
    {
      "epoch": 33.52151781582601,
      "grad_norm": 0.0040896497666835785,
      "learning_rate": 0.03295696436834799,
      "loss": 0.0001,
      "step": 72440
    },
    {
      "epoch": 33.526145303100414,
      "grad_norm": 0.07950881868600845,
      "learning_rate": 0.03294770939379917,
      "loss": 0.0004,
      "step": 72450
    },
    {
      "epoch": 33.530772790374826,
      "grad_norm": 0.0024443233851343393,
      "learning_rate": 0.03293845441925035,
      "loss": 0.0001,
      "step": 72460
    },
    {
      "epoch": 33.53540027764924,
      "grad_norm": 0.049059685319662094,
      "learning_rate": 0.03292919944470153,
      "loss": 0.0001,
      "step": 72470
    },
    {
      "epoch": 33.54002776492365,
      "grad_norm": 0.012893316335976124,
      "learning_rate": 0.03291994447015271,
      "loss": 0.0001,
      "step": 72480
    },
    {
      "epoch": 33.544655252198055,
      "grad_norm": 0.057008251547813416,
      "learning_rate": 0.03291068949560389,
      "loss": 0.0001,
      "step": 72490
    },
    {
      "epoch": 33.54928273947247,
      "grad_norm": 0.004221891053020954,
      "learning_rate": 0.03290143452105507,
      "loss": 0.0001,
      "step": 72500
    },
    {
      "epoch": 33.55391022674688,
      "grad_norm": 0.003603389486670494,
      "learning_rate": 0.032892179546506244,
      "loss": 0.0001,
      "step": 72510
    },
    {
      "epoch": 33.558537714021284,
      "grad_norm": 0.029456138610839844,
      "learning_rate": 0.03288292457195743,
      "loss": 0.0,
      "step": 72520
    },
    {
      "epoch": 33.563165201295696,
      "grad_norm": 0.0213710255920887,
      "learning_rate": 0.03287366959740861,
      "loss": 0.0001,
      "step": 72530
    },
    {
      "epoch": 33.56779268857011,
      "grad_norm": 0.019318222999572754,
      "learning_rate": 0.03286441462285979,
      "loss": 0.0002,
      "step": 72540
    },
    {
      "epoch": 33.57242017584451,
      "grad_norm": 0.0004809408856090158,
      "learning_rate": 0.03285515964831097,
      "loss": 0.0001,
      "step": 72550
    },
    {
      "epoch": 33.577047663118925,
      "grad_norm": 0.03245532140135765,
      "learning_rate": 0.03284590467376215,
      "loss": 0.0004,
      "step": 72560
    },
    {
      "epoch": 33.58167515039334,
      "grad_norm": 0.003381461137905717,
      "learning_rate": 0.03283664969921333,
      "loss": 0.0,
      "step": 72570
    },
    {
      "epoch": 33.58630263766775,
      "grad_norm": 0.011285657063126564,
      "learning_rate": 0.032827394724664505,
      "loss": 0.0001,
      "step": 72580
    },
    {
      "epoch": 33.590930124942155,
      "grad_norm": 0.012757059186697006,
      "learning_rate": 0.03281813975011569,
      "loss": 0.0001,
      "step": 72590
    },
    {
      "epoch": 33.59555761221657,
      "grad_norm": 0.0034410941880196333,
      "learning_rate": 0.032808884775566864,
      "loss": 0.0001,
      "step": 72600
    },
    {
      "epoch": 33.60018509949098,
      "grad_norm": 0.0010686260648071766,
      "learning_rate": 0.03279962980101805,
      "loss": 0.0001,
      "step": 72610
    },
    {
      "epoch": 33.604812586765384,
      "grad_norm": 0.029373561963438988,
      "learning_rate": 0.03279037482646923,
      "loss": 0.0002,
      "step": 72620
    },
    {
      "epoch": 33.609440074039796,
      "grad_norm": 0.0007429347606375813,
      "learning_rate": 0.032781119851920414,
      "loss": 0.0001,
      "step": 72630
    },
    {
      "epoch": 33.61406756131421,
      "grad_norm": 0.001238971366547048,
      "learning_rate": 0.03277186487737159,
      "loss": 0.0001,
      "step": 72640
    },
    {
      "epoch": 33.61869504858861,
      "grad_norm": 0.0029587000608444214,
      "learning_rate": 0.032762609902822766,
      "loss": 0.001,
      "step": 72650
    },
    {
      "epoch": 33.623322535863025,
      "grad_norm": 0.013959810137748718,
      "learning_rate": 0.03275335492827395,
      "loss": 0.0001,
      "step": 72660
    },
    {
      "epoch": 33.62795002313744,
      "grad_norm": 0.0012208487605676055,
      "learning_rate": 0.032744099953725125,
      "loss": 0.0001,
      "step": 72670
    },
    {
      "epoch": 33.63257751041185,
      "grad_norm": 0.0015052182134240866,
      "learning_rate": 0.03273484497917631,
      "loss": 0.0001,
      "step": 72680
    },
    {
      "epoch": 33.637204997686254,
      "grad_norm": 0.03396415337920189,
      "learning_rate": 0.032725590004627485,
      "loss": 0.0002,
      "step": 72690
    },
    {
      "epoch": 33.641832484960666,
      "grad_norm": 0.0014007488498464227,
      "learning_rate": 0.03271633503007867,
      "loss": 0.0001,
      "step": 72700
    },
    {
      "epoch": 33.64645997223508,
      "grad_norm": 0.02168072760105133,
      "learning_rate": 0.03270708005552985,
      "loss": 0.0001,
      "step": 72710
    },
    {
      "epoch": 33.65108745950948,
      "grad_norm": 0.0025020428001880646,
      "learning_rate": 0.032697825080981034,
      "loss": 0.0,
      "step": 72720
    },
    {
      "epoch": 33.655714946783895,
      "grad_norm": 0.012969598174095154,
      "learning_rate": 0.03268857010643221,
      "loss": 0.0001,
      "step": 72730
    },
    {
      "epoch": 33.66034243405831,
      "grad_norm": 0.006440876983106136,
      "learning_rate": 0.03267931513188339,
      "loss": 0.0,
      "step": 72740
    },
    {
      "epoch": 33.66496992133272,
      "grad_norm": 0.009186428040266037,
      "learning_rate": 0.03267006015733457,
      "loss": 0.0,
      "step": 72750
    },
    {
      "epoch": 33.669597408607125,
      "grad_norm": 0.11675937473773956,
      "learning_rate": 0.032660805182785746,
      "loss": 0.0001,
      "step": 72760
    },
    {
      "epoch": 33.67422489588154,
      "grad_norm": 0.0021995508577674627,
      "learning_rate": 0.03265155020823693,
      "loss": 0.0001,
      "step": 72770
    },
    {
      "epoch": 33.67885238315595,
      "grad_norm": 0.024573272094130516,
      "learning_rate": 0.032642295233688105,
      "loss": 0.0001,
      "step": 72780
    },
    {
      "epoch": 33.683479870430354,
      "grad_norm": 0.0012930185766890645,
      "learning_rate": 0.03263304025913929,
      "loss": 0.0001,
      "step": 72790
    },
    {
      "epoch": 33.688107357704766,
      "grad_norm": 0.0027171222027391195,
      "learning_rate": 0.03262378528459047,
      "loss": 0.0021,
      "step": 72800
    },
    {
      "epoch": 33.69273484497918,
      "grad_norm": 0.0006669408176094294,
      "learning_rate": 0.03261453031004165,
      "loss": 0.0001,
      "step": 72810
    },
    {
      "epoch": 33.69736233225358,
      "grad_norm": 0.0016876908484846354,
      "learning_rate": 0.03260527533549283,
      "loss": 0.0001,
      "step": 72820
    },
    {
      "epoch": 33.701989819527995,
      "grad_norm": 0.0007641584379598498,
      "learning_rate": 0.03259602036094401,
      "loss": 0.0003,
      "step": 72830
    },
    {
      "epoch": 33.70661730680241,
      "grad_norm": 0.002051746705546975,
      "learning_rate": 0.03258676538639519,
      "loss": 0.0001,
      "step": 72840
    },
    {
      "epoch": 33.71124479407682,
      "grad_norm": 0.003892214735969901,
      "learning_rate": 0.03257751041184637,
      "loss": 0.0001,
      "step": 72850
    },
    {
      "epoch": 33.715872281351224,
      "grad_norm": 0.03048553504049778,
      "learning_rate": 0.03256825543729755,
      "loss": 0.0001,
      "step": 72860
    },
    {
      "epoch": 33.720499768625636,
      "grad_norm": 0.011408183723688126,
      "learning_rate": 0.032559000462748726,
      "loss": 0.0,
      "step": 72870
    },
    {
      "epoch": 33.72512725590005,
      "grad_norm": 0.007833059877157211,
      "learning_rate": 0.03254974548819991,
      "loss": 0.0001,
      "step": 72880
    },
    {
      "epoch": 33.72975474317445,
      "grad_norm": 0.009703019633889198,
      "learning_rate": 0.03254049051365109,
      "loss": 0.0001,
      "step": 72890
    },
    {
      "epoch": 33.734382230448865,
      "grad_norm": 0.0013360007433220744,
      "learning_rate": 0.03253123553910227,
      "loss": 0.0001,
      "step": 72900
    },
    {
      "epoch": 33.73900971772328,
      "grad_norm": 0.025245849043130875,
      "learning_rate": 0.03252198056455345,
      "loss": 0.0002,
      "step": 72910
    },
    {
      "epoch": 33.74363720499769,
      "grad_norm": 0.0014468515291810036,
      "learning_rate": 0.03251272559000463,
      "loss": 0.0002,
      "step": 72920
    },
    {
      "epoch": 33.748264692272095,
      "grad_norm": 0.0017703021876513958,
      "learning_rate": 0.03250347061545581,
      "loss": 0.0,
      "step": 72930
    },
    {
      "epoch": 33.75289217954651,
      "grad_norm": 0.00731832766905427,
      "learning_rate": 0.03249421564090699,
      "loss": 0.0001,
      "step": 72940
    },
    {
      "epoch": 33.75751966682092,
      "grad_norm": 0.010207569226622581,
      "learning_rate": 0.03248496066635817,
      "loss": 0.0002,
      "step": 72950
    },
    {
      "epoch": 33.762147154095324,
      "grad_norm": 0.004412746988236904,
      "learning_rate": 0.032475705691809346,
      "loss": 0.0002,
      "step": 72960
    },
    {
      "epoch": 33.766774641369736,
      "grad_norm": 0.10064462572336197,
      "learning_rate": 0.03246645071726053,
      "loss": 0.0001,
      "step": 72970
    },
    {
      "epoch": 33.77140212864415,
      "grad_norm": 0.024066947400569916,
      "learning_rate": 0.03245719574271171,
      "loss": 0.0001,
      "step": 72980
    },
    {
      "epoch": 33.77602961591855,
      "grad_norm": 0.024194251745939255,
      "learning_rate": 0.03244794076816289,
      "loss": 0.0001,
      "step": 72990
    },
    {
      "epoch": 33.780657103192965,
      "grad_norm": 0.06579940021038055,
      "learning_rate": 0.03243868579361407,
      "loss": 0.0001,
      "step": 73000
    },
    {
      "epoch": 33.78528459046738,
      "grad_norm": 0.003122166031971574,
      "learning_rate": 0.03242943081906525,
      "loss": 0.0001,
      "step": 73010
    },
    {
      "epoch": 33.78991207774179,
      "grad_norm": 0.0008880301029421389,
      "learning_rate": 0.03242017584451643,
      "loss": 0.0001,
      "step": 73020
    },
    {
      "epoch": 33.794539565016194,
      "grad_norm": 0.003399056615307927,
      "learning_rate": 0.03241092086996761,
      "loss": 0.0003,
      "step": 73030
    },
    {
      "epoch": 33.799167052290606,
      "grad_norm": 0.012779930606484413,
      "learning_rate": 0.032401665895418784,
      "loss": 0.0001,
      "step": 73040
    },
    {
      "epoch": 33.80379453956502,
      "grad_norm": 0.026592964306473732,
      "learning_rate": 0.03239241092086997,
      "loss": 0.0001,
      "step": 73050
    },
    {
      "epoch": 33.80842202683942,
      "grad_norm": 0.03189834579825401,
      "learning_rate": 0.03238315594632115,
      "loss": 0.0001,
      "step": 73060
    },
    {
      "epoch": 33.813049514113835,
      "grad_norm": 0.006326197646558285,
      "learning_rate": 0.03237390097177233,
      "loss": 0.0,
      "step": 73070
    },
    {
      "epoch": 33.81767700138825,
      "grad_norm": 0.027529990300536156,
      "learning_rate": 0.03236464599722351,
      "loss": 0.0,
      "step": 73080
    },
    {
      "epoch": 33.82230448866266,
      "grad_norm": 0.0016113491728901863,
      "learning_rate": 0.03235539102267469,
      "loss": 0.0,
      "step": 73090
    },
    {
      "epoch": 33.826931975937065,
      "grad_norm": 0.21032162010669708,
      "learning_rate": 0.03234613604812587,
      "loss": 0.0001,
      "step": 73100
    },
    {
      "epoch": 33.83155946321148,
      "grad_norm": 0.005930011160671711,
      "learning_rate": 0.032336881073577045,
      "loss": 0.0002,
      "step": 73110
    },
    {
      "epoch": 33.83618695048589,
      "grad_norm": 0.004361744970083237,
      "learning_rate": 0.03232762609902823,
      "loss": 0.0001,
      "step": 73120
    },
    {
      "epoch": 33.840814437760294,
      "grad_norm": 0.004665395710617304,
      "learning_rate": 0.032318371124479404,
      "loss": 0.0001,
      "step": 73130
    },
    {
      "epoch": 33.845441925034706,
      "grad_norm": 0.007486642338335514,
      "learning_rate": 0.03230911614993059,
      "loss": 0.0001,
      "step": 73140
    },
    {
      "epoch": 33.85006941230912,
      "grad_norm": 0.005192211829125881,
      "learning_rate": 0.03229986117538177,
      "loss": 0.0,
      "step": 73150
    },
    {
      "epoch": 33.85469689958352,
      "grad_norm": 0.0066732424311339855,
      "learning_rate": 0.032290606200832954,
      "loss": 0.0001,
      "step": 73160
    },
    {
      "epoch": 33.859324386857935,
      "grad_norm": 0.003920069895684719,
      "learning_rate": 0.03228135122628413,
      "loss": 0.0001,
      "step": 73170
    },
    {
      "epoch": 33.86395187413235,
      "grad_norm": 0.2371416836977005,
      "learning_rate": 0.03227209625173531,
      "loss": 0.0002,
      "step": 73180
    },
    {
      "epoch": 33.86857936140676,
      "grad_norm": 0.0010929545387625694,
      "learning_rate": 0.03226284127718649,
      "loss": 0.0001,
      "step": 73190
    },
    {
      "epoch": 33.873206848681164,
      "grad_norm": 0.007895178161561489,
      "learning_rate": 0.032253586302637666,
      "loss": 0.0001,
      "step": 73200
    },
    {
      "epoch": 33.877834335955576,
      "grad_norm": 0.0013992746826261282,
      "learning_rate": 0.03224433132808885,
      "loss": 0.0,
      "step": 73210
    },
    {
      "epoch": 33.88246182322999,
      "grad_norm": 0.001668794546276331,
      "learning_rate": 0.032235076353540025,
      "loss": 0.0001,
      "step": 73220
    },
    {
      "epoch": 33.88708931050439,
      "grad_norm": 0.014898041263222694,
      "learning_rate": 0.03222582137899121,
      "loss": 0.0001,
      "step": 73230
    },
    {
      "epoch": 33.891716797778805,
      "grad_norm": 0.01315974723547697,
      "learning_rate": 0.03221656640444239,
      "loss": 0.0001,
      "step": 73240
    },
    {
      "epoch": 33.89634428505322,
      "grad_norm": 0.003235890995711088,
      "learning_rate": 0.032207311429893574,
      "loss": 0.0005,
      "step": 73250
    },
    {
      "epoch": 33.90097177232763,
      "grad_norm": 0.029181208461523056,
      "learning_rate": 0.03219805645534475,
      "loss": 0.0001,
      "step": 73260
    },
    {
      "epoch": 33.905599259602035,
      "grad_norm": 0.00038285122718662024,
      "learning_rate": 0.03218880148079593,
      "loss": 0.0001,
      "step": 73270
    },
    {
      "epoch": 33.91022674687645,
      "grad_norm": 0.0010793923866003752,
      "learning_rate": 0.03217954650624711,
      "loss": 0.0001,
      "step": 73280
    },
    {
      "epoch": 33.91485423415086,
      "grad_norm": 0.001952648744918406,
      "learning_rate": 0.032170291531698286,
      "loss": 0.0001,
      "step": 73290
    },
    {
      "epoch": 33.919481721425264,
      "grad_norm": 0.001116355648264289,
      "learning_rate": 0.03216103655714947,
      "loss": 0.0001,
      "step": 73300
    },
    {
      "epoch": 33.924109208699676,
      "grad_norm": 0.02532450295984745,
      "learning_rate": 0.032151781582600646,
      "loss": 0.0001,
      "step": 73310
    },
    {
      "epoch": 33.92873669597409,
      "grad_norm": 0.007676349487155676,
      "learning_rate": 0.03214252660805183,
      "loss": 0.0001,
      "step": 73320
    },
    {
      "epoch": 33.93336418324849,
      "grad_norm": 0.004634686280041933,
      "learning_rate": 0.03213327163350301,
      "loss": 0.0001,
      "step": 73330
    },
    {
      "epoch": 33.937991670522905,
      "grad_norm": 0.00427648751065135,
      "learning_rate": 0.03212401665895419,
      "loss": 0.0001,
      "step": 73340
    },
    {
      "epoch": 33.94261915779732,
      "grad_norm": 0.006847761105746031,
      "learning_rate": 0.03211476168440537,
      "loss": 0.0001,
      "step": 73350
    },
    {
      "epoch": 33.94724664507173,
      "grad_norm": 0.0027666313108056784,
      "learning_rate": 0.03210550670985655,
      "loss": 0.0001,
      "step": 73360
    },
    {
      "epoch": 33.951874132346134,
      "grad_norm": 0.010145578533411026,
      "learning_rate": 0.03209625173530773,
      "loss": 0.0001,
      "step": 73370
    },
    {
      "epoch": 33.956501619620546,
      "grad_norm": 0.0009433699306100607,
      "learning_rate": 0.03208699676075891,
      "loss": 0.0001,
      "step": 73380
    },
    {
      "epoch": 33.96112910689496,
      "grad_norm": 0.02344847284257412,
      "learning_rate": 0.03207774178621009,
      "loss": 0.0001,
      "step": 73390
    },
    {
      "epoch": 33.96575659416936,
      "grad_norm": 0.0014844731194898486,
      "learning_rate": 0.032068486811661266,
      "loss": 0.0001,
      "step": 73400
    },
    {
      "epoch": 33.970384081443775,
      "grad_norm": 0.008066989481449127,
      "learning_rate": 0.03205923183711245,
      "loss": 0.0001,
      "step": 73410
    },
    {
      "epoch": 33.97501156871819,
      "grad_norm": 0.7329956293106079,
      "learning_rate": 0.03204997686256363,
      "loss": 0.0004,
      "step": 73420
    },
    {
      "epoch": 33.97963905599259,
      "grad_norm": 0.001342229312285781,
      "learning_rate": 0.03204072188801481,
      "loss": 0.0,
      "step": 73430
    },
    {
      "epoch": 33.984266543267005,
      "grad_norm": 0.0026408620178699493,
      "learning_rate": 0.03203146691346599,
      "loss": 0.0,
      "step": 73440
    },
    {
      "epoch": 33.98889403054142,
      "grad_norm": 0.4756319224834442,
      "learning_rate": 0.03202221193891717,
      "loss": 0.0002,
      "step": 73450
    },
    {
      "epoch": 33.99352151781583,
      "grad_norm": 0.005211725831031799,
      "learning_rate": 0.03201295696436835,
      "loss": 0.0001,
      "step": 73460
    },
    {
      "epoch": 33.998149005090234,
      "grad_norm": 0.003535754978656769,
      "learning_rate": 0.03200370198981953,
      "loss": 0.0001,
      "step": 73470
    },
    {
      "epoch": 34.0,
      "eval_accuracy_branch1": 0.9894469265136343,
      "eval_accuracy_branch2": 0.5004621784008627,
      "eval_f1_branch1": 0.9904702835074651,
      "eval_f1_branch2": 0.5002040988985985,
      "eval_loss": 0.018819082528352737,
      "eval_precision_branch1": 0.9907060428819224,
      "eval_precision_branch2": 0.500463134996538,
      "eval_recall_branch1": 0.9903556183465199,
      "eval_recall_branch2": 0.5004621784008627,
      "eval_runtime": 29.1635,
      "eval_samples_per_second": 445.146,
      "eval_steps_per_second": 55.652,
      "step": 73474
    },
    {
      "epoch": 34.002776492364646,
      "grad_norm": 0.03093907982110977,
      "learning_rate": 0.03199444701527071,
      "loss": 0.0001,
      "step": 73480
    },
    {
      "epoch": 34.00740397963906,
      "grad_norm": 0.012535718269646168,
      "learning_rate": 0.03198519204072189,
      "loss": 0.0001,
      "step": 73490
    },
    {
      "epoch": 34.01203146691346,
      "grad_norm": 0.0065214368514716625,
      "learning_rate": 0.03197593706617307,
      "loss": 0.0,
      "step": 73500
    },
    {
      "epoch": 34.016658954187875,
      "grad_norm": 0.05559510365128517,
      "learning_rate": 0.03196668209162425,
      "loss": 0.0001,
      "step": 73510
    },
    {
      "epoch": 34.02128644146229,
      "grad_norm": 0.0027596885338425636,
      "learning_rate": 0.03195742711707543,
      "loss": 0.0003,
      "step": 73520
    },
    {
      "epoch": 34.0259139287367,
      "grad_norm": 0.0018002181313931942,
      "learning_rate": 0.03194817214252661,
      "loss": 0.0003,
      "step": 73530
    },
    {
      "epoch": 34.030541416011104,
      "grad_norm": 0.023030031472444534,
      "learning_rate": 0.03193891716797779,
      "loss": 0.0003,
      "step": 73540
    },
    {
      "epoch": 34.035168903285516,
      "grad_norm": 0.0009437526459805667,
      "learning_rate": 0.03192966219342897,
      "loss": 0.0002,
      "step": 73550
    },
    {
      "epoch": 34.03979639055993,
      "grad_norm": 3.3412437438964844,
      "learning_rate": 0.03192040721888015,
      "loss": 0.001,
      "step": 73560
    },
    {
      "epoch": 34.04442387783433,
      "grad_norm": 0.0022927019745111465,
      "learning_rate": 0.031911152244331324,
      "loss": 0.001,
      "step": 73570
    },
    {
      "epoch": 34.049051365108745,
      "grad_norm": 0.0033654440194368362,
      "learning_rate": 0.03190189726978251,
      "loss": 0.0005,
      "step": 73580
    },
    {
      "epoch": 34.05367885238316,
      "grad_norm": 0.0029263703618198633,
      "learning_rate": 0.03189264229523369,
      "loss": 0.0003,
      "step": 73590
    },
    {
      "epoch": 34.05830633965756,
      "grad_norm": 0.011692577041685581,
      "learning_rate": 0.031883387320684874,
      "loss": 0.0001,
      "step": 73600
    },
    {
      "epoch": 34.062933826931975,
      "grad_norm": 0.0006987262167967856,
      "learning_rate": 0.03187413234613605,
      "loss": 0.0001,
      "step": 73610
    },
    {
      "epoch": 34.06756131420639,
      "grad_norm": 0.017886780202388763,
      "learning_rate": 0.03186487737158723,
      "loss": 0.0,
      "step": 73620
    },
    {
      "epoch": 34.0721888014808,
      "grad_norm": 0.004111196845769882,
      "learning_rate": 0.03185562239703841,
      "loss": 0.0001,
      "step": 73630
    },
    {
      "epoch": 34.076816288755204,
      "grad_norm": 0.11030295491218567,
      "learning_rate": 0.031846367422489585,
      "loss": 0.0003,
      "step": 73640
    },
    {
      "epoch": 34.081443776029616,
      "grad_norm": 1.1059640645980835,
      "learning_rate": 0.03183711244794077,
      "loss": 0.0005,
      "step": 73650
    },
    {
      "epoch": 34.08607126330403,
      "grad_norm": 0.0029879428911954165,
      "learning_rate": 0.031827857473391945,
      "loss": 0.0001,
      "step": 73660
    },
    {
      "epoch": 34.09069875057843,
      "grad_norm": 0.0033789193257689476,
      "learning_rate": 0.03181860249884313,
      "loss": 0.0002,
      "step": 73670
    },
    {
      "epoch": 34.095326237852845,
      "grad_norm": 0.04354868084192276,
      "learning_rate": 0.03180934752429431,
      "loss": 0.0001,
      "step": 73680
    },
    {
      "epoch": 34.09995372512726,
      "grad_norm": 0.0019481700146570802,
      "learning_rate": 0.031800092549745494,
      "loss": 0.0001,
      "step": 73690
    },
    {
      "epoch": 34.10458121240167,
      "grad_norm": 0.2208847552537918,
      "learning_rate": 0.03179083757519667,
      "loss": 0.0001,
      "step": 73700
    },
    {
      "epoch": 34.109208699676074,
      "grad_norm": 0.0008610077784396708,
      "learning_rate": 0.031781582600647854,
      "loss": 0.0005,
      "step": 73710
    },
    {
      "epoch": 34.113836186950486,
      "grad_norm": 0.033711694180965424,
      "learning_rate": 0.03177232762609903,
      "loss": 0.0001,
      "step": 73720
    },
    {
      "epoch": 34.1184636742249,
      "grad_norm": 0.005379467736929655,
      "learning_rate": 0.031763072651550206,
      "loss": 0.0003,
      "step": 73730
    },
    {
      "epoch": 34.1230911614993,
      "grad_norm": 0.003077426226809621,
      "learning_rate": 0.03175381767700139,
      "loss": 0.0002,
      "step": 73740
    },
    {
      "epoch": 34.127718648773715,
      "grad_norm": 0.0016269040061160922,
      "learning_rate": 0.031744562702452565,
      "loss": 0.0002,
      "step": 73750
    },
    {
      "epoch": 34.13234613604813,
      "grad_norm": 0.0039879316464066505,
      "learning_rate": 0.03173530772790375,
      "loss": 0.0001,
      "step": 73760
    },
    {
      "epoch": 34.13697362332253,
      "grad_norm": 0.009103345684707165,
      "learning_rate": 0.03172605275335493,
      "loss": 0.0001,
      "step": 73770
    },
    {
      "epoch": 34.141601110596945,
      "grad_norm": 0.04912774637341499,
      "learning_rate": 0.031716797778806115,
      "loss": 0.0,
      "step": 73780
    },
    {
      "epoch": 34.14622859787136,
      "grad_norm": 0.007020061835646629,
      "learning_rate": 0.03170754280425729,
      "loss": 0.0001,
      "step": 73790
    },
    {
      "epoch": 34.15085608514577,
      "grad_norm": 0.01588151976466179,
      "learning_rate": 0.03169828782970847,
      "loss": 0.0001,
      "step": 73800
    },
    {
      "epoch": 34.155483572420174,
      "grad_norm": 0.12031387537717819,
      "learning_rate": 0.03168903285515965,
      "loss": 0.0001,
      "step": 73810
    },
    {
      "epoch": 34.160111059694586,
      "grad_norm": 0.0008210343075916171,
      "learning_rate": 0.03167977788061083,
      "loss": 0.0001,
      "step": 73820
    },
    {
      "epoch": 34.164738546969,
      "grad_norm": 0.011006449349224567,
      "learning_rate": 0.03167052290606201,
      "loss": 0.0003,
      "step": 73830
    },
    {
      "epoch": 34.1693660342434,
      "grad_norm": 0.010608329437673092,
      "learning_rate": 0.031661267931513186,
      "loss": 0.0,
      "step": 73840
    },
    {
      "epoch": 34.173993521517815,
      "grad_norm": 0.0009505204507149756,
      "learning_rate": 0.03165201295696437,
      "loss": 0.0001,
      "step": 73850
    },
    {
      "epoch": 34.17862100879223,
      "grad_norm": 0.006338014267385006,
      "learning_rate": 0.03164275798241555,
      "loss": 0.0001,
      "step": 73860
    },
    {
      "epoch": 34.18324849606664,
      "grad_norm": 0.005010860040783882,
      "learning_rate": 0.03163350300786673,
      "loss": 0.0002,
      "step": 73870
    },
    {
      "epoch": 34.187875983341044,
      "grad_norm": 0.0009686002158559859,
      "learning_rate": 0.03162424803331791,
      "loss": 0.0021,
      "step": 73880
    },
    {
      "epoch": 34.192503470615456,
      "grad_norm": 0.010122106410562992,
      "learning_rate": 0.03161499305876909,
      "loss": 0.0001,
      "step": 73890
    },
    {
      "epoch": 34.19713095788987,
      "grad_norm": 0.001616099034436047,
      "learning_rate": 0.03160573808422027,
      "loss": 0.0002,
      "step": 73900
    },
    {
      "epoch": 34.20175844516427,
      "grad_norm": 0.002366353990510106,
      "learning_rate": 0.03159648310967145,
      "loss": 0.0019,
      "step": 73910
    },
    {
      "epoch": 34.206385932438685,
      "grad_norm": 0.022244224324822426,
      "learning_rate": 0.03158722813512263,
      "loss": 0.0001,
      "step": 73920
    },
    {
      "epoch": 34.2110134197131,
      "grad_norm": 0.06101468205451965,
      "learning_rate": 0.031577973160573806,
      "loss": 0.0002,
      "step": 73930
    },
    {
      "epoch": 34.2156409069875,
      "grad_norm": 0.035391855984926224,
      "learning_rate": 0.03156871818602499,
      "loss": 0.0002,
      "step": 73940
    },
    {
      "epoch": 34.220268394261915,
      "grad_norm": 0.17404848337173462,
      "learning_rate": 0.03155946321147617,
      "loss": 0.0001,
      "step": 73950
    },
    {
      "epoch": 34.22489588153633,
      "grad_norm": 0.0019051890121772885,
      "learning_rate": 0.03155020823692735,
      "loss": 0.0009,
      "step": 73960
    },
    {
      "epoch": 34.22952336881074,
      "grad_norm": 0.07177552580833435,
      "learning_rate": 0.03154095326237853,
      "loss": 0.0003,
      "step": 73970
    },
    {
      "epoch": 34.234150856085144,
      "grad_norm": 0.026199504733085632,
      "learning_rate": 0.03153169828782971,
      "loss": 0.0001,
      "step": 73980
    },
    {
      "epoch": 34.238778343359556,
      "grad_norm": 0.007465810980647802,
      "learning_rate": 0.03152244331328089,
      "loss": 0.0001,
      "step": 73990
    },
    {
      "epoch": 34.24340583063397,
      "grad_norm": 0.008329960517585278,
      "learning_rate": 0.03151318833873207,
      "loss": 0.0,
      "step": 74000
    },
    {
      "epoch": 34.24803331790837,
      "grad_norm": 0.04855208098888397,
      "learning_rate": 0.03150393336418325,
      "loss": 0.0001,
      "step": 74010
    },
    {
      "epoch": 34.252660805182785,
      "grad_norm": 0.0022006244398653507,
      "learning_rate": 0.03149467838963443,
      "loss": 0.0002,
      "step": 74020
    },
    {
      "epoch": 34.2572882924572,
      "grad_norm": 0.0023279553279280663,
      "learning_rate": 0.03148542341508561,
      "loss": 0.0001,
      "step": 74030
    },
    {
      "epoch": 34.26191577973161,
      "grad_norm": 0.015995558351278305,
      "learning_rate": 0.03147616844053679,
      "loss": 0.0,
      "step": 74040
    },
    {
      "epoch": 34.266543267006014,
      "grad_norm": 0.030024966225028038,
      "learning_rate": 0.03146691346598797,
      "loss": 0.0001,
      "step": 74050
    },
    {
      "epoch": 34.271170754280426,
      "grad_norm": 0.0997450202703476,
      "learning_rate": 0.03145765849143915,
      "loss": 0.0001,
      "step": 74060
    },
    {
      "epoch": 34.27579824155484,
      "grad_norm": 0.020308980718255043,
      "learning_rate": 0.03144840351689033,
      "loss": 0.0001,
      "step": 74070
    },
    {
      "epoch": 34.28042572882924,
      "grad_norm": 0.0020917472429573536,
      "learning_rate": 0.03143914854234151,
      "loss": 0.0,
      "step": 74080
    },
    {
      "epoch": 34.285053216103655,
      "grad_norm": 0.023782148957252502,
      "learning_rate": 0.03142989356779269,
      "loss": 0.0002,
      "step": 74090
    },
    {
      "epoch": 34.28968070337807,
      "grad_norm": 0.07024890929460526,
      "learning_rate": 0.031420638593243864,
      "loss": 0.0001,
      "step": 74100
    },
    {
      "epoch": 34.29430819065247,
      "grad_norm": 0.08360714465379715,
      "learning_rate": 0.03141138361869505,
      "loss": 0.0002,
      "step": 74110
    },
    {
      "epoch": 34.298935677926885,
      "grad_norm": 0.011140759103000164,
      "learning_rate": 0.03140212864414623,
      "loss": 0.0001,
      "step": 74120
    },
    {
      "epoch": 34.3035631652013,
      "grad_norm": 0.006636797450482845,
      "learning_rate": 0.031392873669597414,
      "loss": 0.0001,
      "step": 74130
    },
    {
      "epoch": 34.30819065247571,
      "grad_norm": 0.020673038437962532,
      "learning_rate": 0.03138361869504859,
      "loss": 0.0001,
      "step": 74140
    },
    {
      "epoch": 34.312818139750114,
      "grad_norm": 0.004426096566021442,
      "learning_rate": 0.03137436372049977,
      "loss": 0.0008,
      "step": 74150
    },
    {
      "epoch": 34.317445627024526,
      "grad_norm": 0.00653653871268034,
      "learning_rate": 0.03136510874595095,
      "loss": 0.0001,
      "step": 74160
    },
    {
      "epoch": 34.32207311429894,
      "grad_norm": 0.0016081624198704958,
      "learning_rate": 0.03135585377140213,
      "loss": 0.0001,
      "step": 74170
    },
    {
      "epoch": 34.32670060157334,
      "grad_norm": 0.0010639001848176122,
      "learning_rate": 0.03134659879685331,
      "loss": 0.0,
      "step": 74180
    },
    {
      "epoch": 34.331328088847755,
      "grad_norm": 0.01059714425355196,
      "learning_rate": 0.031337343822304485,
      "loss": 0.0002,
      "step": 74190
    },
    {
      "epoch": 34.33595557612217,
      "grad_norm": 0.009511446580290794,
      "learning_rate": 0.03132808884775567,
      "loss": 0.0001,
      "step": 74200
    },
    {
      "epoch": 34.34058306339658,
      "grad_norm": 0.019158730283379555,
      "learning_rate": 0.03131883387320685,
      "loss": 0.0001,
      "step": 74210
    },
    {
      "epoch": 34.345210550670984,
      "grad_norm": 0.0045982226729393005,
      "learning_rate": 0.031309578898658035,
      "loss": 0.0,
      "step": 74220
    },
    {
      "epoch": 34.349838037945396,
      "grad_norm": 0.014308439567685127,
      "learning_rate": 0.03130032392410921,
      "loss": 0.0,
      "step": 74230
    },
    {
      "epoch": 34.35446552521981,
      "grad_norm": 0.001267909654416144,
      "learning_rate": 0.031291068949560394,
      "loss": 0.0001,
      "step": 74240
    },
    {
      "epoch": 34.35909301249421,
      "grad_norm": 0.0012038338463753462,
      "learning_rate": 0.03128181397501157,
      "loss": 0.0,
      "step": 74250
    },
    {
      "epoch": 34.363720499768625,
      "grad_norm": 0.019024211913347244,
      "learning_rate": 0.031272559000462746,
      "loss": 0.0001,
      "step": 74260
    },
    {
      "epoch": 34.36834798704304,
      "grad_norm": 0.43124592304229736,
      "learning_rate": 0.03126330402591393,
      "loss": 0.0002,
      "step": 74270
    },
    {
      "epoch": 34.37297547431744,
      "grad_norm": 0.45255085825920105,
      "learning_rate": 0.031254049051365106,
      "loss": 0.0004,
      "step": 74280
    },
    {
      "epoch": 34.377602961591855,
      "grad_norm": 0.00025433843256905675,
      "learning_rate": 0.031244794076816292,
      "loss": 0.0001,
      "step": 74290
    },
    {
      "epoch": 34.38223044886627,
      "grad_norm": 0.007521558552980423,
      "learning_rate": 0.03123553910226747,
      "loss": 0.0001,
      "step": 74300
    },
    {
      "epoch": 34.38685793614068,
      "grad_norm": 0.0014409433351829648,
      "learning_rate": 0.03122628412771865,
      "loss": 0.0,
      "step": 74310
    },
    {
      "epoch": 34.391485423415084,
      "grad_norm": 0.062240466475486755,
      "learning_rate": 0.03121702915316983,
      "loss": 0.0001,
      "step": 74320
    },
    {
      "epoch": 34.396112910689496,
      "grad_norm": 0.015696464106440544,
      "learning_rate": 0.031207774178621007,
      "loss": 0.0003,
      "step": 74330
    },
    {
      "epoch": 34.40074039796391,
      "grad_norm": 0.0003856536350212991,
      "learning_rate": 0.03119851920407219,
      "loss": 0.0001,
      "step": 74340
    },
    {
      "epoch": 34.40536788523831,
      "grad_norm": 0.008550170809030533,
      "learning_rate": 0.031189264229523367,
      "loss": 0.0001,
      "step": 74350
    },
    {
      "epoch": 34.409995372512725,
      "grad_norm": 0.04835593327879906,
      "learning_rate": 0.03118000925497455,
      "loss": 0.0001,
      "step": 74360
    },
    {
      "epoch": 34.41462285978714,
      "grad_norm": 0.001840637531131506,
      "learning_rate": 0.03117075428042573,
      "loss": 0.0,
      "step": 74370
    },
    {
      "epoch": 34.41925034706154,
      "grad_norm": 0.003992804326117039,
      "learning_rate": 0.031161499305876913,
      "loss": 0.0001,
      "step": 74380
    },
    {
      "epoch": 34.423877834335954,
      "grad_norm": 0.006947949528694153,
      "learning_rate": 0.03115224433132809,
      "loss": 0.0,
      "step": 74390
    },
    {
      "epoch": 34.428505321610366,
      "grad_norm": 0.0043019382283091545,
      "learning_rate": 0.031142989356779272,
      "loss": 0.0,
      "step": 74400
    },
    {
      "epoch": 34.43313280888478,
      "grad_norm": 0.003160440595820546,
      "learning_rate": 0.031133734382230452,
      "loss": 0.0001,
      "step": 74410
    },
    {
      "epoch": 34.43776029615918,
      "grad_norm": 0.009279572404921055,
      "learning_rate": 0.031124479407681628,
      "loss": 0.0001,
      "step": 74420
    },
    {
      "epoch": 34.442387783433595,
      "grad_norm": 0.20495133101940155,
      "learning_rate": 0.03111522443313281,
      "loss": 0.0001,
      "step": 74430
    },
    {
      "epoch": 34.44701527070801,
      "grad_norm": 0.006576539017260075,
      "learning_rate": 0.031105969458583987,
      "loss": 0.0001,
      "step": 74440
    },
    {
      "epoch": 34.45164275798241,
      "grad_norm": 0.021825900301337242,
      "learning_rate": 0.03109671448403517,
      "loss": 0.0001,
      "step": 74450
    },
    {
      "epoch": 34.456270245256825,
      "grad_norm": 0.0015827668830752373,
      "learning_rate": 0.03108745950948635,
      "loss": 0.0001,
      "step": 74460
    },
    {
      "epoch": 34.46089773253124,
      "grad_norm": 0.03888735920190811,
      "learning_rate": 0.031078204534937533,
      "loss": 0.0001,
      "step": 74470
    },
    {
      "epoch": 34.46552521980565,
      "grad_norm": 0.01323938462883234,
      "learning_rate": 0.03106894956038871,
      "loss": 0.0001,
      "step": 74480
    },
    {
      "epoch": 34.470152707080054,
      "grad_norm": 0.005029055289924145,
      "learning_rate": 0.03105969458583989,
      "loss": 0.0,
      "step": 74490
    },
    {
      "epoch": 34.474780194354466,
      "grad_norm": 0.03666596859693527,
      "learning_rate": 0.031050439611291072,
      "loss": 0.0001,
      "step": 74500
    },
    {
      "epoch": 34.47940768162888,
      "grad_norm": 0.04977922886610031,
      "learning_rate": 0.03104118463674225,
      "loss": 0.0003,
      "step": 74510
    },
    {
      "epoch": 34.48403516890328,
      "grad_norm": 0.003541730111464858,
      "learning_rate": 0.031031929662193432,
      "loss": 0.0001,
      "step": 74520
    },
    {
      "epoch": 34.488662656177695,
      "grad_norm": 0.0075627719052135944,
      "learning_rate": 0.031022674687644608,
      "loss": 0.0001,
      "step": 74530
    },
    {
      "epoch": 34.49329014345211,
      "grad_norm": 0.009024156257510185,
      "learning_rate": 0.03101341971309579,
      "loss": 0.0001,
      "step": 74540
    },
    {
      "epoch": 34.49791763072651,
      "grad_norm": 0.08444640785455704,
      "learning_rate": 0.03100416473854697,
      "loss": 0.0001,
      "step": 74550
    },
    {
      "epoch": 34.502545118000924,
      "grad_norm": 0.0023832903243601322,
      "learning_rate": 0.030994909763998147,
      "loss": 0.0,
      "step": 74560
    },
    {
      "epoch": 34.507172605275336,
      "grad_norm": 0.04510495811700821,
      "learning_rate": 0.03098565478944933,
      "loss": 0.0001,
      "step": 74570
    },
    {
      "epoch": 34.51180009254975,
      "grad_norm": 1.8184006214141846,
      "learning_rate": 0.03097639981490051,
      "loss": 0.0003,
      "step": 74580
    },
    {
      "epoch": 34.51642757982415,
      "grad_norm": 0.005690561607480049,
      "learning_rate": 0.030967144840351693,
      "loss": 0.0001,
      "step": 74590
    },
    {
      "epoch": 34.521055067098565,
      "grad_norm": 0.018909411504864693,
      "learning_rate": 0.03095788986580287,
      "loss": 0.0001,
      "step": 74600
    },
    {
      "epoch": 34.52568255437298,
      "grad_norm": 0.003517025150358677,
      "learning_rate": 0.030948634891254052,
      "loss": 0.0012,
      "step": 74610
    },
    {
      "epoch": 34.53031004164738,
      "grad_norm": 0.021177256479859352,
      "learning_rate": 0.03093937991670523,
      "loss": 0.0001,
      "step": 74620
    },
    {
      "epoch": 34.534937528921795,
      "grad_norm": 0.014375491999089718,
      "learning_rate": 0.03093012494215641,
      "loss": 0.0001,
      "step": 74630
    },
    {
      "epoch": 34.53956501619621,
      "grad_norm": 0.06642564386129379,
      "learning_rate": 0.03092086996760759,
      "loss": 0.0002,
      "step": 74640
    },
    {
      "epoch": 34.54419250347062,
      "grad_norm": 0.02049504965543747,
      "learning_rate": 0.030911614993058768,
      "loss": 0.0001,
      "step": 74650
    },
    {
      "epoch": 34.548819990745024,
      "grad_norm": 0.00843117292970419,
      "learning_rate": 0.03090236001850995,
      "loss": 0.0001,
      "step": 74660
    },
    {
      "epoch": 34.553447478019436,
      "grad_norm": 0.005712234415113926,
      "learning_rate": 0.03089310504396113,
      "loss": 0.0001,
      "step": 74670
    },
    {
      "epoch": 34.55807496529385,
      "grad_norm": 0.006362888962030411,
      "learning_rate": 0.030883850069412314,
      "loss": 0.0002,
      "step": 74680
    },
    {
      "epoch": 34.56270245256825,
      "grad_norm": 0.01565072499215603,
      "learning_rate": 0.03087459509486349,
      "loss": 0.0,
      "step": 74690
    },
    {
      "epoch": 34.567329939842665,
      "grad_norm": 0.006817298475652933,
      "learning_rate": 0.030865340120314673,
      "loss": 0.0001,
      "step": 74700
    },
    {
      "epoch": 34.57195742711708,
      "grad_norm": 0.007386486977338791,
      "learning_rate": 0.03085608514576585,
      "loss": 0.0001,
      "step": 74710
    },
    {
      "epoch": 34.57658491439148,
      "grad_norm": 0.001838970696553588,
      "learning_rate": 0.03084683017121703,
      "loss": 0.0001,
      "step": 74720
    },
    {
      "epoch": 34.581212401665894,
      "grad_norm": 0.023038044571876526,
      "learning_rate": 0.030837575196668212,
      "loss": 0.0001,
      "step": 74730
    },
    {
      "epoch": 34.585839888940306,
      "grad_norm": 0.03052663616836071,
      "learning_rate": 0.030828320222119388,
      "loss": 0.0001,
      "step": 74740
    },
    {
      "epoch": 34.59046737621472,
      "grad_norm": 0.008086953312158585,
      "learning_rate": 0.03081906524757057,
      "loss": 0.0001,
      "step": 74750
    },
    {
      "epoch": 34.59509486348912,
      "grad_norm": 0.012447766028344631,
      "learning_rate": 0.03080981027302175,
      "loss": 0.0001,
      "step": 74760
    },
    {
      "epoch": 34.599722350763535,
      "grad_norm": 0.02781999297440052,
      "learning_rate": 0.030800555298472934,
      "loss": 0.0001,
      "step": 74770
    },
    {
      "epoch": 34.60434983803795,
      "grad_norm": 0.01654542237520218,
      "learning_rate": 0.03079130032392411,
      "loss": 0.0002,
      "step": 74780
    },
    {
      "epoch": 34.60897732531235,
      "grad_norm": 0.0277048759162426,
      "learning_rate": 0.030782045349375287,
      "loss": 0.0001,
      "step": 74790
    },
    {
      "epoch": 34.613604812586765,
      "grad_norm": 0.0015939332079142332,
      "learning_rate": 0.03077279037482647,
      "loss": 0.0001,
      "step": 74800
    },
    {
      "epoch": 34.61823229986118,
      "grad_norm": 0.004025445785373449,
      "learning_rate": 0.03076353540027765,
      "loss": 0.0001,
      "step": 74810
    },
    {
      "epoch": 34.62285978713559,
      "grad_norm": 0.006390748545527458,
      "learning_rate": 0.030754280425728833,
      "loss": 0.0001,
      "step": 74820
    },
    {
      "epoch": 34.627487274409994,
      "grad_norm": 0.0550130233168602,
      "learning_rate": 0.03074502545118001,
      "loss": 0.0001,
      "step": 74830
    },
    {
      "epoch": 34.632114761684406,
      "grad_norm": 0.0017169566126540303,
      "learning_rate": 0.030735770476631192,
      "loss": 0.0002,
      "step": 74840
    },
    {
      "epoch": 34.63674224895882,
      "grad_norm": 0.003530017100274563,
      "learning_rate": 0.03072651550208237,
      "loss": 0.0,
      "step": 74850
    },
    {
      "epoch": 34.64136973623322,
      "grad_norm": 0.0011748578399419785,
      "learning_rate": 0.030717260527533555,
      "loss": 0.0001,
      "step": 74860
    },
    {
      "epoch": 34.645997223507635,
      "grad_norm": 0.09886026382446289,
      "learning_rate": 0.03070800555298473,
      "loss": 0.0004,
      "step": 74870
    },
    {
      "epoch": 34.65062471078205,
      "grad_norm": 0.0009884331375360489,
      "learning_rate": 0.030698750578435907,
      "loss": 0.0002,
      "step": 74880
    },
    {
      "epoch": 34.65525219805645,
      "grad_norm": 0.009572113864123821,
      "learning_rate": 0.03068949560388709,
      "loss": 0.0001,
      "step": 74890
    },
    {
      "epoch": 34.659879685330864,
      "grad_norm": 0.0031624094117432833,
      "learning_rate": 0.03068024062933827,
      "loss": 0.0001,
      "step": 74900
    },
    {
      "epoch": 34.664507172605276,
      "grad_norm": 3.3158109188079834,
      "learning_rate": 0.030670985654789453,
      "loss": 0.0008,
      "step": 74910
    },
    {
      "epoch": 34.66913465987969,
      "grad_norm": 0.008542941883206367,
      "learning_rate": 0.03066173068024063,
      "loss": 0.0001,
      "step": 74920
    },
    {
      "epoch": 34.67376214715409,
      "grad_norm": 0.005338464397937059,
      "learning_rate": 0.030652475705691812,
      "loss": 0.0,
      "step": 74930
    },
    {
      "epoch": 34.678389634428505,
      "grad_norm": 0.003045510035008192,
      "learning_rate": 0.030643220731142992,
      "loss": 0.0001,
      "step": 74940
    },
    {
      "epoch": 34.68301712170292,
      "grad_norm": 0.0015923080500215292,
      "learning_rate": 0.03063396575659417,
      "loss": 0.0,
      "step": 74950
    },
    {
      "epoch": 34.68764460897732,
      "grad_norm": 0.004127738066017628,
      "learning_rate": 0.03062471078204535,
      "loss": 0.0001,
      "step": 74960
    },
    {
      "epoch": 34.692272096251735,
      "grad_norm": 0.009956523776054382,
      "learning_rate": 0.030615455807496528,
      "loss": 0.0001,
      "step": 74970
    },
    {
      "epoch": 34.69689958352615,
      "grad_norm": 0.01730743609368801,
      "learning_rate": 0.03060620083294771,
      "loss": 0.0006,
      "step": 74980
    },
    {
      "epoch": 34.70152707080056,
      "grad_norm": 0.06499133259057999,
      "learning_rate": 0.03059694585839889,
      "loss": 0.0004,
      "step": 74990
    },
    {
      "epoch": 34.706154558074964,
      "grad_norm": 0.0027613413985818624,
      "learning_rate": 0.030587690883850074,
      "loss": 0.0,
      "step": 75000
    },
    {
      "epoch": 34.710782045349376,
      "grad_norm": 0.0007119038491509855,
      "learning_rate": 0.03057843590930125,
      "loss": 0.0001,
      "step": 75010
    },
    {
      "epoch": 34.71540953262379,
      "grad_norm": 0.001105044037103653,
      "learning_rate": 0.03056918093475243,
      "loss": 0.0001,
      "step": 75020
    },
    {
      "epoch": 34.72003701989819,
      "grad_norm": 0.0058526769280433655,
      "learning_rate": 0.030559925960203613,
      "loss": 0.0001,
      "step": 75030
    },
    {
      "epoch": 34.724664507172605,
      "grad_norm": 0.08430033177137375,
      "learning_rate": 0.03055067098565479,
      "loss": 0.0004,
      "step": 75040
    },
    {
      "epoch": 34.72929199444702,
      "grad_norm": 0.01654278114438057,
      "learning_rate": 0.030541416011105972,
      "loss": 0.0001,
      "step": 75050
    },
    {
      "epoch": 34.73391948172142,
      "grad_norm": 0.0010993643663823605,
      "learning_rate": 0.03053216103655715,
      "loss": 0.0001,
      "step": 75060
    },
    {
      "epoch": 34.738546968995834,
      "grad_norm": 0.008786472491919994,
      "learning_rate": 0.03052290606200833,
      "loss": 0.0001,
      "step": 75070
    },
    {
      "epoch": 34.743174456270246,
      "grad_norm": 0.003262088866904378,
      "learning_rate": 0.03051365108745951,
      "loss": 0.0001,
      "step": 75080
    },
    {
      "epoch": 34.74780194354466,
      "grad_norm": 0.041510965675115585,
      "learning_rate": 0.030504396112910694,
      "loss": 0.0001,
      "step": 75090
    },
    {
      "epoch": 34.75242943081906,
      "grad_norm": 0.011069738306105137,
      "learning_rate": 0.03049514113836187,
      "loss": 0.0001,
      "step": 75100
    },
    {
      "epoch": 34.757056918093475,
      "grad_norm": 0.012211179360747337,
      "learning_rate": 0.03048588616381305,
      "loss": 0.0002,
      "step": 75110
    },
    {
      "epoch": 34.76168440536789,
      "grad_norm": 0.7302024960517883,
      "learning_rate": 0.030476631189264233,
      "loss": 0.0002,
      "step": 75120
    },
    {
      "epoch": 34.76631189264229,
      "grad_norm": 0.02260369062423706,
      "learning_rate": 0.03046737621471541,
      "loss": 0.0002,
      "step": 75130
    },
    {
      "epoch": 34.770939379916705,
      "grad_norm": 0.004921809304505587,
      "learning_rate": 0.030458121240166593,
      "loss": 0.0001,
      "step": 75140
    },
    {
      "epoch": 34.77556686719112,
      "grad_norm": 0.0008716654265299439,
      "learning_rate": 0.03044886626561777,
      "loss": 0.0001,
      "step": 75150
    },
    {
      "epoch": 34.78019435446552,
      "grad_norm": 0.0021216555032879114,
      "learning_rate": 0.030439611291068952,
      "loss": 0.0001,
      "step": 75160
    },
    {
      "epoch": 34.784821841739934,
      "grad_norm": 0.1064983382821083,
      "learning_rate": 0.03043035631652013,
      "loss": 0.0002,
      "step": 75170
    },
    {
      "epoch": 34.789449329014346,
      "grad_norm": 0.018149444833397865,
      "learning_rate": 0.030421101341971308,
      "loss": 0.0001,
      "step": 75180
    },
    {
      "epoch": 34.79407681628876,
      "grad_norm": 0.03686656430363655,
      "learning_rate": 0.03041184636742249,
      "loss": 0.0001,
      "step": 75190
    },
    {
      "epoch": 34.79870430356316,
      "grad_norm": 0.0797724574804306,
      "learning_rate": 0.03040259139287367,
      "loss": 0.0002,
      "step": 75200
    },
    {
      "epoch": 34.803331790837575,
      "grad_norm": 0.0034146399702876806,
      "learning_rate": 0.030393336418324854,
      "loss": 0.0001,
      "step": 75210
    },
    {
      "epoch": 34.80795927811199,
      "grad_norm": 0.007170849945396185,
      "learning_rate": 0.03038408144377603,
      "loss": 0.0003,
      "step": 75220
    },
    {
      "epoch": 34.81258676538639,
      "grad_norm": 0.024533076211810112,
      "learning_rate": 0.030374826469227213,
      "loss": 0.0001,
      "step": 75230
    },
    {
      "epoch": 34.817214252660804,
      "grad_norm": 0.002342014340683818,
      "learning_rate": 0.03036557149467839,
      "loss": 0.0001,
      "step": 75240
    },
    {
      "epoch": 34.821841739935216,
      "grad_norm": 0.004117359407246113,
      "learning_rate": 0.03035631652012957,
      "loss": 0.0001,
      "step": 75250
    },
    {
      "epoch": 34.82646922720963,
      "grad_norm": 0.11257560551166534,
      "learning_rate": 0.030347061545580752,
      "loss": 0.0005,
      "step": 75260
    },
    {
      "epoch": 34.83109671448403,
      "grad_norm": 0.005164088681340218,
      "learning_rate": 0.03033780657103193,
      "loss": 0.0001,
      "step": 75270
    },
    {
      "epoch": 34.835724201758445,
      "grad_norm": 0.011323005892336369,
      "learning_rate": 0.03032855159648311,
      "loss": 0.0001,
      "step": 75280
    },
    {
      "epoch": 34.84035168903286,
      "grad_norm": 0.004830933175981045,
      "learning_rate": 0.03031929662193429,
      "loss": 0.0001,
      "step": 75290
    },
    {
      "epoch": 34.84497917630726,
      "grad_norm": 0.010318741202354431,
      "learning_rate": 0.030310041647385474,
      "loss": 0.0002,
      "step": 75300
    },
    {
      "epoch": 34.849606663581675,
      "grad_norm": 0.0014962578425183892,
      "learning_rate": 0.03030078667283665,
      "loss": 0.0,
      "step": 75310
    },
    {
      "epoch": 34.85423415085609,
      "grad_norm": 0.0017987689934670925,
      "learning_rate": 0.030291531698287834,
      "loss": 0.0002,
      "step": 75320
    },
    {
      "epoch": 34.85886163813049,
      "grad_norm": 0.005023301113396883,
      "learning_rate": 0.03028227672373901,
      "loss": 0.0008,
      "step": 75330
    },
    {
      "epoch": 34.863489125404904,
      "grad_norm": 0.01502055861055851,
      "learning_rate": 0.03027302174919019,
      "loss": 0.0001,
      "step": 75340
    },
    {
      "epoch": 34.868116612679316,
      "grad_norm": 0.011768151074647903,
      "learning_rate": 0.030263766774641373,
      "loss": 0.0001,
      "step": 75350
    },
    {
      "epoch": 34.87274409995373,
      "grad_norm": 0.0028903670608997345,
      "learning_rate": 0.03025451180009255,
      "loss": 0.0,
      "step": 75360
    },
    {
      "epoch": 34.87737158722813,
      "grad_norm": 0.0056351106613874435,
      "learning_rate": 0.030245256825543732,
      "loss": 0.0001,
      "step": 75370
    },
    {
      "epoch": 34.881999074502545,
      "grad_norm": 0.0032793693244457245,
      "learning_rate": 0.030236001850994912,
      "loss": 0.0001,
      "step": 75380
    },
    {
      "epoch": 34.88662656177696,
      "grad_norm": 1.2626017332077026,
      "learning_rate": 0.030226746876446095,
      "loss": 0.0004,
      "step": 75390
    },
    {
      "epoch": 34.89125404905136,
      "grad_norm": 0.0035302075557410717,
      "learning_rate": 0.03021749190189727,
      "loss": 0.0,
      "step": 75400
    },
    {
      "epoch": 34.895881536325774,
      "grad_norm": 0.04957644268870354,
      "learning_rate": 0.030208236927348447,
      "loss": 0.0001,
      "step": 75410
    },
    {
      "epoch": 34.900509023600186,
      "grad_norm": 0.07796327024698257,
      "learning_rate": 0.03019898195279963,
      "loss": 0.0002,
      "step": 75420
    },
    {
      "epoch": 34.9051365108746,
      "grad_norm": 0.006941666826605797,
      "learning_rate": 0.03018972697825081,
      "loss": 0.0,
      "step": 75430
    },
    {
      "epoch": 34.909763998149,
      "grad_norm": 0.0037672037724405527,
      "learning_rate": 0.030180472003701993,
      "loss": 0.0001,
      "step": 75440
    },
    {
      "epoch": 34.914391485423415,
      "grad_norm": 0.0008501929696649313,
      "learning_rate": 0.03017121702915317,
      "loss": 0.0002,
      "step": 75450
    },
    {
      "epoch": 34.91901897269783,
      "grad_norm": 0.010647665709257126,
      "learning_rate": 0.030161962054604353,
      "loss": 0.0002,
      "step": 75460
    },
    {
      "epoch": 34.92364645997223,
      "grad_norm": 0.009226326830685139,
      "learning_rate": 0.030152707080055532,
      "loss": 0.0001,
      "step": 75470
    },
    {
      "epoch": 34.928273947246645,
      "grad_norm": 0.03791488707065582,
      "learning_rate": 0.03014345210550671,
      "loss": 0.0002,
      "step": 75480
    },
    {
      "epoch": 34.93290143452106,
      "grad_norm": 0.002504638396203518,
      "learning_rate": 0.030134197130957892,
      "loss": 0.0001,
      "step": 75490
    },
    {
      "epoch": 34.93752892179546,
      "grad_norm": 0.0013068282278254628,
      "learning_rate": 0.030124942156409068,
      "loss": 0.0001,
      "step": 75500
    },
    {
      "epoch": 34.942156409069874,
      "grad_norm": 0.10125775635242462,
      "learning_rate": 0.03011568718186025,
      "loss": 0.0001,
      "step": 75510
    },
    {
      "epoch": 34.946783896344286,
      "grad_norm": 0.0036213400308042765,
      "learning_rate": 0.03010643220731143,
      "loss": 0.0005,
      "step": 75520
    },
    {
      "epoch": 34.9514113836187,
      "grad_norm": 0.0017934987554326653,
      "learning_rate": 0.030097177232762614,
      "loss": 0.0001,
      "step": 75530
    },
    {
      "epoch": 34.9560388708931,
      "grad_norm": 0.002890843665227294,
      "learning_rate": 0.03008792225821379,
      "loss": 0.0001,
      "step": 75540
    },
    {
      "epoch": 34.960666358167515,
      "grad_norm": 0.011658906936645508,
      "learning_rate": 0.030078667283664973,
      "loss": 0.0001,
      "step": 75550
    },
    {
      "epoch": 34.96529384544193,
      "grad_norm": 0.03874460607767105,
      "learning_rate": 0.030069412309116153,
      "loss": 0.0001,
      "step": 75560
    },
    {
      "epoch": 34.96992133271633,
      "grad_norm": 0.014864827506244183,
      "learning_rate": 0.03006015733456733,
      "loss": 0.0001,
      "step": 75570
    },
    {
      "epoch": 34.974548819990744,
      "grad_norm": 0.0036607864312827587,
      "learning_rate": 0.030050902360018512,
      "loss": 0.0001,
      "step": 75580
    },
    {
      "epoch": 34.979176307265156,
      "grad_norm": 0.0060790120624005795,
      "learning_rate": 0.03004164738546969,
      "loss": 0.0,
      "step": 75590
    },
    {
      "epoch": 34.98380379453957,
      "grad_norm": 0.009550448507070541,
      "learning_rate": 0.030032392410920872,
      "loss": 0.0001,
      "step": 75600
    },
    {
      "epoch": 34.98843128181397,
      "grad_norm": 0.008402926847338676,
      "learning_rate": 0.03002313743637205,
      "loss": 0.0001,
      "step": 75610
    },
    {
      "epoch": 34.993058769088385,
      "grad_norm": 0.011406555771827698,
      "learning_rate": 0.030013882461823235,
      "loss": 0.0003,
      "step": 75620
    },
    {
      "epoch": 34.9976862563628,
      "grad_norm": 0.0033629240933805704,
      "learning_rate": 0.03000462748727441,
      "loss": 0.0001,
      "step": 75630
    },
    {
      "epoch": 35.0,
      "eval_accuracy_branch1": 0.9902172238484055,
      "eval_accuracy_branch2": 0.4993067323987059,
      "eval_f1_branch1": 0.9910446655927104,
      "eval_f1_branch2": 0.49895900231427825,
      "eval_loss": 0.01851900853216648,
      "eval_precision_branch1": 0.9912556701375618,
      "eval_precision_branch2": 0.49930480248805476,
      "eval_recall_branch1": 0.9909764297419847,
      "eval_recall_branch2": 0.49930673239870593,
      "eval_runtime": 29.0893,
      "eval_samples_per_second": 446.281,
      "eval_steps_per_second": 55.794,
      "step": 75635
    },
    {
      "epoch": 35.0023137436372,
      "grad_norm": 0.027722034603357315,
      "learning_rate": 0.02999537251272559,
      "loss": 0.0383,
      "step": 75640
    },
    {
      "epoch": 35.006941230911615,
      "grad_norm": 0.008478470146656036,
      "learning_rate": 0.029986117538176774,
      "loss": 0.0002,
      "step": 75650
    },
    {
      "epoch": 35.01156871818603,
      "grad_norm": 0.010622096247971058,
      "learning_rate": 0.02997686256362795,
      "loss": 0.0001,
      "step": 75660
    },
    {
      "epoch": 35.01619620546043,
      "grad_norm": 0.005322740413248539,
      "learning_rate": 0.029967607589079133,
      "loss": 0.0001,
      "step": 75670
    },
    {
      "epoch": 35.020823692734844,
      "grad_norm": 0.007148203905671835,
      "learning_rate": 0.02995835261453031,
      "loss": 0.0003,
      "step": 75680
    },
    {
      "epoch": 35.025451180009256,
      "grad_norm": 0.01045103371143341,
      "learning_rate": 0.029949097639981492,
      "loss": 0.0002,
      "step": 75690
    },
    {
      "epoch": 35.03007866728367,
      "grad_norm": 0.0028422423638403416,
      "learning_rate": 0.029939842665432672,
      "loss": 0.0002,
      "step": 75700
    },
    {
      "epoch": 35.03470615455807,
      "grad_norm": 0.0007857083692215383,
      "learning_rate": 0.029930587690883848,
      "loss": 0.0002,
      "step": 75710
    },
    {
      "epoch": 35.039333641832485,
      "grad_norm": 0.0057824295945465565,
      "learning_rate": 0.02992133271633503,
      "loss": 0.0001,
      "step": 75720
    },
    {
      "epoch": 35.0439611291069,
      "grad_norm": 0.004220691509544849,
      "learning_rate": 0.02991207774178621,
      "loss": 0.0001,
      "step": 75730
    },
    {
      "epoch": 35.0485886163813,
      "grad_norm": 0.004337785299867392,
      "learning_rate": 0.029902822767237394,
      "loss": 0.0,
      "step": 75740
    },
    {
      "epoch": 35.053216103655714,
      "grad_norm": 0.014079057611525059,
      "learning_rate": 0.02989356779268857,
      "loss": 0.0,
      "step": 75750
    },
    {
      "epoch": 35.057843590930126,
      "grad_norm": 0.05102880671620369,
      "learning_rate": 0.029884312818139754,
      "loss": 0.0003,
      "step": 75760
    },
    {
      "epoch": 35.06247107820454,
      "grad_norm": 0.06255032867193222,
      "learning_rate": 0.02987505784359093,
      "loss": 0.0001,
      "step": 75770
    },
    {
      "epoch": 35.06709856547894,
      "grad_norm": 0.0075696418061852455,
      "learning_rate": 0.029865802869042113,
      "loss": 0.0001,
      "step": 75780
    },
    {
      "epoch": 35.071726052753355,
      "grad_norm": 0.007558939978480339,
      "learning_rate": 0.029856547894493293,
      "loss": 0.0001,
      "step": 75790
    },
    {
      "epoch": 35.07635354002777,
      "grad_norm": 0.004648308269679546,
      "learning_rate": 0.02984729291994447,
      "loss": 0.0001,
      "step": 75800
    },
    {
      "epoch": 35.08098102730217,
      "grad_norm": 0.005718925967812538,
      "learning_rate": 0.029838037945395652,
      "loss": 0.0002,
      "step": 75810
    },
    {
      "epoch": 35.085608514576585,
      "grad_norm": 0.011074593290686607,
      "learning_rate": 0.02982878297084683,
      "loss": 0.0003,
      "step": 75820
    },
    {
      "epoch": 35.090236001851,
      "grad_norm": 0.013727548532187939,
      "learning_rate": 0.029819527996298015,
      "loss": 0.0001,
      "step": 75830
    },
    {
      "epoch": 35.0948634891254,
      "grad_norm": 0.35296887159347534,
      "learning_rate": 0.02981027302174919,
      "loss": 0.0002,
      "step": 75840
    },
    {
      "epoch": 35.099490976399814,
      "grad_norm": 0.2797434329986572,
      "learning_rate": 0.029801018047200374,
      "loss": 0.0002,
      "step": 75850
    },
    {
      "epoch": 35.104118463674226,
      "grad_norm": 0.005550308618694544,
      "learning_rate": 0.02979176307265155,
      "loss": 0.0001,
      "step": 75860
    },
    {
      "epoch": 35.10874595094864,
      "grad_norm": 0.03644677996635437,
      "learning_rate": 0.02978250809810273,
      "loss": 0.0001,
      "step": 75870
    },
    {
      "epoch": 35.11337343822304,
      "grad_norm": 0.004778238013386726,
      "learning_rate": 0.029773253123553913,
      "loss": 0.0,
      "step": 75880
    },
    {
      "epoch": 35.118000925497455,
      "grad_norm": 0.013281790539622307,
      "learning_rate": 0.02976399814900509,
      "loss": 0.0001,
      "step": 75890
    },
    {
      "epoch": 35.12262841277187,
      "grad_norm": 0.007330492604523897,
      "learning_rate": 0.029754743174456273,
      "loss": 0.0001,
      "step": 75900
    },
    {
      "epoch": 35.12725590004627,
      "grad_norm": 0.003212308045476675,
      "learning_rate": 0.029745488199907452,
      "loss": 0.0001,
      "step": 75910
    },
    {
      "epoch": 35.131883387320684,
      "grad_norm": 0.02329695224761963,
      "learning_rate": 0.029736233225358635,
      "loss": 0.0001,
      "step": 75920
    },
    {
      "epoch": 35.136510874595096,
      "grad_norm": 0.04918743669986725,
      "learning_rate": 0.02972697825080981,
      "loss": 0.0001,
      "step": 75930
    },
    {
      "epoch": 35.14113836186951,
      "grad_norm": 0.010767892934381962,
      "learning_rate": 0.029717723276260988,
      "loss": 0.0001,
      "step": 75940
    },
    {
      "epoch": 35.14576584914391,
      "grad_norm": 0.0023748043458908796,
      "learning_rate": 0.02970846830171217,
      "loss": 0.0019,
      "step": 75950
    },
    {
      "epoch": 35.150393336418325,
      "grad_norm": 0.0008969808113761246,
      "learning_rate": 0.02969921332716335,
      "loss": 0.0002,
      "step": 75960
    },
    {
      "epoch": 35.15502082369274,
      "grad_norm": 0.003983171656727791,
      "learning_rate": 0.029689958352614534,
      "loss": 0.0002,
      "step": 75970
    },
    {
      "epoch": 35.15964831096714,
      "grad_norm": 0.18715892732143402,
      "learning_rate": 0.02968070337806571,
      "loss": 0.0005,
      "step": 75980
    },
    {
      "epoch": 35.164275798241555,
      "grad_norm": 0.0027877085376530886,
      "learning_rate": 0.029671448403516893,
      "loss": 0.0002,
      "step": 75990
    },
    {
      "epoch": 35.16890328551597,
      "grad_norm": 0.010617848485708237,
      "learning_rate": 0.029662193428968073,
      "loss": 0.0001,
      "step": 76000
    },
    {
      "epoch": 35.17353077279037,
      "grad_norm": 0.0054012504406273365,
      "learning_rate": 0.02965293845441925,
      "loss": 0.0001,
      "step": 76010
    },
    {
      "epoch": 35.178158260064784,
      "grad_norm": 0.08892476558685303,
      "learning_rate": 0.029643683479870432,
      "loss": 0.0003,
      "step": 76020
    },
    {
      "epoch": 35.182785747339196,
      "grad_norm": 0.0016272180946543813,
      "learning_rate": 0.02963442850532161,
      "loss": 0.0001,
      "step": 76030
    },
    {
      "epoch": 35.18741323461361,
      "grad_norm": 0.020040670409798622,
      "learning_rate": 0.02962517353077279,
      "loss": 0.0002,
      "step": 76040
    },
    {
      "epoch": 35.19204072188801,
      "grad_norm": 0.005005380604416132,
      "learning_rate": 0.02961591855622397,
      "loss": 0.0002,
      "step": 76050
    },
    {
      "epoch": 35.196668209162425,
      "grad_norm": 0.008281942456960678,
      "learning_rate": 0.029606663581675154,
      "loss": 0.0001,
      "step": 76060
    },
    {
      "epoch": 35.20129569643684,
      "grad_norm": 0.0040634023025631905,
      "learning_rate": 0.02959740860712633,
      "loss": 0.0002,
      "step": 76070
    },
    {
      "epoch": 35.20592318371124,
      "grad_norm": 0.0012994783464819193,
      "learning_rate": 0.029588153632577514,
      "loss": 0.0001,
      "step": 76080
    },
    {
      "epoch": 35.210550670985654,
      "grad_norm": 0.012821645475924015,
      "learning_rate": 0.029578898658028693,
      "loss": 0.0002,
      "step": 76090
    },
    {
      "epoch": 35.215178158260066,
      "grad_norm": 0.01294629741460085,
      "learning_rate": 0.02956964368347987,
      "loss": 0.0,
      "step": 76100
    },
    {
      "epoch": 35.21980564553448,
      "grad_norm": 0.0010292857186868787,
      "learning_rate": 0.029560388708931053,
      "loss": 0.0001,
      "step": 76110
    },
    {
      "epoch": 35.22443313280888,
      "grad_norm": 0.0006986072403378785,
      "learning_rate": 0.02955113373438223,
      "loss": 0.0,
      "step": 76120
    },
    {
      "epoch": 35.229060620083295,
      "grad_norm": 0.0140794413164258,
      "learning_rate": 0.029541878759833412,
      "loss": 0.0,
      "step": 76130
    },
    {
      "epoch": 35.23368810735771,
      "grad_norm": 0.006627960596233606,
      "learning_rate": 0.02953262378528459,
      "loss": 0.0001,
      "step": 76140
    },
    {
      "epoch": 35.23831559463211,
      "grad_norm": 0.00863915029913187,
      "learning_rate": 0.029523368810735775,
      "loss": 0.0,
      "step": 76150
    },
    {
      "epoch": 35.242943081906525,
      "grad_norm": 0.002087692730128765,
      "learning_rate": 0.02951411383618695,
      "loss": 0.0002,
      "step": 76160
    },
    {
      "epoch": 35.24757056918094,
      "grad_norm": 0.007175554521381855,
      "learning_rate": 0.02950485886163813,
      "loss": 0.0001,
      "step": 76170
    },
    {
      "epoch": 35.25219805645534,
      "grad_norm": 0.0027472982183098793,
      "learning_rate": 0.029495603887089314,
      "loss": 0.0001,
      "step": 76180
    },
    {
      "epoch": 35.256825543729754,
      "grad_norm": 0.011677540838718414,
      "learning_rate": 0.02948634891254049,
      "loss": 0.0001,
      "step": 76190
    },
    {
      "epoch": 35.261453031004166,
      "grad_norm": 0.03503739833831787,
      "learning_rate": 0.029477093937991673,
      "loss": 0.0001,
      "step": 76200
    },
    {
      "epoch": 35.26608051827858,
      "grad_norm": 0.0028789814095944166,
      "learning_rate": 0.02946783896344285,
      "loss": 0.0001,
      "step": 76210
    },
    {
      "epoch": 35.27070800555298,
      "grad_norm": 0.03768209367990494,
      "learning_rate": 0.029458583988894033,
      "loss": 0.0,
      "step": 76220
    },
    {
      "epoch": 35.275335492827395,
      "grad_norm": 0.011975115165114403,
      "learning_rate": 0.029449329014345212,
      "loss": 0.0003,
      "step": 76230
    },
    {
      "epoch": 35.27996298010181,
      "grad_norm": 0.01537980604916811,
      "learning_rate": 0.02944007403979639,
      "loss": 0.0002,
      "step": 76240
    },
    {
      "epoch": 35.28459046737621,
      "grad_norm": 0.009435239247977734,
      "learning_rate": 0.02943081906524757,
      "loss": 0.0001,
      "step": 76250
    },
    {
      "epoch": 35.289217954650624,
      "grad_norm": 0.021376019343733788,
      "learning_rate": 0.02942156409069875,
      "loss": 0.0005,
      "step": 76260
    },
    {
      "epoch": 35.293845441925036,
      "grad_norm": 0.010415565222501755,
      "learning_rate": 0.029412309116149934,
      "loss": 0.0007,
      "step": 76270
    },
    {
      "epoch": 35.29847292919944,
      "grad_norm": 0.08053649216890335,
      "learning_rate": 0.02940305414160111,
      "loss": 0.0001,
      "step": 76280
    },
    {
      "epoch": 35.30310041647385,
      "grad_norm": 0.008794400840997696,
      "learning_rate": 0.029393799167052294,
      "loss": 0.0004,
      "step": 76290
    },
    {
      "epoch": 35.307727903748265,
      "grad_norm": 0.006564269308000803,
      "learning_rate": 0.02938454419250347,
      "loss": 0.0001,
      "step": 76300
    },
    {
      "epoch": 35.31235539102268,
      "grad_norm": 0.056616026908159256,
      "learning_rate": 0.029375289217954653,
      "loss": 0.0002,
      "step": 76310
    },
    {
      "epoch": 35.31698287829708,
      "grad_norm": 0.0046512670814991,
      "learning_rate": 0.029366034243405833,
      "loss": 0.0001,
      "step": 76320
    },
    {
      "epoch": 35.321610365571495,
      "grad_norm": 0.0794791504740715,
      "learning_rate": 0.02935677926885701,
      "loss": 0.0,
      "step": 76330
    },
    {
      "epoch": 35.32623785284591,
      "grad_norm": 0.005063667427748442,
      "learning_rate": 0.029347524294308192,
      "loss": 0.0001,
      "step": 76340
    },
    {
      "epoch": 35.33086534012031,
      "grad_norm": 0.0021257391199469566,
      "learning_rate": 0.029338269319759372,
      "loss": 0.0001,
      "step": 76350
    },
    {
      "epoch": 35.335492827394724,
      "grad_norm": 0.08194898813962936,
      "learning_rate": 0.029329014345210555,
      "loss": 0.0002,
      "step": 76360
    },
    {
      "epoch": 35.340120314669136,
      "grad_norm": 0.01726764440536499,
      "learning_rate": 0.02931975937066173,
      "loss": 0.0,
      "step": 76370
    },
    {
      "epoch": 35.34474780194355,
      "grad_norm": 0.16655126214027405,
      "learning_rate": 0.029310504396112914,
      "loss": 0.0001,
      "step": 76380
    },
    {
      "epoch": 35.34937528921795,
      "grad_norm": 0.002179153962060809,
      "learning_rate": 0.02930124942156409,
      "loss": 0.0003,
      "step": 76390
    },
    {
      "epoch": 35.354002776492365,
      "grad_norm": 0.0025760536082088947,
      "learning_rate": 0.02929199444701527,
      "loss": 0.0001,
      "step": 76400
    },
    {
      "epoch": 35.35863026376678,
      "grad_norm": 0.004670924507081509,
      "learning_rate": 0.029282739472466453,
      "loss": 0.0001,
      "step": 76410
    },
    {
      "epoch": 35.36325775104118,
      "grad_norm": 0.008777018636465073,
      "learning_rate": 0.02927348449791763,
      "loss": 0.0001,
      "step": 76420
    },
    {
      "epoch": 35.367885238315594,
      "grad_norm": 0.002762310206890106,
      "learning_rate": 0.029264229523368813,
      "loss": 0.0001,
      "step": 76430
    },
    {
      "epoch": 35.372512725590006,
      "grad_norm": 0.025763865560293198,
      "learning_rate": 0.029254974548819992,
      "loss": 0.0,
      "step": 76440
    },
    {
      "epoch": 35.37714021286441,
      "grad_norm": 0.00272675184533,
      "learning_rate": 0.029245719574271176,
      "loss": 0.0001,
      "step": 76450
    },
    {
      "epoch": 35.38176770013882,
      "grad_norm": 0.004612883552908897,
      "learning_rate": 0.029236464599722352,
      "loss": 0.0001,
      "step": 76460
    },
    {
      "epoch": 35.386395187413235,
      "grad_norm": 0.0031366937328130007,
      "learning_rate": 0.029227209625173528,
      "loss": 0.0001,
      "step": 76470
    },
    {
      "epoch": 35.39102267468765,
      "grad_norm": 0.02704482711851597,
      "learning_rate": 0.02921795465062471,
      "loss": 0.0001,
      "step": 76480
    },
    {
      "epoch": 35.39565016196205,
      "grad_norm": 0.004408028442412615,
      "learning_rate": 0.02920869967607589,
      "loss": 0.0001,
      "step": 76490
    },
    {
      "epoch": 35.400277649236465,
      "grad_norm": 0.00653449073433876,
      "learning_rate": 0.029199444701527074,
      "loss": 0.0004,
      "step": 76500
    },
    {
      "epoch": 35.40490513651088,
      "grad_norm": 0.05748945102095604,
      "learning_rate": 0.02919018972697825,
      "loss": 0.0002,
      "step": 76510
    },
    {
      "epoch": 35.40953262378528,
      "grad_norm": 0.0022649962920695543,
      "learning_rate": 0.029180934752429433,
      "loss": 0.0004,
      "step": 76520
    },
    {
      "epoch": 35.414160111059694,
      "grad_norm": 0.04852414131164551,
      "learning_rate": 0.029171679777880613,
      "loss": 0.0001,
      "step": 76530
    },
    {
      "epoch": 35.418787598334106,
      "grad_norm": 0.2779035270214081,
      "learning_rate": 0.029162424803331796,
      "loss": 0.0001,
      "step": 76540
    },
    {
      "epoch": 35.42341508560852,
      "grad_norm": 0.00299674179404974,
      "learning_rate": 0.029153169828782972,
      "loss": 0.0001,
      "step": 76550
    },
    {
      "epoch": 35.42804257288292,
      "grad_norm": 0.005402765236794949,
      "learning_rate": 0.02914391485423415,
      "loss": 0.0001,
      "step": 76560
    },
    {
      "epoch": 35.432670060157335,
      "grad_norm": 0.008230326697230339,
      "learning_rate": 0.029134659879685332,
      "loss": 0.0,
      "step": 76570
    },
    {
      "epoch": 35.43729754743175,
      "grad_norm": 0.08023137599229813,
      "learning_rate": 0.02912540490513651,
      "loss": 0.0003,
      "step": 76580
    },
    {
      "epoch": 35.44192503470615,
      "grad_norm": 0.0030993709806352854,
      "learning_rate": 0.029116149930587695,
      "loss": 0.0001,
      "step": 76590
    },
    {
      "epoch": 35.446552521980564,
      "grad_norm": 0.004679396282881498,
      "learning_rate": 0.02910689495603887,
      "loss": 0.0002,
      "step": 76600
    },
    {
      "epoch": 35.451180009254976,
      "grad_norm": 0.00374511256814003,
      "learning_rate": 0.029097639981490054,
      "loss": 0.0001,
      "step": 76610
    },
    {
      "epoch": 35.45580749652938,
      "grad_norm": 0.022405525669455528,
      "learning_rate": 0.029088385006941234,
      "loss": 0.0,
      "step": 76620
    },
    {
      "epoch": 35.46043498380379,
      "grad_norm": 0.0005420091329142451,
      "learning_rate": 0.02907913003239241,
      "loss": 0.0001,
      "step": 76630
    },
    {
      "epoch": 35.465062471078205,
      "grad_norm": 0.0426652729511261,
      "learning_rate": 0.029069875057843593,
      "loss": 0.0001,
      "step": 76640
    },
    {
      "epoch": 35.46968995835262,
      "grad_norm": 0.002626419998705387,
      "learning_rate": 0.02906062008329477,
      "loss": 0.0,
      "step": 76650
    },
    {
      "epoch": 35.47431744562702,
      "grad_norm": 0.0032333722338080406,
      "learning_rate": 0.029051365108745952,
      "loss": 0.0,
      "step": 76660
    },
    {
      "epoch": 35.478944932901435,
      "grad_norm": 0.0012127868831157684,
      "learning_rate": 0.029042110134197132,
      "loss": 0.0,
      "step": 76670
    },
    {
      "epoch": 35.48357242017585,
      "grad_norm": 0.0030549908988177776,
      "learning_rate": 0.029032855159648315,
      "loss": 0.0001,
      "step": 76680
    },
    {
      "epoch": 35.48819990745025,
      "grad_norm": 0.316159188747406,
      "learning_rate": 0.02902360018509949,
      "loss": 0.0002,
      "step": 76690
    },
    {
      "epoch": 35.492827394724664,
      "grad_norm": 0.004388756584376097,
      "learning_rate": 0.02901434521055067,
      "loss": 0.0,
      "step": 76700
    },
    {
      "epoch": 35.497454881999076,
      "grad_norm": 0.36793053150177,
      "learning_rate": 0.029005090236001854,
      "loss": 0.0001,
      "step": 76710
    },
    {
      "epoch": 35.50208236927349,
      "grad_norm": 0.007366945501416922,
      "learning_rate": 0.02899583526145303,
      "loss": 0.0001,
      "step": 76720
    },
    {
      "epoch": 35.50670985654789,
      "grad_norm": 0.0051061068661510944,
      "learning_rate": 0.028986580286904214,
      "loss": 0.0003,
      "step": 76730
    },
    {
      "epoch": 35.511337343822305,
      "grad_norm": 0.008300241082906723,
      "learning_rate": 0.02897732531235539,
      "loss": 0.0001,
      "step": 76740
    },
    {
      "epoch": 35.51596483109672,
      "grad_norm": 0.007162802387028933,
      "learning_rate": 0.028968070337806573,
      "loss": 0.0001,
      "step": 76750
    },
    {
      "epoch": 35.52059231837112,
      "grad_norm": 0.011363250203430653,
      "learning_rate": 0.028958815363257753,
      "loss": 0.0002,
      "step": 76760
    },
    {
      "epoch": 35.525219805645534,
      "grad_norm": 0.007349728140980005,
      "learning_rate": 0.028949560388708936,
      "loss": 0.0,
      "step": 76770
    },
    {
      "epoch": 35.529847292919946,
      "grad_norm": 0.0027066438924521208,
      "learning_rate": 0.028940305414160112,
      "loss": 0.0001,
      "step": 76780
    },
    {
      "epoch": 35.53447478019435,
      "grad_norm": 0.024392670020461082,
      "learning_rate": 0.02893105043961129,
      "loss": 0.0001,
      "step": 76790
    },
    {
      "epoch": 35.53910226746876,
      "grad_norm": 0.09786000102758408,
      "learning_rate": 0.028921795465062475,
      "loss": 0.0001,
      "step": 76800
    },
    {
      "epoch": 35.543729754743175,
      "grad_norm": 0.03328230604529381,
      "learning_rate": 0.02891254049051365,
      "loss": 0.0001,
      "step": 76810
    },
    {
      "epoch": 35.54835724201759,
      "grad_norm": 0.0038653381634503603,
      "learning_rate": 0.028903285515964834,
      "loss": 0.0001,
      "step": 76820
    },
    {
      "epoch": 35.55298472929199,
      "grad_norm": 0.0796261727809906,
      "learning_rate": 0.02889403054141601,
      "loss": 0.0001,
      "step": 76830
    },
    {
      "epoch": 35.557612216566405,
      "grad_norm": 0.023286616429686546,
      "learning_rate": 0.028884775566867193,
      "loss": 0.0003,
      "step": 76840
    },
    {
      "epoch": 35.56223970384082,
      "grad_norm": 0.024353619664907455,
      "learning_rate": 0.028875520592318373,
      "loss": 0.0002,
      "step": 76850
    },
    {
      "epoch": 35.56686719111522,
      "grad_norm": 0.06759313493967056,
      "learning_rate": 0.02886626561776955,
      "loss": 0.0001,
      "step": 76860
    },
    {
      "epoch": 35.571494678389634,
      "grad_norm": 0.009055105037987232,
      "learning_rate": 0.028857010643220733,
      "loss": 0.0001,
      "step": 76870
    },
    {
      "epoch": 35.576122165664046,
      "grad_norm": 0.028982369229197502,
      "learning_rate": 0.028847755668671912,
      "loss": 0.0001,
      "step": 76880
    },
    {
      "epoch": 35.58074965293845,
      "grad_norm": 0.006320839747786522,
      "learning_rate": 0.028838500694123095,
      "loss": 0.0001,
      "step": 76890
    },
    {
      "epoch": 35.58537714021286,
      "grad_norm": 0.007037714123725891,
      "learning_rate": 0.02882924571957427,
      "loss": 0.0003,
      "step": 76900
    },
    {
      "epoch": 35.590004627487275,
      "grad_norm": 0.004700325895100832,
      "learning_rate": 0.028819990745025455,
      "loss": 0.0,
      "step": 76910
    },
    {
      "epoch": 35.59463211476169,
      "grad_norm": 0.00611648615449667,
      "learning_rate": 0.02881073577047663,
      "loss": 0.0,
      "step": 76920
    },
    {
      "epoch": 35.59925960203609,
      "grad_norm": 0.0038075183983892202,
      "learning_rate": 0.02880148079592781,
      "loss": 0.0001,
      "step": 76930
    },
    {
      "epoch": 35.603887089310504,
      "grad_norm": 0.0025225873105227947,
      "learning_rate": 0.028792225821378994,
      "loss": 0.0,
      "step": 76940
    },
    {
      "epoch": 35.608514576584916,
      "grad_norm": 0.010769753716886044,
      "learning_rate": 0.02878297084683017,
      "loss": 0.0002,
      "step": 76950
    },
    {
      "epoch": 35.61314206385932,
      "grad_norm": 0.002003511181101203,
      "learning_rate": 0.028773715872281353,
      "loss": 0.0001,
      "step": 76960
    },
    {
      "epoch": 35.61776955113373,
      "grad_norm": 0.005799606908112764,
      "learning_rate": 0.028764460897732533,
      "loss": 0.0001,
      "step": 76970
    },
    {
      "epoch": 35.622397038408145,
      "grad_norm": 0.005710854195058346,
      "learning_rate": 0.028755205923183716,
      "loss": 0.0001,
      "step": 76980
    },
    {
      "epoch": 35.62702452568256,
      "grad_norm": 0.007958795875310898,
      "learning_rate": 0.028745950948634892,
      "loss": 0.0,
      "step": 76990
    },
    {
      "epoch": 35.63165201295696,
      "grad_norm": 0.008792894892394543,
      "learning_rate": 0.028736695974086075,
      "loss": 0.0001,
      "step": 77000
    },
    {
      "epoch": 35.636279500231375,
      "grad_norm": 0.0714646503329277,
      "learning_rate": 0.02872744099953725,
      "loss": 0.0001,
      "step": 77010
    },
    {
      "epoch": 35.64090698750579,
      "grad_norm": 0.002113868948072195,
      "learning_rate": 0.02871818602498843,
      "loss": 0.0001,
      "step": 77020
    },
    {
      "epoch": 35.64553447478019,
      "grad_norm": 0.12383897602558136,
      "learning_rate": 0.028708931050439614,
      "loss": 0.0001,
      "step": 77030
    },
    {
      "epoch": 35.650161962054604,
      "grad_norm": 0.002912048250436783,
      "learning_rate": 0.02869967607589079,
      "loss": 0.0001,
      "step": 77040
    },
    {
      "epoch": 35.654789449329016,
      "grad_norm": 0.0027053242083638906,
      "learning_rate": 0.028690421101341974,
      "loss": 0.0001,
      "step": 77050
    },
    {
      "epoch": 35.65941693660342,
      "grad_norm": 0.00160968245472759,
      "learning_rate": 0.028681166126793153,
      "loss": 0.0001,
      "step": 77060
    },
    {
      "epoch": 35.66404442387783,
      "grad_norm": 0.012314102612435818,
      "learning_rate": 0.028671911152244337,
      "loss": 0.0001,
      "step": 77070
    },
    {
      "epoch": 35.668671911152245,
      "grad_norm": 0.026938283815979958,
      "learning_rate": 0.028662656177695513,
      "loss": 0.0001,
      "step": 77080
    },
    {
      "epoch": 35.67329939842666,
      "grad_norm": 0.01117453258484602,
      "learning_rate": 0.02865340120314669,
      "loss": 0.0001,
      "step": 77090
    },
    {
      "epoch": 35.67792688570106,
      "grad_norm": 0.012420537881553173,
      "learning_rate": 0.028644146228597872,
      "loss": 0.0002,
      "step": 77100
    },
    {
      "epoch": 35.682554372975474,
      "grad_norm": 0.005359221715480089,
      "learning_rate": 0.028634891254049052,
      "loss": 0.0001,
      "step": 77110
    },
    {
      "epoch": 35.687181860249886,
      "grad_norm": 0.0032824568916112185,
      "learning_rate": 0.028625636279500235,
      "loss": 0.0002,
      "step": 77120
    },
    {
      "epoch": 35.69180934752429,
      "grad_norm": 0.021366819739341736,
      "learning_rate": 0.02861638130495141,
      "loss": 0.0001,
      "step": 77130
    },
    {
      "epoch": 35.6964368347987,
      "grad_norm": 0.0023249234072864056,
      "learning_rate": 0.028607126330402594,
      "loss": 0.0,
      "step": 77140
    },
    {
      "epoch": 35.701064322073115,
      "grad_norm": 0.005379807204008102,
      "learning_rate": 0.028597871355853774,
      "loss": 0.0002,
      "step": 77150
    },
    {
      "epoch": 35.70569180934753,
      "grad_norm": 0.006146529223769903,
      "learning_rate": 0.02858861638130495,
      "loss": 0.0003,
      "step": 77160
    },
    {
      "epoch": 35.71031929662193,
      "grad_norm": 0.03630548343062401,
      "learning_rate": 0.028579361406756133,
      "loss": 0.0001,
      "step": 77170
    },
    {
      "epoch": 35.714946783896345,
      "grad_norm": 0.003207302652299404,
      "learning_rate": 0.02857010643220731,
      "loss": 0.0003,
      "step": 77180
    },
    {
      "epoch": 35.71957427117076,
      "grad_norm": 0.01122234482318163,
      "learning_rate": 0.028560851457658493,
      "loss": 0.0039,
      "step": 77190
    },
    {
      "epoch": 35.72420175844516,
      "grad_norm": 0.007577921263873577,
      "learning_rate": 0.028551596483109672,
      "loss": 0.0001,
      "step": 77200
    },
    {
      "epoch": 35.728829245719574,
      "grad_norm": 0.028636831790208817,
      "learning_rate": 0.028542341508560855,
      "loss": 0.0001,
      "step": 77210
    },
    {
      "epoch": 35.733456732993986,
      "grad_norm": 0.02249802276492119,
      "learning_rate": 0.02853308653401203,
      "loss": 0.001,
      "step": 77220
    },
    {
      "epoch": 35.73808422026839,
      "grad_norm": 0.005999161396175623,
      "learning_rate": 0.028523831559463215,
      "loss": 0.0001,
      "step": 77230
    },
    {
      "epoch": 35.7427117075428,
      "grad_norm": 0.004270561039447784,
      "learning_rate": 0.028514576584914395,
      "loss": 0.0001,
      "step": 77240
    },
    {
      "epoch": 35.747339194817215,
      "grad_norm": 0.0034838118590414524,
      "learning_rate": 0.02850532161036557,
      "loss": 0.0001,
      "step": 77250
    },
    {
      "epoch": 35.75196668209163,
      "grad_norm": 0.024865474551916122,
      "learning_rate": 0.028496066635816754,
      "loss": 0.0003,
      "step": 77260
    },
    {
      "epoch": 35.75659416936603,
      "grad_norm": 0.019797228276729584,
      "learning_rate": 0.02848681166126793,
      "loss": 0.0001,
      "step": 77270
    },
    {
      "epoch": 35.761221656640444,
      "grad_norm": 0.005940201226621866,
      "learning_rate": 0.028477556686719113,
      "loss": 0.0,
      "step": 77280
    },
    {
      "epoch": 35.765849143914856,
      "grad_norm": 1.65459406375885,
      "learning_rate": 0.028468301712170293,
      "loss": 0.0005,
      "step": 77290
    },
    {
      "epoch": 35.77047663118926,
      "grad_norm": 0.010727688670158386,
      "learning_rate": 0.028459046737621476,
      "loss": 0.0001,
      "step": 77300
    },
    {
      "epoch": 35.77510411846367,
      "grad_norm": 0.02900521829724312,
      "learning_rate": 0.028449791763072652,
      "loss": 0.0001,
      "step": 77310
    },
    {
      "epoch": 35.779731605738085,
      "grad_norm": 0.004564519971609116,
      "learning_rate": 0.028440536788523832,
      "loss": 0.0001,
      "step": 77320
    },
    {
      "epoch": 35.7843590930125,
      "grad_norm": 0.03962752968072891,
      "learning_rate": 0.028431281813975015,
      "loss": 0.0001,
      "step": 77330
    },
    {
      "epoch": 35.7889865802869,
      "grad_norm": 0.021958786994218826,
      "learning_rate": 0.02842202683942619,
      "loss": 0.0003,
      "step": 77340
    },
    {
      "epoch": 35.793614067561315,
      "grad_norm": 0.08401460200548172,
      "learning_rate": 0.028412771864877374,
      "loss": 0.0,
      "step": 77350
    },
    {
      "epoch": 35.79824155483573,
      "grad_norm": 0.40923011302948,
      "learning_rate": 0.02840351689032855,
      "loss": 0.0002,
      "step": 77360
    },
    {
      "epoch": 35.80286904211013,
      "grad_norm": 0.006136646028608084,
      "learning_rate": 0.028394261915779734,
      "loss": 0.0001,
      "step": 77370
    },
    {
      "epoch": 35.807496529384544,
      "grad_norm": 0.012225374579429626,
      "learning_rate": 0.028385006941230913,
      "loss": 0.0001,
      "step": 77380
    },
    {
      "epoch": 35.812124016658956,
      "grad_norm": 0.035174500197172165,
      "learning_rate": 0.02837575196668209,
      "loss": 0.0002,
      "step": 77390
    },
    {
      "epoch": 35.81675150393336,
      "grad_norm": 0.004025342408567667,
      "learning_rate": 0.028366496992133273,
      "loss": 0.0001,
      "step": 77400
    },
    {
      "epoch": 35.82137899120777,
      "grad_norm": 0.0059456308372318745,
      "learning_rate": 0.028357242017584453,
      "loss": 0.0001,
      "step": 77410
    },
    {
      "epoch": 35.826006478482185,
      "grad_norm": 0.015831103548407555,
      "learning_rate": 0.028347987043035636,
      "loss": 0.0001,
      "step": 77420
    },
    {
      "epoch": 35.8306339657566,
      "grad_norm": 0.004097399767488241,
      "learning_rate": 0.028338732068486812,
      "loss": 0.0001,
      "step": 77430
    },
    {
      "epoch": 35.835261453031,
      "grad_norm": 0.009440511465072632,
      "learning_rate": 0.028329477093937995,
      "loss": 0.0001,
      "step": 77440
    },
    {
      "epoch": 35.839888940305414,
      "grad_norm": 0.004797578789293766,
      "learning_rate": 0.02832022211938917,
      "loss": 0.0001,
      "step": 77450
    },
    {
      "epoch": 35.844516427579826,
      "grad_norm": 0.003105935174971819,
      "learning_rate": 0.028310967144840354,
      "loss": 0.0,
      "step": 77460
    },
    {
      "epoch": 35.84914391485423,
      "grad_norm": 0.249929741024971,
      "learning_rate": 0.028301712170291534,
      "loss": 0.0002,
      "step": 77470
    },
    {
      "epoch": 35.85377140212864,
      "grad_norm": 0.002520407550036907,
      "learning_rate": 0.02829245719574271,
      "loss": 0.0001,
      "step": 77480
    },
    {
      "epoch": 35.858398889403055,
      "grad_norm": 0.002344603417441249,
      "learning_rate": 0.028283202221193893,
      "loss": 0.0001,
      "step": 77490
    },
    {
      "epoch": 35.86302637667747,
      "grad_norm": 0.15704993903636932,
      "learning_rate": 0.028273947246645073,
      "loss": 0.0001,
      "step": 77500
    },
    {
      "epoch": 35.86765386395187,
      "grad_norm": 0.10316036641597748,
      "learning_rate": 0.028264692272096256,
      "loss": 0.0001,
      "step": 77510
    },
    {
      "epoch": 35.872281351226285,
      "grad_norm": 0.00876324437558651,
      "learning_rate": 0.028255437297547432,
      "loss": 0.0001,
      "step": 77520
    },
    {
      "epoch": 35.8769088385007,
      "grad_norm": 0.01203369814902544,
      "learning_rate": 0.028246182322998616,
      "loss": 0.0001,
      "step": 77530
    },
    {
      "epoch": 35.8815363257751,
      "grad_norm": 0.004662633407860994,
      "learning_rate": 0.028236927348449792,
      "loss": 0.0001,
      "step": 77540
    },
    {
      "epoch": 35.886163813049514,
      "grad_norm": 0.06569083034992218,
      "learning_rate": 0.02822767237390097,
      "loss": 0.0001,
      "step": 77550
    },
    {
      "epoch": 35.890791300323926,
      "grad_norm": 0.009260701015591621,
      "learning_rate": 0.028218417399352155,
      "loss": 0.0,
      "step": 77560
    },
    {
      "epoch": 35.89541878759833,
      "grad_norm": 0.009579340927302837,
      "learning_rate": 0.02820916242480333,
      "loss": 0.0001,
      "step": 77570
    },
    {
      "epoch": 35.90004627487274,
      "grad_norm": 0.0030108592472970486,
      "learning_rate": 0.028199907450254514,
      "loss": 0.0,
      "step": 77580
    },
    {
      "epoch": 35.904673762147155,
      "grad_norm": 0.049629900604486465,
      "learning_rate": 0.028190652475705694,
      "loss": 0.0002,
      "step": 77590
    },
    {
      "epoch": 35.90930124942157,
      "grad_norm": 1.668556809425354,
      "learning_rate": 0.028181397501156877,
      "loss": 0.0005,
      "step": 77600
    },
    {
      "epoch": 35.91392873669597,
      "grad_norm": 0.00233482732437551,
      "learning_rate": 0.028172142526608053,
      "loss": 0.0001,
      "step": 77610
    },
    {
      "epoch": 35.918556223970384,
      "grad_norm": 0.020224232226610184,
      "learning_rate": 0.02816288755205923,
      "loss": 0.0001,
      "step": 77620
    },
    {
      "epoch": 35.923183711244796,
      "grad_norm": 0.015363780781626701,
      "learning_rate": 0.028153632577510412,
      "loss": 0.0003,
      "step": 77630
    },
    {
      "epoch": 35.9278111985192,
      "grad_norm": 0.03818243741989136,
      "learning_rate": 0.028144377602961592,
      "loss": 0.0001,
      "step": 77640
    },
    {
      "epoch": 35.93243868579361,
      "grad_norm": 0.000873762764967978,
      "learning_rate": 0.028135122628412775,
      "loss": 0.0001,
      "step": 77650
    },
    {
      "epoch": 35.937066173068025,
      "grad_norm": 0.004318645689636469,
      "learning_rate": 0.02812586765386395,
      "loss": 0.0001,
      "step": 77660
    },
    {
      "epoch": 35.94169366034244,
      "grad_norm": 0.004286834970116615,
      "learning_rate": 0.028116612679315135,
      "loss": 0.0001,
      "step": 77670
    },
    {
      "epoch": 35.94632114761684,
      "grad_norm": 0.019386783242225647,
      "learning_rate": 0.028107357704766314,
      "loss": 0.0001,
      "step": 77680
    },
    {
      "epoch": 35.950948634891255,
      "grad_norm": 0.0018006737809628248,
      "learning_rate": 0.028098102730217497,
      "loss": 0.0001,
      "step": 77690
    },
    {
      "epoch": 35.95557612216567,
      "grad_norm": 0.002229607431218028,
      "learning_rate": 0.028088847755668674,
      "loss": 0.0001,
      "step": 77700
    },
    {
      "epoch": 35.96020360944007,
      "grad_norm": 0.0024836244992911816,
      "learning_rate": 0.02807959278111985,
      "loss": 0.0001,
      "step": 77710
    },
    {
      "epoch": 35.964831096714484,
      "grad_norm": 0.016758177429437637,
      "learning_rate": 0.028070337806571033,
      "loss": 0.0001,
      "step": 77720
    },
    {
      "epoch": 35.969458583988896,
      "grad_norm": 0.05959799140691757,
      "learning_rate": 0.028061082832022213,
      "loss": 0.0004,
      "step": 77730
    },
    {
      "epoch": 35.9740860712633,
      "grad_norm": 0.0008077847887761891,
      "learning_rate": 0.028051827857473396,
      "loss": 0.0001,
      "step": 77740
    },
    {
      "epoch": 35.97871355853771,
      "grad_norm": 0.0168216060847044,
      "learning_rate": 0.028042572882924572,
      "loss": 0.0002,
      "step": 77750
    },
    {
      "epoch": 35.983341045812125,
      "grad_norm": 5.94985818862915,
      "learning_rate": 0.028033317908375755,
      "loss": 0.002,
      "step": 77760
    },
    {
      "epoch": 35.98796853308654,
      "grad_norm": 0.011006936430931091,
      "learning_rate": 0.028024062933826935,
      "loss": 0.0002,
      "step": 77770
    },
    {
      "epoch": 35.99259602036094,
      "grad_norm": 0.009089781902730465,
      "learning_rate": 0.02801480795927811,
      "loss": 0.0002,
      "step": 77780
    },
    {
      "epoch": 35.997223507635354,
      "grad_norm": 0.025326306000351906,
      "learning_rate": 0.028005552984729294,
      "loss": 0.0003,
      "step": 77790
    },
    {
      "epoch": 36.0,
      "eval_accuracy_branch1": 0.9919889077183793,
      "eval_accuracy_branch2": 0.5002310892004314,
      "eval_f1_branch1": 0.9922230621367082,
      "eval_f1_branch2": 0.5001412920512397,
      "eval_loss": 0.015585833229124546,
      "eval_precision_branch1": 0.99257281075095,
      "eval_precision_branch2": 0.5002312553759779,
      "eval_recall_branch1": 0.9919627701317698,
      "eval_recall_branch2": 0.5002310892004314,
      "eval_runtime": 28.9196,
      "eval_samples_per_second": 448.9,
      "eval_steps_per_second": 56.121,
      "step": 77796
    },
    {
      "epoch": 36.001850994909766,
      "grad_norm": 0.0033142445608973503,
      "learning_rate": 0.02799629801018047,
      "loss": 0.0104,
      "step": 77800
    },
    {
      "epoch": 36.00647848218417,
      "grad_norm": 0.0353890024125576,
      "learning_rate": 0.027987043035631654,
      "loss": 0.0001,
      "step": 77810
    },
    {
      "epoch": 36.01110596945858,
      "grad_norm": 0.0042494842782616615,
      "learning_rate": 0.027977788061082833,
      "loss": 0.0001,
      "step": 77820
    },
    {
      "epoch": 36.015733456732995,
      "grad_norm": 0.0038355491124093533,
      "learning_rate": 0.027968533086534016,
      "loss": 0.0001,
      "step": 77830
    },
    {
      "epoch": 36.02036094400741,
      "grad_norm": 0.003827760461717844,
      "learning_rate": 0.027959278111985193,
      "loss": 0.0001,
      "step": 77840
    },
    {
      "epoch": 36.02498843128181,
      "grad_norm": 0.03916545957326889,
      "learning_rate": 0.027950023137436372,
      "loss": 0.0001,
      "step": 77850
    },
    {
      "epoch": 36.029615918556225,
      "grad_norm": 0.01129921805113554,
      "learning_rate": 0.027940768162887555,
      "loss": 0.0001,
      "step": 77860
    },
    {
      "epoch": 36.03424340583064,
      "grad_norm": 0.0025547982659190893,
      "learning_rate": 0.02793151318833873,
      "loss": 0.0001,
      "step": 77870
    },
    {
      "epoch": 36.03887089310504,
      "grad_norm": 0.12125195562839508,
      "learning_rate": 0.027922258213789915,
      "loss": 0.0001,
      "step": 77880
    },
    {
      "epoch": 36.043498380379454,
      "grad_norm": 0.021083930507302284,
      "learning_rate": 0.02791300323924109,
      "loss": 0.0001,
      "step": 77890
    },
    {
      "epoch": 36.048125867653866,
      "grad_norm": 3.22629714012146,
      "learning_rate": 0.027903748264692274,
      "loss": 0.0005,
      "step": 77900
    },
    {
      "epoch": 36.05275335492827,
      "grad_norm": 0.009052742272615433,
      "learning_rate": 0.027894493290143454,
      "loss": 0.0001,
      "step": 77910
    },
    {
      "epoch": 36.05738084220268,
      "grad_norm": 0.0005581305013038218,
      "learning_rate": 0.027885238315594637,
      "loss": 0.0001,
      "step": 77920
    },
    {
      "epoch": 36.062008329477095,
      "grad_norm": 0.0667034313082695,
      "learning_rate": 0.027875983341045813,
      "loss": 0.0001,
      "step": 77930
    },
    {
      "epoch": 36.06663581675151,
      "grad_norm": 0.020814670249819756,
      "learning_rate": 0.027866728366496993,
      "loss": 0.0001,
      "step": 77940
    },
    {
      "epoch": 36.07126330402591,
      "grad_norm": 0.0989045649766922,
      "learning_rate": 0.027857473391948176,
      "loss": 0.0001,
      "step": 77950
    },
    {
      "epoch": 36.075890791300324,
      "grad_norm": 0.0015439913840964437,
      "learning_rate": 0.027848218417399352,
      "loss": 0.0,
      "step": 77960
    },
    {
      "epoch": 36.080518278574736,
      "grad_norm": 0.003868027124553919,
      "learning_rate": 0.027838963442850535,
      "loss": 0.0001,
      "step": 77970
    },
    {
      "epoch": 36.08514576584914,
      "grad_norm": 0.04685673862695694,
      "learning_rate": 0.02782970846830171,
      "loss": 0.0001,
      "step": 77980
    },
    {
      "epoch": 36.08977325312355,
      "grad_norm": 0.023333311080932617,
      "learning_rate": 0.027820453493752895,
      "loss": 0.0007,
      "step": 77990
    },
    {
      "epoch": 36.094400740397965,
      "grad_norm": 0.005035947076976299,
      "learning_rate": 0.027811198519204074,
      "loss": 0.0001,
      "step": 78000
    },
    {
      "epoch": 36.09902822767237,
      "grad_norm": 0.013890348374843597,
      "learning_rate": 0.02780194354465525,
      "loss": 0.0001,
      "step": 78010
    },
    {
      "epoch": 36.10365571494678,
      "grad_norm": 0.008674460463225842,
      "learning_rate": 0.027792688570106434,
      "loss": 0.0001,
      "step": 78020
    },
    {
      "epoch": 36.108283202221195,
      "grad_norm": 0.019391775131225586,
      "learning_rate": 0.027783433595557613,
      "loss": 0.0001,
      "step": 78030
    },
    {
      "epoch": 36.11291068949561,
      "grad_norm": 0.0009705707780085504,
      "learning_rate": 0.027774178621008797,
      "loss": 0.0001,
      "step": 78040
    },
    {
      "epoch": 36.11753817677001,
      "grad_norm": 0.0012470566434785724,
      "learning_rate": 0.027764923646459973,
      "loss": 0.0,
      "step": 78050
    },
    {
      "epoch": 36.122165664044424,
      "grad_norm": 0.0020866275299340487,
      "learning_rate": 0.027755668671911156,
      "loss": 0.0001,
      "step": 78060
    },
    {
      "epoch": 36.126793151318836,
      "grad_norm": 0.9994057416915894,
      "learning_rate": 0.027746413697362332,
      "loss": 0.0005,
      "step": 78070
    },
    {
      "epoch": 36.13142063859324,
      "grad_norm": 0.008000189438462257,
      "learning_rate": 0.027737158722813512,
      "loss": 0.0005,
      "step": 78080
    },
    {
      "epoch": 36.13604812586765,
      "grad_norm": 0.059874147176742554,
      "learning_rate": 0.027727903748264695,
      "loss": 0.0001,
      "step": 78090
    },
    {
      "epoch": 36.140675613142065,
      "grad_norm": 0.005894587840884924,
      "learning_rate": 0.02771864877371587,
      "loss": 0.0001,
      "step": 78100
    },
    {
      "epoch": 36.14530310041648,
      "grad_norm": 0.0023507094010710716,
      "learning_rate": 0.027709393799167054,
      "loss": 0.0001,
      "step": 78110
    },
    {
      "epoch": 36.14993058769088,
      "grad_norm": 0.06597815454006195,
      "learning_rate": 0.027700138824618234,
      "loss": 0.0002,
      "step": 78120
    },
    {
      "epoch": 36.154558074965294,
      "grad_norm": 0.004967161919921637,
      "learning_rate": 0.027690883850069417,
      "loss": 0.0003,
      "step": 78130
    },
    {
      "epoch": 36.159185562239706,
      "grad_norm": 0.0007940715295262635,
      "learning_rate": 0.027681628875520593,
      "loss": 0.0001,
      "step": 78140
    },
    {
      "epoch": 36.16381304951411,
      "grad_norm": 0.014178125187754631,
      "learning_rate": 0.027672373900971776,
      "loss": 0.0001,
      "step": 78150
    },
    {
      "epoch": 36.16844053678852,
      "grad_norm": 0.003656095825135708,
      "learning_rate": 0.027663118926422953,
      "loss": 0.0001,
      "step": 78160
    },
    {
      "epoch": 36.173068024062935,
      "grad_norm": 0.0016415412537753582,
      "learning_rate": 0.027653863951874132,
      "loss": 0.0002,
      "step": 78170
    },
    {
      "epoch": 36.17769551133734,
      "grad_norm": 0.09051398187875748,
      "learning_rate": 0.027644608977325315,
      "loss": 0.0003,
      "step": 78180
    },
    {
      "epoch": 36.18232299861175,
      "grad_norm": 0.002792924176901579,
      "learning_rate": 0.02763535400277649,
      "loss": 0.0,
      "step": 78190
    },
    {
      "epoch": 36.186950485886165,
      "grad_norm": 0.004193163942545652,
      "learning_rate": 0.027626099028227675,
      "loss": 0.0001,
      "step": 78200
    },
    {
      "epoch": 36.19157797316058,
      "grad_norm": 0.012557690031826496,
      "learning_rate": 0.027616844053678855,
      "loss": 0.0002,
      "step": 78210
    },
    {
      "epoch": 36.19620546043498,
      "grad_norm": 0.021544508635997772,
      "learning_rate": 0.027607589079130038,
      "loss": 0.0001,
      "step": 78220
    },
    {
      "epoch": 36.200832947709394,
      "grad_norm": 0.0166301466524601,
      "learning_rate": 0.027598334104581214,
      "loss": 0.0002,
      "step": 78230
    },
    {
      "epoch": 36.205460434983806,
      "grad_norm": 0.3005211651325226,
      "learning_rate": 0.02758907913003239,
      "loss": 0.0002,
      "step": 78240
    },
    {
      "epoch": 36.21008792225821,
      "grad_norm": 0.003117634216323495,
      "learning_rate": 0.027579824155483573,
      "loss": 0.0001,
      "step": 78250
    },
    {
      "epoch": 36.21471540953262,
      "grad_norm": 0.010054835118353367,
      "learning_rate": 0.027570569180934753,
      "loss": 0.0001,
      "step": 78260
    },
    {
      "epoch": 36.219342896807035,
      "grad_norm": 0.45757317543029785,
      "learning_rate": 0.027561314206385936,
      "loss": 0.0002,
      "step": 78270
    },
    {
      "epoch": 36.22397038408145,
      "grad_norm": 0.016157230362296104,
      "learning_rate": 0.027552059231837112,
      "loss": 0.0001,
      "step": 78280
    },
    {
      "epoch": 36.22859787135585,
      "grad_norm": 0.019090469926595688,
      "learning_rate": 0.027542804257288295,
      "loss": 0.0001,
      "step": 78290
    },
    {
      "epoch": 36.233225358630264,
      "grad_norm": 0.011559352278709412,
      "learning_rate": 0.027533549282739475,
      "loss": 0.0001,
      "step": 78300
    },
    {
      "epoch": 36.237852845904676,
      "grad_norm": 0.015386794693768024,
      "learning_rate": 0.02752429430819065,
      "loss": 0.0001,
      "step": 78310
    },
    {
      "epoch": 36.24248033317908,
      "grad_norm": 0.026403574272990227,
      "learning_rate": 0.027515039333641834,
      "loss": 0.0001,
      "step": 78320
    },
    {
      "epoch": 36.24710782045349,
      "grad_norm": 0.01367507129907608,
      "learning_rate": 0.02750578435909301,
      "loss": 0.0001,
      "step": 78330
    },
    {
      "epoch": 36.251735307727905,
      "grad_norm": 0.0025182340759783983,
      "learning_rate": 0.027496529384544194,
      "loss": 0.0002,
      "step": 78340
    },
    {
      "epoch": 36.25636279500231,
      "grad_norm": 0.03182532265782356,
      "learning_rate": 0.027487274409995373,
      "loss": 0.0001,
      "step": 78350
    },
    {
      "epoch": 36.26099028227672,
      "grad_norm": 0.008665377274155617,
      "learning_rate": 0.027478019435446557,
      "loss": 0.0007,
      "step": 78360
    },
    {
      "epoch": 36.265617769551135,
      "grad_norm": 0.00482227373868227,
      "learning_rate": 0.027468764460897733,
      "loss": 0.0001,
      "step": 78370
    },
    {
      "epoch": 36.27024525682555,
      "grad_norm": 0.014372293837368488,
      "learning_rate": 0.027459509486348916,
      "loss": 0.0001,
      "step": 78380
    },
    {
      "epoch": 36.27487274409995,
      "grad_norm": 0.004543025977909565,
      "learning_rate": 0.027450254511800096,
      "loss": 0.0002,
      "step": 78390
    },
    {
      "epoch": 36.279500231374364,
      "grad_norm": 0.0050115203484892845,
      "learning_rate": 0.027440999537251272,
      "loss": 0.0001,
      "step": 78400
    },
    {
      "epoch": 36.284127718648776,
      "grad_norm": 0.010304247960448265,
      "learning_rate": 0.027431744562702455,
      "loss": 0.0002,
      "step": 78410
    },
    {
      "epoch": 36.28875520592318,
      "grad_norm": 0.0019507822580635548,
      "learning_rate": 0.02742248958815363,
      "loss": 0.0001,
      "step": 78420
    },
    {
      "epoch": 36.29338269319759,
      "grad_norm": 0.6264941692352295,
      "learning_rate": 0.027413234613604814,
      "loss": 0.0003,
      "step": 78430
    },
    {
      "epoch": 36.298010180472005,
      "grad_norm": 0.0018467799527570605,
      "learning_rate": 0.027403979639055994,
      "loss": 0.0,
      "step": 78440
    },
    {
      "epoch": 36.30263766774642,
      "grad_norm": 0.004075691569596529,
      "learning_rate": 0.027394724664507177,
      "loss": 0.0001,
      "step": 78450
    },
    {
      "epoch": 36.30726515502082,
      "grad_norm": 0.014786896295845509,
      "learning_rate": 0.027385469689958353,
      "loss": 0.0001,
      "step": 78460
    },
    {
      "epoch": 36.311892642295234,
      "grad_norm": 0.01820593699812889,
      "learning_rate": 0.027376214715409533,
      "loss": 0.0001,
      "step": 78470
    },
    {
      "epoch": 36.316520129569646,
      "grad_norm": 0.008696995675563812,
      "learning_rate": 0.027366959740860716,
      "loss": 0.0002,
      "step": 78480
    },
    {
      "epoch": 36.32114761684405,
      "grad_norm": 0.015624134801328182,
      "learning_rate": 0.027357704766311892,
      "loss": 0.0003,
      "step": 78490
    },
    {
      "epoch": 36.32577510411846,
      "grad_norm": 0.00173029990401119,
      "learning_rate": 0.027348449791763076,
      "loss": 0.0001,
      "step": 78500
    },
    {
      "epoch": 36.330402591392875,
      "grad_norm": 0.10858532041311264,
      "learning_rate": 0.027339194817214252,
      "loss": 0.0001,
      "step": 78510
    },
    {
      "epoch": 36.33503007866728,
      "grad_norm": 0.030976684764027596,
      "learning_rate": 0.027329939842665435,
      "loss": 0.0001,
      "step": 78520
    },
    {
      "epoch": 36.33965756594169,
      "grad_norm": 0.015343568287789822,
      "learning_rate": 0.027320684868116615,
      "loss": 0.0001,
      "step": 78530
    },
    {
      "epoch": 36.344285053216105,
      "grad_norm": 0.00328867812640965,
      "learning_rate": 0.02731142989356779,
      "loss": 0.0,
      "step": 78540
    },
    {
      "epoch": 36.34891254049052,
      "grad_norm": 0.011962766759097576,
      "learning_rate": 0.027302174919018974,
      "loss": 0.0001,
      "step": 78550
    },
    {
      "epoch": 36.35354002776492,
      "grad_norm": 0.1718161404132843,
      "learning_rate": 0.027292919944470154,
      "loss": 0.0002,
      "step": 78560
    },
    {
      "epoch": 36.358167515039334,
      "grad_norm": 0.05011530965566635,
      "learning_rate": 0.027283664969921337,
      "loss": 0.0,
      "step": 78570
    },
    {
      "epoch": 36.362795002313746,
      "grad_norm": 0.0054993219673633575,
      "learning_rate": 0.027274409995372513,
      "loss": 0.0001,
      "step": 78580
    },
    {
      "epoch": 36.36742248958815,
      "grad_norm": 0.019298017024993896,
      "learning_rate": 0.027265155020823696,
      "loss": 0.0001,
      "step": 78590
    },
    {
      "epoch": 36.37204997686256,
      "grad_norm": 0.0031572459265589714,
      "learning_rate": 0.027255900046274872,
      "loss": 0.0002,
      "step": 78600
    },
    {
      "epoch": 36.376677464136975,
      "grad_norm": 0.04355562478303909,
      "learning_rate": 0.027246645071726052,
      "loss": 0.0001,
      "step": 78610
    },
    {
      "epoch": 36.38130495141139,
      "grad_norm": 0.025914322584867477,
      "learning_rate": 0.027237390097177235,
      "loss": 0.0002,
      "step": 78620
    },
    {
      "epoch": 36.38593243868579,
      "grad_norm": 0.004944286774843931,
      "learning_rate": 0.02722813512262841,
      "loss": 0.0003,
      "step": 78630
    },
    {
      "epoch": 36.390559925960204,
      "grad_norm": 0.0022082009818404913,
      "learning_rate": 0.027218880148079595,
      "loss": 0.0001,
      "step": 78640
    },
    {
      "epoch": 36.395187413234616,
      "grad_norm": 0.0016245171427726746,
      "learning_rate": 0.027209625173530774,
      "loss": 0.0001,
      "step": 78650
    },
    {
      "epoch": 36.39981490050902,
      "grad_norm": 0.0009619283373467624,
      "learning_rate": 0.027200370198981957,
      "loss": 0.0001,
      "step": 78660
    },
    {
      "epoch": 36.40444238778343,
      "grad_norm": 0.011156282387673855,
      "learning_rate": 0.027191115224433134,
      "loss": 0.0001,
      "step": 78670
    },
    {
      "epoch": 36.409069875057845,
      "grad_norm": 0.00670621870085597,
      "learning_rate": 0.027181860249884317,
      "loss": 0.0001,
      "step": 78680
    },
    {
      "epoch": 36.41369736233225,
      "grad_norm": 0.0032054735347628593,
      "learning_rate": 0.027172605275335493,
      "loss": 0.0001,
      "step": 78690
    },
    {
      "epoch": 36.41832484960666,
      "grad_norm": 0.04704353213310242,
      "learning_rate": 0.027163350300786673,
      "loss": 0.0001,
      "step": 78700
    },
    {
      "epoch": 36.422952336881075,
      "grad_norm": 0.09608050435781479,
      "learning_rate": 0.027154095326237856,
      "loss": 0.0001,
      "step": 78710
    },
    {
      "epoch": 36.42757982415549,
      "grad_norm": 0.0006004199967719615,
      "learning_rate": 0.027144840351689032,
      "loss": 0.0001,
      "step": 78720
    },
    {
      "epoch": 36.43220731142989,
      "grad_norm": 0.011952215805649757,
      "learning_rate": 0.027135585377140215,
      "loss": 0.0001,
      "step": 78730
    },
    {
      "epoch": 36.436834798704304,
      "grad_norm": 0.004061982501298189,
      "learning_rate": 0.027126330402591395,
      "loss": 0.0001,
      "step": 78740
    },
    {
      "epoch": 36.441462285978716,
      "grad_norm": 0.0014713654527440667,
      "learning_rate": 0.027117075428042578,
      "loss": 0.0051,
      "step": 78750
    },
    {
      "epoch": 36.44608977325312,
      "grad_norm": 0.019291285425424576,
      "learning_rate": 0.027107820453493754,
      "loss": 0.0001,
      "step": 78760
    },
    {
      "epoch": 36.45071726052753,
      "grad_norm": 0.0004290440119802952,
      "learning_rate": 0.02709856547894493,
      "loss": 0.0003,
      "step": 78770
    },
    {
      "epoch": 36.455344747801945,
      "grad_norm": 0.0015787454321980476,
      "learning_rate": 0.027089310504396114,
      "loss": 0.0001,
      "step": 78780
    },
    {
      "epoch": 36.45997223507635,
      "grad_norm": 0.010065174661576748,
      "learning_rate": 0.027080055529847293,
      "loss": 0.0004,
      "step": 78790
    },
    {
      "epoch": 36.46459972235076,
      "grad_norm": 0.004625654313713312,
      "learning_rate": 0.027070800555298476,
      "loss": 0.0,
      "step": 78800
    },
    {
      "epoch": 36.469227209625174,
      "grad_norm": 0.010336870327591896,
      "learning_rate": 0.027061545580749653,
      "loss": 0.0003,
      "step": 78810
    },
    {
      "epoch": 36.473854696899586,
      "grad_norm": 0.0025694789364933968,
      "learning_rate": 0.027052290606200836,
      "loss": 0.0001,
      "step": 78820
    },
    {
      "epoch": 36.47848218417399,
      "grad_norm": 0.009624316357076168,
      "learning_rate": 0.027043035631652015,
      "loss": 0.0001,
      "step": 78830
    },
    {
      "epoch": 36.4831096714484,
      "grad_norm": 0.072932168841362,
      "learning_rate": 0.02703378065710319,
      "loss": 0.0001,
      "step": 78840
    },
    {
      "epoch": 36.487737158722815,
      "grad_norm": 0.29299691319465637,
      "learning_rate": 0.027024525682554375,
      "loss": 0.0002,
      "step": 78850
    },
    {
      "epoch": 36.49236464599722,
      "grad_norm": 0.005644245073199272,
      "learning_rate": 0.02701527070800555,
      "loss": 0.0001,
      "step": 78860
    },
    {
      "epoch": 36.49699213327163,
      "grad_norm": 0.003825318068265915,
      "learning_rate": 0.027006015733456734,
      "loss": 0.0003,
      "step": 78870
    },
    {
      "epoch": 36.501619620546045,
      "grad_norm": 0.04642060026526451,
      "learning_rate": 0.026996760758907914,
      "loss": 0.0001,
      "step": 78880
    },
    {
      "epoch": 36.50624710782046,
      "grad_norm": 0.0014747914392501116,
      "learning_rate": 0.026987505784359097,
      "loss": 0.0003,
      "step": 78890
    },
    {
      "epoch": 36.51087459509486,
      "grad_norm": 0.0004648322646971792,
      "learning_rate": 0.026978250809810273,
      "loss": 0.0001,
      "step": 78900
    },
    {
      "epoch": 36.515502082369274,
      "grad_norm": 0.00759267108514905,
      "learning_rate": 0.026968995835261456,
      "loss": 0.0001,
      "step": 78910
    },
    {
      "epoch": 36.520129569643686,
      "grad_norm": 0.07109851390123367,
      "learning_rate": 0.026959740860712636,
      "loss": 0.0001,
      "step": 78920
    },
    {
      "epoch": 36.52475705691809,
      "grad_norm": 0.0037782711442559958,
      "learning_rate": 0.026950485886163812,
      "loss": 0.0001,
      "step": 78930
    },
    {
      "epoch": 36.5293845441925,
      "grad_norm": 0.005250109825283289,
      "learning_rate": 0.026941230911614995,
      "loss": 0.0001,
      "step": 78940
    },
    {
      "epoch": 36.534012031466915,
      "grad_norm": 0.8043832778930664,
      "learning_rate": 0.02693197593706617,
      "loss": 0.0005,
      "step": 78950
    },
    {
      "epoch": 36.53863951874132,
      "grad_norm": 0.014569194987416267,
      "learning_rate": 0.026922720962517355,
      "loss": 0.0007,
      "step": 78960
    },
    {
      "epoch": 36.54326700601573,
      "grad_norm": 0.024405499920248985,
      "learning_rate": 0.026913465987968534,
      "loss": 0.0001,
      "step": 78970
    },
    {
      "epoch": 36.547894493290144,
      "grad_norm": 0.13864703476428986,
      "learning_rate": 0.026904211013419718,
      "loss": 0.0003,
      "step": 78980
    },
    {
      "epoch": 36.552521980564556,
      "grad_norm": 0.0756474956870079,
      "learning_rate": 0.026894956038870894,
      "loss": 0.0001,
      "step": 78990
    },
    {
      "epoch": 36.55714946783896,
      "grad_norm": 0.08703242987394333,
      "learning_rate": 0.026885701064322073,
      "loss": 0.0001,
      "step": 79000
    },
    {
      "epoch": 36.56177695511337,
      "grad_norm": 0.012709512375295162,
      "learning_rate": 0.026876446089773257,
      "loss": 0.0001,
      "step": 79010
    },
    {
      "epoch": 36.566404442387785,
      "grad_norm": 0.010702314786612988,
      "learning_rate": 0.026867191115224433,
      "loss": 0.0001,
      "step": 79020
    },
    {
      "epoch": 36.57103192966219,
      "grad_norm": 0.019067054614424706,
      "learning_rate": 0.026857936140675616,
      "loss": 0.0002,
      "step": 79030
    },
    {
      "epoch": 36.5756594169366,
      "grad_norm": 0.0010071846190840006,
      "learning_rate": 0.026848681166126792,
      "loss": 0.0001,
      "step": 79040
    },
    {
      "epoch": 36.580286904211015,
      "grad_norm": 0.0014972996432334185,
      "learning_rate": 0.026839426191577975,
      "loss": 0.0002,
      "step": 79050
    },
    {
      "epoch": 36.58491439148543,
      "grad_norm": 0.005899935960769653,
      "learning_rate": 0.026830171217029155,
      "loss": 0.0025,
      "step": 79060
    },
    {
      "epoch": 36.58954187875983,
      "grad_norm": 0.0027878605760633945,
      "learning_rate": 0.02682091624248033,
      "loss": 0.0005,
      "step": 79070
    },
    {
      "epoch": 36.594169366034244,
      "grad_norm": 0.006433920469135046,
      "learning_rate": 0.026811661267931514,
      "loss": 0.0001,
      "step": 79080
    },
    {
      "epoch": 36.598796853308656,
      "grad_norm": 2.5158867835998535,
      "learning_rate": 0.026802406293382694,
      "loss": 0.0005,
      "step": 79090
    },
    {
      "epoch": 36.60342434058306,
      "grad_norm": 0.003080915194004774,
      "learning_rate": 0.026793151318833877,
      "loss": 0.0001,
      "step": 79100
    },
    {
      "epoch": 36.60805182785747,
      "grad_norm": 0.0016679564723744988,
      "learning_rate": 0.026783896344285053,
      "loss": 0.0,
      "step": 79110
    },
    {
      "epoch": 36.612679315131885,
      "grad_norm": 0.003636864246800542,
      "learning_rate": 0.026774641369736236,
      "loss": 0.0001,
      "step": 79120
    },
    {
      "epoch": 36.61730680240629,
      "grad_norm": 0.008249221369624138,
      "learning_rate": 0.026765386395187413,
      "loss": 0.0001,
      "step": 79130
    },
    {
      "epoch": 36.6219342896807,
      "grad_norm": 0.008238770067691803,
      "learning_rate": 0.026756131420638596,
      "loss": 0.0001,
      "step": 79140
    },
    {
      "epoch": 36.626561776955114,
      "grad_norm": 0.0052623990923166275,
      "learning_rate": 0.026746876446089776,
      "loss": 0.0,
      "step": 79150
    },
    {
      "epoch": 36.631189264229526,
      "grad_norm": 0.0028734670486301184,
      "learning_rate": 0.02673762147154095,
      "loss": 0.0001,
      "step": 79160
    },
    {
      "epoch": 36.63581675150393,
      "grad_norm": 0.0006287364521995187,
      "learning_rate": 0.026728366496992135,
      "loss": 0.0,
      "step": 79170
    },
    {
      "epoch": 36.64044423877834,
      "grad_norm": 0.02155228890478611,
      "learning_rate": 0.026719111522443315,
      "loss": 0.0001,
      "step": 79180
    },
    {
      "epoch": 36.645071726052755,
      "grad_norm": 0.003056555287912488,
      "learning_rate": 0.026709856547894498,
      "loss": 0.0001,
      "step": 79190
    },
    {
      "epoch": 36.64969921332716,
      "grad_norm": 0.013345166109502316,
      "learning_rate": 0.026700601573345674,
      "loss": 0.0002,
      "step": 79200
    },
    {
      "epoch": 36.65432670060157,
      "grad_norm": 0.00828778836876154,
      "learning_rate": 0.026691346598796857,
      "loss": 0.0,
      "step": 79210
    },
    {
      "epoch": 36.658954187875985,
      "grad_norm": 0.012715497985482216,
      "learning_rate": 0.026682091624248033,
      "loss": 0.0001,
      "step": 79220
    },
    {
      "epoch": 36.6635816751504,
      "grad_norm": 0.002603150438517332,
      "learning_rate": 0.026672836649699213,
      "loss": 0.0001,
      "step": 79230
    },
    {
      "epoch": 36.6682091624248,
      "grad_norm": 0.00566687760874629,
      "learning_rate": 0.026663581675150396,
      "loss": 0.0012,
      "step": 79240
    },
    {
      "epoch": 36.672836649699214,
      "grad_norm": 0.12424086034297943,
      "learning_rate": 0.026654326700601572,
      "loss": 0.0002,
      "step": 79250
    },
    {
      "epoch": 36.677464136973626,
      "grad_norm": 0.03370071202516556,
      "learning_rate": 0.026645071726052755,
      "loss": 0.0001,
      "step": 79260
    },
    {
      "epoch": 36.68209162424803,
      "grad_norm": 0.000765513104852289,
      "learning_rate": 0.026635816751503935,
      "loss": 0.0002,
      "step": 79270
    },
    {
      "epoch": 36.68671911152244,
      "grad_norm": 0.0011661192402243614,
      "learning_rate": 0.02662656177695512,
      "loss": 0.0001,
      "step": 79280
    },
    {
      "epoch": 36.691346598796855,
      "grad_norm": 0.00018918499699793756,
      "learning_rate": 0.026617306802406294,
      "loss": 0.0005,
      "step": 79290
    },
    {
      "epoch": 36.69597408607126,
      "grad_norm": 0.00747290812432766,
      "learning_rate": 0.02660805182785747,
      "loss": 0.0006,
      "step": 79300
    },
    {
      "epoch": 36.70060157334567,
      "grad_norm": 0.0036969592329114676,
      "learning_rate": 0.026598796853308654,
      "loss": 0.0,
      "step": 79310
    },
    {
      "epoch": 36.705229060620084,
      "grad_norm": 0.006072335410863161,
      "learning_rate": 0.026589541878759834,
      "loss": 0.0001,
      "step": 79320
    },
    {
      "epoch": 36.709856547894496,
      "grad_norm": 0.014171048998832703,
      "learning_rate": 0.026580286904211017,
      "loss": 0.0001,
      "step": 79330
    },
    {
      "epoch": 36.7144840351689,
      "grad_norm": 0.01942634955048561,
      "learning_rate": 0.026571031929662193,
      "loss": 0.0001,
      "step": 79340
    },
    {
      "epoch": 36.71911152244331,
      "grad_norm": 0.0017954065697267652,
      "learning_rate": 0.026561776955113376,
      "loss": 0.0,
      "step": 79350
    },
    {
      "epoch": 36.723739009717725,
      "grad_norm": 0.057948123663663864,
      "learning_rate": 0.026552521980564556,
      "loss": 0.0001,
      "step": 79360
    },
    {
      "epoch": 36.72836649699213,
      "grad_norm": 0.004330352880060673,
      "learning_rate": 0.02654326700601574,
      "loss": 0.0001,
      "step": 79370
    },
    {
      "epoch": 36.73299398426654,
      "grad_norm": 0.10436150431632996,
      "learning_rate": 0.026534012031466915,
      "loss": 0.0001,
      "step": 79380
    },
    {
      "epoch": 36.737621471540955,
      "grad_norm": 0.05443523824214935,
      "learning_rate": 0.02652475705691809,
      "loss": 0.0001,
      "step": 79390
    },
    {
      "epoch": 36.74224895881537,
      "grad_norm": 0.022696828469634056,
      "learning_rate": 0.026515502082369274,
      "loss": 0.0001,
      "step": 79400
    },
    {
      "epoch": 36.74687644608977,
      "grad_norm": 0.004113693255931139,
      "learning_rate": 0.026506247107820454,
      "loss": 0.0001,
      "step": 79410
    },
    {
      "epoch": 36.751503933364184,
      "grad_norm": 0.03299785777926445,
      "learning_rate": 0.026496992133271637,
      "loss": 0.0001,
      "step": 79420
    },
    {
      "epoch": 36.756131420638596,
      "grad_norm": 0.011808204464614391,
      "learning_rate": 0.026487737158722813,
      "loss": 0.0001,
      "step": 79430
    },
    {
      "epoch": 36.760758907913,
      "grad_norm": 0.008395969867706299,
      "learning_rate": 0.026478482184173997,
      "loss": 0.0003,
      "step": 79440
    },
    {
      "epoch": 36.76538639518741,
      "grad_norm": 0.007241240236908197,
      "learning_rate": 0.026469227209625176,
      "loss": 0.0002,
      "step": 79450
    },
    {
      "epoch": 36.770013882461825,
      "grad_norm": 0.002304456429556012,
      "learning_rate": 0.026459972235076352,
      "loss": 0.0002,
      "step": 79460
    },
    {
      "epoch": 36.77464136973623,
      "grad_norm": 0.03210890293121338,
      "learning_rate": 0.026450717260527536,
      "loss": 0.0001,
      "step": 79470
    },
    {
      "epoch": 36.77926885701064,
      "grad_norm": 0.004372698720544577,
      "learning_rate": 0.026441462285978712,
      "loss": 0.0001,
      "step": 79480
    },
    {
      "epoch": 36.783896344285054,
      "grad_norm": 0.002349782269448042,
      "learning_rate": 0.026432207311429895,
      "loss": 0.0008,
      "step": 79490
    },
    {
      "epoch": 36.788523831559466,
      "grad_norm": 0.006658815313130617,
      "learning_rate": 0.026422952336881075,
      "loss": 0.0001,
      "step": 79500
    },
    {
      "epoch": 36.79315131883387,
      "grad_norm": 0.14038404822349548,
      "learning_rate": 0.026413697362332258,
      "loss": 0.002,
      "step": 79510
    },
    {
      "epoch": 36.79777880610828,
      "grad_norm": 0.006887141615152359,
      "learning_rate": 0.026404442387783434,
      "loss": 0.0002,
      "step": 79520
    },
    {
      "epoch": 36.802406293382695,
      "grad_norm": 0.016368165612220764,
      "learning_rate": 0.026395187413234614,
      "loss": 0.0003,
      "step": 79530
    },
    {
      "epoch": 36.8070337806571,
      "grad_norm": 0.019772270694375038,
      "learning_rate": 0.026385932438685797,
      "loss": 0.0001,
      "step": 79540
    },
    {
      "epoch": 36.81166126793151,
      "grad_norm": 0.02328171394765377,
      "learning_rate": 0.026376677464136973,
      "loss": 0.0001,
      "step": 79550
    },
    {
      "epoch": 36.816288755205925,
      "grad_norm": 0.012727534398436546,
      "learning_rate": 0.026367422489588156,
      "loss": 0.0001,
      "step": 79560
    },
    {
      "epoch": 36.82091624248034,
      "grad_norm": 0.01836586371064186,
      "learning_rate": 0.026358167515039332,
      "loss": 0.0001,
      "step": 79570
    },
    {
      "epoch": 36.82554372975474,
      "grad_norm": 0.027237627655267715,
      "learning_rate": 0.026348912540490516,
      "loss": 0.0001,
      "step": 79580
    },
    {
      "epoch": 36.830171217029154,
      "grad_norm": 0.007711449638009071,
      "learning_rate": 0.026339657565941695,
      "loss": 0.0002,
      "step": 79590
    },
    {
      "epoch": 36.834798704303566,
      "grad_norm": 0.00893297791481018,
      "learning_rate": 0.02633040259139288,
      "loss": 0.0001,
      "step": 79600
    },
    {
      "epoch": 36.83942619157797,
      "grad_norm": 0.018676267936825752,
      "learning_rate": 0.026321147616844055,
      "loss": 0.0001,
      "step": 79610
    },
    {
      "epoch": 36.84405367885238,
      "grad_norm": 0.03712770715355873,
      "learning_rate": 0.026311892642295234,
      "loss": 0.0001,
      "step": 79620
    },
    {
      "epoch": 36.848681166126795,
      "grad_norm": 0.012154637835919857,
      "learning_rate": 0.026302637667746417,
      "loss": 0.0001,
      "step": 79630
    },
    {
      "epoch": 36.8533086534012,
      "grad_norm": 0.0021808992605656385,
      "learning_rate": 0.026293382693197594,
      "loss": 0.0001,
      "step": 79640
    },
    {
      "epoch": 36.85793614067561,
      "grad_norm": 0.006926627829670906,
      "learning_rate": 0.026284127718648777,
      "loss": 0.0001,
      "step": 79650
    },
    {
      "epoch": 36.862563627950024,
      "grad_norm": 0.0013609110610559583,
      "learning_rate": 0.026274872744099953,
      "loss": 0.0002,
      "step": 79660
    },
    {
      "epoch": 36.867191115224436,
      "grad_norm": 0.002758690156042576,
      "learning_rate": 0.026265617769551136,
      "loss": 0.0001,
      "step": 79670
    },
    {
      "epoch": 36.87181860249884,
      "grad_norm": 0.0005611872184090316,
      "learning_rate": 0.026256362795002316,
      "loss": 0.0,
      "step": 79680
    },
    {
      "epoch": 36.87644608977325,
      "grad_norm": 0.01993853598833084,
      "learning_rate": 0.026247107820453492,
      "loss": 0.0001,
      "step": 79690
    },
    {
      "epoch": 36.881073577047665,
      "grad_norm": 0.005318330135196447,
      "learning_rate": 0.026237852845904675,
      "loss": 0.0001,
      "step": 79700
    },
    {
      "epoch": 36.88570106432207,
      "grad_norm": 0.14443401992321014,
      "learning_rate": 0.026228597871355855,
      "loss": 0.0002,
      "step": 79710
    },
    {
      "epoch": 36.89032855159648,
      "grad_norm": 0.12655030190944672,
      "learning_rate": 0.026219342896807038,
      "loss": 0.0001,
      "step": 79720
    },
    {
      "epoch": 36.894956038870895,
      "grad_norm": 0.004796464927494526,
      "learning_rate": 0.026210087922258214,
      "loss": 0.0001,
      "step": 79730
    },
    {
      "epoch": 36.89958352614531,
      "grad_norm": 0.029364313930273056,
      "learning_rate": 0.026200832947709397,
      "loss": 0.0001,
      "step": 79740
    },
    {
      "epoch": 36.90421101341971,
      "grad_norm": 0.0031430188100785017,
      "learning_rate": 0.026191577973160574,
      "loss": 0.0001,
      "step": 79750
    },
    {
      "epoch": 36.908838500694124,
      "grad_norm": 0.002092410111799836,
      "learning_rate": 0.026182322998611753,
      "loss": 0.0001,
      "step": 79760
    },
    {
      "epoch": 36.913465987968536,
      "grad_norm": 0.05559765174984932,
      "learning_rate": 0.026173068024062936,
      "loss": 0.0001,
      "step": 79770
    },
    {
      "epoch": 36.91809347524294,
      "grad_norm": 0.0002484526194166392,
      "learning_rate": 0.026163813049514113,
      "loss": 0.0001,
      "step": 79780
    },
    {
      "epoch": 36.92272096251735,
      "grad_norm": 0.0021925615146756172,
      "learning_rate": 0.026154558074965296,
      "loss": 0.0001,
      "step": 79790
    },
    {
      "epoch": 36.927348449791765,
      "grad_norm": 0.0022667930461466312,
      "learning_rate": 0.026145303100416475,
      "loss": 0.0001,
      "step": 79800
    },
    {
      "epoch": 36.93197593706617,
      "grad_norm": 0.009765654802322388,
      "learning_rate": 0.02613604812586766,
      "loss": 0.0001,
      "step": 79810
    },
    {
      "epoch": 36.93660342434058,
      "grad_norm": 0.007391920313239098,
      "learning_rate": 0.026126793151318835,
      "loss": 0.0001,
      "step": 79820
    },
    {
      "epoch": 36.941230911614994,
      "grad_norm": 0.0022272018250077963,
      "learning_rate": 0.026117538176770018,
      "loss": 0.0,
      "step": 79830
    },
    {
      "epoch": 36.945858398889406,
      "grad_norm": 0.0857105478644371,
      "learning_rate": 0.026108283202221194,
      "loss": 0.0001,
      "step": 79840
    },
    {
      "epoch": 36.95048588616381,
      "grad_norm": 0.002567161340266466,
      "learning_rate": 0.026099028227672374,
      "loss": 0.0001,
      "step": 79850
    },
    {
      "epoch": 36.95511337343822,
      "grad_norm": 0.5548498034477234,
      "learning_rate": 0.026089773253123557,
      "loss": 0.0002,
      "step": 79860
    },
    {
      "epoch": 36.959740860712635,
      "grad_norm": 0.002014001365751028,
      "learning_rate": 0.026080518278574733,
      "loss": 0.0001,
      "step": 79870
    },
    {
      "epoch": 36.96436834798704,
      "grad_norm": 0.0007746809278614819,
      "learning_rate": 0.026071263304025916,
      "loss": 0.0001,
      "step": 79880
    },
    {
      "epoch": 36.96899583526145,
      "grad_norm": 0.004772736690938473,
      "learning_rate": 0.026062008329477096,
      "loss": 0.0001,
      "step": 79890
    },
    {
      "epoch": 36.973623322535865,
      "grad_norm": 0.10113993287086487,
      "learning_rate": 0.02605275335492828,
      "loss": 0.0001,
      "step": 79900
    },
    {
      "epoch": 36.97825080981027,
      "grad_norm": 0.050120797008275986,
      "learning_rate": 0.026043498380379455,
      "loss": 0.0001,
      "step": 79910
    },
    {
      "epoch": 36.98287829708468,
      "grad_norm": 0.003378442954272032,
      "learning_rate": 0.02603424340583063,
      "loss": 0.0001,
      "step": 79920
    },
    {
      "epoch": 36.987505784359094,
      "grad_norm": 0.0017005052650347352,
      "learning_rate": 0.026024988431281815,
      "loss": 0.0001,
      "step": 79930
    },
    {
      "epoch": 36.992133271633506,
      "grad_norm": 0.009633013978600502,
      "learning_rate": 0.026015733456732994,
      "loss": 0.0001,
      "step": 79940
    },
    {
      "epoch": 36.99676075890791,
      "grad_norm": 0.008831305429339409,
      "learning_rate": 0.026006478482184178,
      "loss": 0.0001,
      "step": 79950
    },
    {
      "epoch": 37.0,
      "eval_accuracy_branch1": 0.9911415806501309,
      "eval_accuracy_branch2": 0.5003081189339085,
      "eval_f1_branch1": 0.992283954799806,
      "eval_f1_branch2": 0.5002940023526004,
      "eval_loss": 0.017931072041392326,
      "eval_precision_branch1": 0.9924353471132551,
      "eval_precision_branch2": 0.5003081537550038,
      "eval_recall_branch1": 0.9922460136852536,
      "eval_recall_branch2": 0.5003081189339085,
      "eval_runtime": 28.8822,
      "eval_samples_per_second": 449.481,
      "eval_steps_per_second": 56.194,
      "step": 79957
    },
    {
      "epoch": 37.00138824618232,
      "grad_norm": 0.0038075579795986414,
      "learning_rate": 0.025997223507635354,
      "loss": 0.0001,
      "step": 79960
    },
    {
      "epoch": 37.006015733456735,
      "grad_norm": 0.037419743835926056,
      "learning_rate": 0.025987968533086537,
      "loss": 0.0001,
      "step": 79970
    },
    {
      "epoch": 37.01064322073114,
      "grad_norm": 0.013095074333250523,
      "learning_rate": 0.025978713558537717,
      "loss": 0.0001,
      "step": 79980
    },
    {
      "epoch": 37.01527070800555,
      "grad_norm": 0.004923374857753515,
      "learning_rate": 0.025969458583988893,
      "loss": 0.0001,
      "step": 79990
    },
    {
      "epoch": 37.019898195279964,
      "grad_norm": 0.013355029746890068,
      "learning_rate": 0.025960203609440076,
      "loss": 0.0,
      "step": 80000
    },
    {
      "epoch": 37.024525682554376,
      "grad_norm": 0.0018144516507163644,
      "learning_rate": 0.025950948634891252,
      "loss": 0.0002,
      "step": 80010
    },
    {
      "epoch": 37.02915316982878,
      "grad_norm": 0.003518909914419055,
      "learning_rate": 0.025941693660342435,
      "loss": 0.0,
      "step": 80020
    },
    {
      "epoch": 37.03378065710319,
      "grad_norm": 0.0009356264490634203,
      "learning_rate": 0.025932438685793615,
      "loss": 0.0006,
      "step": 80030
    },
    {
      "epoch": 37.038408144377605,
      "grad_norm": 0.02990022860467434,
      "learning_rate": 0.025923183711244798,
      "loss": 0.0001,
      "step": 80040
    },
    {
      "epoch": 37.04303563165201,
      "grad_norm": 0.02706151083111763,
      "learning_rate": 0.025913928736695974,
      "loss": 0.0001,
      "step": 80050
    },
    {
      "epoch": 37.04766311892642,
      "grad_norm": 0.03879886120557785,
      "learning_rate": 0.025904673762147157,
      "loss": 0.0001,
      "step": 80060
    },
    {
      "epoch": 37.052290606200835,
      "grad_norm": 0.03726736456155777,
      "learning_rate": 0.025895418787598337,
      "loss": 0.0001,
      "step": 80070
    },
    {
      "epoch": 37.05691809347524,
      "grad_norm": 0.0010733860544860363,
      "learning_rate": 0.025886163813049513,
      "loss": 0.0001,
      "step": 80080
    },
    {
      "epoch": 37.06154558074965,
      "grad_norm": 0.05353508144617081,
      "learning_rate": 0.025876908838500697,
      "loss": 0.0001,
      "step": 80090
    },
    {
      "epoch": 37.066173068024064,
      "grad_norm": 0.0034996166359633207,
      "learning_rate": 0.025867653863951873,
      "loss": 0.0001,
      "step": 80100
    },
    {
      "epoch": 37.070800555298476,
      "grad_norm": 0.009998396039009094,
      "learning_rate": 0.025858398889403056,
      "loss": 0.0001,
      "step": 80110
    },
    {
      "epoch": 37.07542804257288,
      "grad_norm": 0.030022382736206055,
      "learning_rate": 0.025849143914854236,
      "loss": 0.0001,
      "step": 80120
    },
    {
      "epoch": 37.08005552984729,
      "grad_norm": 0.5449494123458862,
      "learning_rate": 0.02583988894030542,
      "loss": 0.0002,
      "step": 80130
    },
    {
      "epoch": 37.084683017121705,
      "grad_norm": 0.020163586363196373,
      "learning_rate": 0.025830633965756595,
      "loss": 0.0001,
      "step": 80140
    },
    {
      "epoch": 37.08931050439611,
      "grad_norm": 0.005226552486419678,
      "learning_rate": 0.025821378991207775,
      "loss": 0.0001,
      "step": 80150
    },
    {
      "epoch": 37.09393799167052,
      "grad_norm": 0.0017229168443009257,
      "learning_rate": 0.025812124016658958,
      "loss": 0.0001,
      "step": 80160
    },
    {
      "epoch": 37.098565478944934,
      "grad_norm": 0.003904199693351984,
      "learning_rate": 0.025802869042110134,
      "loss": 0.0001,
      "step": 80170
    },
    {
      "epoch": 37.103192966219346,
      "grad_norm": 0.0060668871738016605,
      "learning_rate": 0.025793614067561317,
      "loss": 0.0001,
      "step": 80180
    },
    {
      "epoch": 37.10782045349375,
      "grad_norm": 0.01504662074148655,
      "learning_rate": 0.025784359093012493,
      "loss": 0.0001,
      "step": 80190
    },
    {
      "epoch": 37.11244794076816,
      "grad_norm": 0.012050910852849483,
      "learning_rate": 0.025775104118463676,
      "loss": 0.0001,
      "step": 80200
    },
    {
      "epoch": 37.117075428042575,
      "grad_norm": 0.003363890340551734,
      "learning_rate": 0.025765849143914856,
      "loss": 0.0001,
      "step": 80210
    },
    {
      "epoch": 37.12170291531698,
      "grad_norm": 0.004689163062721491,
      "learning_rate": 0.025756594169366032,
      "loss": 0.0,
      "step": 80220
    },
    {
      "epoch": 37.12633040259139,
      "grad_norm": 0.022162863984704018,
      "learning_rate": 0.025747339194817215,
      "loss": 0.0001,
      "step": 80230
    },
    {
      "epoch": 37.130957889865805,
      "grad_norm": 0.003672859398648143,
      "learning_rate": 0.025738084220268395,
      "loss": 0.0,
      "step": 80240
    },
    {
      "epoch": 37.13558537714021,
      "grad_norm": 0.01281238254159689,
      "learning_rate": 0.02572882924571958,
      "loss": 0.0002,
      "step": 80250
    },
    {
      "epoch": 37.14021286441462,
      "grad_norm": 0.006429691333323717,
      "learning_rate": 0.025719574271170755,
      "loss": 0.0,
      "step": 80260
    },
    {
      "epoch": 37.144840351689034,
      "grad_norm": 0.001984566915780306,
      "learning_rate": 0.025710319296621938,
      "loss": 0.0001,
      "step": 80270
    },
    {
      "epoch": 37.149467838963446,
      "grad_norm": 0.014398830011487007,
      "learning_rate": 0.025701064322073114,
      "loss": 0.0001,
      "step": 80280
    },
    {
      "epoch": 37.15409532623785,
      "grad_norm": 0.014512577094137669,
      "learning_rate": 0.025691809347524297,
      "loss": 0.0001,
      "step": 80290
    },
    {
      "epoch": 37.15872281351226,
      "grad_norm": 0.004026863258332014,
      "learning_rate": 0.025682554372975477,
      "loss": 0.0,
      "step": 80300
    },
    {
      "epoch": 37.163350300786675,
      "grad_norm": 0.15068912506103516,
      "learning_rate": 0.025673299398426653,
      "loss": 0.0001,
      "step": 80310
    },
    {
      "epoch": 37.16797778806108,
      "grad_norm": 0.0014497489901259542,
      "learning_rate": 0.025664044423877836,
      "loss": 0.0001,
      "step": 80320
    },
    {
      "epoch": 37.17260527533549,
      "grad_norm": 0.002730573760345578,
      "learning_rate": 0.025654789449329016,
      "loss": 0.0,
      "step": 80330
    },
    {
      "epoch": 37.177232762609904,
      "grad_norm": 0.0013925947714596987,
      "learning_rate": 0.0256455344747802,
      "loss": 0.0001,
      "step": 80340
    },
    {
      "epoch": 37.181860249884316,
      "grad_norm": 0.03537353128194809,
      "learning_rate": 0.025636279500231375,
      "loss": 0.0001,
      "step": 80350
    },
    {
      "epoch": 37.18648773715872,
      "grad_norm": 0.0029202366713434458,
      "learning_rate": 0.025627024525682558,
      "loss": 0.0,
      "step": 80360
    },
    {
      "epoch": 37.19111522443313,
      "grad_norm": 0.001498981611803174,
      "learning_rate": 0.025617769551133734,
      "loss": 0.0004,
      "step": 80370
    },
    {
      "epoch": 37.195742711707545,
      "grad_norm": 0.0005520310951396823,
      "learning_rate": 0.025608514576584914,
      "loss": 0.0001,
      "step": 80380
    },
    {
      "epoch": 37.20037019898195,
      "grad_norm": 0.0016093833837658167,
      "learning_rate": 0.025599259602036097,
      "loss": 0.0001,
      "step": 80390
    },
    {
      "epoch": 37.20499768625636,
      "grad_norm": 0.0008994715753942728,
      "learning_rate": 0.025590004627487273,
      "loss": 0.0001,
      "step": 80400
    },
    {
      "epoch": 37.209625173530775,
      "grad_norm": 0.03572358563542366,
      "learning_rate": 0.025580749652938457,
      "loss": 0.001,
      "step": 80410
    },
    {
      "epoch": 37.21425266080518,
      "grad_norm": 0.003135831793770194,
      "learning_rate": 0.025571494678389636,
      "loss": 0.0001,
      "step": 80420
    },
    {
      "epoch": 37.21888014807959,
      "grad_norm": 0.0016567094717174768,
      "learning_rate": 0.02556223970384082,
      "loss": 0.0,
      "step": 80430
    },
    {
      "epoch": 37.223507635354004,
      "grad_norm": 0.005795058328658342,
      "learning_rate": 0.025552984729291996,
      "loss": 0.0001,
      "step": 80440
    },
    {
      "epoch": 37.228135122628416,
      "grad_norm": 0.09203279763460159,
      "learning_rate": 0.025543729754743172,
      "loss": 0.0001,
      "step": 80450
    },
    {
      "epoch": 37.23276260990282,
      "grad_norm": 0.0017334290314465761,
      "learning_rate": 0.025534474780194355,
      "loss": 0.001,
      "step": 80460
    },
    {
      "epoch": 37.23739009717723,
      "grad_norm": 0.007265191525220871,
      "learning_rate": 0.025525219805645535,
      "loss": 0.0001,
      "step": 80470
    },
    {
      "epoch": 37.242017584451645,
      "grad_norm": 0.0025071112904697657,
      "learning_rate": 0.025515964831096718,
      "loss": 0.0001,
      "step": 80480
    },
    {
      "epoch": 37.24664507172605,
      "grad_norm": 0.0029275414999574423,
      "learning_rate": 0.025506709856547894,
      "loss": 0.0001,
      "step": 80490
    },
    {
      "epoch": 37.25127255900046,
      "grad_norm": 0.005006368737667799,
      "learning_rate": 0.025497454881999077,
      "loss": 0.0001,
      "step": 80500
    },
    {
      "epoch": 37.255900046274874,
      "grad_norm": 0.002428090665489435,
      "learning_rate": 0.025488199907450257,
      "loss": 0.0,
      "step": 80510
    },
    {
      "epoch": 37.260527533549286,
      "grad_norm": 0.008127451874315739,
      "learning_rate": 0.02547894493290144,
      "loss": 0.0001,
      "step": 80520
    },
    {
      "epoch": 37.26515502082369,
      "grad_norm": 0.02665061131119728,
      "learning_rate": 0.025469689958352616,
      "loss": 0.0002,
      "step": 80530
    },
    {
      "epoch": 37.2697825080981,
      "grad_norm": 0.001474189106374979,
      "learning_rate": 0.025460434983803792,
      "loss": 0.0,
      "step": 80540
    },
    {
      "epoch": 37.274409995372515,
      "grad_norm": 0.0007144985720515251,
      "learning_rate": 0.025451180009254976,
      "loss": 0.0001,
      "step": 80550
    },
    {
      "epoch": 37.27903748264692,
      "grad_norm": 0.004342465661466122,
      "learning_rate": 0.025441925034706155,
      "loss": 0.0001,
      "step": 80560
    },
    {
      "epoch": 37.28366496992133,
      "grad_norm": 0.007740614470094442,
      "learning_rate": 0.02543267006015734,
      "loss": 0.0001,
      "step": 80570
    },
    {
      "epoch": 37.288292457195745,
      "grad_norm": 0.0073050521314144135,
      "learning_rate": 0.025423415085608515,
      "loss": 0.0001,
      "step": 80580
    },
    {
      "epoch": 37.29291994447015,
      "grad_norm": 0.018021315336227417,
      "learning_rate": 0.025414160111059698,
      "loss": 0.0001,
      "step": 80590
    },
    {
      "epoch": 37.29754743174456,
      "grad_norm": 0.0024014399386942387,
      "learning_rate": 0.025404905136510877,
      "loss": 0.0001,
      "step": 80600
    },
    {
      "epoch": 37.302174919018974,
      "grad_norm": 0.009288007393479347,
      "learning_rate": 0.025395650161962054,
      "loss": 0.0,
      "step": 80610
    },
    {
      "epoch": 37.306802406293386,
      "grad_norm": 0.002997915493324399,
      "learning_rate": 0.025386395187413237,
      "loss": 0.0001,
      "step": 80620
    },
    {
      "epoch": 37.31142989356779,
      "grad_norm": 0.002756744623184204,
      "learning_rate": 0.025377140212864413,
      "loss": 0.0014,
      "step": 80630
    },
    {
      "epoch": 37.3160573808422,
      "grad_norm": 0.004439203534275293,
      "learning_rate": 0.025367885238315596,
      "loss": 0.0001,
      "step": 80640
    },
    {
      "epoch": 37.320684868116615,
      "grad_norm": 0.22094421088695526,
      "learning_rate": 0.025358630263766776,
      "loss": 0.0001,
      "step": 80650
    },
    {
      "epoch": 37.32531235539102,
      "grad_norm": 0.0007437268504872918,
      "learning_rate": 0.02534937528921796,
      "loss": 0.0001,
      "step": 80660
    },
    {
      "epoch": 37.32993984266543,
      "grad_norm": 0.010317090898752213,
      "learning_rate": 0.025340120314669135,
      "loss": 0.0001,
      "step": 80670
    },
    {
      "epoch": 37.334567329939844,
      "grad_norm": 0.010324590839445591,
      "learning_rate": 0.025330865340120315,
      "loss": 0.0001,
      "step": 80680
    },
    {
      "epoch": 37.33919481721425,
      "grad_norm": 0.12418132275342941,
      "learning_rate": 0.025321610365571498,
      "loss": 0.0001,
      "step": 80690
    },
    {
      "epoch": 37.34382230448866,
      "grad_norm": 0.0032307649962604046,
      "learning_rate": 0.025312355391022674,
      "loss": 0.0001,
      "step": 80700
    },
    {
      "epoch": 37.34844979176307,
      "grad_norm": 0.002668160479515791,
      "learning_rate": 0.025303100416473857,
      "loss": 0.0001,
      "step": 80710
    },
    {
      "epoch": 37.353077279037485,
      "grad_norm": 0.016340089961886406,
      "learning_rate": 0.025293845441925034,
      "loss": 0.0,
      "step": 80720
    },
    {
      "epoch": 37.35770476631189,
      "grad_norm": 0.011020041070878506,
      "learning_rate": 0.025284590467376217,
      "loss": 0.0001,
      "step": 80730
    },
    {
      "epoch": 37.3623322535863,
      "grad_norm": 0.013566730543971062,
      "learning_rate": 0.025275335492827396,
      "loss": 0.0,
      "step": 80740
    },
    {
      "epoch": 37.366959740860715,
      "grad_norm": 0.010393050499260426,
      "learning_rate": 0.02526608051827858,
      "loss": 0.0004,
      "step": 80750
    },
    {
      "epoch": 37.37158722813512,
      "grad_norm": 0.0034473813138902187,
      "learning_rate": 0.025256825543729756,
      "loss": 0.0001,
      "step": 80760
    },
    {
      "epoch": 37.37621471540953,
      "grad_norm": 0.013965110294520855,
      "learning_rate": 0.025247570569180935,
      "loss": 0.001,
      "step": 80770
    },
    {
      "epoch": 37.380842202683944,
      "grad_norm": 0.0032805101945996284,
      "learning_rate": 0.02523831559463212,
      "loss": 0.0001,
      "step": 80780
    },
    {
      "epoch": 37.385469689958356,
      "grad_norm": 0.002959303092211485,
      "learning_rate": 0.025229060620083295,
      "loss": 0.0001,
      "step": 80790
    },
    {
      "epoch": 37.39009717723276,
      "grad_norm": 0.017988698557019234,
      "learning_rate": 0.025219805645534478,
      "loss": 0.0001,
      "step": 80800
    },
    {
      "epoch": 37.39472466450717,
      "grad_norm": 0.006597320083528757,
      "learning_rate": 0.025210550670985654,
      "loss": 0.0003,
      "step": 80810
    },
    {
      "epoch": 37.399352151781585,
      "grad_norm": 0.003920609597116709,
      "learning_rate": 0.025201295696436837,
      "loss": 0.0001,
      "step": 80820
    },
    {
      "epoch": 37.40397963905599,
      "grad_norm": 0.007149958983063698,
      "learning_rate": 0.025192040721888017,
      "loss": 0.0001,
      "step": 80830
    },
    {
      "epoch": 37.4086071263304,
      "grad_norm": 0.019731031730771065,
      "learning_rate": 0.025182785747339193,
      "loss": 0.0001,
      "step": 80840
    },
    {
      "epoch": 37.413234613604814,
      "grad_norm": 0.0029166419990360737,
      "learning_rate": 0.025173530772790376,
      "loss": 0.0001,
      "step": 80850
    },
    {
      "epoch": 37.41786210087922,
      "grad_norm": 0.005367173347622156,
      "learning_rate": 0.025164275798241556,
      "loss": 0.0001,
      "step": 80860
    },
    {
      "epoch": 37.42248958815363,
      "grad_norm": 0.0016930046258494258,
      "learning_rate": 0.02515502082369274,
      "loss": 0.0,
      "step": 80870
    },
    {
      "epoch": 37.42711707542804,
      "grad_norm": 0.013481377623975277,
      "learning_rate": 0.025145765849143915,
      "loss": 0.0001,
      "step": 80880
    },
    {
      "epoch": 37.431744562702455,
      "grad_norm": 0.00202691787853837,
      "learning_rate": 0.0251365108745951,
      "loss": 0.0141,
      "step": 80890
    },
    {
      "epoch": 37.43637204997686,
      "grad_norm": 0.021775072440505028,
      "learning_rate": 0.025127255900046275,
      "loss": 0.0002,
      "step": 80900
    },
    {
      "epoch": 37.44099953725127,
      "grad_norm": 0.00994476955384016,
      "learning_rate": 0.025118000925497454,
      "loss": 0.0003,
      "step": 80910
    },
    {
      "epoch": 37.445627024525685,
      "grad_norm": 0.020258067175745964,
      "learning_rate": 0.025108745950948638,
      "loss": 0.0001,
      "step": 80920
    },
    {
      "epoch": 37.45025451180009,
      "grad_norm": 0.017907138913869858,
      "learning_rate": 0.025099490976399814,
      "loss": 0.0002,
      "step": 80930
    },
    {
      "epoch": 37.4548819990745,
      "grad_norm": 0.004977477248758078,
      "learning_rate": 0.025090236001850997,
      "loss": 0.0001,
      "step": 80940
    },
    {
      "epoch": 37.459509486348914,
      "grad_norm": 0.00248957728035748,
      "learning_rate": 0.025080981027302177,
      "loss": 0.0001,
      "step": 80950
    },
    {
      "epoch": 37.464136973623326,
      "grad_norm": 0.06787999719381332,
      "learning_rate": 0.02507172605275336,
      "loss": 0.0003,
      "step": 80960
    },
    {
      "epoch": 37.46876446089773,
      "grad_norm": 0.0023753836285322905,
      "learning_rate": 0.025062471078204536,
      "loss": 0.0001,
      "step": 80970
    },
    {
      "epoch": 37.47339194817214,
      "grad_norm": 0.004583287052810192,
      "learning_rate": 0.02505321610365572,
      "loss": 0.0002,
      "step": 80980
    },
    {
      "epoch": 37.478019435446555,
      "grad_norm": 0.0029752692207694054,
      "learning_rate": 0.025043961129106895,
      "loss": 0.0001,
      "step": 80990
    },
    {
      "epoch": 37.48264692272096,
      "grad_norm": 0.11450104415416718,
      "learning_rate": 0.025034706154558075,
      "loss": 0.0002,
      "step": 81000
    },
    {
      "epoch": 37.48727440999537,
      "grad_norm": 0.006922400090843439,
      "learning_rate": 0.025025451180009258,
      "loss": 0.0,
      "step": 81010
    },
    {
      "epoch": 37.491901897269784,
      "grad_norm": 0.007803793530911207,
      "learning_rate": 0.025016196205460434,
      "loss": 0.0003,
      "step": 81020
    },
    {
      "epoch": 37.49652938454419,
      "grad_norm": 0.0038203552830964327,
      "learning_rate": 0.025006941230911617,
      "loss": 0.0001,
      "step": 81030
    },
    {
      "epoch": 37.5011568718186,
      "grad_norm": 0.005400485824793577,
      "learning_rate": 0.024997686256362797,
      "loss": 0.0003,
      "step": 81040
    },
    {
      "epoch": 37.50578435909301,
      "grad_norm": 0.0107492133975029,
      "learning_rate": 0.024988431281813977,
      "loss": 0.0001,
      "step": 81050
    },
    {
      "epoch": 37.510411846367425,
      "grad_norm": 0.00473565049469471,
      "learning_rate": 0.024979176307265157,
      "loss": 0.0001,
      "step": 81060
    },
    {
      "epoch": 37.51503933364183,
      "grad_norm": 0.001976113533601165,
      "learning_rate": 0.024969921332716336,
      "loss": 0.0001,
      "step": 81070
    },
    {
      "epoch": 37.51966682091624,
      "grad_norm": 0.004017690662294626,
      "learning_rate": 0.024960666358167516,
      "loss": 0.0001,
      "step": 81080
    },
    {
      "epoch": 37.524294308190655,
      "grad_norm": 0.012167396955192089,
      "learning_rate": 0.0249514113836187,
      "loss": 0.0001,
      "step": 81090
    },
    {
      "epoch": 37.52892179546506,
      "grad_norm": 0.0025837994180619717,
      "learning_rate": 0.024942156409069875,
      "loss": 0.0003,
      "step": 81100
    },
    {
      "epoch": 37.53354928273947,
      "grad_norm": 0.00611762935295701,
      "learning_rate": 0.024932901434521055,
      "loss": 0.0001,
      "step": 81110
    },
    {
      "epoch": 37.538176770013884,
      "grad_norm": 0.12446986138820648,
      "learning_rate": 0.024923646459972235,
      "loss": 0.0002,
      "step": 81120
    },
    {
      "epoch": 37.542804257288296,
      "grad_norm": 0.006125530693680048,
      "learning_rate": 0.024914391485423418,
      "loss": 0.0,
      "step": 81130
    },
    {
      "epoch": 37.5474317445627,
      "grad_norm": 0.0033668559044599533,
      "learning_rate": 0.024905136510874597,
      "loss": 0.0001,
      "step": 81140
    },
    {
      "epoch": 37.55205923183711,
      "grad_norm": 0.0052613322623074055,
      "learning_rate": 0.024895881536325777,
      "loss": 0.0001,
      "step": 81150
    },
    {
      "epoch": 37.556686719111525,
      "grad_norm": 0.0007305294857360423,
      "learning_rate": 0.024886626561776957,
      "loss": 0.0001,
      "step": 81160
    },
    {
      "epoch": 37.56131420638593,
      "grad_norm": 0.012929743155837059,
      "learning_rate": 0.024877371587228136,
      "loss": 0.0002,
      "step": 81170
    },
    {
      "epoch": 37.56594169366034,
      "grad_norm": 0.009798899292945862,
      "learning_rate": 0.024868116612679316,
      "loss": 0.0007,
      "step": 81180
    },
    {
      "epoch": 37.570569180934754,
      "grad_norm": 0.03159123659133911,
      "learning_rate": 0.024858861638130496,
      "loss": 0.0003,
      "step": 81190
    },
    {
      "epoch": 37.57519666820916,
      "grad_norm": 0.0018819902325049043,
      "learning_rate": 0.024849606663581675,
      "loss": 0.0037,
      "step": 81200
    },
    {
      "epoch": 37.57982415548357,
      "grad_norm": 0.012046799063682556,
      "learning_rate": 0.024840351689032855,
      "loss": 0.0001,
      "step": 81210
    },
    {
      "epoch": 37.58445164275798,
      "grad_norm": 0.002088713925331831,
      "learning_rate": 0.02483109671448404,
      "loss": 0.0,
      "step": 81220
    },
    {
      "epoch": 37.589079130032395,
      "grad_norm": 0.005223682150244713,
      "learning_rate": 0.024821841739935218,
      "loss": 0.0001,
      "step": 81230
    },
    {
      "epoch": 37.5937066173068,
      "grad_norm": 0.02443843148648739,
      "learning_rate": 0.024812586765386398,
      "loss": 0.0004,
      "step": 81240
    },
    {
      "epoch": 37.59833410458121,
      "grad_norm": 0.010771053843200207,
      "learning_rate": 0.024803331790837574,
      "loss": 0.0002,
      "step": 81250
    },
    {
      "epoch": 37.602961591855625,
      "grad_norm": 0.011114885099232197,
      "learning_rate": 0.024794076816288757,
      "loss": 0.0001,
      "step": 81260
    },
    {
      "epoch": 37.60758907913003,
      "grad_norm": 0.009172791615128517,
      "learning_rate": 0.024784821841739937,
      "loss": 0.0003,
      "step": 81270
    },
    {
      "epoch": 37.61221656640444,
      "grad_norm": 0.01771603897213936,
      "learning_rate": 0.024775566867191116,
      "loss": 0.0002,
      "step": 81280
    },
    {
      "epoch": 37.616844053678854,
      "grad_norm": 0.013987842947244644,
      "learning_rate": 0.024766311892642296,
      "loss": 0.0001,
      "step": 81290
    },
    {
      "epoch": 37.621471540953266,
      "grad_norm": 0.005285348743200302,
      "learning_rate": 0.024757056918093476,
      "loss": 0.0001,
      "step": 81300
    },
    {
      "epoch": 37.62609902822767,
      "grad_norm": 0.007602540776133537,
      "learning_rate": 0.02474780194354466,
      "loss": 0.0001,
      "step": 81310
    },
    {
      "epoch": 37.63072651550208,
      "grad_norm": 0.009543520398437977,
      "learning_rate": 0.02473854696899584,
      "loss": 0.0001,
      "step": 81320
    },
    {
      "epoch": 37.635354002776495,
      "grad_norm": 0.03090573661029339,
      "learning_rate": 0.024729291994447015,
      "loss": 0.0002,
      "step": 81330
    },
    {
      "epoch": 37.6399814900509,
      "grad_norm": 0.004499133210629225,
      "learning_rate": 0.024720037019898194,
      "loss": 0.0002,
      "step": 81340
    },
    {
      "epoch": 37.64460897732531,
      "grad_norm": 0.002336678793653846,
      "learning_rate": 0.024710782045349378,
      "loss": 0.0,
      "step": 81350
    },
    {
      "epoch": 37.649236464599724,
      "grad_norm": 0.012854788452386856,
      "learning_rate": 0.024701527070800557,
      "loss": 0.0001,
      "step": 81360
    },
    {
      "epoch": 37.65386395187413,
      "grad_norm": 0.0018175080185756087,
      "learning_rate": 0.024692272096251737,
      "loss": 0.0,
      "step": 81370
    },
    {
      "epoch": 37.65849143914854,
      "grad_norm": 0.006954881362617016,
      "learning_rate": 0.024683017121702917,
      "loss": 0.0001,
      "step": 81380
    },
    {
      "epoch": 37.66311892642295,
      "grad_norm": 0.008576937019824982,
      "learning_rate": 0.024673762147154096,
      "loss": 0.0001,
      "step": 81390
    },
    {
      "epoch": 37.667746413697365,
      "grad_norm": 0.0016827202634885907,
      "learning_rate": 0.02466450717260528,
      "loss": 0.0002,
      "step": 81400
    },
    {
      "epoch": 37.67237390097177,
      "grad_norm": 0.018026502802968025,
      "learning_rate": 0.024655252198056456,
      "loss": 0.0001,
      "step": 81410
    },
    {
      "epoch": 37.67700138824618,
      "grad_norm": 0.006046209018677473,
      "learning_rate": 0.024645997223507635,
      "loss": 0.0005,
      "step": 81420
    },
    {
      "epoch": 37.681628875520595,
      "grad_norm": 0.2810676693916321,
      "learning_rate": 0.024636742248958815,
      "loss": 0.0002,
      "step": 81430
    },
    {
      "epoch": 37.686256362795,
      "grad_norm": 0.019101133570075035,
      "learning_rate": 0.024627487274409998,
      "loss": 0.0001,
      "step": 81440
    },
    {
      "epoch": 37.69088385006941,
      "grad_norm": 0.010076393373310566,
      "learning_rate": 0.024618232299861178,
      "loss": 0.0001,
      "step": 81450
    },
    {
      "epoch": 37.695511337343824,
      "grad_norm": 0.0636892169713974,
      "learning_rate": 0.024608977325312358,
      "loss": 0.0001,
      "step": 81460
    },
    {
      "epoch": 37.700138824618236,
      "grad_norm": 0.0014020565431565046,
      "learning_rate": 0.024599722350763537,
      "loss": 0.0004,
      "step": 81470
    },
    {
      "epoch": 37.70476631189264,
      "grad_norm": 0.04510853812098503,
      "learning_rate": 0.024590467376214717,
      "loss": 0.0002,
      "step": 81480
    },
    {
      "epoch": 37.70939379916705,
      "grad_norm": 0.008166797459125519,
      "learning_rate": 0.024581212401665897,
      "loss": 0.0001,
      "step": 81490
    },
    {
      "epoch": 37.714021286441465,
      "grad_norm": 0.004726657178252935,
      "learning_rate": 0.024571957427117076,
      "loss": 0.0001,
      "step": 81500
    },
    {
      "epoch": 37.71864877371587,
      "grad_norm": 0.006285807117819786,
      "learning_rate": 0.024562702452568256,
      "loss": 0.0002,
      "step": 81510
    },
    {
      "epoch": 37.72327626099028,
      "grad_norm": 0.026646586135029793,
      "learning_rate": 0.024553447478019436,
      "loss": 0.0001,
      "step": 81520
    },
    {
      "epoch": 37.727903748264694,
      "grad_norm": 0.001805803389288485,
      "learning_rate": 0.02454419250347062,
      "loss": 0.0001,
      "step": 81530
    },
    {
      "epoch": 37.7325312355391,
      "grad_norm": 0.020676545798778534,
      "learning_rate": 0.0245349375289218,
      "loss": 0.0001,
      "step": 81540
    },
    {
      "epoch": 37.73715872281351,
      "grad_norm": 0.002394972601905465,
      "learning_rate": 0.024525682554372978,
      "loss": 0.0001,
      "step": 81550
    },
    {
      "epoch": 37.74178621008792,
      "grad_norm": 0.0018265389371663332,
      "learning_rate": 0.024516427579824154,
      "loss": 0.0002,
      "step": 81560
    },
    {
      "epoch": 37.746413697362335,
      "grad_norm": 0.006457414012402296,
      "learning_rate": 0.024507172605275337,
      "loss": 0.0003,
      "step": 81570
    },
    {
      "epoch": 37.75104118463674,
      "grad_norm": 0.008130342699587345,
      "learning_rate": 0.024497917630726517,
      "loss": 0.0,
      "step": 81580
    },
    {
      "epoch": 37.75566867191115,
      "grad_norm": 0.0023558710236102343,
      "learning_rate": 0.024488662656177697,
      "loss": 0.0,
      "step": 81590
    },
    {
      "epoch": 37.760296159185565,
      "grad_norm": 0.022677818313241005,
      "learning_rate": 0.024479407681628877,
      "loss": 0.0001,
      "step": 81600
    },
    {
      "epoch": 37.76492364645997,
      "grad_norm": 0.00196168664842844,
      "learning_rate": 0.024470152707080056,
      "loss": 0.0001,
      "step": 81610
    },
    {
      "epoch": 37.76955113373438,
      "grad_norm": 0.01474617887288332,
      "learning_rate": 0.02446089773253124,
      "loss": 0.0,
      "step": 81620
    },
    {
      "epoch": 37.774178621008794,
      "grad_norm": 0.006592386402189732,
      "learning_rate": 0.02445164275798242,
      "loss": 0.0,
      "step": 81630
    },
    {
      "epoch": 37.7788061082832,
      "grad_norm": 0.018947377800941467,
      "learning_rate": 0.024442387783433595,
      "loss": 0.0001,
      "step": 81640
    },
    {
      "epoch": 37.78343359555761,
      "grad_norm": 0.001844138139858842,
      "learning_rate": 0.024433132808884775,
      "loss": 0.0001,
      "step": 81650
    },
    {
      "epoch": 37.78806108283202,
      "grad_norm": 0.03279761224985123,
      "learning_rate": 0.024423877834335958,
      "loss": 0.0001,
      "step": 81660
    },
    {
      "epoch": 37.792688570106435,
      "grad_norm": 0.014016126282513142,
      "learning_rate": 0.024414622859787138,
      "loss": 0.0001,
      "step": 81670
    },
    {
      "epoch": 37.79731605738084,
      "grad_norm": 0.007813720032572746,
      "learning_rate": 0.024405367885238317,
      "loss": 0.0001,
      "step": 81680
    },
    {
      "epoch": 37.80194354465525,
      "grad_norm": 0.004134378395974636,
      "learning_rate": 0.024396112910689497,
      "loss": 0.0001,
      "step": 81690
    },
    {
      "epoch": 37.806571031929664,
      "grad_norm": 0.0007421193877235055,
      "learning_rate": 0.024386857936140677,
      "loss": 0.0001,
      "step": 81700
    },
    {
      "epoch": 37.81119851920407,
      "grad_norm": 0.005375994835048914,
      "learning_rate": 0.024377602961591856,
      "loss": 0.0001,
      "step": 81710
    },
    {
      "epoch": 37.81582600647848,
      "grad_norm": 0.003602313809096813,
      "learning_rate": 0.024368347987043036,
      "loss": 0.0001,
      "step": 81720
    },
    {
      "epoch": 37.82045349375289,
      "grad_norm": 0.00996117852628231,
      "learning_rate": 0.024359093012494216,
      "loss": 0.0002,
      "step": 81730
    },
    {
      "epoch": 37.825080981027305,
      "grad_norm": 0.007052468601614237,
      "learning_rate": 0.024349838037945395,
      "loss": 0.0001,
      "step": 81740
    },
    {
      "epoch": 37.82970846830171,
      "grad_norm": 0.016799084842205048,
      "learning_rate": 0.02434058306339658,
      "loss": 0.0001,
      "step": 81750
    },
    {
      "epoch": 37.83433595557612,
      "grad_norm": 0.005991910584270954,
      "learning_rate": 0.02433132808884776,
      "loss": 0.0,
      "step": 81760
    },
    {
      "epoch": 37.838963442850535,
      "grad_norm": 0.0038228309713304043,
      "learning_rate": 0.024322073114298938,
      "loss": 0.0001,
      "step": 81770
    },
    {
      "epoch": 37.84359093012494,
      "grad_norm": 0.003915122244507074,
      "learning_rate": 0.024312818139750118,
      "loss": 0.0001,
      "step": 81780
    },
    {
      "epoch": 37.84821841739935,
      "grad_norm": 0.019671764224767685,
      "learning_rate": 0.024303563165201297,
      "loss": 0.0001,
      "step": 81790
    },
    {
      "epoch": 37.852845904673764,
      "grad_norm": 0.003070937003940344,
      "learning_rate": 0.024294308190652477,
      "loss": 0.0,
      "step": 81800
    },
    {
      "epoch": 37.85747339194817,
      "grad_norm": 0.006876680999994278,
      "learning_rate": 0.024285053216103657,
      "loss": 0.0069,
      "step": 81810
    },
    {
      "epoch": 37.86210087922258,
      "grad_norm": 0.005113841500133276,
      "learning_rate": 0.024275798241554836,
      "loss": 0.0001,
      "step": 81820
    },
    {
      "epoch": 37.86672836649699,
      "grad_norm": 0.05699609965085983,
      "learning_rate": 0.024266543267006016,
      "loss": 0.0002,
      "step": 81830
    },
    {
      "epoch": 37.871355853771405,
      "grad_norm": 0.0009251843439415097,
      "learning_rate": 0.0242572882924572,
      "loss": 0.0001,
      "step": 81840
    },
    {
      "epoch": 37.87598334104581,
      "grad_norm": 0.007018295582383871,
      "learning_rate": 0.02424803331790838,
      "loss": 0.0001,
      "step": 81850
    },
    {
      "epoch": 37.88061082832022,
      "grad_norm": 0.0041245813481509686,
      "learning_rate": 0.024238778343359555,
      "loss": 0.0001,
      "step": 81860
    },
    {
      "epoch": 37.885238315594634,
      "grad_norm": 0.012609030120074749,
      "learning_rate": 0.024229523368810735,
      "loss": 0.0,
      "step": 81870
    },
    {
      "epoch": 37.88986580286904,
      "grad_norm": 0.0033283766824752092,
      "learning_rate": 0.024220268394261918,
      "loss": 0.0001,
      "step": 81880
    },
    {
      "epoch": 37.89449329014345,
      "grad_norm": 0.003649349557235837,
      "learning_rate": 0.024211013419713098,
      "loss": 0.0001,
      "step": 81890
    },
    {
      "epoch": 37.89912077741786,
      "grad_norm": 0.0015335996868088841,
      "learning_rate": 0.024201758445164277,
      "loss": 0.0001,
      "step": 81900
    },
    {
      "epoch": 37.903748264692275,
      "grad_norm": 0.15261593461036682,
      "learning_rate": 0.024192503470615457,
      "loss": 0.0001,
      "step": 81910
    },
    {
      "epoch": 37.90837575196668,
      "grad_norm": 0.014781808480620384,
      "learning_rate": 0.024183248496066637,
      "loss": 0.0001,
      "step": 81920
    },
    {
      "epoch": 37.91300323924109,
      "grad_norm": 0.021817896515130997,
      "learning_rate": 0.02417399352151782,
      "loss": 0.0001,
      "step": 81930
    },
    {
      "epoch": 37.917630726515505,
      "grad_norm": 0.011389823630452156,
      "learning_rate": 0.024164738546968996,
      "loss": 0.0001,
      "step": 81940
    },
    {
      "epoch": 37.92225821378991,
      "grad_norm": 0.020420679822564125,
      "learning_rate": 0.024155483572420176,
      "loss": 0.0001,
      "step": 81950
    },
    {
      "epoch": 37.92688570106432,
      "grad_norm": 0.016898877918720245,
      "learning_rate": 0.024146228597871355,
      "loss": 0.0002,
      "step": 81960
    },
    {
      "epoch": 37.931513188338734,
      "grad_norm": 0.0017563566798344254,
      "learning_rate": 0.02413697362332254,
      "loss": 0.0001,
      "step": 81970
    },
    {
      "epoch": 37.93614067561314,
      "grad_norm": 0.00961297657340765,
      "learning_rate": 0.024127718648773718,
      "loss": 0.0002,
      "step": 81980
    },
    {
      "epoch": 37.94076816288755,
      "grad_norm": 0.010889469645917416,
      "learning_rate": 0.024118463674224898,
      "loss": 0.0,
      "step": 81990
    },
    {
      "epoch": 37.94539565016196,
      "grad_norm": 0.0026025702245533466,
      "learning_rate": 0.024109208699676078,
      "loss": 0.0001,
      "step": 82000
    },
    {
      "epoch": 37.950023137436375,
      "grad_norm": 0.003874991787597537,
      "learning_rate": 0.024099953725127257,
      "loss": 0.0001,
      "step": 82010
    },
    {
      "epoch": 37.95465062471078,
      "grad_norm": 0.04408927634358406,
      "learning_rate": 0.024090698750578437,
      "loss": 0.0,
      "step": 82020
    },
    {
      "epoch": 37.95927811198519,
      "grad_norm": 0.0021796468645334244,
      "learning_rate": 0.024081443776029617,
      "loss": 0.0001,
      "step": 82030
    },
    {
      "epoch": 37.963905599259604,
      "grad_norm": 0.013150121085345745,
      "learning_rate": 0.024072188801480796,
      "loss": 0.0,
      "step": 82040
    },
    {
      "epoch": 37.96853308653401,
      "grad_norm": 0.0030494537204504013,
      "learning_rate": 0.024062933826931976,
      "loss": 0.0,
      "step": 82050
    },
    {
      "epoch": 37.97316057380842,
      "grad_norm": 0.005727933254092932,
      "learning_rate": 0.02405367885238316,
      "loss": 0.0,
      "step": 82060
    },
    {
      "epoch": 37.97778806108283,
      "grad_norm": 0.005240374710410833,
      "learning_rate": 0.02404442387783434,
      "loss": 0.0002,
      "step": 82070
    },
    {
      "epoch": 37.982415548357245,
      "grad_norm": 0.009327194653451443,
      "learning_rate": 0.02403516890328552,
      "loss": 0.0001,
      "step": 82080
    },
    {
      "epoch": 37.98704303563165,
      "grad_norm": 0.0033948488999158144,
      "learning_rate": 0.024025913928736695,
      "loss": 0.0001,
      "step": 82090
    },
    {
      "epoch": 37.99167052290606,
      "grad_norm": 0.03393639251589775,
      "learning_rate": 0.024016658954187878,
      "loss": 0.0001,
      "step": 82100
    },
    {
      "epoch": 37.996298010180475,
      "grad_norm": 0.016965553164482117,
      "learning_rate": 0.024007403979639057,
      "loss": 0.0001,
      "step": 82110
    },
    {
      "epoch": 38.0,
      "eval_accuracy_branch1": 0.9903712833153597,
      "eval_accuracy_branch2": 0.5006932676012941,
      "eval_f1_branch1": 0.9917048931687863,
      "eval_f1_branch2": 0.5006681902491433,
      "eval_loss": 0.017640309408307076,
      "eval_precision_branch1": 0.9917783346001321,
      "eval_precision_branch2": 0.5006934068979192,
      "eval_recall_branch1": 0.991778213784645,
      "eval_recall_branch2": 0.5006932676012941,
      "eval_runtime": 29.101,
      "eval_samples_per_second": 446.102,
      "eval_steps_per_second": 55.771,
      "step": 82118
    },
    {
      "epoch": 38.00092549745488,
      "grad_norm": 0.004358832258731127,
      "learning_rate": 0.023998149005090237,
      "loss": 0.2082,
      "step": 82120
    },
    {
      "epoch": 38.00555298472929,
      "grad_norm": 0.017862681299448013,
      "learning_rate": 0.023988894030541417,
      "loss": 0.0001,
      "step": 82130
    },
    {
      "epoch": 38.010180472003704,
      "grad_norm": 0.0031486554071307182,
      "learning_rate": 0.023979639055992596,
      "loss": 0.0001,
      "step": 82140
    },
    {
      "epoch": 38.01480795927811,
      "grad_norm": 0.016318755224347115,
      "learning_rate": 0.02397038408144378,
      "loss": 0.0001,
      "step": 82150
    },
    {
      "epoch": 38.01943544655252,
      "grad_norm": 0.19298405945301056,
      "learning_rate": 0.02396112910689496,
      "loss": 0.0001,
      "step": 82160
    },
    {
      "epoch": 38.02406293382693,
      "grad_norm": 0.031104115769267082,
      "learning_rate": 0.023951874132346136,
      "loss": 0.0001,
      "step": 82170
    },
    {
      "epoch": 38.028690421101345,
      "grad_norm": 0.002646352630108595,
      "learning_rate": 0.023942619157797315,
      "loss": 0.0,
      "step": 82180
    },
    {
      "epoch": 38.03331790837575,
      "grad_norm": 0.0024297707714140415,
      "learning_rate": 0.0239333641832485,
      "loss": 0.0002,
      "step": 82190
    },
    {
      "epoch": 38.03794539565016,
      "grad_norm": 0.006296159699559212,
      "learning_rate": 0.023924109208699678,
      "loss": 0.0002,
      "step": 82200
    },
    {
      "epoch": 38.042572882924574,
      "grad_norm": 0.009079608134925365,
      "learning_rate": 0.023914854234150858,
      "loss": 0.0001,
      "step": 82210
    },
    {
      "epoch": 38.04720037019898,
      "grad_norm": 0.0037662952672690153,
      "learning_rate": 0.023905599259602037,
      "loss": 0.0001,
      "step": 82220
    },
    {
      "epoch": 38.05182785747339,
      "grad_norm": 0.0026057397481054068,
      "learning_rate": 0.023896344285053217,
      "loss": 0.0001,
      "step": 82230
    },
    {
      "epoch": 38.0564553447478,
      "grad_norm": 0.0032548748422414064,
      "learning_rate": 0.0238870893105044,
      "loss": 0.0001,
      "step": 82240
    },
    {
      "epoch": 38.061082832022215,
      "grad_norm": 0.004601225256919861,
      "learning_rate": 0.023877834335955576,
      "loss": 0.0011,
      "step": 82250
    },
    {
      "epoch": 38.06571031929662,
      "grad_norm": 0.040035977959632874,
      "learning_rate": 0.023868579361406756,
      "loss": 0.0001,
      "step": 82260
    },
    {
      "epoch": 38.07033780657103,
      "grad_norm": 0.0027653176803141832,
      "learning_rate": 0.023859324386857936,
      "loss": 0.0002,
      "step": 82270
    },
    {
      "epoch": 38.074965293845445,
      "grad_norm": 0.03184890002012253,
      "learning_rate": 0.02385006941230912,
      "loss": 0.0001,
      "step": 82280
    },
    {
      "epoch": 38.07959278111985,
      "grad_norm": 0.0011067354353144765,
      "learning_rate": 0.0238408144377603,
      "loss": 0.0001,
      "step": 82290
    },
    {
      "epoch": 38.08422026839426,
      "grad_norm": 0.0022053318098187447,
      "learning_rate": 0.02383155946321148,
      "loss": 0.0002,
      "step": 82300
    },
    {
      "epoch": 38.088847755668674,
      "grad_norm": 0.5962186455726624,
      "learning_rate": 0.023822304488662658,
      "loss": 0.0003,
      "step": 82310
    },
    {
      "epoch": 38.09347524294308,
      "grad_norm": 0.2897733449935913,
      "learning_rate": 0.023813049514113838,
      "loss": 0.0003,
      "step": 82320
    },
    {
      "epoch": 38.09810273021749,
      "grad_norm": 0.0022817878052592278,
      "learning_rate": 0.023803794539565017,
      "loss": 0.0001,
      "step": 82330
    },
    {
      "epoch": 38.1027302174919,
      "grad_norm": 0.01332042645663023,
      "learning_rate": 0.023794539565016197,
      "loss": 0.0001,
      "step": 82340
    },
    {
      "epoch": 38.107357704766315,
      "grad_norm": 0.0029667485505342484,
      "learning_rate": 0.023785284590467377,
      "loss": 0.0002,
      "step": 82350
    },
    {
      "epoch": 38.11198519204072,
      "grad_norm": 0.003857692703604698,
      "learning_rate": 0.023776029615918556,
      "loss": 0.0002,
      "step": 82360
    },
    {
      "epoch": 38.11661267931513,
      "grad_norm": 0.026883555576205254,
      "learning_rate": 0.02376677464136974,
      "loss": 0.0001,
      "step": 82370
    },
    {
      "epoch": 38.121240166589544,
      "grad_norm": 0.008930058218538761,
      "learning_rate": 0.02375751966682092,
      "loss": 0.0001,
      "step": 82380
    },
    {
      "epoch": 38.12586765386395,
      "grad_norm": 0.0028732146602123976,
      "learning_rate": 0.0237482646922721,
      "loss": 0.0,
      "step": 82390
    },
    {
      "epoch": 38.13049514113836,
      "grad_norm": 0.007890690118074417,
      "learning_rate": 0.023739009717723275,
      "loss": 0.0003,
      "step": 82400
    },
    {
      "epoch": 38.13512262841277,
      "grad_norm": 0.02606741338968277,
      "learning_rate": 0.023729754743174458,
      "loss": 0.0001,
      "step": 82410
    },
    {
      "epoch": 38.13975011568718,
      "grad_norm": 0.002736163791269064,
      "learning_rate": 0.023720499768625638,
      "loss": 0.0,
      "step": 82420
    },
    {
      "epoch": 38.14437760296159,
      "grad_norm": 0.0077886939980089664,
      "learning_rate": 0.023711244794076818,
      "loss": 0.0,
      "step": 82430
    },
    {
      "epoch": 38.149005090236,
      "grad_norm": 0.002230857964605093,
      "learning_rate": 0.023701989819527997,
      "loss": 0.0001,
      "step": 82440
    },
    {
      "epoch": 38.153632577510415,
      "grad_norm": 0.012804446741938591,
      "learning_rate": 0.023692734844979177,
      "loss": 0.0001,
      "step": 82450
    },
    {
      "epoch": 38.15826006478482,
      "grad_norm": 0.0007232504431158304,
      "learning_rate": 0.02368347987043036,
      "loss": 0.0002,
      "step": 82460
    },
    {
      "epoch": 38.16288755205923,
      "grad_norm": 0.002619402715936303,
      "learning_rate": 0.02367422489588154,
      "loss": 0.0001,
      "step": 82470
    },
    {
      "epoch": 38.167515039333644,
      "grad_norm": 0.02258034609258175,
      "learning_rate": 0.023664969921332716,
      "loss": 0.0001,
      "step": 82480
    },
    {
      "epoch": 38.17214252660805,
      "grad_norm": 0.0022389458026736975,
      "learning_rate": 0.023655714946783896,
      "loss": 0.0001,
      "step": 82490
    },
    {
      "epoch": 38.17677001388246,
      "grad_norm": 0.0018197739263996482,
      "learning_rate": 0.02364645997223508,
      "loss": 0.0001,
      "step": 82500
    },
    {
      "epoch": 38.18139750115687,
      "grad_norm": 0.0018368407618254423,
      "learning_rate": 0.02363720499768626,
      "loss": 0.0001,
      "step": 82510
    },
    {
      "epoch": 38.186024988431285,
      "grad_norm": 0.006778898648917675,
      "learning_rate": 0.023627950023137438,
      "loss": 0.0001,
      "step": 82520
    },
    {
      "epoch": 38.19065247570569,
      "grad_norm": 0.02357528917491436,
      "learning_rate": 0.023618695048588618,
      "loss": 0.0001,
      "step": 82530
    },
    {
      "epoch": 38.1952799629801,
      "grad_norm": 0.02366253174841404,
      "learning_rate": 0.023609440074039797,
      "loss": 0.0002,
      "step": 82540
    },
    {
      "epoch": 38.199907450254514,
      "grad_norm": 0.003010099520906806,
      "learning_rate": 0.023600185099490977,
      "loss": 0.0001,
      "step": 82550
    },
    {
      "epoch": 38.20453493752892,
      "grad_norm": 0.003562672296538949,
      "learning_rate": 0.023590930124942157,
      "loss": 0.0001,
      "step": 82560
    },
    {
      "epoch": 38.20916242480333,
      "grad_norm": 0.18253850936889648,
      "learning_rate": 0.023581675150393337,
      "loss": 0.0003,
      "step": 82570
    },
    {
      "epoch": 38.21378991207774,
      "grad_norm": 0.08709712326526642,
      "learning_rate": 0.023572420175844516,
      "loss": 0.0002,
      "step": 82580
    },
    {
      "epoch": 38.21841739935215,
      "grad_norm": 0.029913660138845444,
      "learning_rate": 0.0235631652012957,
      "loss": 0.0001,
      "step": 82590
    },
    {
      "epoch": 38.22304488662656,
      "grad_norm": 0.007899163290858269,
      "learning_rate": 0.02355391022674688,
      "loss": 0.0001,
      "step": 82600
    },
    {
      "epoch": 38.22767237390097,
      "grad_norm": 0.439657062292099,
      "learning_rate": 0.02354465525219806,
      "loss": 0.0002,
      "step": 82610
    },
    {
      "epoch": 38.232299861175385,
      "grad_norm": 0.014049387536942959,
      "learning_rate": 0.02353540027764924,
      "loss": 0.0001,
      "step": 82620
    },
    {
      "epoch": 38.23692734844979,
      "grad_norm": 0.018033789470791817,
      "learning_rate": 0.023526145303100418,
      "loss": 0.0002,
      "step": 82630
    },
    {
      "epoch": 38.2415548357242,
      "grad_norm": 0.005593133624643087,
      "learning_rate": 0.023516890328551598,
      "loss": 0.0001,
      "step": 82640
    },
    {
      "epoch": 38.246182322998614,
      "grad_norm": 0.00726931681856513,
      "learning_rate": 0.023507635354002777,
      "loss": 0.0001,
      "step": 82650
    },
    {
      "epoch": 38.25080981027302,
      "grad_norm": 0.001199275371618569,
      "learning_rate": 0.023498380379453957,
      "loss": 0.0001,
      "step": 82660
    },
    {
      "epoch": 38.25543729754743,
      "grad_norm": 0.005724412854760885,
      "learning_rate": 0.023489125404905137,
      "loss": 0.0001,
      "step": 82670
    },
    {
      "epoch": 38.26006478482184,
      "grad_norm": 0.008623153902590275,
      "learning_rate": 0.02347987043035632,
      "loss": 0.0001,
      "step": 82680
    },
    {
      "epoch": 38.264692272096255,
      "grad_norm": 0.0054013594053685665,
      "learning_rate": 0.0234706154558075,
      "loss": 0.0001,
      "step": 82690
    },
    {
      "epoch": 38.26931975937066,
      "grad_norm": 0.02696094661951065,
      "learning_rate": 0.02346136048125868,
      "loss": 0.0001,
      "step": 82700
    },
    {
      "epoch": 38.27394724664507,
      "grad_norm": 0.10723717510700226,
      "learning_rate": 0.023452105506709855,
      "loss": 0.0001,
      "step": 82710
    },
    {
      "epoch": 38.278574733919484,
      "grad_norm": 0.10925538092851639,
      "learning_rate": 0.02344285053216104,
      "loss": 0.0001,
      "step": 82720
    },
    {
      "epoch": 38.28320222119389,
      "grad_norm": 0.0008735928568057716,
      "learning_rate": 0.02343359555761222,
      "loss": 0.0001,
      "step": 82730
    },
    {
      "epoch": 38.2878297084683,
      "grad_norm": 0.005205498076975346,
      "learning_rate": 0.023424340583063398,
      "loss": 0.0002,
      "step": 82740
    },
    {
      "epoch": 38.29245719574271,
      "grad_norm": 0.009763859212398529,
      "learning_rate": 0.023415085608514578,
      "loss": 0.0002,
      "step": 82750
    },
    {
      "epoch": 38.29708468301712,
      "grad_norm": 0.009400731883943081,
      "learning_rate": 0.023405830633965757,
      "loss": 0.0,
      "step": 82760
    },
    {
      "epoch": 38.30171217029153,
      "grad_norm": 0.0066342223435640335,
      "learning_rate": 0.02339657565941694,
      "loss": 0.0001,
      "step": 82770
    },
    {
      "epoch": 38.30633965756594,
      "grad_norm": 0.003697954351082444,
      "learning_rate": 0.023387320684868117,
      "loss": 0.0001,
      "step": 82780
    },
    {
      "epoch": 38.310967144840355,
      "grad_norm": 0.002029792871326208,
      "learning_rate": 0.023378065710319296,
      "loss": 0.0001,
      "step": 82790
    },
    {
      "epoch": 38.31559463211476,
      "grad_norm": 0.07163015007972717,
      "learning_rate": 0.023368810735770476,
      "loss": 0.0001,
      "step": 82800
    },
    {
      "epoch": 38.32022211938917,
      "grad_norm": 0.022021368145942688,
      "learning_rate": 0.02335955576122166,
      "loss": 0.0001,
      "step": 82810
    },
    {
      "epoch": 38.324849606663584,
      "grad_norm": 0.062149107456207275,
      "learning_rate": 0.02335030078667284,
      "loss": 0.0002,
      "step": 82820
    },
    {
      "epoch": 38.32947709393799,
      "grad_norm": 0.014384528622031212,
      "learning_rate": 0.02334104581212402,
      "loss": 0.0001,
      "step": 82830
    },
    {
      "epoch": 38.3341045812124,
      "grad_norm": 0.003686316777020693,
      "learning_rate": 0.023331790837575198,
      "loss": 0.0001,
      "step": 82840
    },
    {
      "epoch": 38.33873206848681,
      "grad_norm": 0.019616665318608284,
      "learning_rate": 0.023322535863026378,
      "loss": 0.0001,
      "step": 82850
    },
    {
      "epoch": 38.343359555761225,
      "grad_norm": 0.003290105378255248,
      "learning_rate": 0.023313280888477558,
      "loss": 0.0001,
      "step": 82860
    },
    {
      "epoch": 38.34798704303563,
      "grad_norm": 0.031080100685358047,
      "learning_rate": 0.023304025913928737,
      "loss": 0.0001,
      "step": 82870
    },
    {
      "epoch": 38.35261453031004,
      "grad_norm": 0.018754247575998306,
      "learning_rate": 0.023294770939379917,
      "loss": 0.0001,
      "step": 82880
    },
    {
      "epoch": 38.357242017584454,
      "grad_norm": 0.0022135814651846886,
      "learning_rate": 0.023285515964831097,
      "loss": 0.0002,
      "step": 82890
    },
    {
      "epoch": 38.36186950485886,
      "grad_norm": 0.005971773527562618,
      "learning_rate": 0.02327626099028228,
      "loss": 0.0001,
      "step": 82900
    },
    {
      "epoch": 38.36649699213327,
      "grad_norm": 0.013432137668132782,
      "learning_rate": 0.02326700601573346,
      "loss": 0.0001,
      "step": 82910
    },
    {
      "epoch": 38.37112447940768,
      "grad_norm": 0.004868228919804096,
      "learning_rate": 0.02325775104118464,
      "loss": 0.0002,
      "step": 82920
    },
    {
      "epoch": 38.37575196668209,
      "grad_norm": 0.013285024091601372,
      "learning_rate": 0.02324849606663582,
      "loss": 0.0001,
      "step": 82930
    },
    {
      "epoch": 38.3803794539565,
      "grad_norm": 0.012601940892636776,
      "learning_rate": 0.023239241092087,
      "loss": 0.0002,
      "step": 82940
    },
    {
      "epoch": 38.38500694123091,
      "grad_norm": 0.003829475026577711,
      "learning_rate": 0.023229986117538178,
      "loss": 0.0001,
      "step": 82950
    },
    {
      "epoch": 38.389634428505325,
      "grad_norm": 0.012395535595715046,
      "learning_rate": 0.023220731142989358,
      "loss": 0.0001,
      "step": 82960
    },
    {
      "epoch": 38.39426191577973,
      "grad_norm": 0.036530446261167526,
      "learning_rate": 0.023211476168440538,
      "loss": 0.0001,
      "step": 82970
    },
    {
      "epoch": 38.39888940305414,
      "grad_norm": 0.009327051229774952,
      "learning_rate": 0.023202221193891717,
      "loss": 0.0001,
      "step": 82980
    },
    {
      "epoch": 38.403516890328554,
      "grad_norm": 0.003923295997083187,
      "learning_rate": 0.0231929662193429,
      "loss": 0.0,
      "step": 82990
    },
    {
      "epoch": 38.40814437760296,
      "grad_norm": 0.0030812625773251057,
      "learning_rate": 0.02318371124479408,
      "loss": 0.0002,
      "step": 83000
    },
    {
      "epoch": 38.41277186487737,
      "grad_norm": 1.5114037990570068,
      "learning_rate": 0.023174456270245256,
      "loss": 0.0003,
      "step": 83010
    },
    {
      "epoch": 38.41739935215178,
      "grad_norm": 0.0012215577298775315,
      "learning_rate": 0.023165201295696436,
      "loss": 0.0001,
      "step": 83020
    },
    {
      "epoch": 38.422026839426195,
      "grad_norm": 0.010499903000891209,
      "learning_rate": 0.02315594632114762,
      "loss": 0.0015,
      "step": 83030
    },
    {
      "epoch": 38.4266543267006,
      "grad_norm": 0.006231401115655899,
      "learning_rate": 0.0231466913465988,
      "loss": 0.0001,
      "step": 83040
    },
    {
      "epoch": 38.43128181397501,
      "grad_norm": 0.00916317105293274,
      "learning_rate": 0.02313743637204998,
      "loss": 0.0001,
      "step": 83050
    },
    {
      "epoch": 38.435909301249424,
      "grad_norm": 0.012812050990760326,
      "learning_rate": 0.023128181397501158,
      "loss": 0.0001,
      "step": 83060
    },
    {
      "epoch": 38.44053678852383,
      "grad_norm": 0.005208815447986126,
      "learning_rate": 0.023118926422952338,
      "loss": 0.0002,
      "step": 83070
    },
    {
      "epoch": 38.44516427579824,
      "grad_norm": 0.003681991947814822,
      "learning_rate": 0.02310967144840352,
      "loss": 0.0,
      "step": 83080
    },
    {
      "epoch": 38.44979176307265,
      "grad_norm": 0.007499634753912687,
      "learning_rate": 0.023100416473854697,
      "loss": 0.0001,
      "step": 83090
    },
    {
      "epoch": 38.45441925034706,
      "grad_norm": 0.01619076170027256,
      "learning_rate": 0.023091161499305877,
      "loss": 0.0001,
      "step": 83100
    },
    {
      "epoch": 38.45904673762147,
      "grad_norm": 0.0013162053655833006,
      "learning_rate": 0.023081906524757057,
      "loss": 0.0001,
      "step": 83110
    },
    {
      "epoch": 38.46367422489588,
      "grad_norm": 0.0017136781243607402,
      "learning_rate": 0.02307265155020824,
      "loss": 0.0001,
      "step": 83120
    },
    {
      "epoch": 38.468301712170295,
      "grad_norm": 0.019542792811989784,
      "learning_rate": 0.02306339657565942,
      "loss": 0.0001,
      "step": 83130
    },
    {
      "epoch": 38.4729291994447,
      "grad_norm": 0.06068167835474014,
      "learning_rate": 0.0230541416011106,
      "loss": 0.0002,
      "step": 83140
    },
    {
      "epoch": 38.47755668671911,
      "grad_norm": 0.004615223500877619,
      "learning_rate": 0.02304488662656178,
      "loss": 0.0,
      "step": 83150
    },
    {
      "epoch": 38.482184173993524,
      "grad_norm": 0.043930474668741226,
      "learning_rate": 0.02303563165201296,
      "loss": 0.0001,
      "step": 83160
    },
    {
      "epoch": 38.48681166126793,
      "grad_norm": 0.0032282897736877203,
      "learning_rate": 0.023026376677464138,
      "loss": 0.0001,
      "step": 83170
    },
    {
      "epoch": 38.49143914854234,
      "grad_norm": 0.0011020504171028733,
      "learning_rate": 0.023017121702915318,
      "loss": 0.0,
      "step": 83180
    },
    {
      "epoch": 38.49606663581675,
      "grad_norm": 0.002872014883905649,
      "learning_rate": 0.023007866728366497,
      "loss": 0.0001,
      "step": 83190
    },
    {
      "epoch": 38.500694123091165,
      "grad_norm": 0.005009213928133249,
      "learning_rate": 0.022998611753817677,
      "loss": 0.0001,
      "step": 83200
    },
    {
      "epoch": 38.50532161036557,
      "grad_norm": 0.03117430955171585,
      "learning_rate": 0.02298935677926886,
      "loss": 0.0001,
      "step": 83210
    },
    {
      "epoch": 38.50994909763998,
      "grad_norm": 0.0011629358632490039,
      "learning_rate": 0.02298010180472004,
      "loss": 0.0002,
      "step": 83220
    },
    {
      "epoch": 38.514576584914394,
      "grad_norm": 0.0019443490309640765,
      "learning_rate": 0.02297084683017122,
      "loss": 0.0001,
      "step": 83230
    },
    {
      "epoch": 38.5192040721888,
      "grad_norm": 0.09827955067157745,
      "learning_rate": 0.022961591855622396,
      "loss": 0.0002,
      "step": 83240
    },
    {
      "epoch": 38.52383155946321,
      "grad_norm": 0.0008151487563736737,
      "learning_rate": 0.02295233688107358,
      "loss": 0.0,
      "step": 83250
    },
    {
      "epoch": 38.52845904673762,
      "grad_norm": 0.12238583713769913,
      "learning_rate": 0.02294308190652476,
      "loss": 0.0001,
      "step": 83260
    },
    {
      "epoch": 38.53308653401203,
      "grad_norm": 0.0012328176526352763,
      "learning_rate": 0.02293382693197594,
      "loss": 0.0001,
      "step": 83270
    },
    {
      "epoch": 38.53771402128644,
      "grad_norm": 0.0012045024195685983,
      "learning_rate": 0.022924571957427118,
      "loss": 0.0001,
      "step": 83280
    },
    {
      "epoch": 38.54234150856085,
      "grad_norm": 0.0046115536242723465,
      "learning_rate": 0.022915316982878298,
      "loss": 0.0001,
      "step": 83290
    },
    {
      "epoch": 38.546968995835265,
      "grad_norm": 0.007770773954689503,
      "learning_rate": 0.02290606200832948,
      "loss": 0.0,
      "step": 83300
    },
    {
      "epoch": 38.55159648310967,
      "grad_norm": 0.012846528552472591,
      "learning_rate": 0.02289680703378066,
      "loss": 0.0002,
      "step": 83310
    },
    {
      "epoch": 38.55622397038408,
      "grad_norm": 0.0008100759005174041,
      "learning_rate": 0.022887552059231837,
      "loss": 0.0001,
      "step": 83320
    },
    {
      "epoch": 38.560851457658494,
      "grad_norm": 0.08099197596311569,
      "learning_rate": 0.022878297084683016,
      "loss": 0.0001,
      "step": 83330
    },
    {
      "epoch": 38.5654789449329,
      "grad_norm": 0.021635906770825386,
      "learning_rate": 0.0228690421101342,
      "loss": 0.0008,
      "step": 83340
    },
    {
      "epoch": 38.57010643220731,
      "grad_norm": 0.0011921289842575788,
      "learning_rate": 0.02285978713558538,
      "loss": 0.0001,
      "step": 83350
    },
    {
      "epoch": 38.57473391948172,
      "grad_norm": 0.01385584194213152,
      "learning_rate": 0.02285053216103656,
      "loss": 0.0001,
      "step": 83360
    },
    {
      "epoch": 38.579361406756135,
      "grad_norm": 0.007290665991604328,
      "learning_rate": 0.02284127718648774,
      "loss": 0.0001,
      "step": 83370
    },
    {
      "epoch": 38.58398889403054,
      "grad_norm": 0.06561019271612167,
      "learning_rate": 0.022832022211938918,
      "loss": 0.0001,
      "step": 83380
    },
    {
      "epoch": 38.58861638130495,
      "grad_norm": 0.03880569338798523,
      "learning_rate": 0.022822767237390098,
      "loss": 0.0001,
      "step": 83390
    },
    {
      "epoch": 38.593243868579364,
      "grad_norm": 0.009744093753397465,
      "learning_rate": 0.022813512262841278,
      "loss": 0.0001,
      "step": 83400
    },
    {
      "epoch": 38.59787135585377,
      "grad_norm": 0.006500914692878723,
      "learning_rate": 0.022804257288292457,
      "loss": 0.0003,
      "step": 83410
    },
    {
      "epoch": 38.60249884312818,
      "grad_norm": 0.0036649208050221205,
      "learning_rate": 0.022795002313743637,
      "loss": 0.0001,
      "step": 83420
    },
    {
      "epoch": 38.60712633040259,
      "grad_norm": 0.0008148598717525601,
      "learning_rate": 0.02278574733919482,
      "loss": 0.0001,
      "step": 83430
    },
    {
      "epoch": 38.611753817677,
      "grad_norm": 0.0026559694670140743,
      "learning_rate": 0.022776492364646,
      "loss": 0.0001,
      "step": 83440
    },
    {
      "epoch": 38.61638130495141,
      "grad_norm": 0.0024865148589015007,
      "learning_rate": 0.02276723739009718,
      "loss": 0.0001,
      "step": 83450
    },
    {
      "epoch": 38.62100879222582,
      "grad_norm": 0.015080229379236698,
      "learning_rate": 0.02275798241554836,
      "loss": 0.0001,
      "step": 83460
    },
    {
      "epoch": 38.625636279500235,
      "grad_norm": 0.011499299667775631,
      "learning_rate": 0.02274872744099954,
      "loss": 0.0001,
      "step": 83470
    },
    {
      "epoch": 38.63026376677464,
      "grad_norm": 0.0016956691397354007,
      "learning_rate": 0.02273947246645072,
      "loss": 0.0001,
      "step": 83480
    },
    {
      "epoch": 38.63489125404905,
      "grad_norm": 0.003602711483836174,
      "learning_rate": 0.022730217491901898,
      "loss": 0.0001,
      "step": 83490
    },
    {
      "epoch": 38.639518741323464,
      "grad_norm": 0.005822117440402508,
      "learning_rate": 0.022720962517353078,
      "loss": 0.0001,
      "step": 83500
    },
    {
      "epoch": 38.64414622859787,
      "grad_norm": 0.003156038234010339,
      "learning_rate": 0.022711707542804258,
      "loss": 0.0001,
      "step": 83510
    },
    {
      "epoch": 38.64877371587228,
      "grad_norm": 0.003368215635418892,
      "learning_rate": 0.02270245256825544,
      "loss": 0.0001,
      "step": 83520
    },
    {
      "epoch": 38.65340120314669,
      "grad_norm": 0.12534956634044647,
      "learning_rate": 0.02269319759370662,
      "loss": 0.0002,
      "step": 83530
    },
    {
      "epoch": 38.6580286904211,
      "grad_norm": 0.10986335575580597,
      "learning_rate": 0.0226839426191578,
      "loss": 0.0001,
      "step": 83540
    },
    {
      "epoch": 38.66265617769551,
      "grad_norm": 0.000946468731854111,
      "learning_rate": 0.022674687644608976,
      "loss": 0.0001,
      "step": 83550
    },
    {
      "epoch": 38.66728366496992,
      "grad_norm": 0.0020312706474214792,
      "learning_rate": 0.02266543267006016,
      "loss": 0.0003,
      "step": 83560
    },
    {
      "epoch": 38.671911152244334,
      "grad_norm": 0.01100301742553711,
      "learning_rate": 0.02265617769551134,
      "loss": 0.0001,
      "step": 83570
    },
    {
      "epoch": 38.67653863951874,
      "grad_norm": 0.02869396284222603,
      "learning_rate": 0.02264692272096252,
      "loss": 0.0,
      "step": 83580
    },
    {
      "epoch": 38.68116612679315,
      "grad_norm": 0.05397669970989227,
      "learning_rate": 0.0226376677464137,
      "loss": 0.0001,
      "step": 83590
    },
    {
      "epoch": 38.68579361406756,
      "grad_norm": 0.049668096005916595,
      "learning_rate": 0.022628412771864878,
      "loss": 0.0005,
      "step": 83600
    },
    {
      "epoch": 38.69042110134197,
      "grad_norm": 0.004815320950001478,
      "learning_rate": 0.02261915779731606,
      "loss": 0.0001,
      "step": 83610
    },
    {
      "epoch": 38.69504858861638,
      "grad_norm": 0.005540482234209776,
      "learning_rate": 0.022609902822767237,
      "loss": 0.0003,
      "step": 83620
    },
    {
      "epoch": 38.69967607589079,
      "grad_norm": 0.00459895096719265,
      "learning_rate": 0.022600647848218417,
      "loss": 0.0001,
      "step": 83630
    },
    {
      "epoch": 38.704303563165205,
      "grad_norm": 0.1457575559616089,
      "learning_rate": 0.022591392873669597,
      "loss": 0.0001,
      "step": 83640
    },
    {
      "epoch": 38.70893105043961,
      "grad_norm": 0.002310937736183405,
      "learning_rate": 0.02258213789912078,
      "loss": 0.0001,
      "step": 83650
    },
    {
      "epoch": 38.71355853771402,
      "grad_norm": 0.036001209169626236,
      "learning_rate": 0.02257288292457196,
      "loss": 0.0001,
      "step": 83660
    },
    {
      "epoch": 38.718186024988434,
      "grad_norm": 0.04592743515968323,
      "learning_rate": 0.02256362795002314,
      "loss": 0.0001,
      "step": 83670
    },
    {
      "epoch": 38.72281351226284,
      "grad_norm": 0.0023351646959781647,
      "learning_rate": 0.02255437297547432,
      "loss": 0.0,
      "step": 83680
    },
    {
      "epoch": 38.72744099953725,
      "grad_norm": 0.0019035784061998129,
      "learning_rate": 0.0225451180009255,
      "loss": 0.0,
      "step": 83690
    },
    {
      "epoch": 38.73206848681166,
      "grad_norm": 0.0021019033156335354,
      "learning_rate": 0.02253586302637668,
      "loss": 0.0001,
      "step": 83700
    },
    {
      "epoch": 38.73669597408607,
      "grad_norm": 0.004717403557151556,
      "learning_rate": 0.022526608051827858,
      "loss": 0.0001,
      "step": 83710
    },
    {
      "epoch": 38.74132346136048,
      "grad_norm": 0.0038876112084835768,
      "learning_rate": 0.022517353077279038,
      "loss": 0.0001,
      "step": 83720
    },
    {
      "epoch": 38.74595094863489,
      "grad_norm": 0.012237096205353737,
      "learning_rate": 0.022508098102730217,
      "loss": 0.0,
      "step": 83730
    },
    {
      "epoch": 38.750578435909304,
      "grad_norm": 0.006119874771684408,
      "learning_rate": 0.0224988431281814,
      "loss": 0.0001,
      "step": 83740
    },
    {
      "epoch": 38.75520592318371,
      "grad_norm": 0.04998311027884483,
      "learning_rate": 0.02248958815363258,
      "loss": 0.0001,
      "step": 83750
    },
    {
      "epoch": 38.75983341045812,
      "grad_norm": 0.015573015436530113,
      "learning_rate": 0.02248033317908376,
      "loss": 0.0001,
      "step": 83760
    },
    {
      "epoch": 38.76446089773253,
      "grad_norm": 0.003336158813908696,
      "learning_rate": 0.02247107820453494,
      "loss": 0.0002,
      "step": 83770
    },
    {
      "epoch": 38.76908838500694,
      "grad_norm": 0.09137901663780212,
      "learning_rate": 0.02246182322998612,
      "loss": 0.0001,
      "step": 83780
    },
    {
      "epoch": 38.77371587228135,
      "grad_norm": 0.017306791618466377,
      "learning_rate": 0.0224525682554373,
      "loss": 0.0001,
      "step": 83790
    },
    {
      "epoch": 38.77834335955576,
      "grad_norm": 0.0005575746181420982,
      "learning_rate": 0.02244331328088848,
      "loss": 0.0001,
      "step": 83800
    },
    {
      "epoch": 38.782970846830175,
      "grad_norm": 0.008167717605829239,
      "learning_rate": 0.02243405830633966,
      "loss": 0.0001,
      "step": 83810
    },
    {
      "epoch": 38.78759833410458,
      "grad_norm": 0.05436497926712036,
      "learning_rate": 0.022424803331790838,
      "loss": 0.0001,
      "step": 83820
    },
    {
      "epoch": 38.79222582137899,
      "grad_norm": 0.005348875652998686,
      "learning_rate": 0.02241554835724202,
      "loss": 0.0001,
      "step": 83830
    },
    {
      "epoch": 38.796853308653404,
      "grad_norm": 0.015094746835529804,
      "learning_rate": 0.0224062933826932,
      "loss": 0.0001,
      "step": 83840
    },
    {
      "epoch": 38.80148079592781,
      "grad_norm": 0.001429004012607038,
      "learning_rate": 0.022397038408144377,
      "loss": 0.0,
      "step": 83850
    },
    {
      "epoch": 38.80610828320222,
      "grad_norm": 0.005305489059537649,
      "learning_rate": 0.022387783433595557,
      "loss": 0.0,
      "step": 83860
    },
    {
      "epoch": 38.81073577047663,
      "grad_norm": 0.009053818881511688,
      "learning_rate": 0.02237852845904674,
      "loss": 0.0,
      "step": 83870
    },
    {
      "epoch": 38.81536325775104,
      "grad_norm": 0.008068022318184376,
      "learning_rate": 0.02236927348449792,
      "loss": 0.0001,
      "step": 83880
    },
    {
      "epoch": 38.81999074502545,
      "grad_norm": 0.0023591869976371527,
      "learning_rate": 0.0223600185099491,
      "loss": 0.0,
      "step": 83890
    },
    {
      "epoch": 38.82461823229986,
      "grad_norm": 0.0025775632821023464,
      "learning_rate": 0.02235076353540028,
      "loss": 0.0001,
      "step": 83900
    },
    {
      "epoch": 38.829245719574274,
      "grad_norm": 0.0024459045380353928,
      "learning_rate": 0.02234150856085146,
      "loss": 0.0001,
      "step": 83910
    },
    {
      "epoch": 38.83387320684868,
      "grad_norm": 0.008654749020934105,
      "learning_rate": 0.02233225358630264,
      "loss": 0.0001,
      "step": 83920
    },
    {
      "epoch": 38.83850069412309,
      "grad_norm": 0.0013970048166811466,
      "learning_rate": 0.022322998611753818,
      "loss": 0.0001,
      "step": 83930
    },
    {
      "epoch": 38.8431281813975,
      "grad_norm": 0.006265681236982346,
      "learning_rate": 0.022313743637204998,
      "loss": 0.0,
      "step": 83940
    },
    {
      "epoch": 38.84775566867191,
      "grad_norm": 0.0013452009297907352,
      "learning_rate": 0.022304488662656177,
      "loss": 0.0002,
      "step": 83950
    },
    {
      "epoch": 38.85238315594632,
      "grad_norm": 0.0018491973169147968,
      "learning_rate": 0.02229523368810736,
      "loss": 0.0001,
      "step": 83960
    },
    {
      "epoch": 38.85701064322073,
      "grad_norm": 0.051887497305870056,
      "learning_rate": 0.02228597871355854,
      "loss": 0.0001,
      "step": 83970
    },
    {
      "epoch": 38.861638130495145,
      "grad_norm": 0.36563825607299805,
      "learning_rate": 0.02227672373900972,
      "loss": 0.0002,
      "step": 83980
    },
    {
      "epoch": 38.86626561776955,
      "grad_norm": 0.0010055754100903869,
      "learning_rate": 0.0222674687644609,
      "loss": 0.0001,
      "step": 83990
    },
    {
      "epoch": 38.87089310504396,
      "grad_norm": 0.0066473837941884995,
      "learning_rate": 0.02225821378991208,
      "loss": 0.0,
      "step": 84000
    },
    {
      "epoch": 38.875520592318374,
      "grad_norm": 0.0012088784715160728,
      "learning_rate": 0.02224895881536326,
      "loss": 0.0001,
      "step": 84010
    },
    {
      "epoch": 38.88014807959278,
      "grad_norm": 0.051464907824993134,
      "learning_rate": 0.02223970384081444,
      "loss": 0.0001,
      "step": 84020
    },
    {
      "epoch": 38.88477556686719,
      "grad_norm": 0.002451207721605897,
      "learning_rate": 0.022230448866265618,
      "loss": 0.0001,
      "step": 84030
    },
    {
      "epoch": 38.8894030541416,
      "grad_norm": 0.0054854415357112885,
      "learning_rate": 0.022221193891716798,
      "loss": 0.0001,
      "step": 84040
    },
    {
      "epoch": 38.89403054141601,
      "grad_norm": 0.0018233355367556214,
      "learning_rate": 0.02221193891716798,
      "loss": 0.0001,
      "step": 84050
    },
    {
      "epoch": 38.89865802869042,
      "grad_norm": 0.015092446468770504,
      "learning_rate": 0.02220268394261916,
      "loss": 0.0001,
      "step": 84060
    },
    {
      "epoch": 38.90328551596483,
      "grad_norm": 0.04482383653521538,
      "learning_rate": 0.02219342896807034,
      "loss": 0.0001,
      "step": 84070
    },
    {
      "epoch": 38.907913003239244,
      "grad_norm": 0.005663961637765169,
      "learning_rate": 0.022184173993521517,
      "loss": 0.0001,
      "step": 84080
    },
    {
      "epoch": 38.91254049051365,
      "grad_norm": 0.04165263473987579,
      "learning_rate": 0.0221749190189727,
      "loss": 0.0001,
      "step": 84090
    },
    {
      "epoch": 38.91716797778806,
      "grad_norm": 0.004461693111807108,
      "learning_rate": 0.02216566404442388,
      "loss": 0.0,
      "step": 84100
    },
    {
      "epoch": 38.92179546506247,
      "grad_norm": 0.04940459132194519,
      "learning_rate": 0.02215640906987506,
      "loss": 0.0005,
      "step": 84110
    },
    {
      "epoch": 38.92642295233688,
      "grad_norm": 0.011755340732634068,
      "learning_rate": 0.02214715409532624,
      "loss": 0.0001,
      "step": 84120
    },
    {
      "epoch": 38.93105043961129,
      "grad_norm": 0.01047939620912075,
      "learning_rate": 0.02213789912077742,
      "loss": 0.0002,
      "step": 84130
    },
    {
      "epoch": 38.9356779268857,
      "grad_norm": 0.0029734456911683083,
      "learning_rate": 0.0221286441462286,
      "loss": 0.0001,
      "step": 84140
    },
    {
      "epoch": 38.94030541416011,
      "grad_norm": 0.08062616735696793,
      "learning_rate": 0.02211938917167978,
      "loss": 0.0001,
      "step": 84150
    },
    {
      "epoch": 38.94493290143452,
      "grad_norm": 0.0014414091128855944,
      "learning_rate": 0.022110134197130957,
      "loss": 0.0001,
      "step": 84160
    },
    {
      "epoch": 38.94956038870893,
      "grad_norm": 0.013468479737639427,
      "learning_rate": 0.022100879222582137,
      "loss": 0.0002,
      "step": 84170
    },
    {
      "epoch": 38.954187875983344,
      "grad_norm": 0.021846963092684746,
      "learning_rate": 0.02209162424803332,
      "loss": 0.0001,
      "step": 84180
    },
    {
      "epoch": 38.95881536325775,
      "grad_norm": 0.02322259172797203,
      "learning_rate": 0.0220823692734845,
      "loss": 0.0001,
      "step": 84190
    },
    {
      "epoch": 38.96344285053216,
      "grad_norm": 1.1668838262557983,
      "learning_rate": 0.02207311429893568,
      "loss": 0.0003,
      "step": 84200
    },
    {
      "epoch": 38.96807033780657,
      "grad_norm": 0.0054091233760118484,
      "learning_rate": 0.02206385932438686,
      "loss": 0.0001,
      "step": 84210
    },
    {
      "epoch": 38.97269782508098,
      "grad_norm": 0.003530009649693966,
      "learning_rate": 0.02205460434983804,
      "loss": 0.0001,
      "step": 84220
    },
    {
      "epoch": 38.97732531235539,
      "grad_norm": 0.0023449105210602283,
      "learning_rate": 0.022045349375289222,
      "loss": 0.0003,
      "step": 84230
    },
    {
      "epoch": 38.9819527996298,
      "grad_norm": 0.0038852060679346323,
      "learning_rate": 0.0220360944007404,
      "loss": 0.0001,
      "step": 84240
    },
    {
      "epoch": 38.986580286904214,
      "grad_norm": 0.001869777450338006,
      "learning_rate": 0.022026839426191578,
      "loss": 0.0001,
      "step": 84250
    },
    {
      "epoch": 38.99120777417862,
      "grad_norm": 0.009721743874251842,
      "learning_rate": 0.022017584451642758,
      "loss": 0.0001,
      "step": 84260
    },
    {
      "epoch": 38.99583526145303,
      "grad_norm": 0.012359515763819218,
      "learning_rate": 0.02200832947709394,
      "loss": 0.0001,
      "step": 84270
    },
    {
      "epoch": 39.0,
      "eval_accuracy_branch1": 0.9919118779849022,
      "eval_accuracy_branch2": 0.5007702973347712,
      "eval_f1_branch1": 0.9927848681642513,
      "eval_f1_branch2": 0.5003578262173837,
      "eval_loss": 0.014481043443083763,
      "eval_precision_branch1": 0.9929512668473982,
      "eval_precision_branch2": 0.5007728493855497,
      "eval_recall_branch1": 0.9927115242805148,
      "eval_recall_branch2": 0.5007702973347712,
      "eval_runtime": 29.0374,
      "eval_samples_per_second": 447.079,
      "eval_steps_per_second": 55.893,
      "step": 84279
    },
    {
      "epoch": 39.00046274872744,
      "grad_norm": 0.005228588357567787,
      "learning_rate": 0.02199907450254512,
      "loss": 0.0,
      "step": 84280
    },
    {
      "epoch": 39.00509023600185,
      "grad_norm": 0.001397487474605441,
      "learning_rate": 0.0219898195279963,
      "loss": 0.0001,
      "step": 84290
    },
    {
      "epoch": 39.00971772327626,
      "grad_norm": 0.0024287004489451647,
      "learning_rate": 0.02198056455344748,
      "loss": 0.0001,
      "step": 84300
    },
    {
      "epoch": 39.01434521055067,
      "grad_norm": 0.0040878490544855595,
      "learning_rate": 0.02197130957889866,
      "loss": 0.0004,
      "step": 84310
    },
    {
      "epoch": 39.01897269782508,
      "grad_norm": 0.010186619125306606,
      "learning_rate": 0.02196205460434984,
      "loss": 0.0001,
      "step": 84320
    },
    {
      "epoch": 39.02360018509949,
      "grad_norm": 0.002343611093237996,
      "learning_rate": 0.02195279962980102,
      "loss": 0.0009,
      "step": 84330
    },
    {
      "epoch": 39.0282276723739,
      "grad_norm": 0.007174679543823004,
      "learning_rate": 0.0219435446552522,
      "loss": 0.0,
      "step": 84340
    },
    {
      "epoch": 39.032855159648314,
      "grad_norm": 0.005235104355961084,
      "learning_rate": 0.021934289680703378,
      "loss": 0.0001,
      "step": 84350
    },
    {
      "epoch": 39.03748264692272,
      "grad_norm": 1.2113840579986572,
      "learning_rate": 0.02192503470615456,
      "loss": 0.0004,
      "step": 84360
    },
    {
      "epoch": 39.04211013419713,
      "grad_norm": 0.01617048680782318,
      "learning_rate": 0.02191577973160574,
      "loss": 0.0002,
      "step": 84370
    },
    {
      "epoch": 39.04673762147154,
      "grad_norm": 0.011376320384442806,
      "learning_rate": 0.02190652475705692,
      "loss": 0.0003,
      "step": 84380
    },
    {
      "epoch": 39.05136510874595,
      "grad_norm": 0.016711082309484482,
      "learning_rate": 0.021897269782508097,
      "loss": 0.0001,
      "step": 84390
    },
    {
      "epoch": 39.05599259602036,
      "grad_norm": 0.016388164833188057,
      "learning_rate": 0.02188801480795928,
      "loss": 0.0003,
      "step": 84400
    },
    {
      "epoch": 39.06062008329477,
      "grad_norm": 0.09386055171489716,
      "learning_rate": 0.02187875983341046,
      "loss": 0.0001,
      "step": 84410
    },
    {
      "epoch": 39.065247570569184,
      "grad_norm": 0.656899094581604,
      "learning_rate": 0.02186950485886164,
      "loss": 0.0002,
      "step": 84420
    },
    {
      "epoch": 39.06987505784359,
      "grad_norm": 10.926167488098145,
      "learning_rate": 0.02186024988431282,
      "loss": 0.0022,
      "step": 84430
    },
    {
      "epoch": 39.074502545118,
      "grad_norm": 0.019426701590418816,
      "learning_rate": 0.021850994909764,
      "loss": 0.0001,
      "step": 84440
    },
    {
      "epoch": 39.07913003239241,
      "grad_norm": 0.0009284413536079228,
      "learning_rate": 0.021841739935215182,
      "loss": 0.0001,
      "step": 84450
    },
    {
      "epoch": 39.08375751966682,
      "grad_norm": 0.000708579842466861,
      "learning_rate": 0.021832484960666358,
      "loss": 0.0001,
      "step": 84460
    },
    {
      "epoch": 39.08838500694123,
      "grad_norm": 0.009121810086071491,
      "learning_rate": 0.021823229986117538,
      "loss": 0.0001,
      "step": 84470
    },
    {
      "epoch": 39.09301249421564,
      "grad_norm": 0.012816687114536762,
      "learning_rate": 0.021813975011568718,
      "loss": 0.0002,
      "step": 84480
    },
    {
      "epoch": 39.09763998149005,
      "grad_norm": 0.0007551072631031275,
      "learning_rate": 0.0218047200370199,
      "loss": 0.0001,
      "step": 84490
    },
    {
      "epoch": 39.10226746876446,
      "grad_norm": 0.002555916551500559,
      "learning_rate": 0.02179546506247108,
      "loss": 0.0001,
      "step": 84500
    },
    {
      "epoch": 39.10689495603887,
      "grad_norm": 0.010187666863203049,
      "learning_rate": 0.02178621008792226,
      "loss": 0.0,
      "step": 84510
    },
    {
      "epoch": 39.111522443313284,
      "grad_norm": 0.010021451860666275,
      "learning_rate": 0.02177695511337344,
      "loss": 0.0001,
      "step": 84520
    },
    {
      "epoch": 39.11614993058769,
      "grad_norm": 0.01186735462397337,
      "learning_rate": 0.02176770013882462,
      "loss": 0.0001,
      "step": 84530
    },
    {
      "epoch": 39.1207774178621,
      "grad_norm": 0.03488888591527939,
      "learning_rate": 0.0217584451642758,
      "loss": 0.0001,
      "step": 84540
    },
    {
      "epoch": 39.12540490513651,
      "grad_norm": 0.00129085429944098,
      "learning_rate": 0.02174919018972698,
      "loss": 0.0002,
      "step": 84550
    },
    {
      "epoch": 39.13003239241092,
      "grad_norm": 0.0036695722956210375,
      "learning_rate": 0.02173993521517816,
      "loss": 0.0002,
      "step": 84560
    },
    {
      "epoch": 39.13465987968533,
      "grad_norm": 0.0014495620271191,
      "learning_rate": 0.021730680240629338,
      "loss": 0.0001,
      "step": 84570
    },
    {
      "epoch": 39.13928736695974,
      "grad_norm": 0.028604302555322647,
      "learning_rate": 0.02172142526608052,
      "loss": 0.0001,
      "step": 84580
    },
    {
      "epoch": 39.143914854234154,
      "grad_norm": 0.001153418212197721,
      "learning_rate": 0.0217121702915317,
      "loss": 0.0,
      "step": 84590
    },
    {
      "epoch": 39.14854234150856,
      "grad_norm": 0.004330749157816172,
      "learning_rate": 0.02170291531698288,
      "loss": 0.0,
      "step": 84600
    },
    {
      "epoch": 39.15316982878297,
      "grad_norm": 0.006973641458898783,
      "learning_rate": 0.02169366034243406,
      "loss": 0.0,
      "step": 84610
    },
    {
      "epoch": 39.15779731605738,
      "grad_norm": 0.023509247228503227,
      "learning_rate": 0.02168440536788524,
      "loss": 0.0008,
      "step": 84620
    },
    {
      "epoch": 39.16242480333179,
      "grad_norm": 0.0016316032269969583,
      "learning_rate": 0.02167515039333642,
      "loss": 0.0005,
      "step": 84630
    },
    {
      "epoch": 39.1670522906062,
      "grad_norm": 0.004086457192897797,
      "learning_rate": 0.0216658954187876,
      "loss": 0.0001,
      "step": 84640
    },
    {
      "epoch": 39.17167977788061,
      "grad_norm": 0.02999061346054077,
      "learning_rate": 0.02165664044423878,
      "loss": 0.0001,
      "step": 84650
    },
    {
      "epoch": 39.17630726515502,
      "grad_norm": 0.00211315113119781,
      "learning_rate": 0.02164738546968996,
      "loss": 0.0001,
      "step": 84660
    },
    {
      "epoch": 39.18093475242943,
      "grad_norm": 0.025902800261974335,
      "learning_rate": 0.021638130495141142,
      "loss": 0.0002,
      "step": 84670
    },
    {
      "epoch": 39.18556223970384,
      "grad_norm": 0.0020372625440359116,
      "learning_rate": 0.02162887552059232,
      "loss": 0.0001,
      "step": 84680
    },
    {
      "epoch": 39.190189726978254,
      "grad_norm": 0.007598592434078455,
      "learning_rate": 0.021619620546043498,
      "loss": 0.0002,
      "step": 84690
    },
    {
      "epoch": 39.19481721425266,
      "grad_norm": 0.00404488667845726,
      "learning_rate": 0.021610365571494677,
      "loss": 0.0001,
      "step": 84700
    },
    {
      "epoch": 39.19944470152707,
      "grad_norm": 0.018870942294597626,
      "learning_rate": 0.02160111059694586,
      "loss": 0.0001,
      "step": 84710
    },
    {
      "epoch": 39.20407218880148,
      "grad_norm": 0.007744590751826763,
      "learning_rate": 0.02159185562239704,
      "loss": 0.0001,
      "step": 84720
    },
    {
      "epoch": 39.20869967607589,
      "grad_norm": 0.014650930650532246,
      "learning_rate": 0.02158260064784822,
      "loss": 0.0001,
      "step": 84730
    },
    {
      "epoch": 39.2133271633503,
      "grad_norm": 0.004524491727352142,
      "learning_rate": 0.0215733456732994,
      "loss": 0.0004,
      "step": 84740
    },
    {
      "epoch": 39.21795465062471,
      "grad_norm": 0.004282071720808744,
      "learning_rate": 0.02156409069875058,
      "loss": 0.0001,
      "step": 84750
    },
    {
      "epoch": 39.222582137899124,
      "grad_norm": 0.009818852879106998,
      "learning_rate": 0.021554835724201762,
      "loss": 0.0001,
      "step": 84760
    },
    {
      "epoch": 39.22720962517353,
      "grad_norm": 0.04491531848907471,
      "learning_rate": 0.02154558074965294,
      "loss": 0.0001,
      "step": 84770
    },
    {
      "epoch": 39.23183711244794,
      "grad_norm": 0.006676807999610901,
      "learning_rate": 0.02153632577510412,
      "loss": 0.0001,
      "step": 84780
    },
    {
      "epoch": 39.23646459972235,
      "grad_norm": 0.0046517858281731606,
      "learning_rate": 0.021527070800555298,
      "loss": 0.0001,
      "step": 84790
    },
    {
      "epoch": 39.24109208699676,
      "grad_norm": 0.0037562591023743153,
      "learning_rate": 0.02151781582600648,
      "loss": 0.0001,
      "step": 84800
    },
    {
      "epoch": 39.24571957427117,
      "grad_norm": 0.004511328414082527,
      "learning_rate": 0.02150856085145766,
      "loss": 0.0,
      "step": 84810
    },
    {
      "epoch": 39.25034706154558,
      "grad_norm": 0.0037141384091228247,
      "learning_rate": 0.02149930587690884,
      "loss": 0.0001,
      "step": 84820
    },
    {
      "epoch": 39.25497454881999,
      "grad_norm": 0.0011208117939531803,
      "learning_rate": 0.02149005090236002,
      "loss": 0.0,
      "step": 84830
    },
    {
      "epoch": 39.2596020360944,
      "grad_norm": 0.009856640361249447,
      "learning_rate": 0.0214807959278112,
      "loss": 0.0001,
      "step": 84840
    },
    {
      "epoch": 39.26422952336881,
      "grad_norm": 0.010907493531703949,
      "learning_rate": 0.02147154095326238,
      "loss": 0.0001,
      "step": 84850
    },
    {
      "epoch": 39.268857010643224,
      "grad_norm": 0.07211827486753464,
      "learning_rate": 0.02146228597871356,
      "loss": 0.0001,
      "step": 84860
    },
    {
      "epoch": 39.27348449791763,
      "grad_norm": 0.014527380466461182,
      "learning_rate": 0.02145303100416474,
      "loss": 0.0001,
      "step": 84870
    },
    {
      "epoch": 39.27811198519204,
      "grad_norm": 0.0038410050328820944,
      "learning_rate": 0.02144377602961592,
      "loss": 0.0003,
      "step": 84880
    },
    {
      "epoch": 39.28273947246645,
      "grad_norm": 0.020688479766249657,
      "learning_rate": 0.0214345210550671,
      "loss": 0.0001,
      "step": 84890
    },
    {
      "epoch": 39.28736695974086,
      "grad_norm": 0.01086195558309555,
      "learning_rate": 0.02142526608051828,
      "loss": 0.0001,
      "step": 84900
    },
    {
      "epoch": 39.29199444701527,
      "grad_norm": 0.0026695590931922197,
      "learning_rate": 0.02141601110596946,
      "loss": 0.0005,
      "step": 84910
    },
    {
      "epoch": 39.29662193428968,
      "grad_norm": 0.012369608506560326,
      "learning_rate": 0.021406756131420637,
      "loss": 0.0002,
      "step": 84920
    },
    {
      "epoch": 39.301249421564094,
      "grad_norm": 0.008450939320027828,
      "learning_rate": 0.02139750115687182,
      "loss": 0.0002,
      "step": 84930
    },
    {
      "epoch": 39.3058769088385,
      "grad_norm": 0.017499124631285667,
      "learning_rate": 0.021388246182323,
      "loss": 0.0001,
      "step": 84940
    },
    {
      "epoch": 39.31050439611291,
      "grad_norm": 0.06603596359491348,
      "learning_rate": 0.02137899120777418,
      "loss": 0.0002,
      "step": 84950
    },
    {
      "epoch": 39.31513188338732,
      "grad_norm": 0.0031553630251437426,
      "learning_rate": 0.02136973623322536,
      "loss": 0.0002,
      "step": 84960
    },
    {
      "epoch": 39.31975937066173,
      "grad_norm": 0.004710005130618811,
      "learning_rate": 0.02136048125867654,
      "loss": 0.0002,
      "step": 84970
    },
    {
      "epoch": 39.32438685793614,
      "grad_norm": 0.006096566095948219,
      "learning_rate": 0.021351226284127722,
      "loss": 0.0003,
      "step": 84980
    },
    {
      "epoch": 39.32901434521055,
      "grad_norm": 0.003494193311780691,
      "learning_rate": 0.021341971309578902,
      "loss": 0.0001,
      "step": 84990
    },
    {
      "epoch": 39.33364183248496,
      "grad_norm": 0.025556575506925583,
      "learning_rate": 0.021332716335030078,
      "loss": 0.0,
      "step": 85000
    },
    {
      "epoch": 39.33826931975937,
      "grad_norm": 0.04042939469218254,
      "learning_rate": 0.021323461360481258,
      "loss": 0.0001,
      "step": 85010
    },
    {
      "epoch": 39.34289680703378,
      "grad_norm": 0.0053451284766197205,
      "learning_rate": 0.02131420638593244,
      "loss": 0.0005,
      "step": 85020
    },
    {
      "epoch": 39.347524294308194,
      "grad_norm": 0.006102396175265312,
      "learning_rate": 0.02130495141138362,
      "loss": 0.0001,
      "step": 85030
    },
    {
      "epoch": 39.3521517815826,
      "grad_norm": 0.06085953116416931,
      "learning_rate": 0.0212956964368348,
      "loss": 0.0001,
      "step": 85040
    },
    {
      "epoch": 39.35677926885701,
      "grad_norm": 0.01074930652976036,
      "learning_rate": 0.02128644146228598,
      "loss": 0.0001,
      "step": 85050
    },
    {
      "epoch": 39.36140675613142,
      "grad_norm": 0.23423834145069122,
      "learning_rate": 0.02127718648773716,
      "loss": 0.0002,
      "step": 85060
    },
    {
      "epoch": 39.36603424340583,
      "grad_norm": 0.0014298709575086832,
      "learning_rate": 0.021267931513188343,
      "loss": 0.0,
      "step": 85070
    },
    {
      "epoch": 39.37066173068024,
      "grad_norm": 0.38374775648117065,
      "learning_rate": 0.02125867653863952,
      "loss": 0.0002,
      "step": 85080
    },
    {
      "epoch": 39.37528921795465,
      "grad_norm": 0.0007305124308913946,
      "learning_rate": 0.0212494215640907,
      "loss": 0.0003,
      "step": 85090
    },
    {
      "epoch": 39.379916705229064,
      "grad_norm": 0.0048105306923389435,
      "learning_rate": 0.02124016658954188,
      "loss": 0.0001,
      "step": 85100
    },
    {
      "epoch": 39.38454419250347,
      "grad_norm": 0.023596860468387604,
      "learning_rate": 0.02123091161499306,
      "loss": 0.0001,
      "step": 85110
    },
    {
      "epoch": 39.38917167977788,
      "grad_norm": 0.0031238235533237457,
      "learning_rate": 0.02122165664044424,
      "loss": 0.0,
      "step": 85120
    },
    {
      "epoch": 39.39379916705229,
      "grad_norm": 0.0028241435065865517,
      "learning_rate": 0.02121240166589542,
      "loss": 0.0005,
      "step": 85130
    },
    {
      "epoch": 39.3984266543267,
      "grad_norm": 0.008879569359123707,
      "learning_rate": 0.0212031466913466,
      "loss": 0.0001,
      "step": 85140
    },
    {
      "epoch": 39.40305414160111,
      "grad_norm": 0.0039812601171433926,
      "learning_rate": 0.02119389171679778,
      "loss": 0.0001,
      "step": 85150
    },
    {
      "epoch": 39.40768162887552,
      "grad_norm": 0.0021038788836449385,
      "learning_rate": 0.02118463674224896,
      "loss": 0.0002,
      "step": 85160
    },
    {
      "epoch": 39.41230911614993,
      "grad_norm": 0.010158064775168896,
      "learning_rate": 0.02117538176770014,
      "loss": 0.0001,
      "step": 85170
    },
    {
      "epoch": 39.41693660342434,
      "grad_norm": 0.002483090152963996,
      "learning_rate": 0.02116612679315132,
      "loss": 0.0004,
      "step": 85180
    },
    {
      "epoch": 39.42156409069875,
      "grad_norm": 0.004572092089802027,
      "learning_rate": 0.0211568718186025,
      "loss": 0.0,
      "step": 85190
    },
    {
      "epoch": 39.426191577973164,
      "grad_norm": 0.005418130196630955,
      "learning_rate": 0.021147616844053682,
      "loss": 0.0001,
      "step": 85200
    },
    {
      "epoch": 39.43081906524757,
      "grad_norm": 0.0074788471683859825,
      "learning_rate": 0.021138361869504862,
      "loss": 0.0001,
      "step": 85210
    },
    {
      "epoch": 39.43544655252198,
      "grad_norm": 0.009021379984915257,
      "learning_rate": 0.02112910689495604,
      "loss": 0.0,
      "step": 85220
    },
    {
      "epoch": 39.44007403979639,
      "grad_norm": 0.030664755031466484,
      "learning_rate": 0.021119851920407218,
      "loss": 0.0001,
      "step": 85230
    },
    {
      "epoch": 39.4447015270708,
      "grad_norm": 0.0006706145359203219,
      "learning_rate": 0.0211105969458584,
      "loss": 0.0,
      "step": 85240
    },
    {
      "epoch": 39.44932901434521,
      "grad_norm": 0.005099516827613115,
      "learning_rate": 0.02110134197130958,
      "loss": 0.0001,
      "step": 85250
    },
    {
      "epoch": 39.45395650161962,
      "grad_norm": 0.0011993739753961563,
      "learning_rate": 0.02109208699676076,
      "loss": 0.0001,
      "step": 85260
    },
    {
      "epoch": 39.45858398889403,
      "grad_norm": 0.0035290326923131943,
      "learning_rate": 0.02108283202221194,
      "loss": 0.0001,
      "step": 85270
    },
    {
      "epoch": 39.46321147616844,
      "grad_norm": 0.018940964713692665,
      "learning_rate": 0.02107357704766312,
      "loss": 0.0001,
      "step": 85280
    },
    {
      "epoch": 39.46783896344285,
      "grad_norm": 0.0022933371365070343,
      "learning_rate": 0.021064322073114303,
      "loss": 0.0016,
      "step": 85290
    },
    {
      "epoch": 39.47246645071726,
      "grad_norm": 0.0027111906092613935,
      "learning_rate": 0.021055067098565482,
      "loss": 0.0002,
      "step": 85300
    },
    {
      "epoch": 39.47709393799167,
      "grad_norm": 0.022959934547543526,
      "learning_rate": 0.02104581212401666,
      "loss": 0.0001,
      "step": 85310
    },
    {
      "epoch": 39.48172142526608,
      "grad_norm": 0.19672788679599762,
      "learning_rate": 0.02103655714946784,
      "loss": 0.0005,
      "step": 85320
    },
    {
      "epoch": 39.48634891254049,
      "grad_norm": 0.0008674534037709236,
      "learning_rate": 0.02102730217491902,
      "loss": 0.0,
      "step": 85330
    },
    {
      "epoch": 39.4909763998149,
      "grad_norm": 0.003461637068539858,
      "learning_rate": 0.0210180472003702,
      "loss": 0.0001,
      "step": 85340
    },
    {
      "epoch": 39.49560388708931,
      "grad_norm": 0.0029597424436360598,
      "learning_rate": 0.02100879222582138,
      "loss": 0.0001,
      "step": 85350
    },
    {
      "epoch": 39.50023137436372,
      "grad_norm": 0.0028798857238143682,
      "learning_rate": 0.02099953725127256,
      "loss": 0.0048,
      "step": 85360
    },
    {
      "epoch": 39.504858861638134,
      "grad_norm": 0.005415347870439291,
      "learning_rate": 0.02099028227672374,
      "loss": 0.0001,
      "step": 85370
    },
    {
      "epoch": 39.50948634891254,
      "grad_norm": 0.0030152718536555767,
      "learning_rate": 0.02098102730217492,
      "loss": 0.0001,
      "step": 85380
    },
    {
      "epoch": 39.51411383618695,
      "grad_norm": 0.02810746245086193,
      "learning_rate": 0.0209717723276261,
      "loss": 0.0001,
      "step": 85390
    },
    {
      "epoch": 39.51874132346136,
      "grad_norm": 0.020606445148587227,
      "learning_rate": 0.02096251735307728,
      "loss": 0.0001,
      "step": 85400
    },
    {
      "epoch": 39.52336881073577,
      "grad_norm": 0.0049042790196835995,
      "learning_rate": 0.02095326237852846,
      "loss": 0.0001,
      "step": 85410
    },
    {
      "epoch": 39.52799629801018,
      "grad_norm": 0.009404703043401241,
      "learning_rate": 0.020944007403979642,
      "loss": 0.0003,
      "step": 85420
    },
    {
      "epoch": 39.53262378528459,
      "grad_norm": 0.04792393743991852,
      "learning_rate": 0.02093475242943082,
      "loss": 0.0002,
      "step": 85430
    },
    {
      "epoch": 39.537251272559,
      "grad_norm": 0.022703880444169044,
      "learning_rate": 0.020925497454882,
      "loss": 0.0,
      "step": 85440
    },
    {
      "epoch": 39.54187875983341,
      "grad_norm": 0.004013041965663433,
      "learning_rate": 0.02091624248033318,
      "loss": 0.0001,
      "step": 85450
    },
    {
      "epoch": 39.54650624710782,
      "grad_norm": 0.026923544704914093,
      "learning_rate": 0.02090698750578436,
      "loss": 0.0001,
      "step": 85460
    },
    {
      "epoch": 39.55113373438223,
      "grad_norm": 0.7709866762161255,
      "learning_rate": 0.02089773253123554,
      "loss": 0.0002,
      "step": 85470
    },
    {
      "epoch": 39.55576122165664,
      "grad_norm": 0.021397240459918976,
      "learning_rate": 0.02088847755668672,
      "loss": 0.0001,
      "step": 85480
    },
    {
      "epoch": 39.56038870893105,
      "grad_norm": 0.0027345342095941305,
      "learning_rate": 0.0208792225821379,
      "loss": 0.001,
      "step": 85490
    },
    {
      "epoch": 39.56501619620546,
      "grad_norm": 0.003327945712953806,
      "learning_rate": 0.02086996760758908,
      "loss": 0.0,
      "step": 85500
    },
    {
      "epoch": 39.56964368347987,
      "grad_norm": 0.020672405138611794,
      "learning_rate": 0.020860712633040263,
      "loss": 0.0001,
      "step": 85510
    },
    {
      "epoch": 39.57427117075428,
      "grad_norm": 0.0038807569071650505,
      "learning_rate": 0.020851457658491442,
      "loss": 0.0001,
      "step": 85520
    },
    {
      "epoch": 39.57889865802869,
      "grad_norm": 0.01516072079539299,
      "learning_rate": 0.020842202683942622,
      "loss": 0.0001,
      "step": 85530
    },
    {
      "epoch": 39.583526145303104,
      "grad_norm": 0.08650597184896469,
      "learning_rate": 0.020832947709393798,
      "loss": 0.0002,
      "step": 85540
    },
    {
      "epoch": 39.58815363257751,
      "grad_norm": 0.010779673233628273,
      "learning_rate": 0.02082369273484498,
      "loss": 0.0001,
      "step": 85550
    },
    {
      "epoch": 39.59278111985192,
      "grad_norm": 0.0014545123558491468,
      "learning_rate": 0.02081443776029616,
      "loss": 0.0001,
      "step": 85560
    },
    {
      "epoch": 39.59740860712633,
      "grad_norm": 0.010907694697380066,
      "learning_rate": 0.02080518278574734,
      "loss": 0.0003,
      "step": 85570
    },
    {
      "epoch": 39.60203609440074,
      "grad_norm": 0.0010302335722371936,
      "learning_rate": 0.02079592781119852,
      "loss": 0.0,
      "step": 85580
    },
    {
      "epoch": 39.60666358167515,
      "grad_norm": 0.003945783246308565,
      "learning_rate": 0.0207866728366497,
      "loss": 0.0001,
      "step": 85590
    },
    {
      "epoch": 39.61129106894956,
      "grad_norm": 0.0066665345802903175,
      "learning_rate": 0.020777417862100883,
      "loss": 0.0,
      "step": 85600
    },
    {
      "epoch": 39.61591855622397,
      "grad_norm": 0.2023548036813736,
      "learning_rate": 0.02076816288755206,
      "loss": 0.0002,
      "step": 85610
    },
    {
      "epoch": 39.62054604349838,
      "grad_norm": 0.0011461875401437283,
      "learning_rate": 0.02075890791300324,
      "loss": 0.0001,
      "step": 85620
    },
    {
      "epoch": 39.62517353077279,
      "grad_norm": 0.01704568788409233,
      "learning_rate": 0.02074965293845442,
      "loss": 0.0003,
      "step": 85630
    },
    {
      "epoch": 39.6298010180472,
      "grad_norm": 0.0184306763112545,
      "learning_rate": 0.020740397963905602,
      "loss": 0.0001,
      "step": 85640
    },
    {
      "epoch": 39.63442850532161,
      "grad_norm": 0.12470895797014236,
      "learning_rate": 0.02073114298935678,
      "loss": 0.0001,
      "step": 85650
    },
    {
      "epoch": 39.63905599259602,
      "grad_norm": 0.0042493208311498165,
      "learning_rate": 0.02072188801480796,
      "loss": 0.0001,
      "step": 85660
    },
    {
      "epoch": 39.64368347987043,
      "grad_norm": 0.0027839355170726776,
      "learning_rate": 0.02071263304025914,
      "loss": 0.0001,
      "step": 85670
    },
    {
      "epoch": 39.64831096714484,
      "grad_norm": 0.000756469729822129,
      "learning_rate": 0.02070337806571032,
      "loss": 0.0001,
      "step": 85680
    },
    {
      "epoch": 39.65293845441925,
      "grad_norm": 0.0013069143751636147,
      "learning_rate": 0.0206941230911615,
      "loss": 0.0001,
      "step": 85690
    },
    {
      "epoch": 39.65756594169366,
      "grad_norm": 0.03139256685972214,
      "learning_rate": 0.02068486811661268,
      "loss": 0.0001,
      "step": 85700
    },
    {
      "epoch": 39.662193428968074,
      "grad_norm": 0.009474202990531921,
      "learning_rate": 0.02067561314206386,
      "loss": 0.0001,
      "step": 85710
    },
    {
      "epoch": 39.66682091624248,
      "grad_norm": 0.08835434913635254,
      "learning_rate": 0.02066635816751504,
      "loss": 0.0003,
      "step": 85720
    },
    {
      "epoch": 39.67144840351689,
      "grad_norm": 0.005021947436034679,
      "learning_rate": 0.020657103192966222,
      "loss": 0.0,
      "step": 85730
    },
    {
      "epoch": 39.6760758907913,
      "grad_norm": 0.009035770781338215,
      "learning_rate": 0.020647848218417402,
      "loss": 0.0001,
      "step": 85740
    },
    {
      "epoch": 39.68070337806571,
      "grad_norm": 0.0045264107175171375,
      "learning_rate": 0.020638593243868582,
      "loss": 0.0001,
      "step": 85750
    },
    {
      "epoch": 39.68533086534012,
      "grad_norm": 0.022926870733499527,
      "learning_rate": 0.020629338269319758,
      "loss": 0.0001,
      "step": 85760
    },
    {
      "epoch": 39.68995835261453,
      "grad_norm": 0.0026938789524137974,
      "learning_rate": 0.02062008329477094,
      "loss": 0.0001,
      "step": 85770
    },
    {
      "epoch": 39.69458583988894,
      "grad_norm": 0.03807688131928444,
      "learning_rate": 0.02061082832022212,
      "loss": 0.0001,
      "step": 85780
    },
    {
      "epoch": 39.69921332716335,
      "grad_norm": 0.006019202992320061,
      "learning_rate": 0.0206015733456733,
      "loss": 0.0001,
      "step": 85790
    },
    {
      "epoch": 39.70384081443776,
      "grad_norm": 0.04198336601257324,
      "learning_rate": 0.02059231837112448,
      "loss": 0.0001,
      "step": 85800
    },
    {
      "epoch": 39.70846830171217,
      "grad_norm": 0.01690853014588356,
      "learning_rate": 0.02058306339657566,
      "loss": 0.0001,
      "step": 85810
    },
    {
      "epoch": 39.71309578898658,
      "grad_norm": 0.05032317340373993,
      "learning_rate": 0.020573808422026843,
      "loss": 0.0001,
      "step": 85820
    },
    {
      "epoch": 39.71772327626099,
      "grad_norm": 0.0008467687875963748,
      "learning_rate": 0.020564553447478023,
      "loss": 0.0,
      "step": 85830
    },
    {
      "epoch": 39.7223507635354,
      "grad_norm": 0.003266827203333378,
      "learning_rate": 0.0205552984729292,
      "loss": 0.0001,
      "step": 85840
    },
    {
      "epoch": 39.72697825080981,
      "grad_norm": 0.03014949895441532,
      "learning_rate": 0.02054604349838038,
      "loss": 0.0001,
      "step": 85850
    },
    {
      "epoch": 39.73160573808422,
      "grad_norm": 0.009211941622197628,
      "learning_rate": 0.02053678852383156,
      "loss": 0.0,
      "step": 85860
    },
    {
      "epoch": 39.73623322535863,
      "grad_norm": 0.055299218744039536,
      "learning_rate": 0.02052753354928274,
      "loss": 0.0001,
      "step": 85870
    },
    {
      "epoch": 39.74086071263304,
      "grad_norm": 0.006755529902875423,
      "learning_rate": 0.02051827857473392,
      "loss": 0.0002,
      "step": 85880
    },
    {
      "epoch": 39.74548819990745,
      "grad_norm": 0.0014565164456143975,
      "learning_rate": 0.0205090236001851,
      "loss": 0.0001,
      "step": 85890
    },
    {
      "epoch": 39.75011568718186,
      "grad_norm": 0.02758626826107502,
      "learning_rate": 0.02049976862563628,
      "loss": 0.0001,
      "step": 85900
    },
    {
      "epoch": 39.75474317445627,
      "grad_norm": 0.0009326705476269126,
      "learning_rate": 0.020490513651087464,
      "loss": 0.0001,
      "step": 85910
    },
    {
      "epoch": 39.75937066173068,
      "grad_norm": 0.003976805601269007,
      "learning_rate": 0.02048125867653864,
      "loss": 0.0,
      "step": 85920
    },
    {
      "epoch": 39.76399814900509,
      "grad_norm": 0.009777949191629887,
      "learning_rate": 0.02047200370198982,
      "loss": 0.0001,
      "step": 85930
    },
    {
      "epoch": 39.7686256362795,
      "grad_norm": 0.00930140633136034,
      "learning_rate": 0.020462748727441,
      "loss": 0.0001,
      "step": 85940
    },
    {
      "epoch": 39.77325312355391,
      "grad_norm": 0.0070239040069282055,
      "learning_rate": 0.020453493752892182,
      "loss": 0.0001,
      "step": 85950
    },
    {
      "epoch": 39.77788061082832,
      "grad_norm": 0.018811078742146492,
      "learning_rate": 0.020444238778343362,
      "loss": 0.0001,
      "step": 85960
    },
    {
      "epoch": 39.78250809810273,
      "grad_norm": 0.009444862604141235,
      "learning_rate": 0.02043498380379454,
      "loss": 0.0001,
      "step": 85970
    },
    {
      "epoch": 39.78713558537714,
      "grad_norm": 0.005678436253219843,
      "learning_rate": 0.02042572882924572,
      "loss": 0.0,
      "step": 85980
    },
    {
      "epoch": 39.79176307265155,
      "grad_norm": 0.0808384120464325,
      "learning_rate": 0.0204164738546969,
      "loss": 0.0001,
      "step": 85990
    },
    {
      "epoch": 39.79639055992596,
      "grad_norm": 0.008020114153623581,
      "learning_rate": 0.02040721888014808,
      "loss": 0.0001,
      "step": 86000
    },
    {
      "epoch": 39.80101804720037,
      "grad_norm": 0.0017874969635158777,
      "learning_rate": 0.02039796390559926,
      "loss": 0.0001,
      "step": 86010
    },
    {
      "epoch": 39.80564553447478,
      "grad_norm": 0.07989755272865295,
      "learning_rate": 0.02038870893105044,
      "loss": 0.0001,
      "step": 86020
    },
    {
      "epoch": 39.81027302174919,
      "grad_norm": 0.003603092860430479,
      "learning_rate": 0.02037945395650162,
      "loss": 0.0001,
      "step": 86030
    },
    {
      "epoch": 39.8149005090236,
      "grad_norm": 0.0014027560828253627,
      "learning_rate": 0.020370198981952803,
      "loss": 0.0001,
      "step": 86040
    },
    {
      "epoch": 39.81952799629801,
      "grad_norm": 0.016485679894685745,
      "learning_rate": 0.020360944007403983,
      "loss": 0.0002,
      "step": 86050
    },
    {
      "epoch": 39.82415548357242,
      "grad_norm": 0.004282285459339619,
      "learning_rate": 0.020351689032855162,
      "loss": 0.0001,
      "step": 86060
    },
    {
      "epoch": 39.82878297084683,
      "grad_norm": 0.3499287962913513,
      "learning_rate": 0.02034243405830634,
      "loss": 0.0002,
      "step": 86070
    },
    {
      "epoch": 39.83341045812124,
      "grad_norm": 0.007076127454638481,
      "learning_rate": 0.02033317908375752,
      "loss": 0.0002,
      "step": 86080
    },
    {
      "epoch": 39.83803794539565,
      "grad_norm": 0.021670488640666008,
      "learning_rate": 0.0203239241092087,
      "loss": 0.0,
      "step": 86090
    },
    {
      "epoch": 39.84266543267006,
      "grad_norm": 0.012325411662459373,
      "learning_rate": 0.02031466913465988,
      "loss": 0.0001,
      "step": 86100
    },
    {
      "epoch": 39.84729291994447,
      "grad_norm": 0.31833621859550476,
      "learning_rate": 0.02030541416011106,
      "loss": 0.0002,
      "step": 86110
    },
    {
      "epoch": 39.85192040721888,
      "grad_norm": 0.007093972060829401,
      "learning_rate": 0.02029615918556224,
      "loss": 0.0001,
      "step": 86120
    },
    {
      "epoch": 39.85654789449329,
      "grad_norm": 0.001976474653929472,
      "learning_rate": 0.020286904211013423,
      "loss": 0.0,
      "step": 86130
    },
    {
      "epoch": 39.8611753817677,
      "grad_norm": 0.0007930388092063367,
      "learning_rate": 0.020277649236464603,
      "loss": 0.0001,
      "step": 86140
    },
    {
      "epoch": 39.86580286904211,
      "grad_norm": 0.00047552710748277605,
      "learning_rate": 0.02026839426191578,
      "loss": 0.0001,
      "step": 86150
    },
    {
      "epoch": 39.87043035631652,
      "grad_norm": 0.003947450779378414,
      "learning_rate": 0.02025913928736696,
      "loss": 0.0,
      "step": 86160
    },
    {
      "epoch": 39.87505784359093,
      "grad_norm": 0.0013694445369765162,
      "learning_rate": 0.020249884312818142,
      "loss": 0.0,
      "step": 86170
    },
    {
      "epoch": 39.87968533086534,
      "grad_norm": 0.012156769633293152,
      "learning_rate": 0.020240629338269322,
      "loss": 0.0001,
      "step": 86180
    },
    {
      "epoch": 39.88431281813975,
      "grad_norm": 0.010496917180716991,
      "learning_rate": 0.0202313743637205,
      "loss": 0.0002,
      "step": 86190
    },
    {
      "epoch": 39.88894030541416,
      "grad_norm": 0.11083200573921204,
      "learning_rate": 0.02022211938917168,
      "loss": 0.0004,
      "step": 86200
    },
    {
      "epoch": 39.89356779268857,
      "grad_norm": 0.013481256552040577,
      "learning_rate": 0.02021286441462286,
      "loss": 0.0001,
      "step": 86210
    },
    {
      "epoch": 39.89819527996298,
      "grad_norm": 0.03322600945830345,
      "learning_rate": 0.02020360944007404,
      "loss": 0.0001,
      "step": 86220
    },
    {
      "epoch": 39.90282276723739,
      "grad_norm": 0.009243533946573734,
      "learning_rate": 0.02019435446552522,
      "loss": 0.0002,
      "step": 86230
    },
    {
      "epoch": 39.9074502545118,
      "grad_norm": 0.019400829449295998,
      "learning_rate": 0.0201850994909764,
      "loss": 0.0001,
      "step": 86240
    },
    {
      "epoch": 39.91207774178621,
      "grad_norm": 0.006286703981459141,
      "learning_rate": 0.02017584451642758,
      "loss": 0.0,
      "step": 86250
    },
    {
      "epoch": 39.91670522906062,
      "grad_norm": 0.0021260653156787157,
      "learning_rate": 0.020166589541878763,
      "loss": 0.0,
      "step": 86260
    },
    {
      "epoch": 39.92133271633503,
      "grad_norm": 0.00914587453007698,
      "learning_rate": 0.020157334567329942,
      "loss": 0.0001,
      "step": 86270
    },
    {
      "epoch": 39.92596020360944,
      "grad_norm": 0.003268797416239977,
      "learning_rate": 0.020148079592781122,
      "loss": 0.0002,
      "step": 86280
    },
    {
      "epoch": 39.93058769088385,
      "grad_norm": 0.028683239594101906,
      "learning_rate": 0.020138824618232302,
      "loss": 0.0001,
      "step": 86290
    },
    {
      "epoch": 39.93521517815826,
      "grad_norm": 0.013584469445049763,
      "learning_rate": 0.02012956964368348,
      "loss": 0.0002,
      "step": 86300
    },
    {
      "epoch": 39.93984266543267,
      "grad_norm": 0.004316449165344238,
      "learning_rate": 0.02012031466913466,
      "loss": 0.0001,
      "step": 86310
    },
    {
      "epoch": 39.94447015270708,
      "grad_norm": 0.034362345933914185,
      "learning_rate": 0.02011105969458584,
      "loss": 0.0001,
      "step": 86320
    },
    {
      "epoch": 39.94909763998149,
      "grad_norm": 0.0030134720727801323,
      "learning_rate": 0.02010180472003702,
      "loss": 0.0001,
      "step": 86330
    },
    {
      "epoch": 39.9537251272559,
      "grad_norm": 1.6803117990493774,
      "learning_rate": 0.0200925497454882,
      "loss": 0.0006,
      "step": 86340
    },
    {
      "epoch": 39.95835261453031,
      "grad_norm": 0.004313024692237377,
      "learning_rate": 0.020083294770939383,
      "loss": 0.0001,
      "step": 86350
    },
    {
      "epoch": 39.96298010180472,
      "grad_norm": 0.010974015109241009,
      "learning_rate": 0.020074039796390563,
      "loss": 0.0,
      "step": 86360
    },
    {
      "epoch": 39.96760758907913,
      "grad_norm": 0.0012350077740848064,
      "learning_rate": 0.020064784821841743,
      "loss": 0.0,
      "step": 86370
    },
    {
      "epoch": 39.97223507635354,
      "grad_norm": 0.0045458534732460976,
      "learning_rate": 0.02005552984729292,
      "loss": 0.0001,
      "step": 86380
    },
    {
      "epoch": 39.97686256362795,
      "grad_norm": 0.04466938599944115,
      "learning_rate": 0.020046274872744102,
      "loss": 0.0001,
      "step": 86390
    },
    {
      "epoch": 39.98149005090236,
      "grad_norm": 0.002964586950838566,
      "learning_rate": 0.02003701989819528,
      "loss": 0.0001,
      "step": 86400
    },
    {
      "epoch": 39.98611753817677,
      "grad_norm": 0.11783842742443085,
      "learning_rate": 0.02002776492364646,
      "loss": 0.0001,
      "step": 86410
    },
    {
      "epoch": 39.99074502545118,
      "grad_norm": 0.00211557955481112,
      "learning_rate": 0.02001850994909764,
      "loss": 0.0001,
      "step": 86420
    },
    {
      "epoch": 39.99537251272559,
      "grad_norm": 0.012038806453347206,
      "learning_rate": 0.02000925497454882,
      "loss": 0.0001,
      "step": 86430
    },
    {
      "epoch": 40.0,
      "grad_norm": 2.454010248184204,
      "learning_rate": 0.020000000000000004,
      "loss": 0.0016,
      "step": 86440
    },
    {
      "epoch": 40.0,
      "eval_accuracy_branch1": 0.9891388075797257,
      "eval_accuracy_branch2": 0.4996918810660915,
      "eval_f1_branch1": 0.9903600795303191,
      "eval_f1_branch2": 0.49926058199579026,
      "eval_loss": 0.022283699363470078,
      "eval_precision_branch1": 0.9907285357291844,
      "eval_precision_branch2": 0.4996908158346496,
      "eval_recall_branch1": 0.9901392298875952,
      "eval_recall_branch2": 0.4996918810660915,
      "eval_runtime": 29.1092,
      "eval_samples_per_second": 445.976,
      "eval_steps_per_second": 55.756,
      "step": 86440
    },
    {
      "epoch": 40.00462748727441,
      "grad_norm": 0.0009100722963921726,
      "learning_rate": 0.01999074502545118,
      "loss": 0.0002,
      "step": 86450
    },
    {
      "epoch": 40.00925497454882,
      "grad_norm": 0.024670587852597237,
      "learning_rate": 0.01998149005090236,
      "loss": 0.0001,
      "step": 86460
    },
    {
      "epoch": 40.01388246182323,
      "grad_norm": 0.003276996547356248,
      "learning_rate": 0.01997223507635354,
      "loss": 0.0001,
      "step": 86470
    },
    {
      "epoch": 40.01850994909764,
      "grad_norm": 0.013532917946577072,
      "learning_rate": 0.019962980101804723,
      "loss": 0.0002,
      "step": 86480
    },
    {
      "epoch": 40.02313743637205,
      "grad_norm": 0.04977474361658096,
      "learning_rate": 0.019953725127255902,
      "loss": 0.0001,
      "step": 86490
    },
    {
      "epoch": 40.02776492364646,
      "grad_norm": 0.0032637363765388727,
      "learning_rate": 0.019944470152707082,
      "loss": 0.0002,
      "step": 86500
    },
    {
      "epoch": 40.03239241092087,
      "grad_norm": 0.020719224587082863,
      "learning_rate": 0.01993521517815826,
      "loss": 0.0,
      "step": 86510
    },
    {
      "epoch": 40.03701989819528,
      "grad_norm": 0.025494610890746117,
      "learning_rate": 0.01992596020360944,
      "loss": 0.0001,
      "step": 86520
    },
    {
      "epoch": 40.04164738546969,
      "grad_norm": 0.002568052615970373,
      "learning_rate": 0.01991670522906062,
      "loss": 0.0003,
      "step": 86530
    },
    {
      "epoch": 40.0462748727441,
      "grad_norm": 0.10579035431146622,
      "learning_rate": 0.0199074502545118,
      "loss": 0.0002,
      "step": 86540
    },
    {
      "epoch": 40.05090236001851,
      "grad_norm": 0.008905572816729546,
      "learning_rate": 0.01989819527996298,
      "loss": 0.0001,
      "step": 86550
    },
    {
      "epoch": 40.05552984729292,
      "grad_norm": 0.005299230571836233,
      "learning_rate": 0.01988894030541416,
      "loss": 0.0001,
      "step": 86560
    },
    {
      "epoch": 40.06015733456733,
      "grad_norm": 0.0006683359388262033,
      "learning_rate": 0.019879685330865343,
      "loss": 0.0001,
      "step": 86570
    },
    {
      "epoch": 40.06478482184174,
      "grad_norm": 0.0014267573133111,
      "learning_rate": 0.019870430356316523,
      "loss": 0.0001,
      "step": 86580
    },
    {
      "epoch": 40.06941230911615,
      "grad_norm": 0.05648421868681908,
      "learning_rate": 0.019861175381767703,
      "loss": 0.0001,
      "step": 86590
    },
    {
      "epoch": 40.07403979639056,
      "grad_norm": 0.00580229377374053,
      "learning_rate": 0.019851920407218882,
      "loss": 0.0001,
      "step": 86600
    },
    {
      "epoch": 40.07866728366497,
      "grad_norm": 0.003772325813770294,
      "learning_rate": 0.019842665432670062,
      "loss": 0.0,
      "step": 86610
    },
    {
      "epoch": 40.08329477093938,
      "grad_norm": 0.07574314624071121,
      "learning_rate": 0.01983341045812124,
      "loss": 0.0001,
      "step": 86620
    },
    {
      "epoch": 40.08792225821379,
      "grad_norm": 0.006591545883566141,
      "learning_rate": 0.01982415548357242,
      "loss": 0.0001,
      "step": 86630
    },
    {
      "epoch": 40.0925497454882,
      "grad_norm": 0.0005568687338382006,
      "learning_rate": 0.0198149005090236,
      "loss": 0.0001,
      "step": 86640
    },
    {
      "epoch": 40.09717723276261,
      "grad_norm": 0.0021186431404203176,
      "learning_rate": 0.01980564553447478,
      "loss": 0.0001,
      "step": 86650
    },
    {
      "epoch": 40.10180472003702,
      "grad_norm": 0.17512203752994537,
      "learning_rate": 0.019796390559925964,
      "loss": 0.0001,
      "step": 86660
    },
    {
      "epoch": 40.10643220731143,
      "grad_norm": 0.0301149133592844,
      "learning_rate": 0.019787135585377143,
      "loss": 0.0,
      "step": 86670
    },
    {
      "epoch": 40.11105969458584,
      "grad_norm": 0.004094460979104042,
      "learning_rate": 0.01977788061082832,
      "loss": 0.0,
      "step": 86680
    },
    {
      "epoch": 40.11568718186025,
      "grad_norm": 0.007355338893830776,
      "learning_rate": 0.0197686256362795,
      "loss": 0.0001,
      "step": 86690
    },
    {
      "epoch": 40.12031466913466,
      "grad_norm": 0.01650826260447502,
      "learning_rate": 0.019759370661730682,
      "loss": 0.0003,
      "step": 86700
    },
    {
      "epoch": 40.12494215640907,
      "grad_norm": 0.009518349543213844,
      "learning_rate": 0.019750115687181862,
      "loss": 0.0017,
      "step": 86710
    },
    {
      "epoch": 40.12956964368348,
      "grad_norm": 0.0007202285341918468,
      "learning_rate": 0.019740860712633042,
      "loss": 0.0001,
      "step": 86720
    },
    {
      "epoch": 40.13419713095789,
      "grad_norm": 0.010705996304750443,
      "learning_rate": 0.01973160573808422,
      "loss": 0.0004,
      "step": 86730
    },
    {
      "epoch": 40.1388246182323,
      "grad_norm": 0.01139815617352724,
      "learning_rate": 0.0197223507635354,
      "loss": 0.0002,
      "step": 86740
    },
    {
      "epoch": 40.14345210550671,
      "grad_norm": 0.009522045962512493,
      "learning_rate": 0.019713095788986584,
      "loss": 0.0001,
      "step": 86750
    },
    {
      "epoch": 40.14807959278112,
      "grad_norm": 0.0022714310325682163,
      "learning_rate": 0.01970384081443776,
      "loss": 0.0,
      "step": 86760
    },
    {
      "epoch": 40.15270708005553,
      "grad_norm": 0.01832141913473606,
      "learning_rate": 0.01969458583988894,
      "loss": 0.0001,
      "step": 86770
    },
    {
      "epoch": 40.15733456732994,
      "grad_norm": 0.0031772551592439413,
      "learning_rate": 0.01968533086534012,
      "loss": 0.0001,
      "step": 86780
    },
    {
      "epoch": 40.16196205460435,
      "grad_norm": 0.004639945924282074,
      "learning_rate": 0.019676075890791303,
      "loss": 0.0001,
      "step": 86790
    },
    {
      "epoch": 40.16658954187876,
      "grad_norm": 0.008832933381199837,
      "learning_rate": 0.019666820916242483,
      "loss": 0.0001,
      "step": 86800
    },
    {
      "epoch": 40.17121702915317,
      "grad_norm": 0.012722698040306568,
      "learning_rate": 0.019657565941693662,
      "loss": 0.0001,
      "step": 86810
    },
    {
      "epoch": 40.17584451642758,
      "grad_norm": 0.0034319786354899406,
      "learning_rate": 0.019648310967144842,
      "loss": 0.0001,
      "step": 86820
    },
    {
      "epoch": 40.18047200370199,
      "grad_norm": 0.00436793128028512,
      "learning_rate": 0.019639055992596022,
      "loss": 0.0007,
      "step": 86830
    },
    {
      "epoch": 40.1850994909764,
      "grad_norm": 0.0007664770819246769,
      "learning_rate": 0.0196298010180472,
      "loss": 0.0001,
      "step": 86840
    },
    {
      "epoch": 40.18972697825081,
      "grad_norm": 0.01512905117124319,
      "learning_rate": 0.01962054604349838,
      "loss": 0.0001,
      "step": 86850
    },
    {
      "epoch": 40.19435446552522,
      "grad_norm": 0.035207316279411316,
      "learning_rate": 0.01961129106894956,
      "loss": 0.0001,
      "step": 86860
    },
    {
      "epoch": 40.19898195279963,
      "grad_norm": 0.007629638072103262,
      "learning_rate": 0.01960203609440074,
      "loss": 0.0001,
      "step": 86870
    },
    {
      "epoch": 40.20360944007404,
      "grad_norm": 0.002037795027717948,
      "learning_rate": 0.019592781119851924,
      "loss": 0.0,
      "step": 86880
    },
    {
      "epoch": 40.20823692734845,
      "grad_norm": 0.0030633455608040094,
      "learning_rate": 0.019583526145303103,
      "loss": 0.0001,
      "step": 86890
    },
    {
      "epoch": 40.21286441462286,
      "grad_norm": 0.006698332726955414,
      "learning_rate": 0.019574271170754283,
      "loss": 0.0001,
      "step": 86900
    },
    {
      "epoch": 40.21749190189727,
      "grad_norm": 0.011415084823966026,
      "learning_rate": 0.01956501619620546,
      "loss": 0.0,
      "step": 86910
    },
    {
      "epoch": 40.22211938917168,
      "grad_norm": 0.01651412434875965,
      "learning_rate": 0.019555761221656642,
      "loss": 0.0001,
      "step": 86920
    },
    {
      "epoch": 40.22674687644609,
      "grad_norm": 0.012539144605398178,
      "learning_rate": 0.019546506247107822,
      "loss": 0.0,
      "step": 86930
    },
    {
      "epoch": 40.2313743637205,
      "grad_norm": 0.020444698631763458,
      "learning_rate": 0.019537251272559,
      "loss": 0.0001,
      "step": 86940
    },
    {
      "epoch": 40.23600185099491,
      "grad_norm": 0.14701561629772186,
      "learning_rate": 0.01952799629801018,
      "loss": 0.0001,
      "step": 86950
    },
    {
      "epoch": 40.24062933826932,
      "grad_norm": 0.0018901013536378741,
      "learning_rate": 0.01951874132346136,
      "loss": 0.0001,
      "step": 86960
    },
    {
      "epoch": 40.24525682554373,
      "grad_norm": 0.037292640656232834,
      "learning_rate": 0.019509486348912544,
      "loss": 0.0004,
      "step": 86970
    },
    {
      "epoch": 40.24988431281814,
      "grad_norm": 0.15620732307434082,
      "learning_rate": 0.019500231374363724,
      "loss": 0.0001,
      "step": 86980
    },
    {
      "epoch": 40.25451180009255,
      "grad_norm": 0.03327466920018196,
      "learning_rate": 0.0194909763998149,
      "loss": 0.0003,
      "step": 86990
    },
    {
      "epoch": 40.259139287366956,
      "grad_norm": 0.017024880275130272,
      "learning_rate": 0.01948172142526608,
      "loss": 0.0001,
      "step": 87000
    },
    {
      "epoch": 40.26376677464137,
      "grad_norm": 0.0046877870336174965,
      "learning_rate": 0.019472466450717263,
      "loss": 0.0002,
      "step": 87010
    },
    {
      "epoch": 40.26839426191578,
      "grad_norm": 0.008544511161744595,
      "learning_rate": 0.019463211476168443,
      "loss": 0.0001,
      "step": 87020
    },
    {
      "epoch": 40.27302174919019,
      "grad_norm": 0.0029346407391130924,
      "learning_rate": 0.019453956501619622,
      "loss": 0.0001,
      "step": 87030
    },
    {
      "epoch": 40.2776492364646,
      "grad_norm": 0.05750579386949539,
      "learning_rate": 0.019444701527070802,
      "loss": 0.0001,
      "step": 87040
    },
    {
      "epoch": 40.28227672373901,
      "grad_norm": 0.00643639313057065,
      "learning_rate": 0.01943544655252198,
      "loss": 0.0,
      "step": 87050
    },
    {
      "epoch": 40.28690421101342,
      "grad_norm": 0.023529881611466408,
      "learning_rate": 0.01942619157797316,
      "loss": 0.0001,
      "step": 87060
    },
    {
      "epoch": 40.29153169828783,
      "grad_norm": 0.060468923300504684,
      "learning_rate": 0.01941693660342434,
      "loss": 0.0002,
      "step": 87070
    },
    {
      "epoch": 40.29615918556224,
      "grad_norm": 0.003415358252823353,
      "learning_rate": 0.01940768162887552,
      "loss": 0.0001,
      "step": 87080
    },
    {
      "epoch": 40.30078667283665,
      "grad_norm": 0.0019690608605742455,
      "learning_rate": 0.0193984266543267,
      "loss": 0.0001,
      "step": 87090
    },
    {
      "epoch": 40.30541416011106,
      "grad_norm": 0.0017206171760335565,
      "learning_rate": 0.019389171679777883,
      "loss": 0.0001,
      "step": 87100
    },
    {
      "epoch": 40.31004164738547,
      "grad_norm": 0.016801485791802406,
      "learning_rate": 0.019379916705229063,
      "loss": 0.0001,
      "step": 87110
    },
    {
      "epoch": 40.31466913465988,
      "grad_norm": 0.0072481692768633366,
      "learning_rate": 0.019370661730680243,
      "loss": 0.0001,
      "step": 87120
    },
    {
      "epoch": 40.31929662193429,
      "grad_norm": 0.0030994636472314596,
      "learning_rate": 0.019361406756131423,
      "loss": 0.0003,
      "step": 87130
    },
    {
      "epoch": 40.3239241092087,
      "grad_norm": 0.0035689943470060825,
      "learning_rate": 0.019352151781582602,
      "loss": 0.0001,
      "step": 87140
    },
    {
      "epoch": 40.32855159648311,
      "grad_norm": 0.012974241748452187,
      "learning_rate": 0.019342896807033782,
      "loss": 0.0003,
      "step": 87150
    },
    {
      "epoch": 40.33317908375752,
      "grad_norm": 0.010233135893940926,
      "learning_rate": 0.01933364183248496,
      "loss": 0.0001,
      "step": 87160
    },
    {
      "epoch": 40.337806571031926,
      "grad_norm": 0.015492437407374382,
      "learning_rate": 0.01932438685793614,
      "loss": 0.0001,
      "step": 87170
    },
    {
      "epoch": 40.34243405830634,
      "grad_norm": 0.029513243585824966,
      "learning_rate": 0.01931513188338732,
      "loss": 0.0001,
      "step": 87180
    },
    {
      "epoch": 40.34706154558075,
      "grad_norm": 0.006791236344724894,
      "learning_rate": 0.019305876908838504,
      "loss": 0.0,
      "step": 87190
    },
    {
      "epoch": 40.35168903285516,
      "grad_norm": 0.027134619653224945,
      "learning_rate": 0.019296621934289684,
      "loss": 0.0001,
      "step": 87200
    },
    {
      "epoch": 40.35631652012957,
      "grad_norm": 0.0024204999208450317,
      "learning_rate": 0.019287366959740863,
      "loss": 0.0001,
      "step": 87210
    },
    {
      "epoch": 40.36094400740398,
      "grad_norm": 0.06944803148508072,
      "learning_rate": 0.01927811198519204,
      "loss": 0.0001,
      "step": 87220
    },
    {
      "epoch": 40.36557149467839,
      "grad_norm": 0.003368764417245984,
      "learning_rate": 0.019268857010643223,
      "loss": 0.0001,
      "step": 87230
    },
    {
      "epoch": 40.3701989819528,
      "grad_norm": 0.038174644112586975,
      "learning_rate": 0.019259602036094402,
      "loss": 0.0001,
      "step": 87240
    },
    {
      "epoch": 40.37482646922721,
      "grad_norm": 0.20162148773670197,
      "learning_rate": 0.019250347061545582,
      "loss": 0.0001,
      "step": 87250
    },
    {
      "epoch": 40.37945395650162,
      "grad_norm": 0.0068236165679991245,
      "learning_rate": 0.019241092086996762,
      "loss": 0.0001,
      "step": 87260
    },
    {
      "epoch": 40.38408144377603,
      "grad_norm": 0.021115992218255997,
      "learning_rate": 0.01923183711244794,
      "loss": 0.0,
      "step": 87270
    },
    {
      "epoch": 40.38870893105044,
      "grad_norm": 0.011708813719451427,
      "learning_rate": 0.019222582137899125,
      "loss": 0.0001,
      "step": 87280
    },
    {
      "epoch": 40.39333641832485,
      "grad_norm": 0.0012538534356281161,
      "learning_rate": 0.0192133271633503,
      "loss": 0.0001,
      "step": 87290
    },
    {
      "epoch": 40.39796390559926,
      "grad_norm": 0.0024805902503430843,
      "learning_rate": 0.01920407218880148,
      "loss": 0.0001,
      "step": 87300
    },
    {
      "epoch": 40.40259139287367,
      "grad_norm": 0.011915761977434158,
      "learning_rate": 0.01919481721425266,
      "loss": 0.0001,
      "step": 87310
    },
    {
      "epoch": 40.40721888014808,
      "grad_norm": 0.00211159186437726,
      "learning_rate": 0.019185562239703843,
      "loss": 0.0001,
      "step": 87320
    },
    {
      "epoch": 40.41184636742249,
      "grad_norm": 0.0007646530284546316,
      "learning_rate": 0.019176307265155023,
      "loss": 0.0,
      "step": 87330
    },
    {
      "epoch": 40.416473854696896,
      "grad_norm": 0.2754157781600952,
      "learning_rate": 0.019167052290606203,
      "loss": 0.0002,
      "step": 87340
    },
    {
      "epoch": 40.42110134197131,
      "grad_norm": 0.0005713641294278204,
      "learning_rate": 0.019157797316057382,
      "loss": 0.0001,
      "step": 87350
    },
    {
      "epoch": 40.42572882924572,
      "grad_norm": 0.0028509930707514286,
      "learning_rate": 0.019148542341508562,
      "loss": 0.0,
      "step": 87360
    },
    {
      "epoch": 40.43035631652013,
      "grad_norm": 0.6045970320701599,
      "learning_rate": 0.01913928736695974,
      "loss": 0.0002,
      "step": 87370
    },
    {
      "epoch": 40.43498380379454,
      "grad_norm": 0.0024612529668956995,
      "learning_rate": 0.01913003239241092,
      "loss": 0.0002,
      "step": 87380
    },
    {
      "epoch": 40.43961129106895,
      "grad_norm": 0.0038352517876774073,
      "learning_rate": 0.0191207774178621,
      "loss": 0.0,
      "step": 87390
    },
    {
      "epoch": 40.44423877834336,
      "grad_norm": 0.0011332344729453325,
      "learning_rate": 0.01911152244331328,
      "loss": 0.0001,
      "step": 87400
    },
    {
      "epoch": 40.44886626561777,
      "grad_norm": 0.017908496782183647,
      "learning_rate": 0.019102267468764464,
      "loss": 0.0001,
      "step": 87410
    },
    {
      "epoch": 40.45349375289218,
      "grad_norm": 0.1261814683675766,
      "learning_rate": 0.019093012494215644,
      "loss": 0.0002,
      "step": 87420
    },
    {
      "epoch": 40.45812124016659,
      "grad_norm": 0.01261599175632,
      "learning_rate": 0.019083757519666823,
      "loss": 0.0001,
      "step": 87430
    },
    {
      "epoch": 40.462748727441,
      "grad_norm": 0.004440435208380222,
      "learning_rate": 0.019074502545118003,
      "loss": 0.0002,
      "step": 87440
    },
    {
      "epoch": 40.46737621471541,
      "grad_norm": 0.002909475937485695,
      "learning_rate": 0.019065247570569183,
      "loss": 0.0001,
      "step": 87450
    },
    {
      "epoch": 40.47200370198982,
      "grad_norm": 0.0026493817567825317,
      "learning_rate": 0.019055992596020362,
      "loss": 0.0007,
      "step": 87460
    },
    {
      "epoch": 40.47663118926423,
      "grad_norm": 0.023873964324593544,
      "learning_rate": 0.019046737621471542,
      "loss": 0.0001,
      "step": 87470
    },
    {
      "epoch": 40.48125867653864,
      "grad_norm": 0.07792944461107254,
      "learning_rate": 0.01903748264692272,
      "loss": 0.0001,
      "step": 87480
    },
    {
      "epoch": 40.48588616381305,
      "grad_norm": 0.0049958620220422745,
      "learning_rate": 0.0190282276723739,
      "loss": 0.0003,
      "step": 87490
    },
    {
      "epoch": 40.49051365108746,
      "grad_norm": 0.0015544157940894365,
      "learning_rate": 0.019018972697825084,
      "loss": 0.0001,
      "step": 87500
    },
    {
      "epoch": 40.495141138361866,
      "grad_norm": 0.028840648010373116,
      "learning_rate": 0.019009717723276264,
      "loss": 0.0001,
      "step": 87510
    },
    {
      "epoch": 40.49976862563628,
      "grad_norm": 0.15621358156204224,
      "learning_rate": 0.01900046274872744,
      "loss": 0.0001,
      "step": 87520
    },
    {
      "epoch": 40.50439611291069,
      "grad_norm": 0.03436754643917084,
      "learning_rate": 0.01899120777417862,
      "loss": 0.0001,
      "step": 87530
    },
    {
      "epoch": 40.5090236001851,
      "grad_norm": 0.002890128642320633,
      "learning_rate": 0.018981952799629803,
      "loss": 0.0001,
      "step": 87540
    },
    {
      "epoch": 40.51365108745951,
      "grad_norm": 0.012194731272757053,
      "learning_rate": 0.018972697825080983,
      "loss": 0.0001,
      "step": 87550
    },
    {
      "epoch": 40.51827857473392,
      "grad_norm": 0.021527333185076714,
      "learning_rate": 0.018963442850532163,
      "loss": 0.0001,
      "step": 87560
    },
    {
      "epoch": 40.52290606200833,
      "grad_norm": 0.002838390413671732,
      "learning_rate": 0.018954187875983342,
      "loss": 0.0002,
      "step": 87570
    },
    {
      "epoch": 40.52753354928274,
      "grad_norm": 0.0017177159897983074,
      "learning_rate": 0.018944932901434522,
      "loss": 0.0001,
      "step": 87580
    },
    {
      "epoch": 40.53216103655715,
      "grad_norm": 1.2090548276901245,
      "learning_rate": 0.018935677926885705,
      "loss": 0.0003,
      "step": 87590
    },
    {
      "epoch": 40.53678852383156,
      "grad_norm": 0.005654092412441969,
      "learning_rate": 0.01892642295233688,
      "loss": 0.0001,
      "step": 87600
    },
    {
      "epoch": 40.54141601110597,
      "grad_norm": 0.00042366780689917505,
      "learning_rate": 0.01891716797778806,
      "loss": 0.0002,
      "step": 87610
    },
    {
      "epoch": 40.54604349838038,
      "grad_norm": 0.0031296759843826294,
      "learning_rate": 0.01890791300323924,
      "loss": 0.0001,
      "step": 87620
    },
    {
      "epoch": 40.55067098565479,
      "grad_norm": 0.0018884279998019338,
      "learning_rate": 0.018898658028690424,
      "loss": 0.0,
      "step": 87630
    },
    {
      "epoch": 40.5552984729292,
      "grad_norm": 0.0009332657209597528,
      "learning_rate": 0.018889403054141603,
      "loss": 0.0,
      "step": 87640
    },
    {
      "epoch": 40.55992596020361,
      "grad_norm": 0.0010297902626916766,
      "learning_rate": 0.018880148079592783,
      "loss": 0.0,
      "step": 87650
    },
    {
      "epoch": 40.56455344747802,
      "grad_norm": 0.0020594443194568157,
      "learning_rate": 0.018870893105043963,
      "loss": 0.0001,
      "step": 87660
    },
    {
      "epoch": 40.56918093475243,
      "grad_norm": 0.026020288467407227,
      "learning_rate": 0.018861638130495142,
      "loss": 0.0001,
      "step": 87670
    },
    {
      "epoch": 40.573808422026836,
      "grad_norm": 0.04947546869516373,
      "learning_rate": 0.018852383155946322,
      "loss": 0.0,
      "step": 87680
    },
    {
      "epoch": 40.57843590930125,
      "grad_norm": 0.005374842323362827,
      "learning_rate": 0.018843128181397502,
      "loss": 0.0001,
      "step": 87690
    },
    {
      "epoch": 40.58306339657566,
      "grad_norm": 0.0028148095589131117,
      "learning_rate": 0.01883387320684868,
      "loss": 0.0006,
      "step": 87700
    },
    {
      "epoch": 40.58769088385007,
      "grad_norm": 0.0021752461325377226,
      "learning_rate": 0.01882461823229986,
      "loss": 0.0001,
      "step": 87710
    },
    {
      "epoch": 40.59231837112448,
      "grad_norm": 0.004816492088139057,
      "learning_rate": 0.018815363257751044,
      "loss": 0.0001,
      "step": 87720
    },
    {
      "epoch": 40.59694585839889,
      "grad_norm": 0.04043284431099892,
      "learning_rate": 0.018806108283202224,
      "loss": 0.0001,
      "step": 87730
    },
    {
      "epoch": 40.6015733456733,
      "grad_norm": 0.04054586961865425,
      "learning_rate": 0.018796853308653404,
      "loss": 0.0002,
      "step": 87740
    },
    {
      "epoch": 40.60620083294771,
      "grad_norm": 0.04946369305253029,
      "learning_rate": 0.01878759833410458,
      "loss": 0.0001,
      "step": 87750
    },
    {
      "epoch": 40.61082832022212,
      "grad_norm": 0.0029320481698960066,
      "learning_rate": 0.018778343359555763,
      "loss": 0.0001,
      "step": 87760
    },
    {
      "epoch": 40.61545580749653,
      "grad_norm": 0.009885871782898903,
      "learning_rate": 0.018769088385006943,
      "loss": 0.0001,
      "step": 87770
    },
    {
      "epoch": 40.620083294770936,
      "grad_norm": 0.018138494342565536,
      "learning_rate": 0.018759833410458122,
      "loss": 0.0001,
      "step": 87780
    },
    {
      "epoch": 40.62471078204535,
      "grad_norm": 0.019816959276795387,
      "learning_rate": 0.018750578435909302,
      "loss": 0.0001,
      "step": 87790
    },
    {
      "epoch": 40.62933826931976,
      "grad_norm": 0.01680053398013115,
      "learning_rate": 0.018741323461360482,
      "loss": 0.0001,
      "step": 87800
    },
    {
      "epoch": 40.63396575659417,
      "grad_norm": 0.0169546976685524,
      "learning_rate": 0.01873206848681166,
      "loss": 0.0001,
      "step": 87810
    },
    {
      "epoch": 40.63859324386858,
      "grad_norm": 0.039076801389455795,
      "learning_rate": 0.018722813512262845,
      "loss": 0.0001,
      "step": 87820
    },
    {
      "epoch": 40.64322073114299,
      "grad_norm": 0.005237034056335688,
      "learning_rate": 0.01871355853771402,
      "loss": 0.0003,
      "step": 87830
    },
    {
      "epoch": 40.6478482184174,
      "grad_norm": 0.00903948675841093,
      "learning_rate": 0.0187043035631652,
      "loss": 0.0001,
      "step": 87840
    },
    {
      "epoch": 40.652475705691806,
      "grad_norm": 0.005190645810216665,
      "learning_rate": 0.01869504858861638,
      "loss": 0.0001,
      "step": 87850
    },
    {
      "epoch": 40.65710319296622,
      "grad_norm": 0.11383616924285889,
      "learning_rate": 0.018685793614067563,
      "loss": 0.0002,
      "step": 87860
    },
    {
      "epoch": 40.66173068024063,
      "grad_norm": 0.01015413273125887,
      "learning_rate": 0.018676538639518743,
      "loss": 0.0001,
      "step": 87870
    },
    {
      "epoch": 40.66635816751504,
      "grad_norm": 0.016266614198684692,
      "learning_rate": 0.018667283664969923,
      "loss": 0.0003,
      "step": 87880
    },
    {
      "epoch": 40.67098565478945,
      "grad_norm": 0.0007612230256199837,
      "learning_rate": 0.018658028690421102,
      "loss": 0.0001,
      "step": 87890
    },
    {
      "epoch": 40.67561314206386,
      "grad_norm": 0.02448250912129879,
      "learning_rate": 0.018648773715872282,
      "loss": 0.0002,
      "step": 87900
    },
    {
      "epoch": 40.68024062933827,
      "grad_norm": 0.019834740087389946,
      "learning_rate": 0.01863951874132346,
      "loss": 0.0001,
      "step": 87910
    },
    {
      "epoch": 40.68486811661268,
      "grad_norm": 0.01136807817965746,
      "learning_rate": 0.01863026376677464,
      "loss": 0.0001,
      "step": 87920
    },
    {
      "epoch": 40.68949560388709,
      "grad_norm": 0.002249055542051792,
      "learning_rate": 0.01862100879222582,
      "loss": 0.0001,
      "step": 87930
    },
    {
      "epoch": 40.6941230911615,
      "grad_norm": 0.4788455069065094,
      "learning_rate": 0.018611753817677,
      "loss": 0.0003,
      "step": 87940
    },
    {
      "epoch": 40.698750578435906,
      "grad_norm": 0.00632834667339921,
      "learning_rate": 0.018602498843128184,
      "loss": 0.0002,
      "step": 87950
    },
    {
      "epoch": 40.70337806571032,
      "grad_norm": 0.0022356684785336256,
      "learning_rate": 0.018593243868579364,
      "loss": 0.0001,
      "step": 87960
    },
    {
      "epoch": 40.70800555298473,
      "grad_norm": 0.007611109409481287,
      "learning_rate": 0.018583988894030543,
      "loss": 0.0001,
      "step": 87970
    },
    {
      "epoch": 40.71263304025914,
      "grad_norm": 0.017670469358563423,
      "learning_rate": 0.01857473391948172,
      "loss": 0.0001,
      "step": 87980
    },
    {
      "epoch": 40.71726052753355,
      "grad_norm": 0.002763471333310008,
      "learning_rate": 0.018565478944932903,
      "loss": 0.0001,
      "step": 87990
    },
    {
      "epoch": 40.72188801480796,
      "grad_norm": 0.0029750841204077005,
      "learning_rate": 0.018556223970384082,
      "loss": 0.0001,
      "step": 88000
    },
    {
      "epoch": 40.72651550208237,
      "grad_norm": 0.0056782872416079044,
      "learning_rate": 0.018546968995835262,
      "loss": 0.0001,
      "step": 88010
    },
    {
      "epoch": 40.731142989356776,
      "grad_norm": 0.007414241787046194,
      "learning_rate": 0.01853771402128644,
      "loss": 0.0001,
      "step": 88020
    },
    {
      "epoch": 40.73577047663119,
      "grad_norm": 0.01241215132176876,
      "learning_rate": 0.01852845904673762,
      "loss": 0.0001,
      "step": 88030
    },
    {
      "epoch": 40.7403979639056,
      "grad_norm": 0.0008107191533781588,
      "learning_rate": 0.018519204072188804,
      "loss": 0.0001,
      "step": 88040
    },
    {
      "epoch": 40.74502545118001,
      "grad_norm": 0.012137151323258877,
      "learning_rate": 0.018509949097639984,
      "loss": 0.0001,
      "step": 88050
    },
    {
      "epoch": 40.74965293845442,
      "grad_norm": 0.004397517070174217,
      "learning_rate": 0.01850069412309116,
      "loss": 0.0001,
      "step": 88060
    },
    {
      "epoch": 40.75428042572883,
      "grad_norm": 0.008434605784714222,
      "learning_rate": 0.01849143914854234,
      "loss": 0.0001,
      "step": 88070
    },
    {
      "epoch": 40.75890791300324,
      "grad_norm": 0.0012341998517513275,
      "learning_rate": 0.018482184173993523,
      "loss": 0.0001,
      "step": 88080
    },
    {
      "epoch": 40.76353540027765,
      "grad_norm": 0.03205318748950958,
      "learning_rate": 0.018472929199444703,
      "loss": 0.0001,
      "step": 88090
    },
    {
      "epoch": 40.76816288755206,
      "grad_norm": 0.12836331129074097,
      "learning_rate": 0.018463674224895883,
      "loss": 0.0001,
      "step": 88100
    },
    {
      "epoch": 40.77279037482647,
      "grad_norm": 0.08438631892204285,
      "learning_rate": 0.018454419250347062,
      "loss": 0.0001,
      "step": 88110
    },
    {
      "epoch": 40.777417862100876,
      "grad_norm": 0.0019354508258402348,
      "learning_rate": 0.018445164275798242,
      "loss": 0.0001,
      "step": 88120
    },
    {
      "epoch": 40.78204534937529,
      "grad_norm": 0.09202460944652557,
      "learning_rate": 0.01843590930124942,
      "loss": 0.0001,
      "step": 88130
    },
    {
      "epoch": 40.7866728366497,
      "grad_norm": 0.009615464136004448,
      "learning_rate": 0.0184266543267006,
      "loss": 0.0001,
      "step": 88140
    },
    {
      "epoch": 40.79130032392411,
      "grad_norm": 0.013130010105669498,
      "learning_rate": 0.01841739935215178,
      "loss": 0.0001,
      "step": 88150
    },
    {
      "epoch": 40.79592781119852,
      "grad_norm": 0.013735106214880943,
      "learning_rate": 0.01840814437760296,
      "loss": 0.0,
      "step": 88160
    },
    {
      "epoch": 40.80055529847293,
      "grad_norm": 0.04095452278852463,
      "learning_rate": 0.018398889403054144,
      "loss": 0.0001,
      "step": 88170
    },
    {
      "epoch": 40.80518278574734,
      "grad_norm": 0.004202309995889664,
      "learning_rate": 0.018389634428505323,
      "loss": 0.0001,
      "step": 88180
    },
    {
      "epoch": 40.809810273021746,
      "grad_norm": 0.008955893106758595,
      "learning_rate": 0.018380379453956503,
      "loss": 0.0003,
      "step": 88190
    },
    {
      "epoch": 40.81443776029616,
      "grad_norm": 0.41640716791152954,
      "learning_rate": 0.018371124479407683,
      "loss": 0.0002,
      "step": 88200
    },
    {
      "epoch": 40.81906524757057,
      "grad_norm": 0.006543871480971575,
      "learning_rate": 0.018361869504858862,
      "loss": 0.0,
      "step": 88210
    },
    {
      "epoch": 40.82369273484498,
      "grad_norm": 0.06137474998831749,
      "learning_rate": 0.018352614530310042,
      "loss": 0.0005,
      "step": 88220
    },
    {
      "epoch": 40.82832022211939,
      "grad_norm": 0.0007349728839471936,
      "learning_rate": 0.018343359555761222,
      "loss": 0.0001,
      "step": 88230
    },
    {
      "epoch": 40.8329477093938,
      "grad_norm": 0.004869472701102495,
      "learning_rate": 0.0183341045812124,
      "loss": 0.0001,
      "step": 88240
    },
    {
      "epoch": 40.83757519666821,
      "grad_norm": 0.02213137224316597,
      "learning_rate": 0.01832484960666358,
      "loss": 0.0001,
      "step": 88250
    },
    {
      "epoch": 40.84220268394262,
      "grad_norm": 0.02349763922393322,
      "learning_rate": 0.018315594632114764,
      "loss": 0.0001,
      "step": 88260
    },
    {
      "epoch": 40.84683017121703,
      "grad_norm": 0.003524204483255744,
      "learning_rate": 0.018306339657565944,
      "loss": 0.0002,
      "step": 88270
    },
    {
      "epoch": 40.85145765849144,
      "grad_norm": 0.0064201271161437035,
      "learning_rate": 0.018297084683017124,
      "loss": 0.0001,
      "step": 88280
    },
    {
      "epoch": 40.856085145765846,
      "grad_norm": 0.009768437594175339,
      "learning_rate": 0.0182878297084683,
      "loss": 0.0001,
      "step": 88290
    },
    {
      "epoch": 40.86071263304026,
      "grad_norm": 0.0018641016213223338,
      "learning_rate": 0.018278574733919483,
      "loss": 0.0025,
      "step": 88300
    },
    {
      "epoch": 40.86534012031467,
      "grad_norm": 0.005946222227066755,
      "learning_rate": 0.018269319759370663,
      "loss": 0.0001,
      "step": 88310
    },
    {
      "epoch": 40.86996760758908,
      "grad_norm": 0.004078834783285856,
      "learning_rate": 0.018260064784821842,
      "loss": 0.0,
      "step": 88320
    },
    {
      "epoch": 40.87459509486349,
      "grad_norm": 0.011122943833470345,
      "learning_rate": 0.018250809810273022,
      "loss": 0.0001,
      "step": 88330
    },
    {
      "epoch": 40.8792225821379,
      "grad_norm": 0.04400825500488281,
      "learning_rate": 0.018241554835724202,
      "loss": 0.0001,
      "step": 88340
    },
    {
      "epoch": 40.88385006941231,
      "grad_norm": 0.014088032767176628,
      "learning_rate": 0.018232299861175385,
      "loss": 0.0001,
      "step": 88350
    },
    {
      "epoch": 40.888477556686716,
      "grad_norm": 0.008787313476204872,
      "learning_rate": 0.01822304488662656,
      "loss": 0.0001,
      "step": 88360
    },
    {
      "epoch": 40.89310504396113,
      "grad_norm": 0.01988181658089161,
      "learning_rate": 0.01821378991207774,
      "loss": 0.0001,
      "step": 88370
    },
    {
      "epoch": 40.89773253123554,
      "grad_norm": 0.012665169313549995,
      "learning_rate": 0.01820453493752892,
      "loss": 0.0001,
      "step": 88380
    },
    {
      "epoch": 40.90236001850995,
      "grad_norm": 0.0039229379035532475,
      "learning_rate": 0.018195279962980104,
      "loss": 0.0001,
      "step": 88390
    },
    {
      "epoch": 40.90698750578436,
      "grad_norm": 0.003328734776005149,
      "learning_rate": 0.018186024988431283,
      "loss": 0.0001,
      "step": 88400
    },
    {
      "epoch": 40.91161499305877,
      "grad_norm": 0.0015868298942223191,
      "learning_rate": 0.018176770013882463,
      "loss": 0.0,
      "step": 88410
    },
    {
      "epoch": 40.91624248033318,
      "grad_norm": 0.010321058332920074,
      "learning_rate": 0.018167515039333643,
      "loss": 0.0001,
      "step": 88420
    },
    {
      "epoch": 40.92086996760759,
      "grad_norm": 0.0034684971906244755,
      "learning_rate": 0.018158260064784822,
      "loss": 0.0001,
      "step": 88430
    },
    {
      "epoch": 40.925497454882,
      "grad_norm": 0.011005756445229053,
      "learning_rate": 0.018149005090236002,
      "loss": 0.0001,
      "step": 88440
    },
    {
      "epoch": 40.93012494215641,
      "grad_norm": 0.035838596522808075,
      "learning_rate": 0.01813975011568718,
      "loss": 0.0001,
      "step": 88450
    },
    {
      "epoch": 40.934752429430816,
      "grad_norm": 0.02654624916613102,
      "learning_rate": 0.01813049514113836,
      "loss": 0.0,
      "step": 88460
    },
    {
      "epoch": 40.93937991670523,
      "grad_norm": 0.0033760531805455685,
      "learning_rate": 0.01812124016658954,
      "loss": 0.0,
      "step": 88470
    },
    {
      "epoch": 40.94400740397964,
      "grad_norm": 0.004243660718202591,
      "learning_rate": 0.018111985192040724,
      "loss": 0.0001,
      "step": 88480
    },
    {
      "epoch": 40.94863489125405,
      "grad_norm": 0.5303059816360474,
      "learning_rate": 0.018102730217491904,
      "loss": 0.0002,
      "step": 88490
    },
    {
      "epoch": 40.95326237852846,
      "grad_norm": 0.012004963122308254,
      "learning_rate": 0.018093475242943084,
      "loss": 0.0001,
      "step": 88500
    },
    {
      "epoch": 40.95788986580287,
      "grad_norm": 0.009654155001044273,
      "learning_rate": 0.018084220268394263,
      "loss": 0.0001,
      "step": 88510
    },
    {
      "epoch": 40.96251735307728,
      "grad_norm": 0.009184558875858784,
      "learning_rate": 0.018074965293845443,
      "loss": 0.0001,
      "step": 88520
    },
    {
      "epoch": 40.967144840351686,
      "grad_norm": 0.06069086119532585,
      "learning_rate": 0.018065710319296623,
      "loss": 0.0002,
      "step": 88530
    },
    {
      "epoch": 40.9717723276261,
      "grad_norm": 0.007502996362745762,
      "learning_rate": 0.018056455344747802,
      "loss": 0.0001,
      "step": 88540
    },
    {
      "epoch": 40.97639981490051,
      "grad_norm": 0.003198662307113409,
      "learning_rate": 0.018047200370198982,
      "loss": 0.0001,
      "step": 88550
    },
    {
      "epoch": 40.98102730217492,
      "grad_norm": 0.06818782538175583,
      "learning_rate": 0.01803794539565016,
      "loss": 0.0001,
      "step": 88560
    },
    {
      "epoch": 40.98565478944933,
      "grad_norm": 0.07072540372610092,
      "learning_rate": 0.018028690421101345,
      "loss": 0.0001,
      "step": 88570
    },
    {
      "epoch": 40.99028227672374,
      "grad_norm": 0.01781909540295601,
      "learning_rate": 0.018019435446552524,
      "loss": 0.0001,
      "step": 88580
    },
    {
      "epoch": 40.99490976399815,
      "grad_norm": 0.005550671834498644,
      "learning_rate": 0.0180101804720037,
      "loss": 0.0001,
      "step": 88590
    },
    {
      "epoch": 40.99953725127256,
      "grad_norm": 0.003275495721027255,
      "learning_rate": 0.01800092549745488,
      "loss": 0.0001,
      "step": 88600
    },
    {
      "epoch": 41.0,
      "eval_accuracy_branch1": 0.989909104914497,
      "eval_accuracy_branch2": 0.5002310892004314,
      "eval_f1_branch1": 0.9909810398123462,
      "eval_f1_branch2": 0.4996667384014901,
      "eval_loss": 0.017915664240717888,
      "eval_precision_branch1": 0.9913361205915138,
      "eval_precision_branch2": 0.5002321365539434,
      "eval_recall_branch1": 0.990739494224253,
      "eval_recall_branch2": 0.5002310892004314,
      "eval_runtime": 28.9701,
      "eval_samples_per_second": 448.117,
      "eval_steps_per_second": 56.023,
      "step": 88601
    },
    {
      "epoch": 41.00416473854697,
      "grad_norm": 0.029680993407964706,
      "learning_rate": 0.017991670522906063,
      "loss": 0.5371,
      "step": 88610
    },
    {
      "epoch": 41.00879222582138,
      "grad_norm": 0.004761324264109135,
      "learning_rate": 0.017982415548357243,
      "loss": 0.0002,
      "step": 88620
    },
    {
      "epoch": 41.013419713095786,
      "grad_norm": 0.015392116270959377,
      "learning_rate": 0.017973160573808423,
      "loss": 0.0001,
      "step": 88630
    },
    {
      "epoch": 41.0180472003702,
      "grad_norm": 0.003188233356922865,
      "learning_rate": 0.017963905599259603,
      "loss": 0.0,
      "step": 88640
    },
    {
      "epoch": 41.02267468764461,
      "grad_norm": 0.00431006820872426,
      "learning_rate": 0.017954650624710782,
      "loss": 0.0001,
      "step": 88650
    },
    {
      "epoch": 41.02730217491902,
      "grad_norm": 0.0013402090407907963,
      "learning_rate": 0.017945395650161965,
      "loss": 0.0001,
      "step": 88660
    },
    {
      "epoch": 41.03192966219343,
      "grad_norm": 0.010936982929706573,
      "learning_rate": 0.01793614067561314,
      "loss": 0.0001,
      "step": 88670
    },
    {
      "epoch": 41.03655714946784,
      "grad_norm": 0.001222844934090972,
      "learning_rate": 0.01792688570106432,
      "loss": 0.0001,
      "step": 88680
    },
    {
      "epoch": 41.04118463674225,
      "grad_norm": 0.0023970359470695257,
      "learning_rate": 0.0179176307265155,
      "loss": 0.0,
      "step": 88690
    },
    {
      "epoch": 41.045812124016656,
      "grad_norm": 0.09532327950000763,
      "learning_rate": 0.017908375751966684,
      "loss": 0.0002,
      "step": 88700
    },
    {
      "epoch": 41.05043961129107,
      "grad_norm": 0.011762595735490322,
      "learning_rate": 0.017899120777417864,
      "loss": 0.0001,
      "step": 88710
    },
    {
      "epoch": 41.05506709856548,
      "grad_norm": 0.00646364688873291,
      "learning_rate": 0.017889865802869043,
      "loss": 0.0001,
      "step": 88720
    },
    {
      "epoch": 41.059694585839885,
      "grad_norm": 0.006838950794190168,
      "learning_rate": 0.017880610828320223,
      "loss": 0.0001,
      "step": 88730
    },
    {
      "epoch": 41.0643220731143,
      "grad_norm": 0.0020463650580495596,
      "learning_rate": 0.017871355853771403,
      "loss": 0.0001,
      "step": 88740
    },
    {
      "epoch": 41.06894956038871,
      "grad_norm": 0.0007653438369743526,
      "learning_rate": 0.017862100879222582,
      "loss": 0.0001,
      "step": 88750
    },
    {
      "epoch": 41.07357704766312,
      "grad_norm": 0.023705443367362022,
      "learning_rate": 0.017852845904673762,
      "loss": 0.0001,
      "step": 88760
    },
    {
      "epoch": 41.07820453493753,
      "grad_norm": 0.011378238908946514,
      "learning_rate": 0.017843590930124942,
      "loss": 0.0,
      "step": 88770
    },
    {
      "epoch": 41.08283202221194,
      "grad_norm": 0.0014847799902781844,
      "learning_rate": 0.01783433595557612,
      "loss": 0.0001,
      "step": 88780
    },
    {
      "epoch": 41.08745950948635,
      "grad_norm": 0.0021709634456783533,
      "learning_rate": 0.017825080981027305,
      "loss": 0.0001,
      "step": 88790
    },
    {
      "epoch": 41.092086996760756,
      "grad_norm": 0.008235245011746883,
      "learning_rate": 0.017815826006478484,
      "loss": 0.0001,
      "step": 88800
    },
    {
      "epoch": 41.09671448403517,
      "grad_norm": 0.006813397631049156,
      "learning_rate": 0.017806571031929664,
      "loss": 0.0001,
      "step": 88810
    },
    {
      "epoch": 41.10134197130958,
      "grad_norm": 0.008468089625239372,
      "learning_rate": 0.01779731605738084,
      "loss": 0.0003,
      "step": 88820
    },
    {
      "epoch": 41.10596945858399,
      "grad_norm": 0.005487062502652407,
      "learning_rate": 0.017788061082832023,
      "loss": 0.0001,
      "step": 88830
    },
    {
      "epoch": 41.1105969458584,
      "grad_norm": 0.0028425436466932297,
      "learning_rate": 0.017778806108283203,
      "loss": 0.0002,
      "step": 88840
    },
    {
      "epoch": 41.11522443313281,
      "grad_norm": 0.0006814739899709821,
      "learning_rate": 0.017769551133734383,
      "loss": 0.0001,
      "step": 88850
    },
    {
      "epoch": 41.11985192040722,
      "grad_norm": 0.020966891199350357,
      "learning_rate": 0.017760296159185562,
      "loss": 0.0001,
      "step": 88860
    },
    {
      "epoch": 41.124479407681626,
      "grad_norm": 0.0018765659769997,
      "learning_rate": 0.017751041184636742,
      "loss": 0.0001,
      "step": 88870
    },
    {
      "epoch": 41.12910689495604,
      "grad_norm": 0.00197647325694561,
      "learning_rate": 0.017741786210087925,
      "loss": 0.0001,
      "step": 88880
    },
    {
      "epoch": 41.13373438223045,
      "grad_norm": 0.011682916432619095,
      "learning_rate": 0.017732531235539105,
      "loss": 0.0001,
      "step": 88890
    },
    {
      "epoch": 41.138361869504855,
      "grad_norm": 0.0161649901419878,
      "learning_rate": 0.01772327626099028,
      "loss": 0.0001,
      "step": 88900
    },
    {
      "epoch": 41.14298935677927,
      "grad_norm": 0.06004786863923073,
      "learning_rate": 0.01771402128644146,
      "loss": 0.0001,
      "step": 88910
    },
    {
      "epoch": 41.14761684405368,
      "grad_norm": 0.02727046236395836,
      "learning_rate": 0.017704766311892644,
      "loss": 0.0002,
      "step": 88920
    },
    {
      "epoch": 41.15224433132809,
      "grad_norm": 0.058327317237854004,
      "learning_rate": 0.017695511337343824,
      "loss": 0.0001,
      "step": 88930
    },
    {
      "epoch": 41.1568718186025,
      "grad_norm": 0.007855379022657871,
      "learning_rate": 0.017686256362795003,
      "loss": 0.0001,
      "step": 88940
    },
    {
      "epoch": 41.16149930587691,
      "grad_norm": 0.007974370382726192,
      "learning_rate": 0.017677001388246183,
      "loss": 0.0001,
      "step": 88950
    },
    {
      "epoch": 41.16612679315132,
      "grad_norm": 0.00654958002269268,
      "learning_rate": 0.017667746413697363,
      "loss": 0.0,
      "step": 88960
    },
    {
      "epoch": 41.170754280425726,
      "grad_norm": 0.010413925163447857,
      "learning_rate": 0.017658491439148546,
      "loss": 0.0001,
      "step": 88970
    },
    {
      "epoch": 41.17538176770014,
      "grad_norm": 0.009848893620073795,
      "learning_rate": 0.017649236464599722,
      "loss": 0.0,
      "step": 88980
    },
    {
      "epoch": 41.18000925497455,
      "grad_norm": 0.009125490672886372,
      "learning_rate": 0.0176399814900509,
      "loss": 0.0002,
      "step": 88990
    },
    {
      "epoch": 41.18463674224896,
      "grad_norm": 0.002850041724741459,
      "learning_rate": 0.01763072651550208,
      "loss": 0.0001,
      "step": 89000
    },
    {
      "epoch": 41.18926422952337,
      "grad_norm": 0.007296669762581587,
      "learning_rate": 0.017621471540953264,
      "loss": 0.0001,
      "step": 89010
    },
    {
      "epoch": 41.19389171679778,
      "grad_norm": 0.004355555400252342,
      "learning_rate": 0.017612216566404444,
      "loss": 0.0001,
      "step": 89020
    },
    {
      "epoch": 41.19851920407219,
      "grad_norm": 0.0037461488973349333,
      "learning_rate": 0.017602961591855624,
      "loss": 0.0001,
      "step": 89030
    },
    {
      "epoch": 41.203146691346596,
      "grad_norm": 0.009507999755442142,
      "learning_rate": 0.017593706617306804,
      "loss": 0.0001,
      "step": 89040
    },
    {
      "epoch": 41.20777417862101,
      "grad_norm": 0.06073124334216118,
      "learning_rate": 0.017584451642757983,
      "loss": 0.0001,
      "step": 89050
    },
    {
      "epoch": 41.21240166589542,
      "grad_norm": 0.0024621249176561832,
      "learning_rate": 0.017575196668209163,
      "loss": 0.0001,
      "step": 89060
    },
    {
      "epoch": 41.217029153169825,
      "grad_norm": 0.002225820906460285,
      "learning_rate": 0.017565941693660343,
      "loss": 0.0,
      "step": 89070
    },
    {
      "epoch": 41.22165664044424,
      "grad_norm": 0.006208283361047506,
      "learning_rate": 0.017556686719111522,
      "loss": 0.0,
      "step": 89080
    },
    {
      "epoch": 41.22628412771865,
      "grad_norm": 0.007027314975857735,
      "learning_rate": 0.017547431744562702,
      "loss": 0.0,
      "step": 89090
    },
    {
      "epoch": 41.23091161499306,
      "grad_norm": 0.013854538090527058,
      "learning_rate": 0.017538176770013885,
      "loss": 0.0001,
      "step": 89100
    },
    {
      "epoch": 41.23553910226747,
      "grad_norm": 0.0018431775970384479,
      "learning_rate": 0.017528921795465065,
      "loss": 0.0,
      "step": 89110
    },
    {
      "epoch": 41.24016658954188,
      "grad_norm": 0.00800857413560152,
      "learning_rate": 0.017519666820916244,
      "loss": 0.0001,
      "step": 89120
    },
    {
      "epoch": 41.24479407681629,
      "grad_norm": 0.01488625630736351,
      "learning_rate": 0.01751041184636742,
      "loss": 0.0001,
      "step": 89130
    },
    {
      "epoch": 41.249421564090696,
      "grad_norm": 0.005029253661632538,
      "learning_rate": 0.017501156871818604,
      "loss": 0.0001,
      "step": 89140
    },
    {
      "epoch": 41.25404905136511,
      "grad_norm": 0.010091272182762623,
      "learning_rate": 0.017491901897269783,
      "loss": 0.0002,
      "step": 89150
    },
    {
      "epoch": 41.25867653863952,
      "grad_norm": 0.001564451609738171,
      "learning_rate": 0.017482646922720963,
      "loss": 0.0002,
      "step": 89160
    },
    {
      "epoch": 41.26330402591393,
      "grad_norm": 0.0012685518013313413,
      "learning_rate": 0.017473391948172143,
      "loss": 0.0,
      "step": 89170
    },
    {
      "epoch": 41.26793151318834,
      "grad_norm": 0.007379225455224514,
      "learning_rate": 0.017464136973623322,
      "loss": 0.0001,
      "step": 89180
    },
    {
      "epoch": 41.27255900046275,
      "grad_norm": 0.02888723835349083,
      "learning_rate": 0.017454881999074506,
      "loss": 0.0002,
      "step": 89190
    },
    {
      "epoch": 41.27718648773716,
      "grad_norm": 0.00727256340906024,
      "learning_rate": 0.017445627024525685,
      "loss": 0.0001,
      "step": 89200
    },
    {
      "epoch": 41.281813975011566,
      "grad_norm": 0.003213876159861684,
      "learning_rate": 0.01743637204997686,
      "loss": 0.0001,
      "step": 89210
    },
    {
      "epoch": 41.28644146228598,
      "grad_norm": 0.00868972297757864,
      "learning_rate": 0.01742711707542804,
      "loss": 0.0,
      "step": 89220
    },
    {
      "epoch": 41.29106894956039,
      "grad_norm": 0.030420776456594467,
      "learning_rate": 0.017417862100879224,
      "loss": 0.0001,
      "step": 89230
    },
    {
      "epoch": 41.295696436834795,
      "grad_norm": 0.017215916886925697,
      "learning_rate": 0.017408607126330404,
      "loss": 0.0001,
      "step": 89240
    },
    {
      "epoch": 41.30032392410921,
      "grad_norm": 0.0012636339524760842,
      "learning_rate": 0.017399352151781584,
      "loss": 0.0,
      "step": 89250
    },
    {
      "epoch": 41.30495141138362,
      "grad_norm": 0.007291780784726143,
      "learning_rate": 0.017390097177232763,
      "loss": 0.0001,
      "step": 89260
    },
    {
      "epoch": 41.30957889865803,
      "grad_norm": 0.10643173009157181,
      "learning_rate": 0.017380842202683943,
      "loss": 0.0001,
      "step": 89270
    },
    {
      "epoch": 41.31420638593244,
      "grad_norm": 0.048463039100170135,
      "learning_rate": 0.017371587228135123,
      "loss": 0.0001,
      "step": 89280
    },
    {
      "epoch": 41.31883387320685,
      "grad_norm": 0.026019040495157242,
      "learning_rate": 0.017362332253586302,
      "loss": 0.0001,
      "step": 89290
    },
    {
      "epoch": 41.32346136048126,
      "grad_norm": 0.003425335744395852,
      "learning_rate": 0.017353077279037482,
      "loss": 0.0003,
      "step": 89300
    },
    {
      "epoch": 41.328088847755666,
      "grad_norm": 0.014718072488904,
      "learning_rate": 0.017343822304488662,
      "loss": 0.0003,
      "step": 89310
    },
    {
      "epoch": 41.33271633503008,
      "grad_norm": 0.0019434846471995115,
      "learning_rate": 0.017334567329939845,
      "loss": 0.0001,
      "step": 89320
    },
    {
      "epoch": 41.33734382230449,
      "grad_norm": 0.008141177706420422,
      "learning_rate": 0.017325312355391025,
      "loss": 0.0001,
      "step": 89330
    },
    {
      "epoch": 41.3419713095789,
      "grad_norm": 0.0038923027459532022,
      "learning_rate": 0.017316057380842204,
      "loss": 0.0,
      "step": 89340
    },
    {
      "epoch": 41.34659879685331,
      "grad_norm": 0.005644728429615498,
      "learning_rate": 0.017306802406293384,
      "loss": 0.0001,
      "step": 89350
    },
    {
      "epoch": 41.35122628412772,
      "grad_norm": 0.0209914930164814,
      "learning_rate": 0.017297547431744564,
      "loss": 0.0001,
      "step": 89360
    },
    {
      "epoch": 41.35585377140213,
      "grad_norm": 0.018717631697654724,
      "learning_rate": 0.017288292457195743,
      "loss": 0.0002,
      "step": 89370
    },
    {
      "epoch": 41.360481258676536,
      "grad_norm": 0.0010419829050078988,
      "learning_rate": 0.017279037482646923,
      "loss": 0.0001,
      "step": 89380
    },
    {
      "epoch": 41.36510874595095,
      "grad_norm": 0.0033580095041543245,
      "learning_rate": 0.017269782508098103,
      "loss": 0.0002,
      "step": 89390
    },
    {
      "epoch": 41.36973623322536,
      "grad_norm": 0.014765148051083088,
      "learning_rate": 0.017260527533549282,
      "loss": 0.0001,
      "step": 89400
    },
    {
      "epoch": 41.374363720499765,
      "grad_norm": 0.004547151271253824,
      "learning_rate": 0.017251272559000465,
      "loss": 0.0001,
      "step": 89410
    },
    {
      "epoch": 41.37899120777418,
      "grad_norm": 0.007224625442177057,
      "learning_rate": 0.017242017584451645,
      "loss": 0.0001,
      "step": 89420
    },
    {
      "epoch": 41.38361869504859,
      "grad_norm": 0.000927193439565599,
      "learning_rate": 0.01723276260990282,
      "loss": 0.0001,
      "step": 89430
    },
    {
      "epoch": 41.388246182323,
      "grad_norm": 0.11905193328857422,
      "learning_rate": 0.017223507635354,
      "loss": 0.0001,
      "step": 89440
    },
    {
      "epoch": 41.39287366959741,
      "grad_norm": 0.11232641339302063,
      "learning_rate": 0.017214252660805184,
      "loss": 0.0001,
      "step": 89450
    },
    {
      "epoch": 41.39750115687182,
      "grad_norm": 0.3365292251110077,
      "learning_rate": 0.017204997686256364,
      "loss": 0.0002,
      "step": 89460
    },
    {
      "epoch": 41.40212864414623,
      "grad_norm": 0.01170304510742426,
      "learning_rate": 0.017195742711707544,
      "loss": 0.0001,
      "step": 89470
    },
    {
      "epoch": 41.406756131420636,
      "grad_norm": 0.004091977141797543,
      "learning_rate": 0.017186487737158723,
      "loss": 0.0,
      "step": 89480
    },
    {
      "epoch": 41.41138361869505,
      "grad_norm": 0.010616715997457504,
      "learning_rate": 0.017177232762609903,
      "loss": 0.0001,
      "step": 89490
    },
    {
      "epoch": 41.41601110596946,
      "grad_norm": 0.001974488142877817,
      "learning_rate": 0.017167977788061086,
      "loss": 0.0,
      "step": 89500
    },
    {
      "epoch": 41.42063859324387,
      "grad_norm": 0.0041999914683401585,
      "learning_rate": 0.017158722813512262,
      "loss": 0.0001,
      "step": 89510
    },
    {
      "epoch": 41.42526608051828,
      "grad_norm": 0.02390759438276291,
      "learning_rate": 0.017149467838963442,
      "loss": 0.0001,
      "step": 89520
    },
    {
      "epoch": 41.42989356779269,
      "grad_norm": 0.001629026373848319,
      "learning_rate": 0.01714021286441462,
      "loss": 0.0,
      "step": 89530
    },
    {
      "epoch": 41.4345210550671,
      "grad_norm": 0.5744686126708984,
      "learning_rate": 0.017130957889865805,
      "loss": 0.0003,
      "step": 89540
    },
    {
      "epoch": 41.439148542341506,
      "grad_norm": 0.04664835333824158,
      "learning_rate": 0.017121702915316984,
      "loss": 0.0001,
      "step": 89550
    },
    {
      "epoch": 41.44377602961592,
      "grad_norm": 0.0010664545698091388,
      "learning_rate": 0.017112447940768164,
      "loss": 0.0003,
      "step": 89560
    },
    {
      "epoch": 41.44840351689033,
      "grad_norm": 0.005254414398223162,
      "learning_rate": 0.017103192966219344,
      "loss": 0.0004,
      "step": 89570
    },
    {
      "epoch": 41.453031004164735,
      "grad_norm": 0.015539290383458138,
      "learning_rate": 0.017093937991670523,
      "loss": 0.0001,
      "step": 89580
    },
    {
      "epoch": 41.45765849143915,
      "grad_norm": 0.035801395773887634,
      "learning_rate": 0.017084683017121703,
      "loss": 0.0001,
      "step": 89590
    },
    {
      "epoch": 41.46228597871356,
      "grad_norm": 0.0020588948391377926,
      "learning_rate": 0.017075428042572883,
      "loss": 0.0,
      "step": 89600
    },
    {
      "epoch": 41.46691346598797,
      "grad_norm": 0.004274716135114431,
      "learning_rate": 0.017066173068024063,
      "loss": 0.0001,
      "step": 89610
    },
    {
      "epoch": 41.47154095326238,
      "grad_norm": 0.0015481666196137667,
      "learning_rate": 0.017056918093475242,
      "loss": 0.0001,
      "step": 89620
    },
    {
      "epoch": 41.47616844053679,
      "grad_norm": 0.004894411191344261,
      "learning_rate": 0.017047663118926425,
      "loss": 0.0,
      "step": 89630
    },
    {
      "epoch": 41.4807959278112,
      "grad_norm": 0.0019683493301272392,
      "learning_rate": 0.017038408144377605,
      "loss": 0.0,
      "step": 89640
    },
    {
      "epoch": 41.485423415085606,
      "grad_norm": 0.042657312005758286,
      "learning_rate": 0.017029153169828785,
      "loss": 0.0001,
      "step": 89650
    },
    {
      "epoch": 41.49005090236002,
      "grad_norm": 0.030263816937804222,
      "learning_rate": 0.01701989819527996,
      "loss": 0.0001,
      "step": 89660
    },
    {
      "epoch": 41.49467838963443,
      "grad_norm": 0.0073187947273254395,
      "learning_rate": 0.017010643220731144,
      "loss": 0.0001,
      "step": 89670
    },
    {
      "epoch": 41.499305876908835,
      "grad_norm": 0.0020132996141910553,
      "learning_rate": 0.017001388246182324,
      "loss": 0.0,
      "step": 89680
    },
    {
      "epoch": 41.50393336418325,
      "grad_norm": 0.0024861604906618595,
      "learning_rate": 0.016992133271633503,
      "loss": 0.0001,
      "step": 89690
    },
    {
      "epoch": 41.50856085145766,
      "grad_norm": 0.007267189212143421,
      "learning_rate": 0.016982878297084683,
      "loss": 0.0001,
      "step": 89700
    },
    {
      "epoch": 41.51318833873207,
      "grad_norm": 0.007338562048971653,
      "learning_rate": 0.016973623322535863,
      "loss": 0.0003,
      "step": 89710
    },
    {
      "epoch": 41.517815826006476,
      "grad_norm": 0.0008992112707346678,
      "learning_rate": 0.016964368347987046,
      "loss": 0.0001,
      "step": 89720
    },
    {
      "epoch": 41.52244331328089,
      "grad_norm": 0.003211521776393056,
      "learning_rate": 0.016955113373438226,
      "loss": 0.0001,
      "step": 89730
    },
    {
      "epoch": 41.5270708005553,
      "grad_norm": 0.03297653794288635,
      "learning_rate": 0.016945858398889402,
      "loss": 0.0001,
      "step": 89740
    },
    {
      "epoch": 41.531698287829705,
      "grad_norm": 0.03339187428355217,
      "learning_rate": 0.01693660342434058,
      "loss": 0.0002,
      "step": 89750
    },
    {
      "epoch": 41.53632577510412,
      "grad_norm": 0.009810843504965305,
      "learning_rate": 0.016927348449791765,
      "loss": 0.0001,
      "step": 89760
    },
    {
      "epoch": 41.54095326237853,
      "grad_norm": 0.1427018940448761,
      "learning_rate": 0.016918093475242944,
      "loss": 0.0002,
      "step": 89770
    },
    {
      "epoch": 41.54558074965294,
      "grad_norm": 0.0013117921771481633,
      "learning_rate": 0.016908838500694124,
      "loss": 0.0001,
      "step": 89780
    },
    {
      "epoch": 41.55020823692735,
      "grad_norm": 0.011112802661955357,
      "learning_rate": 0.016899583526145304,
      "loss": 0.0001,
      "step": 89790
    },
    {
      "epoch": 41.55483572420176,
      "grad_norm": 0.016258371993899345,
      "learning_rate": 0.016890328551596483,
      "loss": 0.0001,
      "step": 89800
    },
    {
      "epoch": 41.55946321147617,
      "grad_norm": 0.0009933405090123415,
      "learning_rate": 0.016881073577047667,
      "loss": 0.0,
      "step": 89810
    },
    {
      "epoch": 41.564090698750576,
      "grad_norm": 0.018398672342300415,
      "learning_rate": 0.016871818602498843,
      "loss": 0.0,
      "step": 89820
    },
    {
      "epoch": 41.56871818602499,
      "grad_norm": 0.4176046550273895,
      "learning_rate": 0.016862563627950022,
      "loss": 0.0002,
      "step": 89830
    },
    {
      "epoch": 41.5733456732994,
      "grad_norm": 0.001396780600771308,
      "learning_rate": 0.016853308653401202,
      "loss": 0.0001,
      "step": 89840
    },
    {
      "epoch": 41.577973160573805,
      "grad_norm": 0.004978898447006941,
      "learning_rate": 0.016844053678852385,
      "loss": 0.0,
      "step": 89850
    },
    {
      "epoch": 41.58260064784822,
      "grad_norm": 0.005925909616053104,
      "learning_rate": 0.016834798704303565,
      "loss": 0.0,
      "step": 89860
    },
    {
      "epoch": 41.58722813512263,
      "grad_norm": 0.001000192598439753,
      "learning_rate": 0.016825543729754745,
      "loss": 0.0001,
      "step": 89870
    },
    {
      "epoch": 41.59185562239704,
      "grad_norm": 0.0007735742838121951,
      "learning_rate": 0.016816288755205924,
      "loss": 0.0001,
      "step": 89880
    },
    {
      "epoch": 41.596483109671446,
      "grad_norm": 0.0070578837767243385,
      "learning_rate": 0.016807033780657104,
      "loss": 0.0001,
      "step": 89890
    },
    {
      "epoch": 41.60111059694586,
      "grad_norm": 0.1840391904115677,
      "learning_rate": 0.016797778806108284,
      "loss": 0.0002,
      "step": 89900
    },
    {
      "epoch": 41.60573808422027,
      "grad_norm": 0.002736874623224139,
      "learning_rate": 0.016788523831559463,
      "loss": 0.0002,
      "step": 89910
    },
    {
      "epoch": 41.610365571494675,
      "grad_norm": 0.022438591346144676,
      "learning_rate": 0.016779268857010643,
      "loss": 0.0001,
      "step": 89920
    },
    {
      "epoch": 41.61499305876909,
      "grad_norm": 0.006493729073554277,
      "learning_rate": 0.016770013882461823,
      "loss": 0.0001,
      "step": 89930
    },
    {
      "epoch": 41.6196205460435,
      "grad_norm": 0.018185952678322792,
      "learning_rate": 0.016760758907913006,
      "loss": 0.0001,
      "step": 89940
    },
    {
      "epoch": 41.62424803331791,
      "grad_norm": 0.0019945886451750994,
      "learning_rate": 0.016751503933364185,
      "loss": 0.0001,
      "step": 89950
    },
    {
      "epoch": 41.62887552059232,
      "grad_norm": 0.015057610347867012,
      "learning_rate": 0.016742248958815365,
      "loss": 0.0001,
      "step": 89960
    },
    {
      "epoch": 41.63350300786673,
      "grad_norm": 0.009985527023673058,
      "learning_rate": 0.01673299398426654,
      "loss": 0.0007,
      "step": 89970
    },
    {
      "epoch": 41.63813049514114,
      "grad_norm": 0.00338306394405663,
      "learning_rate": 0.016723739009717725,
      "loss": 0.0001,
      "step": 89980
    },
    {
      "epoch": 41.642757982415546,
      "grad_norm": 0.002582134213298559,
      "learning_rate": 0.016714484035168904,
      "loss": 0.0003,
      "step": 89990
    },
    {
      "epoch": 41.64738546968996,
      "grad_norm": 0.0015430096536874771,
      "learning_rate": 0.016705229060620084,
      "loss": 0.0001,
      "step": 90000
    },
    {
      "epoch": 41.65201295696437,
      "grad_norm": 0.005929593928158283,
      "learning_rate": 0.016695974086071264,
      "loss": 0.0001,
      "step": 90010
    },
    {
      "epoch": 41.656640444238775,
      "grad_norm": 0.002036512829363346,
      "learning_rate": 0.016686719111522443,
      "loss": 0.0002,
      "step": 90020
    },
    {
      "epoch": 41.66126793151319,
      "grad_norm": 0.19503696262836456,
      "learning_rate": 0.016677464136973626,
      "loss": 0.0002,
      "step": 90030
    },
    {
      "epoch": 41.6658954187876,
      "grad_norm": 0.010348910465836525,
      "learning_rate": 0.016668209162424806,
      "loss": 0.0001,
      "step": 90040
    },
    {
      "epoch": 41.67052290606201,
      "grad_norm": 0.031240910291671753,
      "learning_rate": 0.016658954187875982,
      "loss": 0.0001,
      "step": 90050
    },
    {
      "epoch": 41.675150393336416,
      "grad_norm": 0.005092633422464132,
      "learning_rate": 0.016649699213327162,
      "loss": 0.0001,
      "step": 90060
    },
    {
      "epoch": 41.67977788061083,
      "grad_norm": 0.00043194685713388026,
      "learning_rate": 0.016640444238778345,
      "loss": 0.0001,
      "step": 90070
    },
    {
      "epoch": 41.68440536788524,
      "grad_norm": 0.010147594846785069,
      "learning_rate": 0.016631189264229525,
      "loss": 0.0001,
      "step": 90080
    },
    {
      "epoch": 41.689032855159645,
      "grad_norm": 0.010399317368865013,
      "learning_rate": 0.016621934289680704,
      "loss": 0.0001,
      "step": 90090
    },
    {
      "epoch": 41.69366034243406,
      "grad_norm": 0.005794605240225792,
      "learning_rate": 0.016612679315131884,
      "loss": 0.0003,
      "step": 90100
    },
    {
      "epoch": 41.69828782970847,
      "grad_norm": 0.005319187417626381,
      "learning_rate": 0.016603424340583064,
      "loss": 0.0001,
      "step": 90110
    },
    {
      "epoch": 41.70291531698288,
      "grad_norm": 0.008139043115079403,
      "learning_rate": 0.016594169366034243,
      "loss": 0.0001,
      "step": 90120
    },
    {
      "epoch": 41.70754280425729,
      "grad_norm": 0.0009180779452435672,
      "learning_rate": 0.016584914391485423,
      "loss": 0.0,
      "step": 90130
    },
    {
      "epoch": 41.7121702915317,
      "grad_norm": 0.0033492285292595625,
      "learning_rate": 0.016575659416936603,
      "loss": 0.0002,
      "step": 90140
    },
    {
      "epoch": 41.71679777880611,
      "grad_norm": 0.0021315061021596193,
      "learning_rate": 0.016566404442387783,
      "loss": 0.0017,
      "step": 90150
    },
    {
      "epoch": 41.721425266080516,
      "grad_norm": 0.007019175682216883,
      "learning_rate": 0.016557149467838966,
      "loss": 0.0001,
      "step": 90160
    },
    {
      "epoch": 41.72605275335493,
      "grad_norm": 0.009443460963666439,
      "learning_rate": 0.016547894493290145,
      "loss": 0.0002,
      "step": 90170
    },
    {
      "epoch": 41.73068024062934,
      "grad_norm": 0.021719537675380707,
      "learning_rate": 0.016538639518741325,
      "loss": 0.0002,
      "step": 90180
    },
    {
      "epoch": 41.735307727903745,
      "grad_norm": 0.0024330024607479572,
      "learning_rate": 0.016529384544192505,
      "loss": 0.0003,
      "step": 90190
    },
    {
      "epoch": 41.73993521517816,
      "grad_norm": 0.09114847332239151,
      "learning_rate": 0.016520129569643684,
      "loss": 0.0001,
      "step": 90200
    },
    {
      "epoch": 41.74456270245257,
      "grad_norm": 0.022780749946832657,
      "learning_rate": 0.016510874595094864,
      "loss": 0.0001,
      "step": 90210
    },
    {
      "epoch": 41.74919018972698,
      "grad_norm": 0.0896536111831665,
      "learning_rate": 0.016501619620546044,
      "loss": 0.0001,
      "step": 90220
    },
    {
      "epoch": 41.753817677001386,
      "grad_norm": 0.002030026400461793,
      "learning_rate": 0.016492364645997223,
      "loss": 0.0002,
      "step": 90230
    },
    {
      "epoch": 41.7584451642758,
      "grad_norm": 0.0026547424495220184,
      "learning_rate": 0.016483109671448403,
      "loss": 0.0001,
      "step": 90240
    },
    {
      "epoch": 41.76307265155021,
      "grad_norm": 0.0048844157718122005,
      "learning_rate": 0.016473854696899586,
      "loss": 0.0001,
      "step": 90250
    },
    {
      "epoch": 41.767700138824615,
      "grad_norm": 0.011517398990690708,
      "learning_rate": 0.016464599722350766,
      "loss": 0.0001,
      "step": 90260
    },
    {
      "epoch": 41.77232762609903,
      "grad_norm": 0.0022180459927767515,
      "learning_rate": 0.016455344747801946,
      "loss": 0.0001,
      "step": 90270
    },
    {
      "epoch": 41.77695511337344,
      "grad_norm": 0.672078013420105,
      "learning_rate": 0.016446089773253122,
      "loss": 0.0003,
      "step": 90280
    },
    {
      "epoch": 41.78158260064785,
      "grad_norm": 0.06666714698076248,
      "learning_rate": 0.016436834798704305,
      "loss": 0.0001,
      "step": 90290
    },
    {
      "epoch": 41.78621008792226,
      "grad_norm": 0.005581932608038187,
      "learning_rate": 0.016427579824155485,
      "loss": 0.0001,
      "step": 90300
    },
    {
      "epoch": 41.79083757519667,
      "grad_norm": 0.005391047801822424,
      "learning_rate": 0.016418324849606664,
      "loss": 0.0,
      "step": 90310
    },
    {
      "epoch": 41.79546506247108,
      "grad_norm": 0.0036377853248268366,
      "learning_rate": 0.016409069875057844,
      "loss": 0.0001,
      "step": 90320
    },
    {
      "epoch": 41.800092549745486,
      "grad_norm": 0.01365344412624836,
      "learning_rate": 0.016399814900509024,
      "loss": 0.0001,
      "step": 90330
    },
    {
      "epoch": 41.8047200370199,
      "grad_norm": 0.10457517951726913,
      "learning_rate": 0.016390559925960207,
      "loss": 0.0002,
      "step": 90340
    },
    {
      "epoch": 41.80934752429431,
      "grad_norm": 0.0010188256856054068,
      "learning_rate": 0.016381304951411383,
      "loss": 0.0001,
      "step": 90350
    },
    {
      "epoch": 41.813975011568715,
      "grad_norm": 0.021587083116173744,
      "learning_rate": 0.016372049976862563,
      "loss": 0.0001,
      "step": 90360
    },
    {
      "epoch": 41.81860249884313,
      "grad_norm": 0.0011015970958396792,
      "learning_rate": 0.016362795002313742,
      "loss": 0.0001,
      "step": 90370
    },
    {
      "epoch": 41.82322998611754,
      "grad_norm": 0.0014763792278245091,
      "learning_rate": 0.016353540027764926,
      "loss": 0.0002,
      "step": 90380
    },
    {
      "epoch": 41.82785747339195,
      "grad_norm": 0.004301704000681639,
      "learning_rate": 0.016344285053216105,
      "loss": 0.0002,
      "step": 90390
    },
    {
      "epoch": 41.832484960666356,
      "grad_norm": 0.014883607625961304,
      "learning_rate": 0.016335030078667285,
      "loss": 0.0004,
      "step": 90400
    },
    {
      "epoch": 41.83711244794077,
      "grad_norm": 0.011055641807615757,
      "learning_rate": 0.016325775104118465,
      "loss": 0.0002,
      "step": 90410
    },
    {
      "epoch": 41.84173993521518,
      "grad_norm": 0.004169374704360962,
      "learning_rate": 0.016316520129569644,
      "loss": 0.0001,
      "step": 90420
    },
    {
      "epoch": 41.846367422489585,
      "grad_norm": 0.0023542919661849737,
      "learning_rate": 0.016307265155020824,
      "loss": 0.0,
      "step": 90430
    },
    {
      "epoch": 41.850994909764,
      "grad_norm": 0.01510296668857336,
      "learning_rate": 0.016298010180472004,
      "loss": 0.0001,
      "step": 90440
    },
    {
      "epoch": 41.85562239703841,
      "grad_norm": 0.0008779757190495729,
      "learning_rate": 0.016288755205923183,
      "loss": 0.0001,
      "step": 90450
    },
    {
      "epoch": 41.86024988431282,
      "grad_norm": 0.01938840001821518,
      "learning_rate": 0.016279500231374363,
      "loss": 0.0,
      "step": 90460
    },
    {
      "epoch": 41.86487737158723,
      "grad_norm": 0.0025006826035678387,
      "learning_rate": 0.016270245256825546,
      "loss": 0.0001,
      "step": 90470
    },
    {
      "epoch": 41.86950485886164,
      "grad_norm": 0.0017654175171628594,
      "learning_rate": 0.016260990282276726,
      "loss": 0.0017,
      "step": 90480
    },
    {
      "epoch": 41.87413234613605,
      "grad_norm": 0.00033123872708529234,
      "learning_rate": 0.016251735307727905,
      "loss": 0.0001,
      "step": 90490
    },
    {
      "epoch": 41.878759833410456,
      "grad_norm": 0.09263893961906433,
      "learning_rate": 0.016242480333179085,
      "loss": 0.0001,
      "step": 90500
    },
    {
      "epoch": 41.88338732068487,
      "grad_norm": 0.0051604281179606915,
      "learning_rate": 0.016233225358630265,
      "loss": 0.0,
      "step": 90510
    },
    {
      "epoch": 41.88801480795928,
      "grad_norm": 0.010914402082562447,
      "learning_rate": 0.016223970384081444,
      "loss": 0.0001,
      "step": 90520
    },
    {
      "epoch": 41.892642295233685,
      "grad_norm": 0.05766080319881439,
      "learning_rate": 0.016214715409532624,
      "loss": 0.0001,
      "step": 90530
    },
    {
      "epoch": 41.8972697825081,
      "grad_norm": 0.004049334209412336,
      "learning_rate": 0.016205460434983804,
      "loss": 0.0,
      "step": 90540
    },
    {
      "epoch": 41.90189726978251,
      "grad_norm": 0.002834977814927697,
      "learning_rate": 0.016196205460434984,
      "loss": 0.0001,
      "step": 90550
    },
    {
      "epoch": 41.90652475705692,
      "grad_norm": 0.008761836215853691,
      "learning_rate": 0.016186950485886167,
      "loss": 0.0001,
      "step": 90560
    },
    {
      "epoch": 41.911152244331326,
      "grad_norm": 0.0015435920795425773,
      "learning_rate": 0.016177695511337346,
      "loss": 0.0001,
      "step": 90570
    },
    {
      "epoch": 41.91577973160574,
      "grad_norm": 0.001593755092471838,
      "learning_rate": 0.016168440536788523,
      "loss": 0.0003,
      "step": 90580
    },
    {
      "epoch": 41.92040721888015,
      "grad_norm": 0.0014055134961381555,
      "learning_rate": 0.016159185562239702,
      "loss": 0.0002,
      "step": 90590
    },
    {
      "epoch": 41.925034706154555,
      "grad_norm": 0.049182724207639694,
      "learning_rate": 0.016149930587690885,
      "loss": 0.0,
      "step": 90600
    },
    {
      "epoch": 41.92966219342897,
      "grad_norm": 0.0010659185936674476,
      "learning_rate": 0.016140675613142065,
      "loss": 0.0001,
      "step": 90610
    },
    {
      "epoch": 41.93428968070338,
      "grad_norm": 0.005061182659119368,
      "learning_rate": 0.016131420638593245,
      "loss": 0.0002,
      "step": 90620
    },
    {
      "epoch": 41.938917167977785,
      "grad_norm": 0.0013626181753352284,
      "learning_rate": 0.016122165664044424,
      "loss": 0.0001,
      "step": 90630
    },
    {
      "epoch": 41.9435446552522,
      "grad_norm": 0.025468064472079277,
      "learning_rate": 0.016112910689495604,
      "loss": 0.0004,
      "step": 90640
    },
    {
      "epoch": 41.94817214252661,
      "grad_norm": 0.004481370560824871,
      "learning_rate": 0.016103655714946787,
      "loss": 0.0,
      "step": 90650
    },
    {
      "epoch": 41.95279962980102,
      "grad_norm": 0.0018753588665276766,
      "learning_rate": 0.016094400740397963,
      "loss": 0.0001,
      "step": 90660
    },
    {
      "epoch": 41.957427117075426,
      "grad_norm": 0.05253079906105995,
      "learning_rate": 0.016085145765849143,
      "loss": 0.0001,
      "step": 90670
    },
    {
      "epoch": 41.96205460434984,
      "grad_norm": 0.008715745061635971,
      "learning_rate": 0.016075890791300323,
      "loss": 0.0001,
      "step": 90680
    },
    {
      "epoch": 41.96668209162425,
      "grad_norm": 0.01263486035168171,
      "learning_rate": 0.016066635816751506,
      "loss": 0.0001,
      "step": 90690
    },
    {
      "epoch": 41.971309578898655,
      "grad_norm": 0.008058099076151848,
      "learning_rate": 0.016057380842202686,
      "loss": 0.0001,
      "step": 90700
    },
    {
      "epoch": 41.97593706617307,
      "grad_norm": 0.006024069152772427,
      "learning_rate": 0.016048125867653865,
      "loss": 0.0001,
      "step": 90710
    },
    {
      "epoch": 41.98056455344748,
      "grad_norm": 0.024780279025435448,
      "learning_rate": 0.016038870893105045,
      "loss": 0.0002,
      "step": 90720
    },
    {
      "epoch": 41.98519204072189,
      "grad_norm": 0.010575159452855587,
      "learning_rate": 0.016029615918556225,
      "loss": 0.0001,
      "step": 90730
    },
    {
      "epoch": 41.989819527996296,
      "grad_norm": 0.0023017434868961573,
      "learning_rate": 0.016020360944007404,
      "loss": 0.0001,
      "step": 90740
    },
    {
      "epoch": 41.99444701527071,
      "grad_norm": 0.008364076726138592,
      "learning_rate": 0.016011105969458584,
      "loss": 0.0001,
      "step": 90750
    },
    {
      "epoch": 41.99907450254512,
      "grad_norm": 0.015579696744680405,
      "learning_rate": 0.016001850994909764,
      "loss": 0.0003,
      "step": 90760
    },
    {
      "epoch": 42.0,
      "eval_accuracy_branch1": 0.9899861346479741,
      "eval_accuracy_branch2": 0.4999229702665229,
      "eval_f1_branch1": 0.9913252900566988,
      "eval_f1_branch2": 0.49991004460458455,
      "eval_loss": 0.01999347098171711,
      "eval_precision_branch1": 0.9915509622934827,
      "eval_precision_branch2": 0.49992296230184985,
      "eval_recall_branch1": 0.9912568982184424,
      "eval_recall_branch2": 0.4999229702665229,
      "eval_runtime": 28.8359,
      "eval_samples_per_second": 450.202,
      "eval_steps_per_second": 56.284,
      "step": 90762
    },
    {
      "epoch": 42.003701989819525,
      "grad_norm": 0.002688153414055705,
      "learning_rate": 0.015992596020360943,
      "loss": 0.1252,
      "step": 90770
    },
    {
      "epoch": 42.00832947709394,
      "grad_norm": 0.0075302207842469215,
      "learning_rate": 0.015983341045812127,
      "loss": 0.0,
      "step": 90780
    },
    {
      "epoch": 42.01295696436835,
      "grad_norm": 0.0031672113109380007,
      "learning_rate": 0.015974086071263306,
      "loss": 0.0001,
      "step": 90790
    },
    {
      "epoch": 42.017584451642755,
      "grad_norm": 0.0033233570866286755,
      "learning_rate": 0.015964831096714486,
      "loss": 0.0001,
      "step": 90800
    },
    {
      "epoch": 42.02221193891717,
      "grad_norm": 0.00645958399400115,
      "learning_rate": 0.015955576122165662,
      "loss": 0.0,
      "step": 90810
    },
    {
      "epoch": 42.02683942619158,
      "grad_norm": 0.001633283100090921,
      "learning_rate": 0.015946321147616845,
      "loss": 0.0002,
      "step": 90820
    },
    {
      "epoch": 42.03146691346599,
      "grad_norm": 0.08376893401145935,
      "learning_rate": 0.015937066173068025,
      "loss": 0.0001,
      "step": 90830
    },
    {
      "epoch": 42.036094400740396,
      "grad_norm": 0.014582921750843525,
      "learning_rate": 0.015927811198519205,
      "loss": 0.0001,
      "step": 90840
    },
    {
      "epoch": 42.04072188801481,
      "grad_norm": 0.005771401338279247,
      "learning_rate": 0.015918556223970384,
      "loss": 0.0002,
      "step": 90850
    },
    {
      "epoch": 42.04534937528922,
      "grad_norm": 0.019145524129271507,
      "learning_rate": 0.015909301249421564,
      "loss": 0.0001,
      "step": 90860
    },
    {
      "epoch": 42.049976862563625,
      "grad_norm": 0.008932475931942463,
      "learning_rate": 0.015900046274872747,
      "loss": 0.0002,
      "step": 90870
    },
    {
      "epoch": 42.05460434983804,
      "grad_norm": 0.002909015864133835,
      "learning_rate": 0.015890791300323927,
      "loss": 0.0001,
      "step": 90880
    },
    {
      "epoch": 42.05923183711245,
      "grad_norm": 0.030797123908996582,
      "learning_rate": 0.015881536325775103,
      "loss": 0.0001,
      "step": 90890
    },
    {
      "epoch": 42.06385932438686,
      "grad_norm": 0.004695112816989422,
      "learning_rate": 0.015872281351226283,
      "loss": 0.0001,
      "step": 90900
    },
    {
      "epoch": 42.068486811661266,
      "grad_norm": 0.0023150446359068155,
      "learning_rate": 0.015863026376677466,
      "loss": 0.0,
      "step": 90910
    },
    {
      "epoch": 42.07311429893568,
      "grad_norm": 0.005272735841572285,
      "learning_rate": 0.015853771402128645,
      "loss": 0.0001,
      "step": 90920
    },
    {
      "epoch": 42.07774178621009,
      "grad_norm": 0.00295039894990623,
      "learning_rate": 0.015844516427579825,
      "loss": 0.0016,
      "step": 90930
    },
    {
      "epoch": 42.082369273484495,
      "grad_norm": 0.003854090580716729,
      "learning_rate": 0.015835261453031005,
      "loss": 0.0002,
      "step": 90940
    },
    {
      "epoch": 42.08699676075891,
      "grad_norm": 0.0013862192863598466,
      "learning_rate": 0.015826006478482185,
      "loss": 0.0001,
      "step": 90950
    },
    {
      "epoch": 42.09162424803332,
      "grad_norm": 0.004304668866097927,
      "learning_rate": 0.015816751503933364,
      "loss": 0.0001,
      "step": 90960
    },
    {
      "epoch": 42.096251735307725,
      "grad_norm": 0.005663182586431503,
      "learning_rate": 0.015807496529384544,
      "loss": 0.0001,
      "step": 90970
    },
    {
      "epoch": 42.10087922258214,
      "grad_norm": 0.0023217175621539354,
      "learning_rate": 0.015798241554835724,
      "loss": 0.0014,
      "step": 90980
    },
    {
      "epoch": 42.10550670985655,
      "grad_norm": 0.0021327375434339046,
      "learning_rate": 0.015788986580286903,
      "loss": 0.0001,
      "step": 90990
    },
    {
      "epoch": 42.11013419713096,
      "grad_norm": 0.0026742734480649233,
      "learning_rate": 0.015779731605738086,
      "loss": 0.0001,
      "step": 91000
    },
    {
      "epoch": 42.114761684405366,
      "grad_norm": 0.005028656683862209,
      "learning_rate": 0.015770476631189266,
      "loss": 0.0001,
      "step": 91010
    },
    {
      "epoch": 42.11938917167978,
      "grad_norm": 0.05238466337323189,
      "learning_rate": 0.015761221656640446,
      "loss": 0.0001,
      "step": 91020
    },
    {
      "epoch": 42.12401665895419,
      "grad_norm": 0.001170479692518711,
      "learning_rate": 0.015751966682091625,
      "loss": 0.0001,
      "step": 91030
    },
    {
      "epoch": 42.128644146228595,
      "grad_norm": 0.00717624556273222,
      "learning_rate": 0.015742711707542805,
      "loss": 0.0001,
      "step": 91040
    },
    {
      "epoch": 42.13327163350301,
      "grad_norm": 0.00536505039781332,
      "learning_rate": 0.015733456732993985,
      "loss": 0.0001,
      "step": 91050
    },
    {
      "epoch": 42.13789912077742,
      "grad_norm": 1.855648159980774,
      "learning_rate": 0.015724201758445164,
      "loss": 0.0005,
      "step": 91060
    },
    {
      "epoch": 42.14252660805183,
      "grad_norm": 0.0021979769226163626,
      "learning_rate": 0.015714946783896344,
      "loss": 0.0,
      "step": 91070
    },
    {
      "epoch": 42.147154095326236,
      "grad_norm": 0.0015579372411593795,
      "learning_rate": 0.015705691809347524,
      "loss": 0.0005,
      "step": 91080
    },
    {
      "epoch": 42.15178158260065,
      "grad_norm": 0.0025830003432929516,
      "learning_rate": 0.015696436834798707,
      "loss": 0.0001,
      "step": 91090
    },
    {
      "epoch": 42.15640906987506,
      "grad_norm": 0.18797773122787476,
      "learning_rate": 0.015687181860249887,
      "loss": 0.0001,
      "step": 91100
    },
    {
      "epoch": 42.161036557149465,
      "grad_norm": 0.11244166642427444,
      "learning_rate": 0.015677926885701066,
      "loss": 0.0001,
      "step": 91110
    },
    {
      "epoch": 42.16566404442388,
      "grad_norm": 0.0009420228889212012,
      "learning_rate": 0.015668671911152243,
      "loss": 0.0001,
      "step": 91120
    },
    {
      "epoch": 42.17029153169829,
      "grad_norm": 0.006832021754235029,
      "learning_rate": 0.015659416936603426,
      "loss": 0.0003,
      "step": 91130
    },
    {
      "epoch": 42.174919018972695,
      "grad_norm": 0.011251930147409439,
      "learning_rate": 0.015650161962054605,
      "loss": 0.0001,
      "step": 91140
    },
    {
      "epoch": 42.17954650624711,
      "grad_norm": 0.10724205523729324,
      "learning_rate": 0.015640906987505785,
      "loss": 0.0002,
      "step": 91150
    },
    {
      "epoch": 42.18417399352152,
      "grad_norm": 0.001781280618160963,
      "learning_rate": 0.015631652012956965,
      "loss": 0.0018,
      "step": 91160
    },
    {
      "epoch": 42.18880148079593,
      "grad_norm": 0.001852109213359654,
      "learning_rate": 0.015622397038408146,
      "loss": 0.0001,
      "step": 91170
    },
    {
      "epoch": 42.193428968070336,
      "grad_norm": 0.002137306611984968,
      "learning_rate": 0.015613142063859326,
      "loss": 0.0,
      "step": 91180
    },
    {
      "epoch": 42.19805645534475,
      "grad_norm": 0.0019081737846136093,
      "learning_rate": 0.015603887089310504,
      "loss": 0.0,
      "step": 91190
    },
    {
      "epoch": 42.20268394261916,
      "grad_norm": 0.09349779784679413,
      "learning_rate": 0.015594632114761683,
      "loss": 0.0001,
      "step": 91200
    },
    {
      "epoch": 42.207311429893565,
      "grad_norm": 0.005249203182756901,
      "learning_rate": 0.015585377140212865,
      "loss": 0.0002,
      "step": 91210
    },
    {
      "epoch": 42.21193891716798,
      "grad_norm": 0.004825912415981293,
      "learning_rate": 0.015576122165664045,
      "loss": 0.0001,
      "step": 91220
    },
    {
      "epoch": 42.21656640444239,
      "grad_norm": 0.0019028360256925225,
      "learning_rate": 0.015566867191115226,
      "loss": 0.0001,
      "step": 91230
    },
    {
      "epoch": 42.2211938917168,
      "grad_norm": 0.007704634685069323,
      "learning_rate": 0.015557612216566406,
      "loss": 0.0001,
      "step": 91240
    },
    {
      "epoch": 42.225821378991206,
      "grad_norm": 0.004618867766112089,
      "learning_rate": 0.015548357242017585,
      "loss": 0.0001,
      "step": 91250
    },
    {
      "epoch": 42.23044886626562,
      "grad_norm": 0.004050369840115309,
      "learning_rate": 0.015539102267468767,
      "loss": 0.0001,
      "step": 91260
    },
    {
      "epoch": 42.23507635354003,
      "grad_norm": 0.011046407744288445,
      "learning_rate": 0.015529847292919945,
      "loss": 0.0001,
      "step": 91270
    },
    {
      "epoch": 42.239703840814435,
      "grad_norm": 0.0073058572597801685,
      "learning_rate": 0.015520592318371124,
      "loss": 0.0002,
      "step": 91280
    },
    {
      "epoch": 42.24433132808885,
      "grad_norm": 0.041635408997535706,
      "learning_rate": 0.015511337343822304,
      "loss": 0.0001,
      "step": 91290
    },
    {
      "epoch": 42.24895881536326,
      "grad_norm": 0.015706023201346397,
      "learning_rate": 0.015502082369273485,
      "loss": 0.0002,
      "step": 91300
    },
    {
      "epoch": 42.253586302637665,
      "grad_norm": 0.0030690240673720837,
      "learning_rate": 0.015492827394724665,
      "loss": 0.0001,
      "step": 91310
    },
    {
      "epoch": 42.25821378991208,
      "grad_norm": 0.01446253340691328,
      "learning_rate": 0.015483572420175847,
      "loss": 0.0001,
      "step": 91320
    },
    {
      "epoch": 42.26284127718649,
      "grad_norm": 0.06475140899419785,
      "learning_rate": 0.015474317445627026,
      "loss": 0.0001,
      "step": 91330
    },
    {
      "epoch": 42.2674687644609,
      "grad_norm": 0.027498185634613037,
      "learning_rate": 0.015465062471078206,
      "loss": 0.0001,
      "step": 91340
    },
    {
      "epoch": 42.272096251735306,
      "grad_norm": 0.0031643847469240427,
      "learning_rate": 0.015455807496529384,
      "loss": 0.0002,
      "step": 91350
    },
    {
      "epoch": 42.27672373900972,
      "grad_norm": 0.01338954921811819,
      "learning_rate": 0.015446552521980565,
      "loss": 0.0001,
      "step": 91360
    },
    {
      "epoch": 42.28135122628413,
      "grad_norm": 0.007243734318763018,
      "learning_rate": 0.015437297547431745,
      "loss": 0.0004,
      "step": 91370
    },
    {
      "epoch": 42.285978713558535,
      "grad_norm": 0.0062226345762610435,
      "learning_rate": 0.015428042572882925,
      "loss": 0.0001,
      "step": 91380
    },
    {
      "epoch": 42.29060620083295,
      "grad_norm": 0.004031742922961712,
      "learning_rate": 0.015418787598334106,
      "loss": 0.0001,
      "step": 91390
    },
    {
      "epoch": 42.29523368810736,
      "grad_norm": 0.015637367963790894,
      "learning_rate": 0.015409532623785286,
      "loss": 0.0001,
      "step": 91400
    },
    {
      "epoch": 42.299861175381764,
      "grad_norm": 0.017457719892263412,
      "learning_rate": 0.015400277649236467,
      "loss": 0.0001,
      "step": 91410
    },
    {
      "epoch": 42.304488662656176,
      "grad_norm": 0.008523520082235336,
      "learning_rate": 0.015391022674687643,
      "loss": 0.0001,
      "step": 91420
    },
    {
      "epoch": 42.30911614993059,
      "grad_norm": 0.02232155203819275,
      "learning_rate": 0.015381767700138825,
      "loss": 0.0001,
      "step": 91430
    },
    {
      "epoch": 42.313743637205,
      "grad_norm": 0.03972930833697319,
      "learning_rate": 0.015372512725590004,
      "loss": 0.0002,
      "step": 91440
    },
    {
      "epoch": 42.318371124479405,
      "grad_norm": 0.8066215515136719,
      "learning_rate": 0.015363257751041186,
      "loss": 0.0002,
      "step": 91450
    },
    {
      "epoch": 42.32299861175382,
      "grad_norm": 0.10311001539230347,
      "learning_rate": 0.015354002776492365,
      "loss": 0.0001,
      "step": 91460
    },
    {
      "epoch": 42.32762609902823,
      "grad_norm": 0.025756999850273132,
      "learning_rate": 0.015344747801943545,
      "loss": 0.0,
      "step": 91470
    },
    {
      "epoch": 42.332253586302635,
      "grad_norm": 0.007885058410465717,
      "learning_rate": 0.015335492827394727,
      "loss": 0.0001,
      "step": 91480
    },
    {
      "epoch": 42.33688107357705,
      "grad_norm": 0.0023600051645189524,
      "learning_rate": 0.015326237852845906,
      "loss": 0.0001,
      "step": 91490
    },
    {
      "epoch": 42.34150856085146,
      "grad_norm": 0.006794101558625698,
      "learning_rate": 0.015316982878297084,
      "loss": 0.0002,
      "step": 91500
    },
    {
      "epoch": 42.34613604812587,
      "grad_norm": 0.09049134701490402,
      "learning_rate": 0.015307727903748264,
      "loss": 0.0001,
      "step": 91510
    },
    {
      "epoch": 42.350763535400276,
      "grad_norm": 0.003656108397990465,
      "learning_rate": 0.015298472929199445,
      "loss": 0.0004,
      "step": 91520
    },
    {
      "epoch": 42.35539102267469,
      "grad_norm": 0.009859736077487469,
      "learning_rate": 0.015289217954650625,
      "loss": 0.0001,
      "step": 91530
    },
    {
      "epoch": 42.3600185099491,
      "grad_norm": 0.0044225784949958324,
      "learning_rate": 0.015279962980101806,
      "loss": 0.0,
      "step": 91540
    },
    {
      "epoch": 42.364645997223505,
      "grad_norm": 0.0041087036952376366,
      "learning_rate": 0.015270708005552986,
      "loss": 0.0002,
      "step": 91550
    },
    {
      "epoch": 42.36927348449792,
      "grad_norm": 0.0020271039102226496,
      "learning_rate": 0.015261453031004166,
      "loss": 0.0001,
      "step": 91560
    },
    {
      "epoch": 42.37390097177233,
      "grad_norm": 0.0012542788172140718,
      "learning_rate": 0.015252198056455347,
      "loss": 0.0001,
      "step": 91570
    },
    {
      "epoch": 42.378528459046734,
      "grad_norm": 0.0020052536856383085,
      "learning_rate": 0.015242943081906525,
      "loss": 0.0,
      "step": 91580
    },
    {
      "epoch": 42.383155946321146,
      "grad_norm": 0.002108692191541195,
      "learning_rate": 0.015233688107357705,
      "loss": 0.0001,
      "step": 91590
    },
    {
      "epoch": 42.38778343359556,
      "grad_norm": 0.004304696340113878,
      "learning_rate": 0.015224433132808884,
      "loss": 0.0001,
      "step": 91600
    },
    {
      "epoch": 42.39241092086997,
      "grad_norm": 0.01398716401308775,
      "learning_rate": 0.015215178158260066,
      "loss": 0.0,
      "step": 91610
    },
    {
      "epoch": 42.397038408144375,
      "grad_norm": 0.010063743218779564,
      "learning_rate": 0.015205923183711246,
      "loss": 0.0001,
      "step": 91620
    },
    {
      "epoch": 42.40166589541879,
      "grad_norm": 0.004074010998010635,
      "learning_rate": 0.015196668209162427,
      "loss": 0.0001,
      "step": 91630
    },
    {
      "epoch": 42.4062933826932,
      "grad_norm": 0.004147556144744158,
      "learning_rate": 0.015187413234613607,
      "loss": 0.0,
      "step": 91640
    },
    {
      "epoch": 42.410920869967605,
      "grad_norm": 0.000827597570605576,
      "learning_rate": 0.015178158260064785,
      "loss": 0.0001,
      "step": 91650
    },
    {
      "epoch": 42.41554835724202,
      "grad_norm": 0.004774636123329401,
      "learning_rate": 0.015168903285515964,
      "loss": 0.0,
      "step": 91660
    },
    {
      "epoch": 42.42017584451643,
      "grad_norm": 0.004829517100006342,
      "learning_rate": 0.015159648310967146,
      "loss": 0.0001,
      "step": 91670
    },
    {
      "epoch": 42.42480333179084,
      "grad_norm": 0.05288231372833252,
      "learning_rate": 0.015150393336418325,
      "loss": 0.0001,
      "step": 91680
    },
    {
      "epoch": 42.429430819065246,
      "grad_norm": 0.00777104077860713,
      "learning_rate": 0.015141138361869505,
      "loss": 0.0,
      "step": 91690
    },
    {
      "epoch": 42.43405830633966,
      "grad_norm": 0.0019464633660390973,
      "learning_rate": 0.015131883387320686,
      "loss": 0.0,
      "step": 91700
    },
    {
      "epoch": 42.43868579361407,
      "grad_norm": 0.004431906156241894,
      "learning_rate": 0.015122628412771866,
      "loss": 0.0001,
      "step": 91710
    },
    {
      "epoch": 42.443313280888475,
      "grad_norm": 0.00580345094203949,
      "learning_rate": 0.015113373438223048,
      "loss": 0.0001,
      "step": 91720
    },
    {
      "epoch": 42.44794076816289,
      "grad_norm": 0.004805793985724449,
      "learning_rate": 0.015104118463674224,
      "loss": 0.0001,
      "step": 91730
    },
    {
      "epoch": 42.4525682554373,
      "grad_norm": 0.0021819581743329763,
      "learning_rate": 0.015094863489125405,
      "loss": 0.0001,
      "step": 91740
    },
    {
      "epoch": 42.457195742711704,
      "grad_norm": 0.12046936899423599,
      "learning_rate": 0.015085608514576585,
      "loss": 0.0001,
      "step": 91750
    },
    {
      "epoch": 42.461823229986116,
      "grad_norm": 0.0013605388812720776,
      "learning_rate": 0.015076353540027766,
      "loss": 0.0001,
      "step": 91760
    },
    {
      "epoch": 42.46645071726053,
      "grad_norm": 0.0012291106395423412,
      "learning_rate": 0.015067098565478946,
      "loss": 0.0001,
      "step": 91770
    },
    {
      "epoch": 42.47107820453494,
      "grad_norm": 0.005876054055988789,
      "learning_rate": 0.015057843590930126,
      "loss": 0.0001,
      "step": 91780
    },
    {
      "epoch": 42.475705691809345,
      "grad_norm": 0.03432084992527962,
      "learning_rate": 0.015048588616381307,
      "loss": 0.0001,
      "step": 91790
    },
    {
      "epoch": 42.48033317908376,
      "grad_norm": 0.007595611736178398,
      "learning_rate": 0.015039333641832487,
      "loss": 0.0002,
      "step": 91800
    },
    {
      "epoch": 42.48496066635817,
      "grad_norm": 0.0012964154593646526,
      "learning_rate": 0.015030078667283665,
      "loss": 0.0002,
      "step": 91810
    },
    {
      "epoch": 42.489588153632575,
      "grad_norm": 0.005467692855745554,
      "learning_rate": 0.015020823692734844,
      "loss": 0.0002,
      "step": 91820
    },
    {
      "epoch": 42.49421564090699,
      "grad_norm": 0.0012671869480982423,
      "learning_rate": 0.015011568718186026,
      "loss": 0.0003,
      "step": 91830
    },
    {
      "epoch": 42.4988431281814,
      "grad_norm": 0.029535675421357155,
      "learning_rate": 0.015002313743637205,
      "loss": 0.0001,
      "step": 91840
    },
    {
      "epoch": 42.50347061545581,
      "grad_norm": 0.04030314460396767,
      "learning_rate": 0.014993058769088387,
      "loss": 0.0001,
      "step": 91850
    },
    {
      "epoch": 42.508098102730216,
      "grad_norm": 0.003944857511669397,
      "learning_rate": 0.014983803794539566,
      "loss": 0.0001,
      "step": 91860
    },
    {
      "epoch": 42.51272559000463,
      "grad_norm": 0.0022866923827677965,
      "learning_rate": 0.014974548819990746,
      "loss": 0.0001,
      "step": 91870
    },
    {
      "epoch": 42.51735307727904,
      "grad_norm": 0.021596025675535202,
      "learning_rate": 0.014965293845441924,
      "loss": 0.0001,
      "step": 91880
    },
    {
      "epoch": 42.521980564553445,
      "grad_norm": 0.038212429732084274,
      "learning_rate": 0.014956038870893106,
      "loss": 0.0001,
      "step": 91890
    },
    {
      "epoch": 42.52660805182786,
      "grad_norm": 0.002807921264320612,
      "learning_rate": 0.014946783896344285,
      "loss": 0.0001,
      "step": 91900
    },
    {
      "epoch": 42.53123553910227,
      "grad_norm": 0.0017008193535730243,
      "learning_rate": 0.014937528921795465,
      "loss": 0.0007,
      "step": 91910
    },
    {
      "epoch": 42.535863026376674,
      "grad_norm": 0.06260324269533157,
      "learning_rate": 0.014928273947246646,
      "loss": 0.0001,
      "step": 91920
    },
    {
      "epoch": 42.540490513651086,
      "grad_norm": 0.043502844870090485,
      "learning_rate": 0.014919018972697826,
      "loss": 0.0003,
      "step": 91930
    },
    {
      "epoch": 42.5451180009255,
      "grad_norm": 0.011310001835227013,
      "learning_rate": 0.014909763998149007,
      "loss": 0.0001,
      "step": 91940
    },
    {
      "epoch": 42.54974548819991,
      "grad_norm": 0.0019261499401181936,
      "learning_rate": 0.014900509023600187,
      "loss": 0.0001,
      "step": 91950
    },
    {
      "epoch": 42.554372975474315,
      "grad_norm": 0.0005302145145833492,
      "learning_rate": 0.014891254049051365,
      "loss": 0.0002,
      "step": 91960
    },
    {
      "epoch": 42.55900046274873,
      "grad_norm": 0.0021443760488182306,
      "learning_rate": 0.014881999074502545,
      "loss": 0.0001,
      "step": 91970
    },
    {
      "epoch": 42.56362795002314,
      "grad_norm": 0.013439160771667957,
      "learning_rate": 0.014872744099953726,
      "loss": 0.0001,
      "step": 91980
    },
    {
      "epoch": 42.568255437297545,
      "grad_norm": 0.055754899978637695,
      "learning_rate": 0.014863489125404906,
      "loss": 0.0001,
      "step": 91990
    },
    {
      "epoch": 42.57288292457196,
      "grad_norm": 0.007072074338793755,
      "learning_rate": 0.014854234150856085,
      "loss": 0.0001,
      "step": 92000
    },
    {
      "epoch": 42.57751041184637,
      "grad_norm": 0.003417082829400897,
      "learning_rate": 0.014844979176307267,
      "loss": 0.0001,
      "step": 92010
    },
    {
      "epoch": 42.58213789912078,
      "grad_norm": 0.00715620955452323,
      "learning_rate": 0.014835724201758447,
      "loss": 0.0,
      "step": 92020
    },
    {
      "epoch": 42.586765386395186,
      "grad_norm": 0.000629412941634655,
      "learning_rate": 0.014826469227209624,
      "loss": 0.0,
      "step": 92030
    },
    {
      "epoch": 42.5913928736696,
      "grad_norm": 0.01135173812508583,
      "learning_rate": 0.014817214252660804,
      "loss": 0.0001,
      "step": 92040
    },
    {
      "epoch": 42.59602036094401,
      "grad_norm": 0.0021069052163511515,
      "learning_rate": 0.014807959278111986,
      "loss": 0.0002,
      "step": 92050
    },
    {
      "epoch": 42.600647848218415,
      "grad_norm": 0.018336636945605278,
      "learning_rate": 0.014798704303563165,
      "loss": 0.0,
      "step": 92060
    },
    {
      "epoch": 42.60527533549283,
      "grad_norm": 0.041555006057024,
      "learning_rate": 0.014789449329014347,
      "loss": 0.0028,
      "step": 92070
    },
    {
      "epoch": 42.60990282276724,
      "grad_norm": 0.017597077414393425,
      "learning_rate": 0.014780194354465526,
      "loss": 0.0002,
      "step": 92080
    },
    {
      "epoch": 42.614530310041644,
      "grad_norm": 0.023476997390389442,
      "learning_rate": 0.014770939379916706,
      "loss": 0.0001,
      "step": 92090
    },
    {
      "epoch": 42.619157797316056,
      "grad_norm": 0.01438372302800417,
      "learning_rate": 0.014761684405367887,
      "loss": 0.0001,
      "step": 92100
    },
    {
      "epoch": 42.62378528459047,
      "grad_norm": 0.0398072749376297,
      "learning_rate": 0.014752429430819065,
      "loss": 0.0001,
      "step": 92110
    },
    {
      "epoch": 42.62841277186488,
      "grad_norm": 0.017010467126965523,
      "learning_rate": 0.014743174456270245,
      "loss": 0.0001,
      "step": 92120
    },
    {
      "epoch": 42.633040259139285,
      "grad_norm": 0.004008156713098288,
      "learning_rate": 0.014733919481721425,
      "loss": 0.0001,
      "step": 92130
    },
    {
      "epoch": 42.6376677464137,
      "grad_norm": 0.04069645702838898,
      "learning_rate": 0.014724664507172606,
      "loss": 0.0002,
      "step": 92140
    },
    {
      "epoch": 42.64229523368811,
      "grad_norm": 0.003916497807949781,
      "learning_rate": 0.014715409532623786,
      "loss": 0.0001,
      "step": 92150
    },
    {
      "epoch": 42.646922720962515,
      "grad_norm": 0.011283806525170803,
      "learning_rate": 0.014706154558074967,
      "loss": 0.0006,
      "step": 92160
    },
    {
      "epoch": 42.65155020823693,
      "grad_norm": 0.0016766368644312024,
      "learning_rate": 0.014696899583526147,
      "loss": 0.0012,
      "step": 92170
    },
    {
      "epoch": 42.65617769551134,
      "grad_norm": 0.001994271297007799,
      "learning_rate": 0.014687644608977327,
      "loss": 0.0001,
      "step": 92180
    },
    {
      "epoch": 42.66080518278575,
      "grad_norm": 0.020555652678012848,
      "learning_rate": 0.014678389634428505,
      "loss": 0.0006,
      "step": 92190
    },
    {
      "epoch": 42.665432670060156,
      "grad_norm": 0.0016945557435974479,
      "learning_rate": 0.014669134659879686,
      "loss": 0.0001,
      "step": 92200
    },
    {
      "epoch": 42.67006015733457,
      "grad_norm": 0.0324748158454895,
      "learning_rate": 0.014659879685330866,
      "loss": 0.0001,
      "step": 92210
    },
    {
      "epoch": 42.67468764460898,
      "grad_norm": 0.010767453350126743,
      "learning_rate": 0.014650624710782045,
      "loss": 0.0001,
      "step": 92220
    },
    {
      "epoch": 42.679315131883385,
      "grad_norm": 0.0018966455245390534,
      "learning_rate": 0.014641369736233227,
      "loss": 0.0,
      "step": 92230
    },
    {
      "epoch": 42.6839426191578,
      "grad_norm": 0.04377545416355133,
      "learning_rate": 0.014632114761684406,
      "loss": 0.0003,
      "step": 92240
    },
    {
      "epoch": 42.68857010643221,
      "grad_norm": 0.002562368754297495,
      "learning_rate": 0.014622859787135588,
      "loss": 0.0002,
      "step": 92250
    },
    {
      "epoch": 42.693197593706614,
      "grad_norm": 0.0014004322001710534,
      "learning_rate": 0.014613604812586764,
      "loss": 0.0001,
      "step": 92260
    },
    {
      "epoch": 42.697825080981026,
      "grad_norm": 0.0027254316955804825,
      "learning_rate": 0.014604349838037945,
      "loss": 0.0001,
      "step": 92270
    },
    {
      "epoch": 42.70245256825544,
      "grad_norm": 0.013362612575292587,
      "learning_rate": 0.014595094863489125,
      "loss": 0.0003,
      "step": 92280
    },
    {
      "epoch": 42.70708005552985,
      "grad_norm": 0.00427418015897274,
      "learning_rate": 0.014585839888940307,
      "loss": 0.0001,
      "step": 92290
    },
    {
      "epoch": 42.711707542804255,
      "grad_norm": 0.004348314832895994,
      "learning_rate": 0.014576584914391486,
      "loss": 0.0001,
      "step": 92300
    },
    {
      "epoch": 42.71633503007867,
      "grad_norm": 0.024110982194542885,
      "learning_rate": 0.014567329939842666,
      "loss": 0.0001,
      "step": 92310
    },
    {
      "epoch": 42.72096251735308,
      "grad_norm": 0.004162723198533058,
      "learning_rate": 0.014558074965293847,
      "loss": 0.0,
      "step": 92320
    },
    {
      "epoch": 42.725590004627485,
      "grad_norm": 0.001131150871515274,
      "learning_rate": 0.014548819990745027,
      "loss": 0.0001,
      "step": 92330
    },
    {
      "epoch": 42.7302174919019,
      "grad_norm": 0.00695290882140398,
      "learning_rate": 0.014539565016196205,
      "loss": 0.0001,
      "step": 92340
    },
    {
      "epoch": 42.73484497917631,
      "grad_norm": 0.01923794113099575,
      "learning_rate": 0.014530310041647385,
      "loss": 0.0001,
      "step": 92350
    },
    {
      "epoch": 42.73947246645072,
      "grad_norm": 0.008502387441694736,
      "learning_rate": 0.014521055067098566,
      "loss": 0.0,
      "step": 92360
    },
    {
      "epoch": 42.744099953725126,
      "grad_norm": 0.0060089584439992905,
      "learning_rate": 0.014511800092549746,
      "loss": 0.0001,
      "step": 92370
    },
    {
      "epoch": 42.74872744099954,
      "grad_norm": 0.01731257513165474,
      "learning_rate": 0.014502545118000927,
      "loss": 0.0004,
      "step": 92380
    },
    {
      "epoch": 42.75335492827395,
      "grad_norm": 0.0035141848493367434,
      "learning_rate": 0.014493290143452107,
      "loss": 0.0001,
      "step": 92390
    },
    {
      "epoch": 42.757982415548355,
      "grad_norm": 0.002153831534087658,
      "learning_rate": 0.014484035168903286,
      "loss": 0.0001,
      "step": 92400
    },
    {
      "epoch": 42.76260990282277,
      "grad_norm": 0.0005674064159393311,
      "learning_rate": 0.014474780194354468,
      "loss": 0.0001,
      "step": 92410
    },
    {
      "epoch": 42.76723739009718,
      "grad_norm": 0.009816424921154976,
      "learning_rate": 0.014465525219805646,
      "loss": 0.0001,
      "step": 92420
    },
    {
      "epoch": 42.771864877371584,
      "grad_norm": 0.013918345794081688,
      "learning_rate": 0.014456270245256825,
      "loss": 0.0003,
      "step": 92430
    },
    {
      "epoch": 42.776492364645996,
      "grad_norm": 0.02276512235403061,
      "learning_rate": 0.014447015270708005,
      "loss": 0.0001,
      "step": 92440
    },
    {
      "epoch": 42.78111985192041,
      "grad_norm": 0.12232302129268646,
      "learning_rate": 0.014437760296159187,
      "loss": 0.0001,
      "step": 92450
    },
    {
      "epoch": 42.78574733919482,
      "grad_norm": 0.10329879075288773,
      "learning_rate": 0.014428505321610366,
      "loss": 0.0001,
      "step": 92460
    },
    {
      "epoch": 42.790374826469225,
      "grad_norm": 0.017396358773112297,
      "learning_rate": 0.014419250347061548,
      "loss": 0.0002,
      "step": 92470
    },
    {
      "epoch": 42.79500231374364,
      "grad_norm": 0.11996042728424072,
      "learning_rate": 0.014409995372512727,
      "loss": 0.0002,
      "step": 92480
    },
    {
      "epoch": 42.79962980101805,
      "grad_norm": 0.011574216187000275,
      "learning_rate": 0.014400740397963905,
      "loss": 0.0002,
      "step": 92490
    },
    {
      "epoch": 42.804257288292455,
      "grad_norm": 0.12322433292865753,
      "learning_rate": 0.014391485423415085,
      "loss": 0.0001,
      "step": 92500
    },
    {
      "epoch": 42.80888477556687,
      "grad_norm": 0.004044054541736841,
      "learning_rate": 0.014382230448866266,
      "loss": 0.0001,
      "step": 92510
    },
    {
      "epoch": 42.81351226284128,
      "grad_norm": 0.009947300888597965,
      "learning_rate": 0.014372975474317446,
      "loss": 0.0001,
      "step": 92520
    },
    {
      "epoch": 42.818139750115684,
      "grad_norm": 0.008744321763515472,
      "learning_rate": 0.014363720499768626,
      "loss": 0.0001,
      "step": 92530
    },
    {
      "epoch": 42.822767237390096,
      "grad_norm": 0.0019935935270041227,
      "learning_rate": 0.014354465525219807,
      "loss": 0.0001,
      "step": 92540
    },
    {
      "epoch": 42.82739472466451,
      "grad_norm": 0.21606169641017914,
      "learning_rate": 0.014345210550670987,
      "loss": 0.0002,
      "step": 92550
    },
    {
      "epoch": 42.83202221193892,
      "grad_norm": 0.018519580364227295,
      "learning_rate": 0.014335955576122168,
      "loss": 0.0001,
      "step": 92560
    },
    {
      "epoch": 42.836649699213325,
      "grad_norm": 0.0065860566683113575,
      "learning_rate": 0.014326700601573344,
      "loss": 0.0,
      "step": 92570
    },
    {
      "epoch": 42.84127718648774,
      "grad_norm": 0.0001999064552364871,
      "learning_rate": 0.014317445627024526,
      "loss": 0.0001,
      "step": 92580
    },
    {
      "epoch": 42.84590467376215,
      "grad_norm": 0.009151927195489407,
      "learning_rate": 0.014308190652475706,
      "loss": 0.0001,
      "step": 92590
    },
    {
      "epoch": 42.850532161036554,
      "grad_norm": 0.002478116424754262,
      "learning_rate": 0.014298935677926887,
      "loss": 0.0003,
      "step": 92600
    },
    {
      "epoch": 42.855159648310966,
      "grad_norm": 0.4528251886367798,
      "learning_rate": 0.014289680703378067,
      "loss": 0.0003,
      "step": 92610
    },
    {
      "epoch": 42.85978713558538,
      "grad_norm": 0.034899093210697174,
      "learning_rate": 0.014280425728829246,
      "loss": 0.0001,
      "step": 92620
    },
    {
      "epoch": 42.86441462285979,
      "grad_norm": 0.017201056703925133,
      "learning_rate": 0.014271170754280428,
      "loss": 0.0001,
      "step": 92630
    },
    {
      "epoch": 42.869042110134195,
      "grad_norm": 0.020535334944725037,
      "learning_rate": 0.014261915779731607,
      "loss": 0.0001,
      "step": 92640
    },
    {
      "epoch": 42.87366959740861,
      "grad_norm": 0.002358123892918229,
      "learning_rate": 0.014252660805182785,
      "loss": 0.0001,
      "step": 92650
    },
    {
      "epoch": 42.87829708468302,
      "grad_norm": 0.005475206300616264,
      "learning_rate": 0.014243405830633965,
      "loss": 0.0001,
      "step": 92660
    },
    {
      "epoch": 42.882924571957425,
      "grad_norm": 0.021820055320858955,
      "learning_rate": 0.014234150856085146,
      "loss": 0.0003,
      "step": 92670
    },
    {
      "epoch": 42.88755205923184,
      "grad_norm": 0.012327627278864384,
      "learning_rate": 0.014224895881536326,
      "loss": 0.0,
      "step": 92680
    },
    {
      "epoch": 42.89217954650625,
      "grad_norm": 0.0023180157877504826,
      "learning_rate": 0.014215640906987508,
      "loss": 0.0,
      "step": 92690
    },
    {
      "epoch": 42.896807033780654,
      "grad_norm": 0.002277072286233306,
      "learning_rate": 0.014206385932438687,
      "loss": 0.0,
      "step": 92700
    },
    {
      "epoch": 42.901434521055066,
      "grad_norm": 0.004002464469522238,
      "learning_rate": 0.014197130957889867,
      "loss": 0.0,
      "step": 92710
    },
    {
      "epoch": 42.90606200832948,
      "grad_norm": 0.029480447992682457,
      "learning_rate": 0.014187875983341045,
      "loss": 0.0004,
      "step": 92720
    },
    {
      "epoch": 42.91068949560389,
      "grad_norm": 0.04934917390346527,
      "learning_rate": 0.014178621008792226,
      "loss": 0.0002,
      "step": 92730
    },
    {
      "epoch": 42.915316982878295,
      "grad_norm": 0.0021607421804219484,
      "learning_rate": 0.014169366034243406,
      "loss": 0.0001,
      "step": 92740
    },
    {
      "epoch": 42.91994447015271,
      "grad_norm": 0.017306501045823097,
      "learning_rate": 0.014160111059694586,
      "loss": 0.0001,
      "step": 92750
    },
    {
      "epoch": 42.92457195742712,
      "grad_norm": 0.0030548940412700176,
      "learning_rate": 0.014150856085145767,
      "loss": 0.0007,
      "step": 92760
    },
    {
      "epoch": 42.929199444701524,
      "grad_norm": 0.028035275638103485,
      "learning_rate": 0.014141601110596947,
      "loss": 0.0001,
      "step": 92770
    },
    {
      "epoch": 42.933826931975936,
      "grad_norm": 0.019284818321466446,
      "learning_rate": 0.014132346136048128,
      "loss": 0.0001,
      "step": 92780
    },
    {
      "epoch": 42.93845441925035,
      "grad_norm": 0.0021375289652496576,
      "learning_rate": 0.014123091161499308,
      "loss": 0.0,
      "step": 92790
    },
    {
      "epoch": 42.94308190652476,
      "grad_norm": 0.004161304794251919,
      "learning_rate": 0.014113836186950486,
      "loss": 0.0,
      "step": 92800
    },
    {
      "epoch": 42.947709393799165,
      "grad_norm": 0.013607066124677658,
      "learning_rate": 0.014104581212401665,
      "loss": 0.0001,
      "step": 92810
    },
    {
      "epoch": 42.95233688107358,
      "grad_norm": 0.040930308401584625,
      "learning_rate": 0.014095326237852847,
      "loss": 0.0001,
      "step": 92820
    },
    {
      "epoch": 42.95696436834799,
      "grad_norm": 0.0010927406838163733,
      "learning_rate": 0.014086071263304027,
      "loss": 0.0001,
      "step": 92830
    },
    {
      "epoch": 42.961591855622395,
      "grad_norm": 0.006702728103846312,
      "learning_rate": 0.014076816288755206,
      "loss": 0.0001,
      "step": 92840
    },
    {
      "epoch": 42.96621934289681,
      "grad_norm": 0.18594089150428772,
      "learning_rate": 0.014067561314206388,
      "loss": 0.0001,
      "step": 92850
    },
    {
      "epoch": 42.97084683017122,
      "grad_norm": 0.002387468935921788,
      "learning_rate": 0.014058306339657567,
      "loss": 0.0001,
      "step": 92860
    },
    {
      "epoch": 42.975474317445624,
      "grad_norm": 0.032216571271419525,
      "learning_rate": 0.014049051365108749,
      "loss": 0.0001,
      "step": 92870
    },
    {
      "epoch": 42.980101804720036,
      "grad_norm": 0.014827580191195011,
      "learning_rate": 0.014039796390559925,
      "loss": 0.0002,
      "step": 92880
    },
    {
      "epoch": 42.98472929199445,
      "grad_norm": 0.024566615000367165,
      "learning_rate": 0.014030541416011106,
      "loss": 0.0001,
      "step": 92890
    },
    {
      "epoch": 42.98935677926886,
      "grad_norm": 0.008039579726755619,
      "learning_rate": 0.014021286441462286,
      "loss": 0.0001,
      "step": 92900
    },
    {
      "epoch": 42.993984266543265,
      "grad_norm": 0.01739148609340191,
      "learning_rate": 0.014012031466913467,
      "loss": 0.0003,
      "step": 92910
    },
    {
      "epoch": 42.99861175381768,
      "grad_norm": 0.2315182238817215,
      "learning_rate": 0.014002776492364647,
      "loss": 0.0002,
      "step": 92920
    },
    {
      "epoch": 43.0,
      "eval_accuracy_branch1": 0.9901401941149284,
      "eval_accuracy_branch2": 0.5010013865352025,
      "eval_f1_branch1": 0.9914044647188661,
      "eval_f1_branch2": 0.5008757083392447,
      "eval_loss": 0.017264071851968765,
      "eval_precision_branch1": 0.9916985852625579,
      "eval_precision_branch2": 0.5010023961381442,
      "eval_recall_branch1": 0.9912378799062268,
      "eval_recall_branch2": 0.5010013865352025,
      "eval_runtime": 29.1815,
      "eval_samples_per_second": 444.871,
      "eval_steps_per_second": 55.617,
      "step": 92923
    },
    {
      "epoch": 43.00323924109209,
      "grad_norm": 0.004139818716794252,
      "learning_rate": 0.013993521517815827,
      "loss": 0.0188,
      "step": 92930
    },
    {
      "epoch": 43.007866728366494,
      "grad_norm": 0.0006912979297339916,
      "learning_rate": 0.013984266543267008,
      "loss": 0.0001,
      "step": 92940
    },
    {
      "epoch": 43.012494215640906,
      "grad_norm": 0.0016943031223490834,
      "learning_rate": 0.013975011568718186,
      "loss": 0.0001,
      "step": 92950
    },
    {
      "epoch": 43.01712170291532,
      "grad_norm": 0.013381426222622395,
      "learning_rate": 0.013965756594169366,
      "loss": 0.0002,
      "step": 92960
    },
    {
      "epoch": 43.02174919018973,
      "grad_norm": 0.0017002265667542815,
      "learning_rate": 0.013956501619620545,
      "loss": 0.0,
      "step": 92970
    },
    {
      "epoch": 43.026376677464135,
      "grad_norm": 0.6800070405006409,
      "learning_rate": 0.013947246645071727,
      "loss": 0.0002,
      "step": 92980
    },
    {
      "epoch": 43.03100416473855,
      "grad_norm": 0.004691886715590954,
      "learning_rate": 0.013937991670522907,
      "loss": 0.0,
      "step": 92990
    },
    {
      "epoch": 43.03563165201296,
      "grad_norm": 0.0018542970065027475,
      "learning_rate": 0.013928736695974088,
      "loss": 0.0,
      "step": 93000
    },
    {
      "epoch": 43.040259139287365,
      "grad_norm": 0.025021955370903015,
      "learning_rate": 0.013919481721425268,
      "loss": 0.0,
      "step": 93010
    },
    {
      "epoch": 43.04488662656178,
      "grad_norm": 0.0030545773915946484,
      "learning_rate": 0.013910226746876447,
      "loss": 0.0001,
      "step": 93020
    },
    {
      "epoch": 43.04951411383619,
      "grad_norm": 0.010178767144680023,
      "learning_rate": 0.013900971772327625,
      "loss": 0.0001,
      "step": 93030
    },
    {
      "epoch": 43.054141601110594,
      "grad_norm": 0.020261075347661972,
      "learning_rate": 0.013891716797778807,
      "loss": 0.0009,
      "step": 93040
    },
    {
      "epoch": 43.058769088385006,
      "grad_norm": 0.007601589895784855,
      "learning_rate": 0.013882461823229986,
      "loss": 0.0001,
      "step": 93050
    },
    {
      "epoch": 43.06339657565942,
      "grad_norm": 0.0022158618085086346,
      "learning_rate": 0.013873206848681166,
      "loss": 0.0002,
      "step": 93060
    },
    {
      "epoch": 43.06802406293383,
      "grad_norm": 0.0032267640344798565,
      "learning_rate": 0.013863951874132347,
      "loss": 0.0001,
      "step": 93070
    },
    {
      "epoch": 43.072651550208235,
      "grad_norm": 0.005307327955961227,
      "learning_rate": 0.013854696899583527,
      "loss": 0.0001,
      "step": 93080
    },
    {
      "epoch": 43.07727903748265,
      "grad_norm": 0.004446188919246197,
      "learning_rate": 0.013845441925034709,
      "loss": 0.0001,
      "step": 93090
    },
    {
      "epoch": 43.08190652475706,
      "grad_norm": 0.0009849395137280226,
      "learning_rate": 0.013836186950485888,
      "loss": 0.0,
      "step": 93100
    },
    {
      "epoch": 43.086534012031464,
      "grad_norm": 0.010634169913828373,
      "learning_rate": 0.013826931975937066,
      "loss": 0.0002,
      "step": 93110
    },
    {
      "epoch": 43.091161499305876,
      "grad_norm": 0.0017229745863005519,
      "learning_rate": 0.013817677001388246,
      "loss": 0.0003,
      "step": 93120
    },
    {
      "epoch": 43.09578898658029,
      "grad_norm": 0.03512217104434967,
      "learning_rate": 0.013808422026839427,
      "loss": 0.0001,
      "step": 93130
    },
    {
      "epoch": 43.1004164738547,
      "grad_norm": 0.008976979181170464,
      "learning_rate": 0.013799167052290607,
      "loss": 0.0003,
      "step": 93140
    },
    {
      "epoch": 43.105043961129105,
      "grad_norm": 0.009105040691792965,
      "learning_rate": 0.013789912077741787,
      "loss": 0.0001,
      "step": 93150
    },
    {
      "epoch": 43.10967144840352,
      "grad_norm": 0.6788720488548279,
      "learning_rate": 0.013780657103192968,
      "loss": 0.0003,
      "step": 93160
    },
    {
      "epoch": 43.11429893567793,
      "grad_norm": 0.0050702206790447235,
      "learning_rate": 0.013771402128644148,
      "loss": 0.0001,
      "step": 93170
    },
    {
      "epoch": 43.118926422952335,
      "grad_norm": 0.01337431836873293,
      "learning_rate": 0.013762147154095326,
      "loss": 0.0001,
      "step": 93180
    },
    {
      "epoch": 43.12355391022675,
      "grad_norm": 0.0015197318280115724,
      "learning_rate": 0.013752892179546505,
      "loss": 0.0001,
      "step": 93190
    },
    {
      "epoch": 43.12818139750116,
      "grad_norm": 0.005317173898220062,
      "learning_rate": 0.013743637204997687,
      "loss": 0.0001,
      "step": 93200
    },
    {
      "epoch": 43.132808884775564,
      "grad_norm": 0.015338439494371414,
      "learning_rate": 0.013734382230448866,
      "loss": 0.0001,
      "step": 93210
    },
    {
      "epoch": 43.137436372049976,
      "grad_norm": 0.0005946385208517313,
      "learning_rate": 0.013725127255900048,
      "loss": 0.0,
      "step": 93220
    },
    {
      "epoch": 43.14206385932439,
      "grad_norm": 0.0017400650540366769,
      "learning_rate": 0.013715872281351228,
      "loss": 0.0003,
      "step": 93230
    },
    {
      "epoch": 43.1466913465988,
      "grad_norm": 0.0028685720171779394,
      "learning_rate": 0.013706617306802407,
      "loss": 0.0001,
      "step": 93240
    },
    {
      "epoch": 43.151318833873205,
      "grad_norm": 0.008478092961013317,
      "learning_rate": 0.013697362332253589,
      "loss": 0.0042,
      "step": 93250
    },
    {
      "epoch": 43.15594632114762,
      "grad_norm": 0.014600727707147598,
      "learning_rate": 0.013688107357704767,
      "loss": 0.0001,
      "step": 93260
    },
    {
      "epoch": 43.16057380842203,
      "grad_norm": 0.010696307756006718,
      "learning_rate": 0.013678852383155946,
      "loss": 0.0001,
      "step": 93270
    },
    {
      "epoch": 43.165201295696434,
      "grad_norm": 0.001640072325244546,
      "learning_rate": 0.013669597408607126,
      "loss": 0.0001,
      "step": 93280
    },
    {
      "epoch": 43.169828782970846,
      "grad_norm": 0.002922442741692066,
      "learning_rate": 0.013660342434058307,
      "loss": 0.0001,
      "step": 93290
    },
    {
      "epoch": 43.17445627024526,
      "grad_norm": 0.019692877307534218,
      "learning_rate": 0.013651087459509487,
      "loss": 0.0002,
      "step": 93300
    },
    {
      "epoch": 43.17908375751966,
      "grad_norm": 0.11367416381835938,
      "learning_rate": 0.013641832484960668,
      "loss": 0.0001,
      "step": 93310
    },
    {
      "epoch": 43.183711244794075,
      "grad_norm": 0.0015214908635243773,
      "learning_rate": 0.013632577510411848,
      "loss": 0.0001,
      "step": 93320
    },
    {
      "epoch": 43.18833873206849,
      "grad_norm": 0.019394278526306152,
      "learning_rate": 0.013623322535863026,
      "loss": 0.0001,
      "step": 93330
    },
    {
      "epoch": 43.1929662193429,
      "grad_norm": 0.002647382440045476,
      "learning_rate": 0.013614067561314206,
      "loss": 0.0,
      "step": 93340
    },
    {
      "epoch": 43.197593706617305,
      "grad_norm": 0.002778995083644986,
      "learning_rate": 0.013604812586765387,
      "loss": 0.0001,
      "step": 93350
    },
    {
      "epoch": 43.20222119389172,
      "grad_norm": 0.0036904937587678432,
      "learning_rate": 0.013595557612216567,
      "loss": 0.0,
      "step": 93360
    },
    {
      "epoch": 43.20684868116613,
      "grad_norm": 0.019710317254066467,
      "learning_rate": 0.013586302637667746,
      "loss": 0.0001,
      "step": 93370
    },
    {
      "epoch": 43.211476168440534,
      "grad_norm": 0.0829196572303772,
      "learning_rate": 0.013577047663118928,
      "loss": 0.0001,
      "step": 93380
    },
    {
      "epoch": 43.216103655714946,
      "grad_norm": 0.015396841801702976,
      "learning_rate": 0.013567792688570108,
      "loss": 0.0001,
      "step": 93390
    },
    {
      "epoch": 43.22073114298936,
      "grad_norm": 0.06450098007917404,
      "learning_rate": 0.013558537714021289,
      "loss": 0.0001,
      "step": 93400
    },
    {
      "epoch": 43.22535863026377,
      "grad_norm": 0.0046047186478972435,
      "learning_rate": 0.013549282739472465,
      "loss": 0.0001,
      "step": 93410
    },
    {
      "epoch": 43.229986117538175,
      "grad_norm": 0.0291883684694767,
      "learning_rate": 0.013540027764923647,
      "loss": 0.0001,
      "step": 93420
    },
    {
      "epoch": 43.23461360481259,
      "grad_norm": 0.024741923436522484,
      "learning_rate": 0.013530772790374826,
      "loss": 0.0001,
      "step": 93430
    },
    {
      "epoch": 43.239241092087,
      "grad_norm": 0.0016514798626303673,
      "learning_rate": 0.013521517815826008,
      "loss": 0.0003,
      "step": 93440
    },
    {
      "epoch": 43.243868579361404,
      "grad_norm": 0.017634598538279533,
      "learning_rate": 0.013512262841277187,
      "loss": 0.0001,
      "step": 93450
    },
    {
      "epoch": 43.248496066635816,
      "grad_norm": 0.005484869237989187,
      "learning_rate": 0.013503007866728367,
      "loss": 0.0,
      "step": 93460
    },
    {
      "epoch": 43.25312355391023,
      "grad_norm": 0.0025653450284153223,
      "learning_rate": 0.013493752892179548,
      "loss": 0.0002,
      "step": 93470
    },
    {
      "epoch": 43.25775104118463,
      "grad_norm": 0.0016303263837471604,
      "learning_rate": 0.013484497917630728,
      "loss": 0.0,
      "step": 93480
    },
    {
      "epoch": 43.262378528459045,
      "grad_norm": 0.023932192474603653,
      "learning_rate": 0.013475242943081906,
      "loss": 0.0001,
      "step": 93490
    },
    {
      "epoch": 43.26700601573346,
      "grad_norm": 0.011341145262122154,
      "learning_rate": 0.013465987968533086,
      "loss": 0.0001,
      "step": 93500
    },
    {
      "epoch": 43.27163350300787,
      "grad_norm": 0.01431288756430149,
      "learning_rate": 0.013456732993984267,
      "loss": 0.0,
      "step": 93510
    },
    {
      "epoch": 43.276260990282275,
      "grad_norm": 0.003655304666608572,
      "learning_rate": 0.013447478019435447,
      "loss": 0.0002,
      "step": 93520
    },
    {
      "epoch": 43.28088847755669,
      "grad_norm": 0.0027468667831271887,
      "learning_rate": 0.013438223044886628,
      "loss": 0.0001,
      "step": 93530
    },
    {
      "epoch": 43.2855159648311,
      "grad_norm": 0.04730474203824997,
      "learning_rate": 0.013428968070337808,
      "loss": 0.0001,
      "step": 93540
    },
    {
      "epoch": 43.290143452105504,
      "grad_norm": 0.039989981800317764,
      "learning_rate": 0.013419713095788988,
      "loss": 0.0001,
      "step": 93550
    },
    {
      "epoch": 43.294770939379916,
      "grad_norm": 0.0014745541848242283,
      "learning_rate": 0.013410458121240166,
      "loss": 0.0001,
      "step": 93560
    },
    {
      "epoch": 43.29939842665433,
      "grad_norm": 0.03621530532836914,
      "learning_rate": 0.013401203146691347,
      "loss": 0.0001,
      "step": 93570
    },
    {
      "epoch": 43.30402591392874,
      "grad_norm": 0.006132124457508326,
      "learning_rate": 0.013391948172142527,
      "loss": 0.0001,
      "step": 93580
    },
    {
      "epoch": 43.308653401203145,
      "grad_norm": 0.003155398415401578,
      "learning_rate": 0.013382693197593706,
      "loss": 0.0001,
      "step": 93590
    },
    {
      "epoch": 43.31328088847756,
      "grad_norm": 0.007427528500556946,
      "learning_rate": 0.013373438223044888,
      "loss": 0.0001,
      "step": 93600
    },
    {
      "epoch": 43.31790837575197,
      "grad_norm": 0.004000325687229633,
      "learning_rate": 0.013364183248496067,
      "loss": 0.0001,
      "step": 93610
    },
    {
      "epoch": 43.322535863026374,
      "grad_norm": 0.01159324124455452,
      "learning_rate": 0.013354928273947249,
      "loss": 0.0001,
      "step": 93620
    },
    {
      "epoch": 43.327163350300786,
      "grad_norm": 0.006400399375706911,
      "learning_rate": 0.013345673299398429,
      "loss": 0.0,
      "step": 93630
    },
    {
      "epoch": 43.3317908375752,
      "grad_norm": 0.03348555043339729,
      "learning_rate": 0.013336418324849606,
      "loss": 0.0001,
      "step": 93640
    },
    {
      "epoch": 43.3364183248496,
      "grad_norm": 0.020935479551553726,
      "learning_rate": 0.013327163350300786,
      "loss": 0.0001,
      "step": 93650
    },
    {
      "epoch": 43.341045812124015,
      "grad_norm": 0.005008405074477196,
      "learning_rate": 0.013317908375751968,
      "loss": 0.0001,
      "step": 93660
    },
    {
      "epoch": 43.34567329939843,
      "grad_norm": 0.0934503823518753,
      "learning_rate": 0.013308653401203147,
      "loss": 0.0002,
      "step": 93670
    },
    {
      "epoch": 43.35030078667284,
      "grad_norm": 0.05094558745622635,
      "learning_rate": 0.013299398426654327,
      "loss": 0.0001,
      "step": 93680
    },
    {
      "epoch": 43.354928273947245,
      "grad_norm": 0.12302606552839279,
      "learning_rate": 0.013290143452105508,
      "loss": 0.0001,
      "step": 93690
    },
    {
      "epoch": 43.35955576122166,
      "grad_norm": 0.06785845756530762,
      "learning_rate": 0.013280888477556688,
      "loss": 0.0001,
      "step": 93700
    },
    {
      "epoch": 43.36418324849607,
      "grad_norm": 0.04872195050120354,
      "learning_rate": 0.01327163350300787,
      "loss": 0.0001,
      "step": 93710
    },
    {
      "epoch": 43.368810735770474,
      "grad_norm": 0.0020313176792114973,
      "learning_rate": 0.013262378528459046,
      "loss": 0.0001,
      "step": 93720
    },
    {
      "epoch": 43.373438223044886,
      "grad_norm": 0.011485208757221699,
      "learning_rate": 0.013253123553910227,
      "loss": 0.0002,
      "step": 93730
    },
    {
      "epoch": 43.3780657103193,
      "grad_norm": 0.01201641745865345,
      "learning_rate": 0.013243868579361407,
      "loss": 0.0002,
      "step": 93740
    },
    {
      "epoch": 43.38269319759371,
      "grad_norm": 0.007183713372796774,
      "learning_rate": 0.013234613604812588,
      "loss": 0.0001,
      "step": 93750
    },
    {
      "epoch": 43.387320684868115,
      "grad_norm": 0.005192081443965435,
      "learning_rate": 0.013225358630263768,
      "loss": 0.0001,
      "step": 93760
    },
    {
      "epoch": 43.39194817214253,
      "grad_norm": 0.014159216545522213,
      "learning_rate": 0.013216103655714947,
      "loss": 0.0,
      "step": 93770
    },
    {
      "epoch": 43.39657565941694,
      "grad_norm": 0.0031218668445944786,
      "learning_rate": 0.013206848681166129,
      "loss": 0.0001,
      "step": 93780
    },
    {
      "epoch": 43.401203146691344,
      "grad_norm": 0.0029599119443446398,
      "learning_rate": 0.013197593706617307,
      "loss": 0.0001,
      "step": 93790
    },
    {
      "epoch": 43.405830633965756,
      "grad_norm": 0.09673461318016052,
      "learning_rate": 0.013188338732068487,
      "loss": 0.0001,
      "step": 93800
    },
    {
      "epoch": 43.41045812124017,
      "grad_norm": 0.0333237461745739,
      "learning_rate": 0.013179083757519666,
      "loss": 0.0001,
      "step": 93810
    },
    {
      "epoch": 43.41508560851457,
      "grad_norm": 0.013572931289672852,
      "learning_rate": 0.013169828782970848,
      "loss": 0.0002,
      "step": 93820
    },
    {
      "epoch": 43.419713095788985,
      "grad_norm": 0.0029020903166383505,
      "learning_rate": 0.013160573808422027,
      "loss": 0.0001,
      "step": 93830
    },
    {
      "epoch": 43.4243405830634,
      "grad_norm": 0.004400376230478287,
      "learning_rate": 0.013151318833873209,
      "loss": 0.0001,
      "step": 93840
    },
    {
      "epoch": 43.42896807033781,
      "grad_norm": 0.008278886787593365,
      "learning_rate": 0.013142063859324388,
      "loss": 0.0001,
      "step": 93850
    },
    {
      "epoch": 43.433595557612215,
      "grad_norm": 0.008839667774736881,
      "learning_rate": 0.013132808884775568,
      "loss": 0.0001,
      "step": 93860
    },
    {
      "epoch": 43.43822304488663,
      "grad_norm": 0.0037810157518833876,
      "learning_rate": 0.013123553910226746,
      "loss": 0.0001,
      "step": 93870
    },
    {
      "epoch": 43.44285053216104,
      "grad_norm": 0.007276536431163549,
      "learning_rate": 0.013114298935677927,
      "loss": 0.0002,
      "step": 93880
    },
    {
      "epoch": 43.447478019435444,
      "grad_norm": 0.01809355802834034,
      "learning_rate": 0.013105043961129107,
      "loss": 0.0002,
      "step": 93890
    },
    {
      "epoch": 43.452105506709856,
      "grad_norm": 0.008295202627778053,
      "learning_rate": 0.013095788986580287,
      "loss": 0.0001,
      "step": 93900
    },
    {
      "epoch": 43.45673299398427,
      "grad_norm": 0.016523052006959915,
      "learning_rate": 0.013086534012031468,
      "loss": 0.0004,
      "step": 93910
    },
    {
      "epoch": 43.46136048125868,
      "grad_norm": 0.003045180346816778,
      "learning_rate": 0.013077279037482648,
      "loss": 0.0001,
      "step": 93920
    },
    {
      "epoch": 43.465987968533085,
      "grad_norm": 0.0046649957075715065,
      "learning_rate": 0.01306802406293383,
      "loss": 0.0001,
      "step": 93930
    },
    {
      "epoch": 43.4706154558075,
      "grad_norm": 0.0076949577778577805,
      "learning_rate": 0.013058769088385009,
      "loss": 0.0,
      "step": 93940
    },
    {
      "epoch": 43.47524294308191,
      "grad_norm": 0.08575127273797989,
      "learning_rate": 0.013049514113836187,
      "loss": 0.0002,
      "step": 93950
    },
    {
      "epoch": 43.479870430356314,
      "grad_norm": 0.004489440005272627,
      "learning_rate": 0.013040259139287367,
      "loss": 0.0001,
      "step": 93960
    },
    {
      "epoch": 43.484497917630726,
      "grad_norm": 0.008411184884607792,
      "learning_rate": 0.013031004164738548,
      "loss": 0.0001,
      "step": 93970
    },
    {
      "epoch": 43.48912540490514,
      "grad_norm": 0.003075741231441498,
      "learning_rate": 0.013021749190189728,
      "loss": 0.0001,
      "step": 93980
    },
    {
      "epoch": 43.49375289217954,
      "grad_norm": 0.007460836321115494,
      "learning_rate": 0.013012494215640907,
      "loss": 0.0001,
      "step": 93990
    },
    {
      "epoch": 43.498380379453955,
      "grad_norm": 0.03153390809893608,
      "learning_rate": 0.013003239241092089,
      "loss": 0.0002,
      "step": 94000
    },
    {
      "epoch": 43.50300786672837,
      "grad_norm": 0.18903625011444092,
      "learning_rate": 0.012993984266543268,
      "loss": 0.0001,
      "step": 94010
    },
    {
      "epoch": 43.50763535400278,
      "grad_norm": 0.010842370800673962,
      "learning_rate": 0.012984729291994446,
      "loss": 0.0001,
      "step": 94020
    },
    {
      "epoch": 43.512262841277185,
      "grad_norm": 0.002808230696246028,
      "learning_rate": 0.012975474317445626,
      "loss": 0.0015,
      "step": 94030
    },
    {
      "epoch": 43.5168903285516,
      "grad_norm": 0.002154290908947587,
      "learning_rate": 0.012966219342896807,
      "loss": 0.0001,
      "step": 94040
    },
    {
      "epoch": 43.52151781582601,
      "grad_norm": 0.0026192297227680683,
      "learning_rate": 0.012956964368347987,
      "loss": 0.0001,
      "step": 94050
    },
    {
      "epoch": 43.526145303100414,
      "grad_norm": 0.006530907470732927,
      "learning_rate": 0.012947709393799169,
      "loss": 0.0005,
      "step": 94060
    },
    {
      "epoch": 43.530772790374826,
      "grad_norm": 0.03395693004131317,
      "learning_rate": 0.012938454419250348,
      "loss": 0.0002,
      "step": 94070
    },
    {
      "epoch": 43.53540027764924,
      "grad_norm": 0.003592776134610176,
      "learning_rate": 0.012929199444701528,
      "loss": 0.0002,
      "step": 94080
    },
    {
      "epoch": 43.54002776492365,
      "grad_norm": 0.0023307614028453827,
      "learning_rate": 0.01291994447015271,
      "loss": 0.0001,
      "step": 94090
    },
    {
      "epoch": 43.544655252198055,
      "grad_norm": 0.4158322513103485,
      "learning_rate": 0.012910689495603887,
      "loss": 0.0002,
      "step": 94100
    },
    {
      "epoch": 43.54928273947247,
      "grad_norm": 0.01931246742606163,
      "learning_rate": 0.012901434521055067,
      "loss": 0.0002,
      "step": 94110
    },
    {
      "epoch": 43.55391022674688,
      "grad_norm": 0.005836636293679476,
      "learning_rate": 0.012892179546506247,
      "loss": 0.0001,
      "step": 94120
    },
    {
      "epoch": 43.558537714021284,
      "grad_norm": 0.012970567680895329,
      "learning_rate": 0.012882924571957428,
      "loss": 0.0001,
      "step": 94130
    },
    {
      "epoch": 43.563165201295696,
      "grad_norm": 0.0012832937063649297,
      "learning_rate": 0.012873669597408608,
      "loss": 0.0,
      "step": 94140
    },
    {
      "epoch": 43.56779268857011,
      "grad_norm": 0.01464730966836214,
      "learning_rate": 0.01286441462285979,
      "loss": 0.0001,
      "step": 94150
    },
    {
      "epoch": 43.57242017584451,
      "grad_norm": 0.003448247676715255,
      "learning_rate": 0.012855159648310969,
      "loss": 0.0002,
      "step": 94160
    },
    {
      "epoch": 43.577047663118925,
      "grad_norm": 0.004003074020147324,
      "learning_rate": 0.012845904673762149,
      "loss": 0.0,
      "step": 94170
    },
    {
      "epoch": 43.58167515039334,
      "grad_norm": 0.02244204469025135,
      "learning_rate": 0.012836649699213326,
      "loss": 0.001,
      "step": 94180
    },
    {
      "epoch": 43.58630263766775,
      "grad_norm": 0.013263328932225704,
      "learning_rate": 0.012827394724664508,
      "loss": 0.0001,
      "step": 94190
    },
    {
      "epoch": 43.590930124942155,
      "grad_norm": 0.022990422323346138,
      "learning_rate": 0.012818139750115688,
      "loss": 0.0001,
      "step": 94200
    },
    {
      "epoch": 43.59555761221657,
      "grad_norm": 0.30563482642173767,
      "learning_rate": 0.012808884775566867,
      "loss": 0.0001,
      "step": 94210
    },
    {
      "epoch": 43.60018509949098,
      "grad_norm": 0.009304437786340714,
      "learning_rate": 0.012799629801018049,
      "loss": 0.0,
      "step": 94220
    },
    {
      "epoch": 43.604812586765384,
      "grad_norm": 0.005904348101466894,
      "learning_rate": 0.012790374826469228,
      "loss": 0.0002,
      "step": 94230
    },
    {
      "epoch": 43.609440074039796,
      "grad_norm": 0.0019083413062617183,
      "learning_rate": 0.01278111985192041,
      "loss": 0.0002,
      "step": 94240
    },
    {
      "epoch": 43.61406756131421,
      "grad_norm": 0.007202851586043835,
      "learning_rate": 0.012771864877371586,
      "loss": 0.0001,
      "step": 94250
    },
    {
      "epoch": 43.61869504858861,
      "grad_norm": 0.012075556442141533,
      "learning_rate": 0.012762609902822767,
      "loss": 0.0001,
      "step": 94260
    },
    {
      "epoch": 43.623322535863025,
      "grad_norm": 0.02523210272192955,
      "learning_rate": 0.012753354928273947,
      "loss": 0.0001,
      "step": 94270
    },
    {
      "epoch": 43.62795002313744,
      "grad_norm": 0.0016406929353252053,
      "learning_rate": 0.012744099953725128,
      "loss": 0.0001,
      "step": 94280
    },
    {
      "epoch": 43.63257751041185,
      "grad_norm": 0.12772683799266815,
      "learning_rate": 0.012734844979176308,
      "loss": 0.0002,
      "step": 94290
    },
    {
      "epoch": 43.637204997686254,
      "grad_norm": 0.004931577015668154,
      "learning_rate": 0.012725590004627488,
      "loss": 0.0001,
      "step": 94300
    },
    {
      "epoch": 43.641832484960666,
      "grad_norm": 0.000406725041102618,
      "learning_rate": 0.01271633503007867,
      "loss": 0.0001,
      "step": 94310
    },
    {
      "epoch": 43.64645997223508,
      "grad_norm": 0.01062803715467453,
      "learning_rate": 0.012707080055529849,
      "loss": 0.0001,
      "step": 94320
    },
    {
      "epoch": 43.65108745950948,
      "grad_norm": 0.0024066749028861523,
      "learning_rate": 0.012697825080981027,
      "loss": 0.0001,
      "step": 94330
    },
    {
      "epoch": 43.655714946783895,
      "grad_norm": 0.0026936777867376804,
      "learning_rate": 0.012688570106432207,
      "loss": 0.0001,
      "step": 94340
    },
    {
      "epoch": 43.66034243405831,
      "grad_norm": 0.024619504809379578,
      "learning_rate": 0.012679315131883388,
      "loss": 0.0003,
      "step": 94350
    },
    {
      "epoch": 43.66496992133272,
      "grad_norm": 0.0019378475844860077,
      "learning_rate": 0.012670060157334568,
      "loss": 0.0001,
      "step": 94360
    },
    {
      "epoch": 43.669597408607125,
      "grad_norm": 0.0071112303994596004,
      "learning_rate": 0.012660805182785749,
      "loss": 0.0001,
      "step": 94370
    },
    {
      "epoch": 43.67422489588154,
      "grad_norm": 0.001421135850250721,
      "learning_rate": 0.012651550208236929,
      "loss": 0.0001,
      "step": 94380
    },
    {
      "epoch": 43.67885238315595,
      "grad_norm": 0.017709659412503242,
      "learning_rate": 0.012642295233688108,
      "loss": 0.0001,
      "step": 94390
    },
    {
      "epoch": 43.683479870430354,
      "grad_norm": 0.005244843196123838,
      "learning_rate": 0.01263304025913929,
      "loss": 0.0,
      "step": 94400
    },
    {
      "epoch": 43.688107357704766,
      "grad_norm": 0.006358895916491747,
      "learning_rate": 0.012623785284590468,
      "loss": 0.0001,
      "step": 94410
    },
    {
      "epoch": 43.69273484497918,
      "grad_norm": 0.04972480237483978,
      "learning_rate": 0.012614530310041647,
      "loss": 0.0002,
      "step": 94420
    },
    {
      "epoch": 43.69736233225358,
      "grad_norm": 0.0060116928070783615,
      "learning_rate": 0.012605275335492827,
      "loss": 0.0001,
      "step": 94430
    },
    {
      "epoch": 43.701989819527995,
      "grad_norm": 0.002375708194449544,
      "learning_rate": 0.012596020360944008,
      "loss": 0.0001,
      "step": 94440
    },
    {
      "epoch": 43.70661730680241,
      "grad_norm": 0.0494077205657959,
      "learning_rate": 0.012586765386395188,
      "loss": 0.0001,
      "step": 94450
    },
    {
      "epoch": 43.71124479407682,
      "grad_norm": 0.0010204716818407178,
      "learning_rate": 0.01257751041184637,
      "loss": 0.0001,
      "step": 94460
    },
    {
      "epoch": 43.715872281351224,
      "grad_norm": 0.006240743678063154,
      "learning_rate": 0.01256825543729755,
      "loss": 0.0003,
      "step": 94470
    },
    {
      "epoch": 43.720499768625636,
      "grad_norm": 0.009323285892605782,
      "learning_rate": 0.012559000462748727,
      "loss": 0.0001,
      "step": 94480
    },
    {
      "epoch": 43.72512725590005,
      "grad_norm": 0.004660033620893955,
      "learning_rate": 0.012549745488199907,
      "loss": 0.0001,
      "step": 94490
    },
    {
      "epoch": 43.72975474317445,
      "grad_norm": 0.008492019027471542,
      "learning_rate": 0.012540490513651088,
      "loss": 0.0001,
      "step": 94500
    },
    {
      "epoch": 43.734382230448865,
      "grad_norm": 0.0006409171619452536,
      "learning_rate": 0.012531235539102268,
      "loss": 0.0001,
      "step": 94510
    },
    {
      "epoch": 43.73900971772328,
      "grad_norm": 0.05954158306121826,
      "learning_rate": 0.012521980564553448,
      "loss": 0.0001,
      "step": 94520
    },
    {
      "epoch": 43.74363720499769,
      "grad_norm": 0.03336493670940399,
      "learning_rate": 0.012512725590004629,
      "loss": 0.0002,
      "step": 94530
    },
    {
      "epoch": 43.748264692272095,
      "grad_norm": 0.001376942964270711,
      "learning_rate": 0.012503470615455809,
      "loss": 0.0,
      "step": 94540
    },
    {
      "epoch": 43.75289217954651,
      "grad_norm": 0.002358401892706752,
      "learning_rate": 0.012494215640906988,
      "loss": 0.0001,
      "step": 94550
    },
    {
      "epoch": 43.75751966682092,
      "grad_norm": 0.0060919467359781265,
      "learning_rate": 0.012484960666358168,
      "loss": 0.0001,
      "step": 94560
    },
    {
      "epoch": 43.762147154095324,
      "grad_norm": 0.01166381686925888,
      "learning_rate": 0.01247570569180935,
      "loss": 0.0001,
      "step": 94570
    },
    {
      "epoch": 43.766774641369736,
      "grad_norm": 0.014280090108513832,
      "learning_rate": 0.012466450717260527,
      "loss": 0.0005,
      "step": 94580
    },
    {
      "epoch": 43.77140212864415,
      "grad_norm": 0.02118859440088272,
      "learning_rate": 0.012457195742711709,
      "loss": 0.0001,
      "step": 94590
    },
    {
      "epoch": 43.77602961591855,
      "grad_norm": 0.009663179516792297,
      "learning_rate": 0.012447940768162889,
      "loss": 0.0001,
      "step": 94600
    },
    {
      "epoch": 43.780657103192965,
      "grad_norm": 0.003100366797298193,
      "learning_rate": 0.012438685793614068,
      "loss": 0.0,
      "step": 94610
    },
    {
      "epoch": 43.78528459046738,
      "grad_norm": 0.0025465674698352814,
      "learning_rate": 0.012429430819065248,
      "loss": 0.0001,
      "step": 94620
    },
    {
      "epoch": 43.78991207774179,
      "grad_norm": 0.03597753867506981,
      "learning_rate": 0.012420175844516428,
      "loss": 0.0,
      "step": 94630
    },
    {
      "epoch": 43.794539565016194,
      "grad_norm": 0.021792996674776077,
      "learning_rate": 0.012410920869967609,
      "loss": 0.0001,
      "step": 94640
    },
    {
      "epoch": 43.799167052290606,
      "grad_norm": 0.03348840773105621,
      "learning_rate": 0.012401665895418787,
      "loss": 0.0001,
      "step": 94650
    },
    {
      "epoch": 43.80379453956502,
      "grad_norm": 0.0032671845983713865,
      "learning_rate": 0.012392410920869968,
      "loss": 0.0,
      "step": 94660
    },
    {
      "epoch": 43.80842202683942,
      "grad_norm": 0.006698450539261103,
      "learning_rate": 0.012383155946321148,
      "loss": 0.0001,
      "step": 94670
    },
    {
      "epoch": 43.813049514113835,
      "grad_norm": 0.0011702788760885596,
      "learning_rate": 0.01237390097177233,
      "loss": 0.0,
      "step": 94680
    },
    {
      "epoch": 43.81767700138825,
      "grad_norm": 0.21415191888809204,
      "learning_rate": 0.012364645997223507,
      "loss": 0.0001,
      "step": 94690
    },
    {
      "epoch": 43.82230448866266,
      "grad_norm": 0.002954804804176092,
      "learning_rate": 0.012355391022674689,
      "loss": 0.0001,
      "step": 94700
    },
    {
      "epoch": 43.826931975937065,
      "grad_norm": 0.00689062150195241,
      "learning_rate": 0.012346136048125868,
      "loss": 0.0001,
      "step": 94710
    },
    {
      "epoch": 43.83155946321148,
      "grad_norm": 0.0019912945572286844,
      "learning_rate": 0.012336881073577048,
      "loss": 0.0001,
      "step": 94720
    },
    {
      "epoch": 43.83618695048589,
      "grad_norm": 0.009206023067235947,
      "learning_rate": 0.012327626099028228,
      "loss": 0.0003,
      "step": 94730
    },
    {
      "epoch": 43.840814437760294,
      "grad_norm": 0.015936851501464844,
      "learning_rate": 0.012318371124479408,
      "loss": 0.0001,
      "step": 94740
    },
    {
      "epoch": 43.845441925034706,
      "grad_norm": 0.007630476262420416,
      "learning_rate": 0.012309116149930589,
      "loss": 0.0001,
      "step": 94750
    },
    {
      "epoch": 43.85006941230912,
      "grad_norm": 0.0037969041150063276,
      "learning_rate": 0.012299861175381769,
      "loss": 0.0001,
      "step": 94760
    },
    {
      "epoch": 43.85469689958352,
      "grad_norm": 0.004619897343218327,
      "learning_rate": 0.012290606200832948,
      "loss": 0.0001,
      "step": 94770
    },
    {
      "epoch": 43.859324386857935,
      "grad_norm": 0.016945041716098785,
      "learning_rate": 0.012281351226284128,
      "loss": 0.0001,
      "step": 94780
    },
    {
      "epoch": 43.86395187413235,
      "grad_norm": 0.16736334562301636,
      "learning_rate": 0.01227209625173531,
      "loss": 0.0002,
      "step": 94790
    },
    {
      "epoch": 43.86857936140676,
      "grad_norm": 0.0018807209562510252,
      "learning_rate": 0.012262841277186489,
      "loss": 0.0001,
      "step": 94800
    },
    {
      "epoch": 43.873206848681164,
      "grad_norm": 0.31003278493881226,
      "learning_rate": 0.012253586302637669,
      "loss": 0.0002,
      "step": 94810
    },
    {
      "epoch": 43.877834335955576,
      "grad_norm": 0.016247140243649483,
      "learning_rate": 0.012244331328088848,
      "loss": 0.0002,
      "step": 94820
    },
    {
      "epoch": 43.88246182322999,
      "grad_norm": 0.0193332489579916,
      "learning_rate": 0.012235076353540028,
      "loss": 0.0004,
      "step": 94830
    },
    {
      "epoch": 43.88708931050439,
      "grad_norm": 0.001244344050064683,
      "learning_rate": 0.01222582137899121,
      "loss": 0.0001,
      "step": 94840
    },
    {
      "epoch": 43.891716797778805,
      "grad_norm": 0.009815584868192673,
      "learning_rate": 0.012216566404442387,
      "loss": 0.0001,
      "step": 94850
    },
    {
      "epoch": 43.89634428505322,
      "grad_norm": 0.03708932176232338,
      "learning_rate": 0.012207311429893569,
      "loss": 0.0002,
      "step": 94860
    },
    {
      "epoch": 43.90097177232763,
      "grad_norm": 0.1563325971364975,
      "learning_rate": 0.012198056455344749,
      "loss": 0.0001,
      "step": 94870
    },
    {
      "epoch": 43.905599259602035,
      "grad_norm": 0.0015670000575482845,
      "learning_rate": 0.012188801480795928,
      "loss": 0.0001,
      "step": 94880
    },
    {
      "epoch": 43.91022674687645,
      "grad_norm": 0.011857056990265846,
      "learning_rate": 0.012179546506247108,
      "loss": 0.0001,
      "step": 94890
    },
    {
      "epoch": 43.91485423415086,
      "grad_norm": 0.0025447357911616564,
      "learning_rate": 0.01217029153169829,
      "loss": 0.0001,
      "step": 94900
    },
    {
      "epoch": 43.919481721425264,
      "grad_norm": 0.017711948603391647,
      "learning_rate": 0.012161036557149469,
      "loss": 0.0,
      "step": 94910
    },
    {
      "epoch": 43.924109208699676,
      "grad_norm": 0.019573694095015526,
      "learning_rate": 0.012151781582600649,
      "loss": 0.0001,
      "step": 94920
    },
    {
      "epoch": 43.92873669597409,
      "grad_norm": 0.01213956531137228,
      "learning_rate": 0.012142526608051828,
      "loss": 0.0001,
      "step": 94930
    },
    {
      "epoch": 43.93336418324849,
      "grad_norm": 0.002866168739274144,
      "learning_rate": 0.012133271633503008,
      "loss": 0.0001,
      "step": 94940
    },
    {
      "epoch": 43.937991670522905,
      "grad_norm": 0.014315497130155563,
      "learning_rate": 0.01212401665895419,
      "loss": 0.0001,
      "step": 94950
    },
    {
      "epoch": 43.94261915779732,
      "grad_norm": 0.008238901384174824,
      "learning_rate": 0.012114761684405367,
      "loss": 0.0003,
      "step": 94960
    },
    {
      "epoch": 43.94724664507173,
      "grad_norm": 0.007143282797187567,
      "learning_rate": 0.012105506709856549,
      "loss": 0.0001,
      "step": 94970
    },
    {
      "epoch": 43.951874132346134,
      "grad_norm": 0.03788774833083153,
      "learning_rate": 0.012096251735307728,
      "loss": 0.0,
      "step": 94980
    },
    {
      "epoch": 43.956501619620546,
      "grad_norm": 0.004431408364325762,
      "learning_rate": 0.01208699676075891,
      "loss": 0.0001,
      "step": 94990
    },
    {
      "epoch": 43.96112910689496,
      "grad_norm": 0.005819982849061489,
      "learning_rate": 0.012077741786210088,
      "loss": 0.0001,
      "step": 95000
    },
    {
      "epoch": 43.96575659416936,
      "grad_norm": 0.3139137029647827,
      "learning_rate": 0.01206848681166127,
      "loss": 0.0002,
      "step": 95010
    },
    {
      "epoch": 43.970384081443775,
      "grad_norm": 0.1000814288854599,
      "learning_rate": 0.012059231837112449,
      "loss": 0.0002,
      "step": 95020
    },
    {
      "epoch": 43.97501156871819,
      "grad_norm": 0.006719586905092001,
      "learning_rate": 0.012049976862563629,
      "loss": 0.0001,
      "step": 95030
    },
    {
      "epoch": 43.97963905599259,
      "grad_norm": 0.0018773607444018126,
      "learning_rate": 0.012040721888014808,
      "loss": 0.0003,
      "step": 95040
    },
    {
      "epoch": 43.984266543267005,
      "grad_norm": 0.014228081330657005,
      "learning_rate": 0.012031466913465988,
      "loss": 0.0001,
      "step": 95050
    },
    {
      "epoch": 43.98889403054142,
      "grad_norm": 0.007585644256323576,
      "learning_rate": 0.01202221193891717,
      "loss": 0.0001,
      "step": 95060
    },
    {
      "epoch": 43.99352151781583,
      "grad_norm": 0.000625905639026314,
      "learning_rate": 0.012012956964368347,
      "loss": 0.0001,
      "step": 95070
    },
    {
      "epoch": 43.998149005090234,
      "grad_norm": 0.009512396529316902,
      "learning_rate": 0.012003701989819529,
      "loss": 0.0001,
      "step": 95080
    },
    {
      "epoch": 44.0,
      "eval_accuracy_branch1": 0.9875211831767062,
      "eval_accuracy_branch2": 0.5003081189339085,
      "eval_f1_branch1": 0.9891206206765635,
      "eval_f1_branch2": 0.5002214054445521,
      "eval_loss": 0.023836050182580948,
      "eval_precision_branch1": 0.9895576479753363,
      "eval_precision_branch2": 0.5003083329216528,
      "eval_recall_branch1": 0.9888812785596396,
      "eval_recall_branch2": 0.5003081189339085,
      "eval_runtime": 28.9226,
      "eval_samples_per_second": 448.854,
      "eval_steps_per_second": 56.115,
      "step": 95084
    },
    {
      "epoch": 44.002776492364646,
      "grad_norm": 0.009447935968637466,
      "learning_rate": 0.011994447015270708,
      "loss": 0.1546,
      "step": 95090
    },
    {
      "epoch": 44.00740397963906,
      "grad_norm": 0.001691006007604301,
      "learning_rate": 0.01198519204072189,
      "loss": 0.0,
      "step": 95100
    },
    {
      "epoch": 44.01203146691346,
      "grad_norm": 0.004764285869896412,
      "learning_rate": 0.011975937066173068,
      "loss": 0.0001,
      "step": 95110
    },
    {
      "epoch": 44.016658954187875,
      "grad_norm": 0.001099674147553742,
      "learning_rate": 0.01196668209162425,
      "loss": 0.0001,
      "step": 95120
    },
    {
      "epoch": 44.02128644146229,
      "grad_norm": 0.013485725969076157,
      "learning_rate": 0.011957427117075429,
      "loss": 0.0001,
      "step": 95130
    },
    {
      "epoch": 44.0259139287367,
      "grad_norm": 0.0023934002965688705,
      "learning_rate": 0.011948172142526609,
      "loss": 0.0001,
      "step": 95140
    },
    {
      "epoch": 44.030541416011104,
      "grad_norm": 0.002173075685277581,
      "learning_rate": 0.011938917167977788,
      "loss": 0.0001,
      "step": 95150
    },
    {
      "epoch": 44.035168903285516,
      "grad_norm": 0.015337076038122177,
      "learning_rate": 0.011929662193428968,
      "loss": 0.0001,
      "step": 95160
    },
    {
      "epoch": 44.03979639055993,
      "grad_norm": 0.0035525565035641193,
      "learning_rate": 0.01192040721888015,
      "loss": 0.0001,
      "step": 95170
    },
    {
      "epoch": 44.04442387783433,
      "grad_norm": 0.031332626938819885,
      "learning_rate": 0.011911152244331329,
      "loss": 0.0001,
      "step": 95180
    },
    {
      "epoch": 44.049051365108745,
      "grad_norm": 0.01652376353740692,
      "learning_rate": 0.011901897269782509,
      "loss": 0.0001,
      "step": 95190
    },
    {
      "epoch": 44.05367885238316,
      "grad_norm": 0.020320318639278412,
      "learning_rate": 0.011892642295233688,
      "loss": 0.0001,
      "step": 95200
    },
    {
      "epoch": 44.05830633965756,
      "grad_norm": 0.0029017291963100433,
      "learning_rate": 0.01188338732068487,
      "loss": 0.0,
      "step": 95210
    },
    {
      "epoch": 44.062933826931975,
      "grad_norm": 0.001797976321540773,
      "learning_rate": 0.01187413234613605,
      "loss": 0.0001,
      "step": 95220
    },
    {
      "epoch": 44.06756131420639,
      "grad_norm": 0.003005042439326644,
      "learning_rate": 0.011864877371587229,
      "loss": 0.0001,
      "step": 95230
    },
    {
      "epoch": 44.0721888014808,
      "grad_norm": 0.007527771405875683,
      "learning_rate": 0.011855622397038409,
      "loss": 0.0001,
      "step": 95240
    },
    {
      "epoch": 44.076816288755204,
      "grad_norm": 0.008769970387220383,
      "learning_rate": 0.011846367422489588,
      "loss": 0.0007,
      "step": 95250
    },
    {
      "epoch": 44.081443776029616,
      "grad_norm": 0.003359033027663827,
      "learning_rate": 0.01183711244794077,
      "loss": 0.0021,
      "step": 95260
    },
    {
      "epoch": 44.08607126330403,
      "grad_norm": 0.04807555675506592,
      "learning_rate": 0.011827857473391948,
      "loss": 0.0001,
      "step": 95270
    },
    {
      "epoch": 44.09069875057843,
      "grad_norm": 0.0008631574455648661,
      "learning_rate": 0.01181860249884313,
      "loss": 0.0001,
      "step": 95280
    },
    {
      "epoch": 44.095326237852845,
      "grad_norm": 0.011377728544175625,
      "learning_rate": 0.011809347524294309,
      "loss": 0.0001,
      "step": 95290
    },
    {
      "epoch": 44.09995372512726,
      "grad_norm": 0.0071819438599050045,
      "learning_rate": 0.011800092549745489,
      "loss": 0.0001,
      "step": 95300
    },
    {
      "epoch": 44.10458121240167,
      "grad_norm": 0.00119578477460891,
      "learning_rate": 0.011790837575196668,
      "loss": 0.0003,
      "step": 95310
    },
    {
      "epoch": 44.109208699676074,
      "grad_norm": 0.01473049633204937,
      "learning_rate": 0.01178158260064785,
      "loss": 0.0001,
      "step": 95320
    },
    {
      "epoch": 44.113836186950486,
      "grad_norm": 0.004794541280716658,
      "learning_rate": 0.01177232762609903,
      "loss": 0.0006,
      "step": 95330
    },
    {
      "epoch": 44.1184636742249,
      "grad_norm": 0.011610274203121662,
      "learning_rate": 0.011763072651550209,
      "loss": 0.0002,
      "step": 95340
    },
    {
      "epoch": 44.1230911614993,
      "grad_norm": 0.005090436898171902,
      "learning_rate": 0.011753817677001389,
      "loss": 0.0001,
      "step": 95350
    },
    {
      "epoch": 44.127718648773715,
      "grad_norm": 0.024192258715629578,
      "learning_rate": 0.011744562702452568,
      "loss": 0.0001,
      "step": 95360
    },
    {
      "epoch": 44.13234613604813,
      "grad_norm": 0.01508707832545042,
      "learning_rate": 0.01173530772790375,
      "loss": 0.0001,
      "step": 95370
    },
    {
      "epoch": 44.13697362332253,
      "grad_norm": 0.029651494696736336,
      "learning_rate": 0.011726052753354928,
      "loss": 0.0001,
      "step": 95380
    },
    {
      "epoch": 44.141601110596945,
      "grad_norm": 0.0011607719352468848,
      "learning_rate": 0.01171679777880611,
      "loss": 0.0,
      "step": 95390
    },
    {
      "epoch": 44.14622859787136,
      "grad_norm": 0.0028017533477395773,
      "learning_rate": 0.011707542804257289,
      "loss": 0.0001,
      "step": 95400
    },
    {
      "epoch": 44.15085608514577,
      "grad_norm": 0.0009522715699858963,
      "learning_rate": 0.01169828782970847,
      "loss": 0.0001,
      "step": 95410
    },
    {
      "epoch": 44.155483572420174,
      "grad_norm": 0.009136258624494076,
      "learning_rate": 0.011689032855159648,
      "loss": 0.0003,
      "step": 95420
    },
    {
      "epoch": 44.160111059694586,
      "grad_norm": 0.010952526703476906,
      "learning_rate": 0.01167977788061083,
      "loss": 0.0001,
      "step": 95430
    },
    {
      "epoch": 44.164738546969,
      "grad_norm": 0.03134579211473465,
      "learning_rate": 0.01167052290606201,
      "loss": 0.0003,
      "step": 95440
    },
    {
      "epoch": 44.1693660342434,
      "grad_norm": 0.0046792724169790745,
      "learning_rate": 0.011661267931513189,
      "loss": 0.0,
      "step": 95450
    },
    {
      "epoch": 44.173993521517815,
      "grad_norm": 0.018147112801671028,
      "learning_rate": 0.011652012956964369,
      "loss": 0.0002,
      "step": 95460
    },
    {
      "epoch": 44.17862100879223,
      "grad_norm": 0.004084819927811623,
      "learning_rate": 0.011642757982415548,
      "loss": 0.0001,
      "step": 95470
    },
    {
      "epoch": 44.18324849606664,
      "grad_norm": 0.005147113930433989,
      "learning_rate": 0.01163350300786673,
      "loss": 0.0001,
      "step": 95480
    },
    {
      "epoch": 44.187875983341044,
      "grad_norm": 0.029577579349279404,
      "learning_rate": 0.01162424803331791,
      "loss": 0.0001,
      "step": 95490
    },
    {
      "epoch": 44.192503470615456,
      "grad_norm": 0.02199232205748558,
      "learning_rate": 0.011614993058769089,
      "loss": 0.0001,
      "step": 95500
    },
    {
      "epoch": 44.19713095788987,
      "grad_norm": 0.0020937689114362,
      "learning_rate": 0.011605738084220269,
      "loss": 0.0001,
      "step": 95510
    },
    {
      "epoch": 44.20175844516427,
      "grad_norm": 0.003886852180585265,
      "learning_rate": 0.01159648310967145,
      "loss": 0.0001,
      "step": 95520
    },
    {
      "epoch": 44.206385932438685,
      "grad_norm": 0.02710661105811596,
      "learning_rate": 0.011587228135122628,
      "loss": 0.0007,
      "step": 95530
    },
    {
      "epoch": 44.2110134197131,
      "grad_norm": 0.0025681625120341778,
      "learning_rate": 0.01157797316057381,
      "loss": 0.0001,
      "step": 95540
    },
    {
      "epoch": 44.2156409069875,
      "grad_norm": 0.0011470044264569879,
      "learning_rate": 0.01156871818602499,
      "loss": 0.0001,
      "step": 95550
    },
    {
      "epoch": 44.220268394261915,
      "grad_norm": 0.005060798488557339,
      "learning_rate": 0.011559463211476169,
      "loss": 0.0001,
      "step": 95560
    },
    {
      "epoch": 44.22489588153633,
      "grad_norm": 0.006860550493001938,
      "learning_rate": 0.011550208236927349,
      "loss": 0.0001,
      "step": 95570
    },
    {
      "epoch": 44.22952336881074,
      "grad_norm": 0.001975264400243759,
      "learning_rate": 0.011540953262378528,
      "loss": 0.0002,
      "step": 95580
    },
    {
      "epoch": 44.234150856085144,
      "grad_norm": 0.0023269683588296175,
      "learning_rate": 0.01153169828782971,
      "loss": 0.0001,
      "step": 95590
    },
    {
      "epoch": 44.238778343359556,
      "grad_norm": 0.0014966330491006374,
      "learning_rate": 0.01152244331328089,
      "loss": 0.0001,
      "step": 95600
    },
    {
      "epoch": 44.24340583063397,
      "grad_norm": 0.5239410400390625,
      "learning_rate": 0.011513188338732069,
      "loss": 0.0003,
      "step": 95610
    },
    {
      "epoch": 44.24803331790837,
      "grad_norm": 0.012859064154326916,
      "learning_rate": 0.011503933364183249,
      "loss": 0.0001,
      "step": 95620
    },
    {
      "epoch": 44.252660805182785,
      "grad_norm": 0.008382560685276985,
      "learning_rate": 0.01149467838963443,
      "loss": 0.0001,
      "step": 95630
    },
    {
      "epoch": 44.2572882924572,
      "grad_norm": 0.0022453274577856064,
      "learning_rate": 0.01148542341508561,
      "loss": 0.0001,
      "step": 95640
    },
    {
      "epoch": 44.26191577973161,
      "grad_norm": 0.004274855367839336,
      "learning_rate": 0.01147616844053679,
      "loss": 0.0,
      "step": 95650
    },
    {
      "epoch": 44.266543267006014,
      "grad_norm": 0.0017560403794050217,
      "learning_rate": 0.01146691346598797,
      "loss": 0.0,
      "step": 95660
    },
    {
      "epoch": 44.271170754280426,
      "grad_norm": 0.01321304403245449,
      "learning_rate": 0.011457658491439149,
      "loss": 0.0002,
      "step": 95670
    },
    {
      "epoch": 44.27579824155484,
      "grad_norm": 0.0033546711783856153,
      "learning_rate": 0.01144840351689033,
      "loss": 0.0001,
      "step": 95680
    },
    {
      "epoch": 44.28042572882924,
      "grad_norm": 0.0017131698550656438,
      "learning_rate": 0.011439148542341508,
      "loss": 0.002,
      "step": 95690
    },
    {
      "epoch": 44.285053216103655,
      "grad_norm": 0.034073591232299805,
      "learning_rate": 0.01142989356779269,
      "loss": 0.0001,
      "step": 95700
    },
    {
      "epoch": 44.28968070337807,
      "grad_norm": 0.0030301695223897696,
      "learning_rate": 0.01142063859324387,
      "loss": 0.0001,
      "step": 95710
    },
    {
      "epoch": 44.29430819065247,
      "grad_norm": 0.0006259005167521536,
      "learning_rate": 0.011411383618695049,
      "loss": 0.0001,
      "step": 95720
    },
    {
      "epoch": 44.298935677926885,
      "grad_norm": 0.0030964447651058435,
      "learning_rate": 0.011402128644146229,
      "loss": 0.0001,
      "step": 95730
    },
    {
      "epoch": 44.3035631652013,
      "grad_norm": 0.0046552568674087524,
      "learning_rate": 0.01139287366959741,
      "loss": 0.0001,
      "step": 95740
    },
    {
      "epoch": 44.30819065247571,
      "grad_norm": 0.006179931573569775,
      "learning_rate": 0.01138361869504859,
      "loss": 0.0,
      "step": 95750
    },
    {
      "epoch": 44.312818139750114,
      "grad_norm": 0.0035208435729146004,
      "learning_rate": 0.01137436372049977,
      "loss": 0.0001,
      "step": 95760
    },
    {
      "epoch": 44.317445627024526,
      "grad_norm": 0.012290369719266891,
      "learning_rate": 0.011365108745950949,
      "loss": 0.0001,
      "step": 95770
    },
    {
      "epoch": 44.32207311429894,
      "grad_norm": 0.0068945386447012424,
      "learning_rate": 0.011355853771402129,
      "loss": 0.0001,
      "step": 95780
    },
    {
      "epoch": 44.32670060157334,
      "grad_norm": 0.0034483089111745358,
      "learning_rate": 0.01134659879685331,
      "loss": 0.0013,
      "step": 95790
    },
    {
      "epoch": 44.331328088847755,
      "grad_norm": 0.0011577942641451955,
      "learning_rate": 0.011337343822304488,
      "loss": 0.0001,
      "step": 95800
    },
    {
      "epoch": 44.33595557612217,
      "grad_norm": 0.03410862386226654,
      "learning_rate": 0.01132808884775567,
      "loss": 0.0001,
      "step": 95810
    },
    {
      "epoch": 44.34058306339658,
      "grad_norm": 0.002710191300138831,
      "learning_rate": 0.01131883387320685,
      "loss": 0.0001,
      "step": 95820
    },
    {
      "epoch": 44.345210550670984,
      "grad_norm": 0.005786463618278503,
      "learning_rate": 0.01130957889865803,
      "loss": 0.0001,
      "step": 95830
    },
    {
      "epoch": 44.349838037945396,
      "grad_norm": 0.0024011479690670967,
      "learning_rate": 0.011300323924109209,
      "loss": 0.0001,
      "step": 95840
    },
    {
      "epoch": 44.35446552521981,
      "grad_norm": 0.012941657565534115,
      "learning_rate": 0.01129106894956039,
      "loss": 0.0002,
      "step": 95850
    },
    {
      "epoch": 44.35909301249421,
      "grad_norm": 0.04867416247725487,
      "learning_rate": 0.01128181397501157,
      "loss": 0.0001,
      "step": 95860
    },
    {
      "epoch": 44.363720499768625,
      "grad_norm": 0.0014545778976753354,
      "learning_rate": 0.01127255900046275,
      "loss": 0.0,
      "step": 95870
    },
    {
      "epoch": 44.36834798704304,
      "grad_norm": 0.009094740264117718,
      "learning_rate": 0.011263304025913929,
      "loss": 0.0001,
      "step": 95880
    },
    {
      "epoch": 44.37297547431744,
      "grad_norm": 0.02838924340903759,
      "learning_rate": 0.011254049051365109,
      "loss": 0.0001,
      "step": 95890
    },
    {
      "epoch": 44.377602961591855,
      "grad_norm": 0.005744935479015112,
      "learning_rate": 0.01124479407681629,
      "loss": 0.0001,
      "step": 95900
    },
    {
      "epoch": 44.38223044886627,
      "grad_norm": 0.004196525551378727,
      "learning_rate": 0.01123553910226747,
      "loss": 0.0001,
      "step": 95910
    },
    {
      "epoch": 44.38685793614068,
      "grad_norm": 0.0027003202121704817,
      "learning_rate": 0.01122628412771865,
      "loss": 0.0003,
      "step": 95920
    },
    {
      "epoch": 44.391485423415084,
      "grad_norm": 0.002095982199534774,
      "learning_rate": 0.01121702915316983,
      "loss": 0.0,
      "step": 95930
    },
    {
      "epoch": 44.396112910689496,
      "grad_norm": 0.003620176576077938,
      "learning_rate": 0.01120777417862101,
      "loss": 0.0001,
      "step": 95940
    },
    {
      "epoch": 44.40074039796391,
      "grad_norm": 0.1433064490556717,
      "learning_rate": 0.011198519204072188,
      "loss": 0.0001,
      "step": 95950
    },
    {
      "epoch": 44.40536788523831,
      "grad_norm": 0.008699622005224228,
      "learning_rate": 0.01118926422952337,
      "loss": 0.0001,
      "step": 95960
    },
    {
      "epoch": 44.409995372512725,
      "grad_norm": 0.002482867566868663,
      "learning_rate": 0.01118000925497455,
      "loss": 0.0001,
      "step": 95970
    },
    {
      "epoch": 44.41462285978714,
      "grad_norm": 0.01728086918592453,
      "learning_rate": 0.01117075428042573,
      "loss": 0.0002,
      "step": 95980
    },
    {
      "epoch": 44.41925034706154,
      "grad_norm": 0.003973622340708971,
      "learning_rate": 0.011161499305876909,
      "loss": 0.0001,
      "step": 95990
    },
    {
      "epoch": 44.423877834335954,
      "grad_norm": 0.10301511734724045,
      "learning_rate": 0.011152244331328089,
      "loss": 0.0001,
      "step": 96000
    },
    {
      "epoch": 44.428505321610366,
      "grad_norm": 0.022790782153606415,
      "learning_rate": 0.01114298935677927,
      "loss": 0.0001,
      "step": 96010
    },
    {
      "epoch": 44.43313280888478,
      "grad_norm": 0.017003020271658897,
      "learning_rate": 0.01113373438223045,
      "loss": 0.0002,
      "step": 96020
    },
    {
      "epoch": 44.43776029615918,
      "grad_norm": 0.011203942820429802,
      "learning_rate": 0.01112447940768163,
      "loss": 0.0001,
      "step": 96030
    },
    {
      "epoch": 44.442387783433595,
      "grad_norm": 0.010837102308869362,
      "learning_rate": 0.011115224433132809,
      "loss": 0.0002,
      "step": 96040
    },
    {
      "epoch": 44.44701527070801,
      "grad_norm": 0.0010240519186481833,
      "learning_rate": 0.01110596945858399,
      "loss": 0.0,
      "step": 96050
    },
    {
      "epoch": 44.45164275798241,
      "grad_norm": 0.0018931134836748242,
      "learning_rate": 0.01109671448403517,
      "loss": 0.0001,
      "step": 96060
    },
    {
      "epoch": 44.456270245256825,
      "grad_norm": 0.13947515189647675,
      "learning_rate": 0.01108745950948635,
      "loss": 0.0001,
      "step": 96070
    },
    {
      "epoch": 44.46089773253124,
      "grad_norm": 0.09149494022130966,
      "learning_rate": 0.01107820453493753,
      "loss": 0.0001,
      "step": 96080
    },
    {
      "epoch": 44.46552521980565,
      "grad_norm": 0.23431067168712616,
      "learning_rate": 0.01106894956038871,
      "loss": 0.0001,
      "step": 96090
    },
    {
      "epoch": 44.470152707080054,
      "grad_norm": 0.003203051397576928,
      "learning_rate": 0.01105969458583989,
      "loss": 0.0004,
      "step": 96100
    },
    {
      "epoch": 44.474780194354466,
      "grad_norm": 4.621857643127441,
      "learning_rate": 0.011050439611291069,
      "loss": 0.0012,
      "step": 96110
    },
    {
      "epoch": 44.47940768162888,
      "grad_norm": 0.0010215283837169409,
      "learning_rate": 0.01104118463674225,
      "loss": 0.0,
      "step": 96120
    },
    {
      "epoch": 44.48403516890328,
      "grad_norm": 0.019101077690720558,
      "learning_rate": 0.01103192966219343,
      "loss": 0.0001,
      "step": 96130
    },
    {
      "epoch": 44.488662656177695,
      "grad_norm": 0.002123418962582946,
      "learning_rate": 0.011022674687644611,
      "loss": 0.0001,
      "step": 96140
    },
    {
      "epoch": 44.49329014345211,
      "grad_norm": 0.0034650021698325872,
      "learning_rate": 0.011013419713095789,
      "loss": 0.0001,
      "step": 96150
    },
    {
      "epoch": 44.49791763072651,
      "grad_norm": 0.007148606702685356,
      "learning_rate": 0.01100416473854697,
      "loss": 0.0002,
      "step": 96160
    },
    {
      "epoch": 44.502545118000924,
      "grad_norm": 0.15370972454547882,
      "learning_rate": 0.01099490976399815,
      "loss": 0.0001,
      "step": 96170
    },
    {
      "epoch": 44.507172605275336,
      "grad_norm": 0.0013252056669443846,
      "learning_rate": 0.01098565478944933,
      "loss": 0.0001,
      "step": 96180
    },
    {
      "epoch": 44.51180009254975,
      "grad_norm": 0.11809533089399338,
      "learning_rate": 0.01097639981490051,
      "loss": 0.0001,
      "step": 96190
    },
    {
      "epoch": 44.51642757982415,
      "grad_norm": 0.00660973833873868,
      "learning_rate": 0.010967144840351689,
      "loss": 0.0003,
      "step": 96200
    },
    {
      "epoch": 44.521055067098565,
      "grad_norm": 0.003512138966470957,
      "learning_rate": 0.01095788986580287,
      "loss": 0.0001,
      "step": 96210
    },
    {
      "epoch": 44.52568255437298,
      "grad_norm": 0.0011738189496099949,
      "learning_rate": 0.010948634891254048,
      "loss": 0.0001,
      "step": 96220
    },
    {
      "epoch": 44.53031004164738,
      "grad_norm": 0.0018870485946536064,
      "learning_rate": 0.01093937991670523,
      "loss": 0.0003,
      "step": 96230
    },
    {
      "epoch": 44.534937528921795,
      "grad_norm": 0.011214789934456348,
      "learning_rate": 0.01093012494215641,
      "loss": 0.0001,
      "step": 96240
    },
    {
      "epoch": 44.53956501619621,
      "grad_norm": 0.0042412918992340565,
      "learning_rate": 0.010920869967607591,
      "loss": 0.0,
      "step": 96250
    },
    {
      "epoch": 44.54419250347062,
      "grad_norm": 0.007925814017653465,
      "learning_rate": 0.010911614993058769,
      "loss": 0.0001,
      "step": 96260
    },
    {
      "epoch": 44.548819990745024,
      "grad_norm": 0.0015419331612065434,
      "learning_rate": 0.01090236001850995,
      "loss": 0.0015,
      "step": 96270
    },
    {
      "epoch": 44.553447478019436,
      "grad_norm": 0.011825148947536945,
      "learning_rate": 0.01089310504396113,
      "loss": 0.0001,
      "step": 96280
    },
    {
      "epoch": 44.55807496529385,
      "grad_norm": 0.0013315827818587422,
      "learning_rate": 0.01088385006941231,
      "loss": 0.0002,
      "step": 96290
    },
    {
      "epoch": 44.56270245256825,
      "grad_norm": 0.004556445404887199,
      "learning_rate": 0.01087459509486349,
      "loss": 0.0001,
      "step": 96300
    },
    {
      "epoch": 44.567329939842665,
      "grad_norm": 0.006165734026581049,
      "learning_rate": 0.010865340120314669,
      "loss": 0.0001,
      "step": 96310
    },
    {
      "epoch": 44.57195742711708,
      "grad_norm": 0.0036033601500093937,
      "learning_rate": 0.01085608514576585,
      "loss": 0.0002,
      "step": 96320
    },
    {
      "epoch": 44.57658491439148,
      "grad_norm": 0.0010851307306438684,
      "learning_rate": 0.01084683017121703,
      "loss": 0.0001,
      "step": 96330
    },
    {
      "epoch": 44.581212401665894,
      "grad_norm": 0.007515367120504379,
      "learning_rate": 0.01083757519666821,
      "loss": 0.0002,
      "step": 96340
    },
    {
      "epoch": 44.585839888940306,
      "grad_norm": 0.0006205437821336091,
      "learning_rate": 0.01082832022211939,
      "loss": 0.0001,
      "step": 96350
    },
    {
      "epoch": 44.59046737621472,
      "grad_norm": 0.02480258233845234,
      "learning_rate": 0.010819065247570571,
      "loss": 0.0001,
      "step": 96360
    },
    {
      "epoch": 44.59509486348912,
      "grad_norm": 0.025139182806015015,
      "learning_rate": 0.010809810273021749,
      "loss": 0.0001,
      "step": 96370
    },
    {
      "epoch": 44.599722350763535,
      "grad_norm": 0.02963983081281185,
      "learning_rate": 0.01080055529847293,
      "loss": 0.0001,
      "step": 96380
    },
    {
      "epoch": 44.60434983803795,
      "grad_norm": 0.009190729819238186,
      "learning_rate": 0.01079130032392411,
      "loss": 0.0,
      "step": 96390
    },
    {
      "epoch": 44.60897732531235,
      "grad_norm": 0.0011852524476125836,
      "learning_rate": 0.01078204534937529,
      "loss": 0.0001,
      "step": 96400
    },
    {
      "epoch": 44.613604812586765,
      "grad_norm": 0.040362220257520676,
      "learning_rate": 0.01077279037482647,
      "loss": 0.0003,
      "step": 96410
    },
    {
      "epoch": 44.61823229986118,
      "grad_norm": 0.02552644908428192,
      "learning_rate": 0.010763535400277649,
      "loss": 0.0001,
      "step": 96420
    },
    {
      "epoch": 44.62285978713559,
      "grad_norm": 0.012983818538486958,
      "learning_rate": 0.01075428042572883,
      "loss": 0.0001,
      "step": 96430
    },
    {
      "epoch": 44.627487274409994,
      "grad_norm": 0.004151055123656988,
      "learning_rate": 0.01074502545118001,
      "loss": 0.0001,
      "step": 96440
    },
    {
      "epoch": 44.632114761684406,
      "grad_norm": 0.002884529298171401,
      "learning_rate": 0.01073577047663119,
      "loss": 0.0001,
      "step": 96450
    },
    {
      "epoch": 44.63674224895882,
      "grad_norm": 0.002909119473770261,
      "learning_rate": 0.01072651550208237,
      "loss": 0.0,
      "step": 96460
    },
    {
      "epoch": 44.64136973623322,
      "grad_norm": 0.013092474080622196,
      "learning_rate": 0.01071726052753355,
      "loss": 0.0001,
      "step": 96470
    },
    {
      "epoch": 44.645997223507635,
      "grad_norm": 0.0014444347470998764,
      "learning_rate": 0.01070800555298473,
      "loss": 0.0001,
      "step": 96480
    },
    {
      "epoch": 44.65062471078205,
      "grad_norm": 0.007418468594551086,
      "learning_rate": 0.01069875057843591,
      "loss": 0.0054,
      "step": 96490
    },
    {
      "epoch": 44.65525219805645,
      "grad_norm": 0.002862514927983284,
      "learning_rate": 0.01068949560388709,
      "loss": 0.0001,
      "step": 96500
    },
    {
      "epoch": 44.659879685330864,
      "grad_norm": 0.0007638487149961293,
      "learning_rate": 0.01068024062933827,
      "loss": 0.0001,
      "step": 96510
    },
    {
      "epoch": 44.664507172605276,
      "grad_norm": 0.006743771489709616,
      "learning_rate": 0.010670985654789451,
      "loss": 0.0001,
      "step": 96520
    },
    {
      "epoch": 44.66913465987969,
      "grad_norm": 0.003993822727352381,
      "learning_rate": 0.010661730680240629,
      "loss": 0.0,
      "step": 96530
    },
    {
      "epoch": 44.67376214715409,
      "grad_norm": 6.423019886016846,
      "learning_rate": 0.01065247570569181,
      "loss": 0.0013,
      "step": 96540
    },
    {
      "epoch": 44.678389634428505,
      "grad_norm": 0.0008712278213351965,
      "learning_rate": 0.01064322073114299,
      "loss": 0.0001,
      "step": 96550
    },
    {
      "epoch": 44.68301712170292,
      "grad_norm": 0.02272864431142807,
      "learning_rate": 0.010633965756594171,
      "loss": 0.0001,
      "step": 96560
    },
    {
      "epoch": 44.68764460897732,
      "grad_norm": 0.01927078701555729,
      "learning_rate": 0.01062471078204535,
      "loss": 0.0001,
      "step": 96570
    },
    {
      "epoch": 44.692272096251735,
      "grad_norm": 1.151811122894287,
      "learning_rate": 0.01061545580749653,
      "loss": 0.0003,
      "step": 96580
    },
    {
      "epoch": 44.69689958352615,
      "grad_norm": 0.001477978890761733,
      "learning_rate": 0.01060620083294771,
      "loss": 0.0,
      "step": 96590
    },
    {
      "epoch": 44.70152707080056,
      "grad_norm": 0.001867562416009605,
      "learning_rate": 0.01059694585839889,
      "loss": 0.0,
      "step": 96600
    },
    {
      "epoch": 44.706154558074964,
      "grad_norm": 0.0038786744698882103,
      "learning_rate": 0.01058769088385007,
      "loss": 0.0001,
      "step": 96610
    },
    {
      "epoch": 44.710782045349376,
      "grad_norm": 0.0024758854415267706,
      "learning_rate": 0.01057843590930125,
      "loss": 0.0001,
      "step": 96620
    },
    {
      "epoch": 44.71540953262379,
      "grad_norm": 0.0006661797524429858,
      "learning_rate": 0.010569180934752431,
      "loss": 0.0001,
      "step": 96630
    },
    {
      "epoch": 44.72003701989819,
      "grad_norm": 0.0034705952275544405,
      "learning_rate": 0.010559925960203609,
      "loss": 0.0024,
      "step": 96640
    },
    {
      "epoch": 44.724664507172605,
      "grad_norm": 0.014413212426006794,
      "learning_rate": 0.01055067098565479,
      "loss": 0.0001,
      "step": 96650
    },
    {
      "epoch": 44.72929199444702,
      "grad_norm": 0.011158901266753674,
      "learning_rate": 0.01054141601110597,
      "loss": 0.0004,
      "step": 96660
    },
    {
      "epoch": 44.73391948172142,
      "grad_norm": 0.011453579179942608,
      "learning_rate": 0.010532161036557151,
      "loss": 0.0,
      "step": 96670
    },
    {
      "epoch": 44.738546968995834,
      "grad_norm": 0.0025166915729641914,
      "learning_rate": 0.01052290606200833,
      "loss": 0.0001,
      "step": 96680
    },
    {
      "epoch": 44.743174456270246,
      "grad_norm": 0.009093821980059147,
      "learning_rate": 0.01051365108745951,
      "loss": 0.0,
      "step": 96690
    },
    {
      "epoch": 44.74780194354466,
      "grad_norm": 0.0035575302317738533,
      "learning_rate": 0.01050439611291069,
      "loss": 0.0001,
      "step": 96700
    },
    {
      "epoch": 44.75242943081906,
      "grad_norm": 0.04999824985861778,
      "learning_rate": 0.01049514113836187,
      "loss": 0.0001,
      "step": 96710
    },
    {
      "epoch": 44.757056918093475,
      "grad_norm": 0.004741380922496319,
      "learning_rate": 0.01048588616381305,
      "loss": 0.0,
      "step": 96720
    },
    {
      "epoch": 44.76168440536789,
      "grad_norm": 0.014523083344101906,
      "learning_rate": 0.01047663118926423,
      "loss": 0.0001,
      "step": 96730
    },
    {
      "epoch": 44.76631189264229,
      "grad_norm": 0.002461440861225128,
      "learning_rate": 0.01046737621471541,
      "loss": 0.0001,
      "step": 96740
    },
    {
      "epoch": 44.770939379916705,
      "grad_norm": 0.006815535482019186,
      "learning_rate": 0.01045812124016659,
      "loss": 0.0,
      "step": 96750
    },
    {
      "epoch": 44.77556686719112,
      "grad_norm": 0.004775486420840025,
      "learning_rate": 0.01044886626561777,
      "loss": 0.0,
      "step": 96760
    },
    {
      "epoch": 44.78019435446552,
      "grad_norm": 0.0018988627707585692,
      "learning_rate": 0.01043961129106895,
      "loss": 0.0,
      "step": 96770
    },
    {
      "epoch": 44.784821841739934,
      "grad_norm": 0.0036250008270144463,
      "learning_rate": 0.010430356316520131,
      "loss": 0.0001,
      "step": 96780
    },
    {
      "epoch": 44.789449329014346,
      "grad_norm": 0.06552299857139587,
      "learning_rate": 0.010421101341971311,
      "loss": 0.0001,
      "step": 96790
    },
    {
      "epoch": 44.79407681628876,
      "grad_norm": 0.0017424544785171747,
      "learning_rate": 0.01041184636742249,
      "loss": 0.0001,
      "step": 96800
    },
    {
      "epoch": 44.79870430356316,
      "grad_norm": 0.0027117666322737932,
      "learning_rate": 0.01040259139287367,
      "loss": 0.0001,
      "step": 96810
    },
    {
      "epoch": 44.803331790837575,
      "grad_norm": 15.76926326751709,
      "learning_rate": 0.01039333641832485,
      "loss": 0.0041,
      "step": 96820
    },
    {
      "epoch": 44.80795927811199,
      "grad_norm": 0.1905955821275711,
      "learning_rate": 0.01038408144377603,
      "loss": 0.0001,
      "step": 96830
    },
    {
      "epoch": 44.81258676538639,
      "grad_norm": 0.14997537434101105,
      "learning_rate": 0.01037482646922721,
      "loss": 0.0001,
      "step": 96840
    },
    {
      "epoch": 44.817214252660804,
      "grad_norm": 0.009695892222225666,
      "learning_rate": 0.01036557149467839,
      "loss": 0.0001,
      "step": 96850
    },
    {
      "epoch": 44.821841739935216,
      "grad_norm": 0.014622981660068035,
      "learning_rate": 0.01035631652012957,
      "loss": 0.0007,
      "step": 96860
    },
    {
      "epoch": 44.82646922720963,
      "grad_norm": 0.0033773344475775957,
      "learning_rate": 0.01034706154558075,
      "loss": 0.0001,
      "step": 96870
    },
    {
      "epoch": 44.83109671448403,
      "grad_norm": 0.0028305051382631063,
      "learning_rate": 0.01033780657103193,
      "loss": 0.0001,
      "step": 96880
    },
    {
      "epoch": 44.835724201758445,
      "grad_norm": 0.008036291226744652,
      "learning_rate": 0.010328551596483111,
      "loss": 0.0001,
      "step": 96890
    },
    {
      "epoch": 44.84035168903286,
      "grad_norm": 0.0026877010241150856,
      "learning_rate": 0.010319296621934291,
      "loss": 0.0,
      "step": 96900
    },
    {
      "epoch": 44.84497917630726,
      "grad_norm": 0.0024046541657298803,
      "learning_rate": 0.01031004164738547,
      "loss": 0.0,
      "step": 96910
    },
    {
      "epoch": 44.849606663581675,
      "grad_norm": 0.0024924271274358034,
      "learning_rate": 0.01030078667283665,
      "loss": 0.0003,
      "step": 96920
    },
    {
      "epoch": 44.85423415085609,
      "grad_norm": 0.0019126598490402102,
      "learning_rate": 0.01029153169828783,
      "loss": 0.0,
      "step": 96930
    },
    {
      "epoch": 44.85886163813049,
      "grad_norm": 0.054259832948446274,
      "learning_rate": 0.010282276723739011,
      "loss": 0.0001,
      "step": 96940
    },
    {
      "epoch": 44.863489125404904,
      "grad_norm": 0.008606313727796078,
      "learning_rate": 0.01027302174919019,
      "loss": 0.0002,
      "step": 96950
    },
    {
      "epoch": 44.868116612679316,
      "grad_norm": 0.04465281218290329,
      "learning_rate": 0.01026376677464137,
      "loss": 0.0007,
      "step": 96960
    },
    {
      "epoch": 44.87274409995373,
      "grad_norm": 0.010995680466294289,
      "learning_rate": 0.01025451180009255,
      "loss": 0.0001,
      "step": 96970
    },
    {
      "epoch": 44.87737158722813,
      "grad_norm": 0.4495064914226532,
      "learning_rate": 0.010245256825543732,
      "loss": 0.0001,
      "step": 96980
    },
    {
      "epoch": 44.881999074502545,
      "grad_norm": 1.0337367057800293,
      "learning_rate": 0.01023600185099491,
      "loss": 0.0003,
      "step": 96990
    },
    {
      "epoch": 44.88662656177696,
      "grad_norm": 0.0016570681473240256,
      "learning_rate": 0.010226746876446091,
      "loss": 0.0,
      "step": 97000
    },
    {
      "epoch": 44.89125404905136,
      "grad_norm": 0.021234530955553055,
      "learning_rate": 0.01021749190189727,
      "loss": 0.0001,
      "step": 97010
    },
    {
      "epoch": 44.895881536325774,
      "grad_norm": 0.006215255707502365,
      "learning_rate": 0.01020823692734845,
      "loss": 0.0,
      "step": 97020
    },
    {
      "epoch": 44.900509023600186,
      "grad_norm": 0.0607537217438221,
      "learning_rate": 0.01019898195279963,
      "loss": 0.0001,
      "step": 97030
    },
    {
      "epoch": 44.9051365108746,
      "grad_norm": 0.013188299722969532,
      "learning_rate": 0.01018972697825081,
      "loss": 0.0002,
      "step": 97040
    },
    {
      "epoch": 44.909763998149,
      "grad_norm": 0.003847267245873809,
      "learning_rate": 0.010180472003701991,
      "loss": 0.0001,
      "step": 97050
    },
    {
      "epoch": 44.914391485423415,
      "grad_norm": 0.10782213509082794,
      "learning_rate": 0.01017121702915317,
      "loss": 0.0001,
      "step": 97060
    },
    {
      "epoch": 44.91901897269783,
      "grad_norm": 0.0046004499308764935,
      "learning_rate": 0.01016196205460435,
      "loss": 0.0001,
      "step": 97070
    },
    {
      "epoch": 44.92364645997223,
      "grad_norm": 0.0021531416568905115,
      "learning_rate": 0.01015270708005553,
      "loss": 0.0001,
      "step": 97080
    },
    {
      "epoch": 44.928273947246645,
      "grad_norm": 0.008299600332975388,
      "learning_rate": 0.010143452105506712,
      "loss": 0.0008,
      "step": 97090
    },
    {
      "epoch": 44.93290143452106,
      "grad_norm": 0.06515393406152725,
      "learning_rate": 0.01013419713095789,
      "loss": 0.0012,
      "step": 97100
    },
    {
      "epoch": 44.93752892179546,
      "grad_norm": 0.0012722811661660671,
      "learning_rate": 0.010124942156409071,
      "loss": 0.0001,
      "step": 97110
    },
    {
      "epoch": 44.942156409069874,
      "grad_norm": 0.002131454646587372,
      "learning_rate": 0.01011568718186025,
      "loss": 0.0002,
      "step": 97120
    },
    {
      "epoch": 44.946783896344286,
      "grad_norm": 0.0015522642061114311,
      "learning_rate": 0.01010643220731143,
      "loss": 0.0001,
      "step": 97130
    },
    {
      "epoch": 44.9514113836187,
      "grad_norm": 0.004924639593809843,
      "learning_rate": 0.01009717723276261,
      "loss": 0.0004,
      "step": 97140
    },
    {
      "epoch": 44.9560388708931,
      "grad_norm": 0.04143766313791275,
      "learning_rate": 0.01008792225821379,
      "loss": 0.0001,
      "step": 97150
    },
    {
      "epoch": 44.960666358167515,
      "grad_norm": 0.0022557128686457872,
      "learning_rate": 0.010078667283664971,
      "loss": 0.0002,
      "step": 97160
    },
    {
      "epoch": 44.96529384544193,
      "grad_norm": 0.008083337917923927,
      "learning_rate": 0.010069412309116151,
      "loss": 0.0001,
      "step": 97170
    },
    {
      "epoch": 44.96992133271633,
      "grad_norm": 0.003834610339254141,
      "learning_rate": 0.01006015733456733,
      "loss": 0.0,
      "step": 97180
    },
    {
      "epoch": 44.974548819990744,
      "grad_norm": 0.004784533753991127,
      "learning_rate": 0.01005090236001851,
      "loss": 0.0001,
      "step": 97190
    },
    {
      "epoch": 44.979176307265156,
      "grad_norm": 0.0762120932340622,
      "learning_rate": 0.010041647385469692,
      "loss": 0.0001,
      "step": 97200
    },
    {
      "epoch": 44.98380379453957,
      "grad_norm": 0.01711033284664154,
      "learning_rate": 0.010032392410920871,
      "loss": 0.0003,
      "step": 97210
    },
    {
      "epoch": 44.98843128181397,
      "grad_norm": 0.004402137827128172,
      "learning_rate": 0.010023137436372051,
      "loss": 0.0,
      "step": 97220
    },
    {
      "epoch": 44.993058769088385,
      "grad_norm": 0.012751264497637749,
      "learning_rate": 0.01001388246182323,
      "loss": 0.0002,
      "step": 97230
    },
    {
      "epoch": 44.9976862563628,
      "grad_norm": 0.01196508388966322,
      "learning_rate": 0.01000462748727441,
      "loss": 0.0001,
      "step": 97240
    },
    {
      "epoch": 45.0,
      "eval_accuracy_branch1": 0.9898320751810199,
      "eval_accuracy_branch2": 0.5,
      "eval_f1_branch1": 0.9909848267302295,
      "eval_f1_branch2": 0.49935768176281226,
      "eval_loss": 0.016758138313889503,
      "eval_precision_branch1": 0.9911625655012095,
      "eval_precision_branch2": 0.5,
      "eval_recall_branch1": 0.9909261768775692,
      "eval_recall_branch2": 0.5,
      "eval_runtime": 28.9931,
      "eval_samples_per_second": 447.761,
      "eval_steps_per_second": 55.979,
      "step": 97245
    },
    {
      "epoch": 45.0023137436372,
      "grad_norm": 0.02674052305519581,
      "learning_rate": 0.00999537251272559,
      "loss": 0.0016,
      "step": 97250
    },
    {
      "epoch": 45.006941230911615,
      "grad_norm": 0.011008855886757374,
      "learning_rate": 0.00998611753817677,
      "loss": 0.0001,
      "step": 97260
    },
    {
      "epoch": 45.01156871818603,
      "grad_norm": 0.0015382178826257586,
      "learning_rate": 0.009976862563627951,
      "loss": 0.0001,
      "step": 97270
    },
    {
      "epoch": 45.01619620546043,
      "grad_norm": 0.014765783213078976,
      "learning_rate": 0.00996760758907913,
      "loss": 0.0001,
      "step": 97280
    },
    {
      "epoch": 45.020823692734844,
      "grad_norm": 0.03633512556552887,
      "learning_rate": 0.00995835261453031,
      "loss": 0.0001,
      "step": 97290
    },
    {
      "epoch": 45.025451180009256,
      "grad_norm": 0.007620299234986305,
      "learning_rate": 0.00994909763998149,
      "loss": 0.0001,
      "step": 97300
    },
    {
      "epoch": 45.03007866728367,
      "grad_norm": 0.0010108983842656016,
      "learning_rate": 0.009939842665432672,
      "loss": 0.0,
      "step": 97310
    },
    {
      "epoch": 45.03470615455807,
      "grad_norm": 0.013355760835111141,
      "learning_rate": 0.009930587690883851,
      "loss": 0.0001,
      "step": 97320
    },
    {
      "epoch": 45.039333641832485,
      "grad_norm": 0.009724094532430172,
      "learning_rate": 0.009921332716335031,
      "loss": 0.0001,
      "step": 97330
    },
    {
      "epoch": 45.0439611291069,
      "grad_norm": 0.03870663046836853,
      "learning_rate": 0.00991207774178621,
      "loss": 0.0001,
      "step": 97340
    },
    {
      "epoch": 45.0485886163813,
      "grad_norm": 0.03482965752482414,
      "learning_rate": 0.00990282276723739,
      "loss": 0.0018,
      "step": 97350
    },
    {
      "epoch": 45.053216103655714,
      "grad_norm": 0.0023522323463112116,
      "learning_rate": 0.009893567792688572,
      "loss": 0.0001,
      "step": 97360
    },
    {
      "epoch": 45.057843590930126,
      "grad_norm": 0.0012790919281542301,
      "learning_rate": 0.00988431281813975,
      "loss": 0.0001,
      "step": 97370
    },
    {
      "epoch": 45.06247107820454,
      "grad_norm": 0.0026051639579236507,
      "learning_rate": 0.009875057843590931,
      "loss": 0.0,
      "step": 97380
    },
    {
      "epoch": 45.06709856547894,
      "grad_norm": 0.0023548307362943888,
      "learning_rate": 0.00986580286904211,
      "loss": 0.0,
      "step": 97390
    },
    {
      "epoch": 45.071726052753355,
      "grad_norm": 0.002167715225368738,
      "learning_rate": 0.009856547894493292,
      "loss": 0.0001,
      "step": 97400
    },
    {
      "epoch": 45.07635354002777,
      "grad_norm": 0.002388331340625882,
      "learning_rate": 0.00984729291994447,
      "loss": 0.0001,
      "step": 97410
    },
    {
      "epoch": 45.08098102730217,
      "grad_norm": 0.006635747384279966,
      "learning_rate": 0.009838037945395652,
      "loss": 0.0001,
      "step": 97420
    },
    {
      "epoch": 45.085608514576585,
      "grad_norm": 0.0029126896988600492,
      "learning_rate": 0.009828782970846831,
      "loss": 0.0,
      "step": 97430
    },
    {
      "epoch": 45.090236001851,
      "grad_norm": 0.7218896150588989,
      "learning_rate": 0.009819527996298011,
      "loss": 0.0003,
      "step": 97440
    },
    {
      "epoch": 45.0948634891254,
      "grad_norm": 0.01717076078057289,
      "learning_rate": 0.00981027302174919,
      "loss": 0.0001,
      "step": 97450
    },
    {
      "epoch": 45.099490976399814,
      "grad_norm": 0.17696645855903625,
      "learning_rate": 0.00980101804720037,
      "loss": 0.0002,
      "step": 97460
    },
    {
      "epoch": 45.104118463674226,
      "grad_norm": 0.018253006041049957,
      "learning_rate": 0.009791763072651552,
      "loss": 0.0001,
      "step": 97470
    },
    {
      "epoch": 45.10874595094864,
      "grad_norm": 0.01916600577533245,
      "learning_rate": 0.00978250809810273,
      "loss": 0.0001,
      "step": 97480
    },
    {
      "epoch": 45.11337343822304,
      "grad_norm": 0.002173131098970771,
      "learning_rate": 0.009773253123553911,
      "loss": 0.0002,
      "step": 97490
    },
    {
      "epoch": 45.118000925497455,
      "grad_norm": 0.0029166864696890116,
      "learning_rate": 0.00976399814900509,
      "loss": 0.0001,
      "step": 97500
    },
    {
      "epoch": 45.12262841277187,
      "grad_norm": 0.004631613381206989,
      "learning_rate": 0.009754743174456272,
      "loss": 0.0001,
      "step": 97510
    },
    {
      "epoch": 45.12725590004627,
      "grad_norm": 0.0031653710175305605,
      "learning_rate": 0.00974548819990745,
      "loss": 0.0001,
      "step": 97520
    },
    {
      "epoch": 45.131883387320684,
      "grad_norm": 0.0026101761031895876,
      "learning_rate": 0.009736233225358631,
      "loss": 0.0005,
      "step": 97530
    },
    {
      "epoch": 45.136510874595096,
      "grad_norm": 0.029202435165643692,
      "learning_rate": 0.009726978250809811,
      "loss": 0.0001,
      "step": 97540
    },
    {
      "epoch": 45.14113836186951,
      "grad_norm": 0.0035495909396559,
      "learning_rate": 0.00971772327626099,
      "loss": 0.0,
      "step": 97550
    },
    {
      "epoch": 45.14576584914391,
      "grad_norm": 0.0037644365802407265,
      "learning_rate": 0.00970846830171217,
      "loss": 0.0001,
      "step": 97560
    },
    {
      "epoch": 45.150393336418325,
      "grad_norm": 0.0891634002327919,
      "learning_rate": 0.00969921332716335,
      "loss": 0.0008,
      "step": 97570
    },
    {
      "epoch": 45.15502082369274,
      "grad_norm": 0.015398290939629078,
      "learning_rate": 0.009689958352614532,
      "loss": 0.0001,
      "step": 97580
    },
    {
      "epoch": 45.15964831096714,
      "grad_norm": 0.015738951042294502,
      "learning_rate": 0.009680703378065711,
      "loss": 0.0001,
      "step": 97590
    },
    {
      "epoch": 45.164275798241555,
      "grad_norm": 0.04729703441262245,
      "learning_rate": 0.009671448403516891,
      "loss": 0.0001,
      "step": 97600
    },
    {
      "epoch": 45.16890328551597,
      "grad_norm": 0.06444506347179413,
      "learning_rate": 0.00966219342896807,
      "loss": 0.0001,
      "step": 97610
    },
    {
      "epoch": 45.17353077279037,
      "grad_norm": 0.0024031561333686113,
      "learning_rate": 0.009652938454419252,
      "loss": 0.0005,
      "step": 97620
    },
    {
      "epoch": 45.178158260064784,
      "grad_norm": 0.003338238224387169,
      "learning_rate": 0.009643683479870432,
      "loss": 0.0001,
      "step": 97630
    },
    {
      "epoch": 45.182785747339196,
      "grad_norm": 0.013402316719293594,
      "learning_rate": 0.009634428505321611,
      "loss": 0.0,
      "step": 97640
    },
    {
      "epoch": 45.18741323461361,
      "grad_norm": 0.004433212336152792,
      "learning_rate": 0.009625173530772791,
      "loss": 0.0,
      "step": 97650
    },
    {
      "epoch": 45.19204072188801,
      "grad_norm": 0.005167887080460787,
      "learning_rate": 0.00961591855622397,
      "loss": 0.0001,
      "step": 97660
    },
    {
      "epoch": 45.196668209162425,
      "grad_norm": 0.0022408575750887394,
      "learning_rate": 0.00960666358167515,
      "loss": 0.0001,
      "step": 97670
    },
    {
      "epoch": 45.20129569643684,
      "grad_norm": 0.009986579418182373,
      "learning_rate": 0.00959740860712633,
      "loss": 0.0003,
      "step": 97680
    },
    {
      "epoch": 45.20592318371124,
      "grad_norm": 0.006638522259891033,
      "learning_rate": 0.009588153632577512,
      "loss": 0.0001,
      "step": 97690
    },
    {
      "epoch": 45.210550670985654,
      "grad_norm": 0.0019439851166680455,
      "learning_rate": 0.009578898658028691,
      "loss": 0.0,
      "step": 97700
    },
    {
      "epoch": 45.215178158260066,
      "grad_norm": 0.009241056628525257,
      "learning_rate": 0.00956964368347987,
      "loss": 0.0001,
      "step": 97710
    },
    {
      "epoch": 45.21980564553448,
      "grad_norm": 0.005430884659290314,
      "learning_rate": 0.00956038870893105,
      "loss": 0.0001,
      "step": 97720
    },
    {
      "epoch": 45.22443313280888,
      "grad_norm": 0.01214585267007351,
      "learning_rate": 0.009551133734382232,
      "loss": 0.0002,
      "step": 97730
    },
    {
      "epoch": 45.229060620083295,
      "grad_norm": 0.013781707733869553,
      "learning_rate": 0.009541878759833412,
      "loss": 0.0,
      "step": 97740
    },
    {
      "epoch": 45.23368810735771,
      "grad_norm": 0.0033774131443351507,
      "learning_rate": 0.009532623785284591,
      "loss": 0.0001,
      "step": 97750
    },
    {
      "epoch": 45.23831559463211,
      "grad_norm": 0.02754751406610012,
      "learning_rate": 0.009523368810735771,
      "loss": 0.0001,
      "step": 97760
    },
    {
      "epoch": 45.242943081906525,
      "grad_norm": 0.0010781411547213793,
      "learning_rate": 0.00951411383618695,
      "loss": 0.0,
      "step": 97770
    },
    {
      "epoch": 45.24757056918094,
      "grad_norm": 0.07235980033874512,
      "learning_rate": 0.009504858861638132,
      "loss": 0.0001,
      "step": 97780
    },
    {
      "epoch": 45.25219805645534,
      "grad_norm": 0.03878685086965561,
      "learning_rate": 0.00949560388708931,
      "loss": 0.0002,
      "step": 97790
    },
    {
      "epoch": 45.256825543729754,
      "grad_norm": 0.0023107589222490788,
      "learning_rate": 0.009486348912540491,
      "loss": 0.0001,
      "step": 97800
    },
    {
      "epoch": 45.261453031004166,
      "grad_norm": 0.005450444296002388,
      "learning_rate": 0.009477093937991671,
      "loss": 0.0001,
      "step": 97810
    },
    {
      "epoch": 45.26608051827858,
      "grad_norm": 0.1860632747411728,
      "learning_rate": 0.009467838963442853,
      "loss": 0.0001,
      "step": 97820
    },
    {
      "epoch": 45.27070800555298,
      "grad_norm": 0.02491128258407116,
      "learning_rate": 0.00945858398889403,
      "loss": 0.0001,
      "step": 97830
    },
    {
      "epoch": 45.275335492827395,
      "grad_norm": 0.04585107043385506,
      "learning_rate": 0.009449329014345212,
      "loss": 0.0002,
      "step": 97840
    },
    {
      "epoch": 45.27996298010181,
      "grad_norm": 0.0028815551195293665,
      "learning_rate": 0.009440074039796392,
      "loss": 0.0001,
      "step": 97850
    },
    {
      "epoch": 45.28459046737621,
      "grad_norm": 0.002443550620228052,
      "learning_rate": 0.009430819065247571,
      "loss": 0.0001,
      "step": 97860
    },
    {
      "epoch": 45.289217954650624,
      "grad_norm": 0.02048821747303009,
      "learning_rate": 0.009421564090698751,
      "loss": 0.0001,
      "step": 97870
    },
    {
      "epoch": 45.293845441925036,
      "grad_norm": 0.008117375895380974,
      "learning_rate": 0.00941230911614993,
      "loss": 0.0003,
      "step": 97880
    },
    {
      "epoch": 45.29847292919944,
      "grad_norm": 0.0003346102894283831,
      "learning_rate": 0.009403054141601112,
      "loss": 0.0001,
      "step": 97890
    },
    {
      "epoch": 45.30310041647385,
      "grad_norm": 0.003422400914132595,
      "learning_rate": 0.00939379916705229,
      "loss": 0.0001,
      "step": 97900
    },
    {
      "epoch": 45.307727903748265,
      "grad_norm": 0.0008301728521473706,
      "learning_rate": 0.009384544192503471,
      "loss": 0.0002,
      "step": 97910
    },
    {
      "epoch": 45.31235539102268,
      "grad_norm": 0.00988710392266512,
      "learning_rate": 0.009375289217954651,
      "loss": 0.0002,
      "step": 97920
    },
    {
      "epoch": 45.31698287829708,
      "grad_norm": 0.007869335822761059,
      "learning_rate": 0.00936603424340583,
      "loss": 0.0,
      "step": 97930
    },
    {
      "epoch": 45.321610365571495,
      "grad_norm": 0.030640441924333572,
      "learning_rate": 0.00935677926885701,
      "loss": 0.0004,
      "step": 97940
    },
    {
      "epoch": 45.32623785284591,
      "grad_norm": 0.015127778984606266,
      "learning_rate": 0.00934752429430819,
      "loss": 0.0001,
      "step": 97950
    },
    {
      "epoch": 45.33086534012031,
      "grad_norm": 0.029225772246718407,
      "learning_rate": 0.009338269319759371,
      "loss": 0.0001,
      "step": 97960
    },
    {
      "epoch": 45.335492827394724,
      "grad_norm": 0.006426203530281782,
      "learning_rate": 0.009329014345210551,
      "loss": 0.0003,
      "step": 97970
    },
    {
      "epoch": 45.340120314669136,
      "grad_norm": 0.016517382115125656,
      "learning_rate": 0.00931975937066173,
      "loss": 0.0001,
      "step": 97980
    },
    {
      "epoch": 45.34474780194355,
      "grad_norm": 0.002288949443027377,
      "learning_rate": 0.00931050439611291,
      "loss": 0.0,
      "step": 97990
    },
    {
      "epoch": 45.34937528921795,
      "grad_norm": 0.0025750186759978533,
      "learning_rate": 0.009301249421564092,
      "loss": 0.0001,
      "step": 98000
    },
    {
      "epoch": 45.354002776492365,
      "grad_norm": 0.0010360187152400613,
      "learning_rate": 0.009291994447015272,
      "loss": 0.0001,
      "step": 98010
    },
    {
      "epoch": 45.35863026376678,
      "grad_norm": 0.002804744755849242,
      "learning_rate": 0.009282739472466451,
      "loss": 0.0007,
      "step": 98020
    },
    {
      "epoch": 45.36325775104118,
      "grad_norm": 0.011611019261181355,
      "learning_rate": 0.009273484497917631,
      "loss": 0.0001,
      "step": 98030
    },
    {
      "epoch": 45.367885238315594,
      "grad_norm": 0.0015745629789307714,
      "learning_rate": 0.00926422952336881,
      "loss": 0.0001,
      "step": 98040
    },
    {
      "epoch": 45.372512725590006,
      "grad_norm": 0.016371097415685654,
      "learning_rate": 0.009254974548819992,
      "loss": 0.0002,
      "step": 98050
    },
    {
      "epoch": 45.37714021286441,
      "grad_norm": 0.00898069329559803,
      "learning_rate": 0.00924571957427117,
      "loss": 0.0002,
      "step": 98060
    },
    {
      "epoch": 45.38176770013882,
      "grad_norm": 0.015726905316114426,
      "learning_rate": 0.009236464599722351,
      "loss": 0.0001,
      "step": 98070
    },
    {
      "epoch": 45.386395187413235,
      "grad_norm": 0.0070603652857244015,
      "learning_rate": 0.009227209625173531,
      "loss": 0.0002,
      "step": 98080
    },
    {
      "epoch": 45.39102267468765,
      "grad_norm": 0.008311194367706776,
      "learning_rate": 0.00921795465062471,
      "loss": 0.0003,
      "step": 98090
    },
    {
      "epoch": 45.39565016196205,
      "grad_norm": 0.023169072344899178,
      "learning_rate": 0.00920869967607589,
      "loss": 0.0001,
      "step": 98100
    },
    {
      "epoch": 45.400277649236465,
      "grad_norm": 0.0003987877571489662,
      "learning_rate": 0.009199444701527072,
      "loss": 0.0001,
      "step": 98110
    },
    {
      "epoch": 45.40490513651088,
      "grad_norm": 0.010205721482634544,
      "learning_rate": 0.009190189726978252,
      "loss": 0.0001,
      "step": 98120
    },
    {
      "epoch": 45.40953262378528,
      "grad_norm": 0.0030180870089679956,
      "learning_rate": 0.009180934752429431,
      "loss": 0.0001,
      "step": 98130
    },
    {
      "epoch": 45.414160111059694,
      "grad_norm": 0.002542197937145829,
      "learning_rate": 0.009171679777880611,
      "loss": 0.0001,
      "step": 98140
    },
    {
      "epoch": 45.418787598334106,
      "grad_norm": 0.04891123250126839,
      "learning_rate": 0.00916242480333179,
      "loss": 0.0001,
      "step": 98150
    },
    {
      "epoch": 45.42341508560852,
      "grad_norm": 0.0016432623378932476,
      "learning_rate": 0.009153169828782972,
      "loss": 0.0001,
      "step": 98160
    },
    {
      "epoch": 45.42804257288292,
      "grad_norm": 0.0039948970079422,
      "learning_rate": 0.00914391485423415,
      "loss": 0.0,
      "step": 98170
    },
    {
      "epoch": 45.432670060157335,
      "grad_norm": 0.014279365539550781,
      "learning_rate": 0.009134659879685331,
      "loss": 0.0001,
      "step": 98180
    },
    {
      "epoch": 45.43729754743175,
      "grad_norm": 0.05192161351442337,
      "learning_rate": 0.009125404905136511,
      "loss": 0.0001,
      "step": 98190
    },
    {
      "epoch": 45.44192503470615,
      "grad_norm": 0.0018066203920170665,
      "learning_rate": 0.009116149930587692,
      "loss": 0.0002,
      "step": 98200
    },
    {
      "epoch": 45.446552521980564,
      "grad_norm": 0.23714202642440796,
      "learning_rate": 0.00910689495603887,
      "loss": 0.0002,
      "step": 98210
    },
    {
      "epoch": 45.451180009254976,
      "grad_norm": 0.014428000897169113,
      "learning_rate": 0.009097639981490052,
      "loss": 0.0,
      "step": 98220
    },
    {
      "epoch": 45.45580749652938,
      "grad_norm": 0.0052754245698452,
      "learning_rate": 0.009088385006941231,
      "loss": 0.0001,
      "step": 98230
    },
    {
      "epoch": 45.46043498380379,
      "grad_norm": 0.007465017028152943,
      "learning_rate": 0.009079130032392411,
      "loss": 0.0001,
      "step": 98240
    },
    {
      "epoch": 45.465062471078205,
      "grad_norm": 0.003092604922130704,
      "learning_rate": 0.00906987505784359,
      "loss": 0.0001,
      "step": 98250
    },
    {
      "epoch": 45.46968995835262,
      "grad_norm": 0.005295176524668932,
      "learning_rate": 0.00906062008329477,
      "loss": 0.0001,
      "step": 98260
    },
    {
      "epoch": 45.47431744562702,
      "grad_norm": 0.040041521191596985,
      "learning_rate": 0.009051365108745952,
      "loss": 0.0002,
      "step": 98270
    },
    {
      "epoch": 45.478944932901435,
      "grad_norm": 0.0005030173342674971,
      "learning_rate": 0.009042110134197132,
      "loss": 0.0001,
      "step": 98280
    },
    {
      "epoch": 45.48357242017585,
      "grad_norm": 0.028825828805565834,
      "learning_rate": 0.009032855159648311,
      "loss": 0.0001,
      "step": 98290
    },
    {
      "epoch": 45.48819990745025,
      "grad_norm": 0.005648140795528889,
      "learning_rate": 0.009023600185099491,
      "loss": 0.0001,
      "step": 98300
    },
    {
      "epoch": 45.492827394724664,
      "grad_norm": 0.03245477005839348,
      "learning_rate": 0.009014345210550672,
      "loss": 0.0001,
      "step": 98310
    },
    {
      "epoch": 45.497454881999076,
      "grad_norm": 0.0006819196278229356,
      "learning_rate": 0.00900509023600185,
      "loss": 0.0001,
      "step": 98320
    },
    {
      "epoch": 45.50208236927349,
      "grad_norm": 0.008538060821592808,
      "learning_rate": 0.008995835261453032,
      "loss": 0.0001,
      "step": 98330
    },
    {
      "epoch": 45.50670985654789,
      "grad_norm": 0.07977285236120224,
      "learning_rate": 0.008986580286904211,
      "loss": 0.0001,
      "step": 98340
    },
    {
      "epoch": 45.511337343822305,
      "grad_norm": 0.0013952696463093162,
      "learning_rate": 0.008977325312355391,
      "loss": 0.0001,
      "step": 98350
    },
    {
      "epoch": 45.51596483109672,
      "grad_norm": 0.007010759320110083,
      "learning_rate": 0.00896807033780657,
      "loss": 0.0001,
      "step": 98360
    },
    {
      "epoch": 45.52059231837112,
      "grad_norm": 0.0973418578505516,
      "learning_rate": 0.00895881536325775,
      "loss": 0.0002,
      "step": 98370
    },
    {
      "epoch": 45.525219805645534,
      "grad_norm": 0.0031500132754445076,
      "learning_rate": 0.008949560388708932,
      "loss": 0.0,
      "step": 98380
    },
    {
      "epoch": 45.529847292919946,
      "grad_norm": 0.001763310399837792,
      "learning_rate": 0.008940305414160112,
      "loss": 0.0001,
      "step": 98390
    },
    {
      "epoch": 45.53447478019435,
      "grad_norm": 0.0014405378606170416,
      "learning_rate": 0.008931050439611291,
      "loss": 0.0001,
      "step": 98400
    },
    {
      "epoch": 45.53910226746876,
      "grad_norm": 0.03287715092301369,
      "learning_rate": 0.008921795465062471,
      "loss": 0.0001,
      "step": 98410
    },
    {
      "epoch": 45.543729754743175,
      "grad_norm": 0.13143089413642883,
      "learning_rate": 0.008912540490513652,
      "loss": 0.0001,
      "step": 98420
    },
    {
      "epoch": 45.54835724201759,
      "grad_norm": 0.03314516320824623,
      "learning_rate": 0.008903285515964832,
      "loss": 0.0001,
      "step": 98430
    },
    {
      "epoch": 45.55298472929199,
      "grad_norm": 0.024803869426250458,
      "learning_rate": 0.008894030541416012,
      "loss": 0.0001,
      "step": 98440
    },
    {
      "epoch": 45.557612216566405,
      "grad_norm": 0.05007635056972504,
      "learning_rate": 0.008884775566867191,
      "loss": 0.0001,
      "step": 98450
    },
    {
      "epoch": 45.56223970384082,
      "grad_norm": 0.0795140489935875,
      "learning_rate": 0.008875520592318371,
      "loss": 0.0001,
      "step": 98460
    },
    {
      "epoch": 45.56686719111522,
      "grad_norm": 0.020834393799304962,
      "learning_rate": 0.008866265617769552,
      "loss": 0.0001,
      "step": 98470
    },
    {
      "epoch": 45.571494678389634,
      "grad_norm": 0.0013082600198686123,
      "learning_rate": 0.00885701064322073,
      "loss": 0.0002,
      "step": 98480
    },
    {
      "epoch": 45.576122165664046,
      "grad_norm": 0.0046006725169718266,
      "learning_rate": 0.008847755668671912,
      "loss": 0.0,
      "step": 98490
    },
    {
      "epoch": 45.58074965293845,
      "grad_norm": 0.002725361380726099,
      "learning_rate": 0.008838500694123091,
      "loss": 0.0004,
      "step": 98500
    },
    {
      "epoch": 45.58537714021286,
      "grad_norm": 0.0025786259211599827,
      "learning_rate": 0.008829245719574273,
      "loss": 0.0001,
      "step": 98510
    },
    {
      "epoch": 45.590004627487275,
      "grad_norm": 0.0018111445242539048,
      "learning_rate": 0.00881999074502545,
      "loss": 0.0001,
      "step": 98520
    },
    {
      "epoch": 45.59463211476169,
      "grad_norm": 0.006159419193863869,
      "learning_rate": 0.008810735770476632,
      "loss": 0.0,
      "step": 98530
    },
    {
      "epoch": 45.59925960203609,
      "grad_norm": 0.021922271698713303,
      "learning_rate": 0.008801480795927812,
      "loss": 0.0001,
      "step": 98540
    },
    {
      "epoch": 45.603887089310504,
      "grad_norm": 0.001617093221284449,
      "learning_rate": 0.008792225821378992,
      "loss": 0.0001,
      "step": 98550
    },
    {
      "epoch": 45.608514576584916,
      "grad_norm": 0.08481958508491516,
      "learning_rate": 0.008782970846830171,
      "loss": 0.0001,
      "step": 98560
    },
    {
      "epoch": 45.61314206385932,
      "grad_norm": 0.018364546820521355,
      "learning_rate": 0.008773715872281351,
      "loss": 0.0001,
      "step": 98570
    },
    {
      "epoch": 45.61776955113373,
      "grad_norm": 0.001689308788627386,
      "learning_rate": 0.008764460897732532,
      "loss": 0.0001,
      "step": 98580
    },
    {
      "epoch": 45.622397038408145,
      "grad_norm": 0.006361626088619232,
      "learning_rate": 0.00875520592318371,
      "loss": 0.0003,
      "step": 98590
    },
    {
      "epoch": 45.62702452568256,
      "grad_norm": 0.006217070855200291,
      "learning_rate": 0.008745950948634892,
      "loss": 0.0001,
      "step": 98600
    },
    {
      "epoch": 45.63165201295696,
      "grad_norm": 0.1565423160791397,
      "learning_rate": 0.008736695974086071,
      "loss": 0.0001,
      "step": 98610
    },
    {
      "epoch": 45.636279500231375,
      "grad_norm": 0.004555571358650923,
      "learning_rate": 0.008727440999537253,
      "loss": 0.0001,
      "step": 98620
    },
    {
      "epoch": 45.64090698750579,
      "grad_norm": 0.038108933717012405,
      "learning_rate": 0.00871818602498843,
      "loss": 0.0002,
      "step": 98630
    },
    {
      "epoch": 45.64553447478019,
      "grad_norm": 0.02186383306980133,
      "learning_rate": 0.008708931050439612,
      "loss": 0.0001,
      "step": 98640
    },
    {
      "epoch": 45.650161962054604,
      "grad_norm": 0.0015559069579467177,
      "learning_rate": 0.008699676075890792,
      "loss": 0.0001,
      "step": 98650
    },
    {
      "epoch": 45.654789449329016,
      "grad_norm": 0.007747613359242678,
      "learning_rate": 0.008690421101341972,
      "loss": 0.0001,
      "step": 98660
    },
    {
      "epoch": 45.65941693660342,
      "grad_norm": 0.0017408085986971855,
      "learning_rate": 0.008681166126793151,
      "loss": 0.0,
      "step": 98670
    },
    {
      "epoch": 45.66404442387783,
      "grad_norm": 0.029470261186361313,
      "learning_rate": 0.008671911152244331,
      "loss": 0.0002,
      "step": 98680
    },
    {
      "epoch": 45.668671911152245,
      "grad_norm": 0.0023687102366238832,
      "learning_rate": 0.008662656177695512,
      "loss": 0.0001,
      "step": 98690
    },
    {
      "epoch": 45.67329939842666,
      "grad_norm": 0.007018950767815113,
      "learning_rate": 0.008653401203146692,
      "loss": 0.0,
      "step": 98700
    },
    {
      "epoch": 45.67792688570106,
      "grad_norm": 0.0013334976974874735,
      "learning_rate": 0.008644146228597872,
      "loss": 0.0001,
      "step": 98710
    },
    {
      "epoch": 45.682554372975474,
      "grad_norm": 0.2915973365306854,
      "learning_rate": 0.008634891254049051,
      "loss": 0.0002,
      "step": 98720
    },
    {
      "epoch": 45.687181860249886,
      "grad_norm": 0.0022348682396113873,
      "learning_rate": 0.008625636279500233,
      "loss": 0.0001,
      "step": 98730
    },
    {
      "epoch": 45.69180934752429,
      "grad_norm": 0.004703347571194172,
      "learning_rate": 0.00861638130495141,
      "loss": 0.0001,
      "step": 98740
    },
    {
      "epoch": 45.6964368347987,
      "grad_norm": 0.016040511429309845,
      "learning_rate": 0.008607126330402592,
      "loss": 0.0,
      "step": 98750
    },
    {
      "epoch": 45.701064322073115,
      "grad_norm": 0.012035351246595383,
      "learning_rate": 0.008597871355853772,
      "loss": 0.0001,
      "step": 98760
    },
    {
      "epoch": 45.70569180934753,
      "grad_norm": 0.3271386921405792,
      "learning_rate": 0.008588616381304951,
      "loss": 0.0002,
      "step": 98770
    },
    {
      "epoch": 45.71031929662193,
      "grad_norm": 0.027127917855978012,
      "learning_rate": 0.008579361406756131,
      "loss": 0.0001,
      "step": 98780
    },
    {
      "epoch": 45.714946783896345,
      "grad_norm": 0.0004365074564702809,
      "learning_rate": 0.00857010643220731,
      "loss": 0.0,
      "step": 98790
    },
    {
      "epoch": 45.71957427117076,
      "grad_norm": 0.0020742660854011774,
      "learning_rate": 0.008560851457658492,
      "loss": 0.0,
      "step": 98800
    },
    {
      "epoch": 45.72420175844516,
      "grad_norm": 0.0015617064200341702,
      "learning_rate": 0.008551596483109672,
      "loss": 0.0001,
      "step": 98810
    },
    {
      "epoch": 45.728829245719574,
      "grad_norm": 0.009162881411612034,
      "learning_rate": 0.008542341508560852,
      "loss": 0.0002,
      "step": 98820
    },
    {
      "epoch": 45.733456732993986,
      "grad_norm": 0.006089409813284874,
      "learning_rate": 0.008533086534012031,
      "loss": 0.0003,
      "step": 98830
    },
    {
      "epoch": 45.73808422026839,
      "grad_norm": 0.04424227774143219,
      "learning_rate": 0.008523831559463213,
      "loss": 0.0001,
      "step": 98840
    },
    {
      "epoch": 45.7427117075428,
      "grad_norm": 0.014842008240520954,
      "learning_rate": 0.008514576584914392,
      "loss": 0.0001,
      "step": 98850
    },
    {
      "epoch": 45.747339194817215,
      "grad_norm": 0.0024928459897637367,
      "learning_rate": 0.008505321610365572,
      "loss": 0.0001,
      "step": 98860
    },
    {
      "epoch": 45.75196668209163,
      "grad_norm": 0.002968997461721301,
      "learning_rate": 0.008496066635816752,
      "loss": 0.0001,
      "step": 98870
    },
    {
      "epoch": 45.75659416936603,
      "grad_norm": 0.0023914030753076077,
      "learning_rate": 0.008486811661267931,
      "loss": 0.0001,
      "step": 98880
    },
    {
      "epoch": 45.761221656640444,
      "grad_norm": 0.017946898937225342,
      "learning_rate": 0.008477556686719113,
      "loss": 0.0002,
      "step": 98890
    },
    {
      "epoch": 45.765849143914856,
      "grad_norm": 0.11629011482000351,
      "learning_rate": 0.00846830171217029,
      "loss": 0.0001,
      "step": 98900
    },
    {
      "epoch": 45.77047663118926,
      "grad_norm": 0.00889801699668169,
      "learning_rate": 0.008459046737621472,
      "loss": 0.0002,
      "step": 98910
    },
    {
      "epoch": 45.77510411846367,
      "grad_norm": 0.016245564445853233,
      "learning_rate": 0.008449791763072652,
      "loss": 0.0001,
      "step": 98920
    },
    {
      "epoch": 45.779731605738085,
      "grad_norm": 0.0019110925495624542,
      "learning_rate": 0.008440536788523833,
      "loss": 0.0001,
      "step": 98930
    },
    {
      "epoch": 45.7843590930125,
      "grad_norm": 0.007058309391140938,
      "learning_rate": 0.008431281813975011,
      "loss": 0.0,
      "step": 98940
    },
    {
      "epoch": 45.7889865802869,
      "grad_norm": 0.0033482415601611137,
      "learning_rate": 0.008422026839426193,
      "loss": 0.0003,
      "step": 98950
    },
    {
      "epoch": 45.793614067561315,
      "grad_norm": 0.008488651365041733,
      "learning_rate": 0.008412771864877372,
      "loss": 0.0001,
      "step": 98960
    },
    {
      "epoch": 45.79824155483573,
      "grad_norm": 0.0207830797880888,
      "learning_rate": 0.008403516890328552,
      "loss": 0.0002,
      "step": 98970
    },
    {
      "epoch": 45.80286904211013,
      "grad_norm": 0.027075637131929398,
      "learning_rate": 0.008394261915779732,
      "loss": 0.0003,
      "step": 98980
    },
    {
      "epoch": 45.807496529384544,
      "grad_norm": 0.018428977578878403,
      "learning_rate": 0.008385006941230911,
      "loss": 0.0,
      "step": 98990
    },
    {
      "epoch": 45.812124016658956,
      "grad_norm": 0.004853139165788889,
      "learning_rate": 0.008375751966682093,
      "loss": 0.0,
      "step": 99000
    },
    {
      "epoch": 45.81675150393336,
      "grad_norm": 0.03259607031941414,
      "learning_rate": 0.00836649699213327,
      "loss": 0.0001,
      "step": 99010
    },
    {
      "epoch": 45.82137899120777,
      "grad_norm": 0.0013060945784673095,
      "learning_rate": 0.008357242017584452,
      "loss": 0.0001,
      "step": 99020
    },
    {
      "epoch": 45.826006478482185,
      "grad_norm": 0.0010305261239409447,
      "learning_rate": 0.008347987043035632,
      "loss": 0.0001,
      "step": 99030
    },
    {
      "epoch": 45.8306339657566,
      "grad_norm": 0.005642800126224756,
      "learning_rate": 0.008338732068486813,
      "loss": 0.0002,
      "step": 99040
    },
    {
      "epoch": 45.835261453031,
      "grad_norm": 0.008935741148889065,
      "learning_rate": 0.008329477093937991,
      "loss": 0.0,
      "step": 99050
    },
    {
      "epoch": 45.839888940305414,
      "grad_norm": 0.0010481602512300014,
      "learning_rate": 0.008320222119389173,
      "loss": 0.0,
      "step": 99060
    },
    {
      "epoch": 45.844516427579826,
      "grad_norm": 0.45310723781585693,
      "learning_rate": 0.008310967144840352,
      "loss": 0.0002,
      "step": 99070
    },
    {
      "epoch": 45.84914391485423,
      "grad_norm": 0.001425889553502202,
      "learning_rate": 0.008301712170291532,
      "loss": 0.0001,
      "step": 99080
    },
    {
      "epoch": 45.85377140212864,
      "grad_norm": 0.00370751041918993,
      "learning_rate": 0.008292457195742712,
      "loss": 0.0001,
      "step": 99090
    },
    {
      "epoch": 45.858398889403055,
      "grad_norm": 0.004525667987763882,
      "learning_rate": 0.008283202221193891,
      "loss": 0.0002,
      "step": 99100
    },
    {
      "epoch": 45.86302637667747,
      "grad_norm": 0.0009855276439338923,
      "learning_rate": 0.008273947246645073,
      "loss": 0.0001,
      "step": 99110
    },
    {
      "epoch": 45.86765386395187,
      "grad_norm": 0.006133648566901684,
      "learning_rate": 0.008264692272096252,
      "loss": 0.0001,
      "step": 99120
    },
    {
      "epoch": 45.872281351226285,
      "grad_norm": 0.0016407509101554751,
      "learning_rate": 0.008255437297547432,
      "loss": 0.0001,
      "step": 99130
    },
    {
      "epoch": 45.8769088385007,
      "grad_norm": 0.006341461092233658,
      "learning_rate": 0.008246182322998612,
      "loss": 0.0012,
      "step": 99140
    },
    {
      "epoch": 45.8815363257751,
      "grad_norm": 0.017536213621497154,
      "learning_rate": 0.008236927348449793,
      "loss": 0.0001,
      "step": 99150
    },
    {
      "epoch": 45.886163813049514,
      "grad_norm": 0.014396337792277336,
      "learning_rate": 0.008227672373900973,
      "loss": 0.0001,
      "step": 99160
    },
    {
      "epoch": 45.890791300323926,
      "grad_norm": 0.0045765978284180164,
      "learning_rate": 0.008218417399352152,
      "loss": 0.0002,
      "step": 99170
    },
    {
      "epoch": 45.89541878759833,
      "grad_norm": 0.017918497323989868,
      "learning_rate": 0.008209162424803332,
      "loss": 0.0001,
      "step": 99180
    },
    {
      "epoch": 45.90004627487274,
      "grad_norm": 0.02611573599278927,
      "learning_rate": 0.008199907450254512,
      "loss": 0.0002,
      "step": 99190
    },
    {
      "epoch": 45.904673762147155,
      "grad_norm": 0.005082324147224426,
      "learning_rate": 0.008190652475705692,
      "loss": 0.0001,
      "step": 99200
    },
    {
      "epoch": 45.90930124942157,
      "grad_norm": 0.007691964972764254,
      "learning_rate": 0.008181397501156871,
      "loss": 0.0,
      "step": 99210
    },
    {
      "epoch": 45.91392873669597,
      "grad_norm": 0.03694135695695877,
      "learning_rate": 0.008172142526608053,
      "loss": 0.0001,
      "step": 99220
    },
    {
      "epoch": 45.918556223970384,
      "grad_norm": 0.22982855141162872,
      "learning_rate": 0.008162887552059232,
      "loss": 0.0001,
      "step": 99230
    },
    {
      "epoch": 45.923183711244796,
      "grad_norm": 0.0019358155550435185,
      "learning_rate": 0.008153632577510412,
      "loss": 0.0001,
      "step": 99240
    },
    {
      "epoch": 45.9278111985192,
      "grad_norm": 0.0012057399144396186,
      "learning_rate": 0.008144377602961592,
      "loss": 0.0001,
      "step": 99250
    },
    {
      "epoch": 45.93243868579361,
      "grad_norm": 0.004822202492505312,
      "learning_rate": 0.008135122628412773,
      "loss": 0.0001,
      "step": 99260
    },
    {
      "epoch": 45.937066173068025,
      "grad_norm": 0.06312514841556549,
      "learning_rate": 0.008125867653863953,
      "loss": 0.0001,
      "step": 99270
    },
    {
      "epoch": 45.94169366034244,
      "grad_norm": 0.005881452467292547,
      "learning_rate": 0.008116612679315132,
      "loss": 0.0001,
      "step": 99280
    },
    {
      "epoch": 45.94632114761684,
      "grad_norm": 0.06198694929480553,
      "learning_rate": 0.008107357704766312,
      "loss": 0.0002,
      "step": 99290
    },
    {
      "epoch": 45.950948634891255,
      "grad_norm": 0.001021868665702641,
      "learning_rate": 0.008098102730217492,
      "loss": 0.0001,
      "step": 99300
    },
    {
      "epoch": 45.95557612216567,
      "grad_norm": 0.003547740401700139,
      "learning_rate": 0.008088847755668673,
      "loss": 0.0001,
      "step": 99310
    },
    {
      "epoch": 45.96020360944007,
      "grad_norm": 0.004994145128875971,
      "learning_rate": 0.008079592781119851,
      "loss": 0.0009,
      "step": 99320
    },
    {
      "epoch": 45.964831096714484,
      "grad_norm": 0.004482933320105076,
      "learning_rate": 0.008070337806571033,
      "loss": 0.0001,
      "step": 99330
    },
    {
      "epoch": 45.969458583988896,
      "grad_norm": 0.016693828627467155,
      "learning_rate": 0.008061082832022212,
      "loss": 0.0001,
      "step": 99340
    },
    {
      "epoch": 45.9740860712633,
      "grad_norm": 0.0006772879860363901,
      "learning_rate": 0.008051827857473394,
      "loss": 0.0,
      "step": 99350
    },
    {
      "epoch": 45.97871355853771,
      "grad_norm": 0.007818920537829399,
      "learning_rate": 0.008042572882924572,
      "loss": 0.0001,
      "step": 99360
    },
    {
      "epoch": 45.983341045812125,
      "grad_norm": 0.0018774376949295402,
      "learning_rate": 0.008033317908375753,
      "loss": 0.0001,
      "step": 99370
    },
    {
      "epoch": 45.98796853308654,
      "grad_norm": 0.04402642697095871,
      "learning_rate": 0.008024062933826933,
      "loss": 0.0001,
      "step": 99380
    },
    {
      "epoch": 45.99259602036094,
      "grad_norm": 0.009003954008221626,
      "learning_rate": 0.008014807959278112,
      "loss": 0.0006,
      "step": 99390
    },
    {
      "epoch": 45.997223507635354,
      "grad_norm": 0.0045406874269247055,
      "learning_rate": 0.008005552984729292,
      "loss": 0.0,
      "step": 99400
    },
    {
      "epoch": 46.0,
      "eval_accuracy_branch1": 0.9898320751810199,
      "eval_accuracy_branch2": 0.4996148513326144,
      "eval_f1_branch1": 0.9907745131015986,
      "eval_f1_branch2": 0.49919920399280615,
      "eval_loss": 0.019204430282115936,
      "eval_precision_branch1": 0.9909000385891141,
      "eval_precision_branch2": 0.4996135684332739,
      "eval_recall_branch1": 0.9907600752548763,
      "eval_recall_branch2": 0.4996148513326144,
      "eval_runtime": 28.8688,
      "eval_samples_per_second": 449.689,
      "eval_steps_per_second": 56.22,
      "step": 99406
    },
    {
      "epoch": 46.001850994909766,
      "grad_norm": 0.020563604310154915,
      "learning_rate": 0.007996298010180472,
      "loss": 0.1201,
      "step": 99410
    },
    {
      "epoch": 46.00647848218417,
      "grad_norm": 0.007793799974024296,
      "learning_rate": 0.007987043035631653,
      "loss": 0.0001,
      "step": 99420
    },
    {
      "epoch": 46.01110596945858,
      "grad_norm": 0.005765990354120731,
      "learning_rate": 0.007977788061082831,
      "loss": 0.0001,
      "step": 99430
    },
    {
      "epoch": 46.015733456732995,
      "grad_norm": 0.017330309376120567,
      "learning_rate": 0.007968533086534012,
      "loss": 0.0004,
      "step": 99440
    },
    {
      "epoch": 46.02036094400741,
      "grad_norm": 0.0010197849478572607,
      "learning_rate": 0.007959278111985192,
      "loss": 0.0001,
      "step": 99450
    },
    {
      "epoch": 46.02498843128181,
      "grad_norm": 0.0024208207614719868,
      "learning_rate": 0.007950023137436374,
      "loss": 0.0001,
      "step": 99460
    },
    {
      "epoch": 46.029615918556225,
      "grad_norm": 0.003993329592049122,
      "learning_rate": 0.007940768162887551,
      "loss": 0.0003,
      "step": 99470
    },
    {
      "epoch": 46.03424340583064,
      "grad_norm": 0.0031382793094962835,
      "learning_rate": 0.007931513188338733,
      "loss": 0.0001,
      "step": 99480
    },
    {
      "epoch": 46.03887089310504,
      "grad_norm": 0.009455880150198936,
      "learning_rate": 0.007922258213789913,
      "loss": 0.0001,
      "step": 99490
    },
    {
      "epoch": 46.043498380379454,
      "grad_norm": 0.0728967934846878,
      "learning_rate": 0.007913003239241092,
      "loss": 0.0001,
      "step": 99500
    },
    {
      "epoch": 46.048125867653866,
      "grad_norm": 0.005458979867398739,
      "learning_rate": 0.007903748264692272,
      "loss": 0.0002,
      "step": 99510
    },
    {
      "epoch": 46.05275335492827,
      "grad_norm": 0.028270632028579712,
      "learning_rate": 0.007894493290143452,
      "loss": 0.0001,
      "step": 99520
    },
    {
      "epoch": 46.05738084220268,
      "grad_norm": 0.0063656107522547245,
      "learning_rate": 0.007885238315594633,
      "loss": 0.0001,
      "step": 99530
    },
    {
      "epoch": 46.062008329477095,
      "grad_norm": 0.004195301793515682,
      "learning_rate": 0.007875983341045813,
      "loss": 0.0034,
      "step": 99540
    },
    {
      "epoch": 46.06663581675151,
      "grad_norm": 0.006355654448270798,
      "learning_rate": 0.007866728366496992,
      "loss": 0.0001,
      "step": 99550
    },
    {
      "epoch": 46.07126330402591,
      "grad_norm": 0.0007213323260657489,
      "learning_rate": 0.007857473391948172,
      "loss": 0.0001,
      "step": 99560
    },
    {
      "epoch": 46.075890791300324,
      "grad_norm": 0.05913412198424339,
      "learning_rate": 0.007848218417399353,
      "loss": 0.0001,
      "step": 99570
    },
    {
      "epoch": 46.080518278574736,
      "grad_norm": 0.0655829980969429,
      "learning_rate": 0.007838963442850533,
      "loss": 0.0003,
      "step": 99580
    },
    {
      "epoch": 46.08514576584914,
      "grad_norm": 0.005056559573858976,
      "learning_rate": 0.007829708468301713,
      "loss": 0.0,
      "step": 99590
    },
    {
      "epoch": 46.08977325312355,
      "grad_norm": 0.005952151026576757,
      "learning_rate": 0.007820453493752893,
      "loss": 0.0,
      "step": 99600
    },
    {
      "epoch": 46.094400740397965,
      "grad_norm": 0.0033961141016334295,
      "learning_rate": 0.007811198519204073,
      "loss": 0.0,
      "step": 99610
    },
    {
      "epoch": 46.09902822767237,
      "grad_norm": 0.006739172153174877,
      "learning_rate": 0.007801943544655252,
      "loss": 0.0001,
      "step": 99620
    },
    {
      "epoch": 46.10365571494678,
      "grad_norm": 0.0007541990489698946,
      "learning_rate": 0.007792688570106432,
      "loss": 0.0001,
      "step": 99630
    },
    {
      "epoch": 46.108283202221195,
      "grad_norm": 0.0048210411332547665,
      "learning_rate": 0.007783433595557613,
      "loss": 0.0003,
      "step": 99640
    },
    {
      "epoch": 46.11291068949561,
      "grad_norm": 0.005459002684801817,
      "learning_rate": 0.007774178621008793,
      "loss": 0.0001,
      "step": 99650
    },
    {
      "epoch": 46.11753817677001,
      "grad_norm": 0.02697330340743065,
      "learning_rate": 0.007764923646459972,
      "loss": 0.0002,
      "step": 99660
    },
    {
      "epoch": 46.122165664044424,
      "grad_norm": 0.0012729666195809841,
      "learning_rate": 0.007755668671911152,
      "loss": 0.0001,
      "step": 99670
    },
    {
      "epoch": 46.126793151318836,
      "grad_norm": 0.038704149425029755,
      "learning_rate": 0.0077464136973623325,
      "loss": 0.0002,
      "step": 99680
    },
    {
      "epoch": 46.13142063859324,
      "grad_norm": 0.007038897834718227,
      "learning_rate": 0.007737158722813513,
      "loss": 0.0,
      "step": 99690
    },
    {
      "epoch": 46.13604812586765,
      "grad_norm": 0.04130545258522034,
      "learning_rate": 0.007727903748264692,
      "loss": 0.0001,
      "step": 99700
    },
    {
      "epoch": 46.140675613142065,
      "grad_norm": 0.003537421580404043,
      "learning_rate": 0.0077186487737158724,
      "loss": 0.0001,
      "step": 99710
    },
    {
      "epoch": 46.14530310041648,
      "grad_norm": 0.0037070177495479584,
      "learning_rate": 0.007709393799167053,
      "loss": 0.0005,
      "step": 99720
    },
    {
      "epoch": 46.14993058769088,
      "grad_norm": 0.0038384736981242895,
      "learning_rate": 0.0077001388246182335,
      "loss": 0.0,
      "step": 99730
    },
    {
      "epoch": 46.154558074965294,
      "grad_norm": 0.005502897314727306,
      "learning_rate": 0.007690883850069412,
      "loss": 0.0001,
      "step": 99740
    },
    {
      "epoch": 46.159185562239706,
      "grad_norm": 0.0011474814964458346,
      "learning_rate": 0.007681628875520593,
      "loss": 0.0002,
      "step": 99750
    },
    {
      "epoch": 46.16381304951411,
      "grad_norm": 0.005230163224041462,
      "learning_rate": 0.007672373900971773,
      "loss": 0.0001,
      "step": 99760
    },
    {
      "epoch": 46.16844053678852,
      "grad_norm": 0.0014453051844611764,
      "learning_rate": 0.007663118926422953,
      "loss": 0.0,
      "step": 99770
    },
    {
      "epoch": 46.173068024062935,
      "grad_norm": 0.0031049130484461784,
      "learning_rate": 0.007653863951874132,
      "loss": 0.0001,
      "step": 99780
    },
    {
      "epoch": 46.17769551133734,
      "grad_norm": 0.0030203089118003845,
      "learning_rate": 0.0076446089773253125,
      "loss": 0.0001,
      "step": 99790
    },
    {
      "epoch": 46.18232299861175,
      "grad_norm": 0.0028425492346286774,
      "learning_rate": 0.007635354002776493,
      "loss": 0.0001,
      "step": 99800
    },
    {
      "epoch": 46.186950485886165,
      "grad_norm": 0.00316817220300436,
      "learning_rate": 0.007626099028227674,
      "loss": 0.0001,
      "step": 99810
    },
    {
      "epoch": 46.19157797316058,
      "grad_norm": 0.027823392301797867,
      "learning_rate": 0.007616844053678852,
      "loss": 0.0001,
      "step": 99820
    },
    {
      "epoch": 46.19620546043498,
      "grad_norm": 0.0031979032792150974,
      "learning_rate": 0.007607589079130033,
      "loss": 0.0001,
      "step": 99830
    },
    {
      "epoch": 46.200832947709394,
      "grad_norm": 0.003451198572292924,
      "learning_rate": 0.0075983341045812135,
      "loss": 0.0001,
      "step": 99840
    },
    {
      "epoch": 46.205460434983806,
      "grad_norm": 0.048423267900943756,
      "learning_rate": 0.007589079130032392,
      "loss": 0.0001,
      "step": 99850
    },
    {
      "epoch": 46.21008792225821,
      "grad_norm": 0.004576674662530422,
      "learning_rate": 0.007579824155483573,
      "loss": 0.0001,
      "step": 99860
    },
    {
      "epoch": 46.21471540953262,
      "grad_norm": 0.0008833447936922312,
      "learning_rate": 0.0075705691809347525,
      "loss": 0.0,
      "step": 99870
    },
    {
      "epoch": 46.219342896807035,
      "grad_norm": 0.7165498733520508,
      "learning_rate": 0.007561314206385933,
      "loss": 0.0003,
      "step": 99880
    },
    {
      "epoch": 46.22397038408145,
      "grad_norm": 0.001116554718464613,
      "learning_rate": 0.007552059231837112,
      "loss": 0.0001,
      "step": 99890
    },
    {
      "epoch": 46.22859787135585,
      "grad_norm": 0.006825269665569067,
      "learning_rate": 0.007542804257288292,
      "loss": 0.0001,
      "step": 99900
    },
    {
      "epoch": 46.233225358630264,
      "grad_norm": 0.007835562340915203,
      "learning_rate": 0.007533549282739473,
      "loss": 0.0001,
      "step": 99910
    },
    {
      "epoch": 46.237852845904676,
      "grad_norm": 0.0037637583445757627,
      "learning_rate": 0.0075242943081906535,
      "loss": 0.0001,
      "step": 99920
    },
    {
      "epoch": 46.24248033317908,
      "grad_norm": 0.004733209498226643,
      "learning_rate": 0.007515039333641832,
      "loss": 0.0003,
      "step": 99930
    },
    {
      "epoch": 46.24710782045349,
      "grad_norm": 0.0017962070414796472,
      "learning_rate": 0.007505784359093013,
      "loss": 0.0,
      "step": 99940
    },
    {
      "epoch": 46.251735307727905,
      "grad_norm": 0.002455934416502714,
      "learning_rate": 0.007496529384544193,
      "loss": 0.0001,
      "step": 99950
    },
    {
      "epoch": 46.25636279500231,
      "grad_norm": 0.021396098658442497,
      "learning_rate": 0.007487274409995373,
      "loss": 0.0001,
      "step": 99960
    },
    {
      "epoch": 46.26099028227672,
      "grad_norm": 0.004262621980160475,
      "learning_rate": 0.007478019435446553,
      "loss": 0.0001,
      "step": 99970
    },
    {
      "epoch": 46.265617769551135,
      "grad_norm": 0.038461510092020035,
      "learning_rate": 0.0074687644608977324,
      "loss": 0.0001,
      "step": 99980
    },
    {
      "epoch": 46.27024525682555,
      "grad_norm": 0.003169916570186615,
      "learning_rate": 0.007459509486348913,
      "loss": 0.0001,
      "step": 99990
    },
    {
      "epoch": 46.27487274409995,
      "grad_norm": 0.0036854390054941177,
      "learning_rate": 0.0074502545118000935,
      "loss": 0.0,
      "step": 100000
    },
    {
      "epoch": 46.279500231374364,
      "grad_norm": 0.012654262594878674,
      "learning_rate": 0.007440999537251272,
      "loss": 0.0001,
      "step": 100010
    },
    {
      "epoch": 46.284127718648776,
      "grad_norm": 0.002444873098284006,
      "learning_rate": 0.007431744562702453,
      "loss": 0.0001,
      "step": 100020
    },
    {
      "epoch": 46.28875520592318,
      "grad_norm": 0.004701604135334492,
      "learning_rate": 0.007422489588153633,
      "loss": 0.0002,
      "step": 100030
    },
    {
      "epoch": 46.29338269319759,
      "grad_norm": 0.0033265370875597,
      "learning_rate": 0.007413234613604812,
      "loss": 0.0,
      "step": 100040
    },
    {
      "epoch": 46.298010180472005,
      "grad_norm": 0.007287731394171715,
      "learning_rate": 0.007403979639055993,
      "loss": 0.0,
      "step": 100050
    },
    {
      "epoch": 46.30263766774642,
      "grad_norm": 0.012385704554617405,
      "learning_rate": 0.007394724664507173,
      "loss": 0.0002,
      "step": 100060
    },
    {
      "epoch": 46.30726515502082,
      "grad_norm": 0.27319207787513733,
      "learning_rate": 0.007385469689958353,
      "loss": 0.0001,
      "step": 100070
    },
    {
      "epoch": 46.311892642295234,
      "grad_norm": 0.013363485224545002,
      "learning_rate": 0.007376214715409533,
      "loss": 0.0001,
      "step": 100080
    },
    {
      "epoch": 46.316520129569646,
      "grad_norm": 0.000743329874239862,
      "learning_rate": 0.007366959740860712,
      "loss": 0.0001,
      "step": 100090
    },
    {
      "epoch": 46.32114761684405,
      "grad_norm": 0.009542274288833141,
      "learning_rate": 0.007357704766311893,
      "loss": 0.0001,
      "step": 100100
    },
    {
      "epoch": 46.32577510411846,
      "grad_norm": 0.012402969412505627,
      "learning_rate": 0.0073484497917630735,
      "loss": 0.0,
      "step": 100110
    },
    {
      "epoch": 46.330402591392875,
      "grad_norm": 0.0019874509889632463,
      "learning_rate": 0.007339194817214252,
      "loss": 0.0001,
      "step": 100120
    },
    {
      "epoch": 46.33503007866728,
      "grad_norm": 0.006462727207690477,
      "learning_rate": 0.007329939842665433,
      "loss": 0.0001,
      "step": 100130
    },
    {
      "epoch": 46.33965756594169,
      "grad_norm": 0.019155504181981087,
      "learning_rate": 0.007320684868116613,
      "loss": 0.0002,
      "step": 100140
    },
    {
      "epoch": 46.344285053216105,
      "grad_norm": 0.004183179698884487,
      "learning_rate": 0.007311429893567794,
      "loss": 0.0001,
      "step": 100150
    },
    {
      "epoch": 46.34891254049052,
      "grad_norm": 0.024792572483420372,
      "learning_rate": 0.007302174919018973,
      "loss": 0.0001,
      "step": 100160
    },
    {
      "epoch": 46.35354002776492,
      "grad_norm": 0.004895718302577734,
      "learning_rate": 0.007292919944470153,
      "loss": 0.0001,
      "step": 100170
    },
    {
      "epoch": 46.358167515039334,
      "grad_norm": 0.014895847998559475,
      "learning_rate": 0.007283664969921333,
      "loss": 0.0001,
      "step": 100180
    },
    {
      "epoch": 46.362795002313746,
      "grad_norm": 0.007828067056834698,
      "learning_rate": 0.0072744099953725135,
      "loss": 0.0001,
      "step": 100190
    },
    {
      "epoch": 46.36742248958815,
      "grad_norm": 0.0017215178813785315,
      "learning_rate": 0.007265155020823692,
      "loss": 0.0,
      "step": 100200
    },
    {
      "epoch": 46.37204997686256,
      "grad_norm": 0.1641969531774521,
      "learning_rate": 0.007255900046274873,
      "loss": 0.0001,
      "step": 100210
    },
    {
      "epoch": 46.376677464136975,
      "grad_norm": 0.0010927841067314148,
      "learning_rate": 0.007246645071726053,
      "loss": 0.0001,
      "step": 100220
    },
    {
      "epoch": 46.38130495141139,
      "grad_norm": 0.006188023369759321,
      "learning_rate": 0.007237390097177234,
      "loss": 0.0001,
      "step": 100230
    },
    {
      "epoch": 46.38593243868579,
      "grad_norm": 0.01786300167441368,
      "learning_rate": 0.007228135122628413,
      "loss": 0.0001,
      "step": 100240
    },
    {
      "epoch": 46.390559925960204,
      "grad_norm": 0.011104070581495762,
      "learning_rate": 0.007218880148079593,
      "loss": 0.0003,
      "step": 100250
    },
    {
      "epoch": 46.395187413234616,
      "grad_norm": 0.010791325010359287,
      "learning_rate": 0.007209625173530774,
      "loss": 0.0001,
      "step": 100260
    },
    {
      "epoch": 46.39981490050902,
      "grad_norm": 0.001987891271710396,
      "learning_rate": 0.007200370198981953,
      "loss": 0.0001,
      "step": 100270
    },
    {
      "epoch": 46.40444238778343,
      "grad_norm": 1.767116904258728,
      "learning_rate": 0.007191115224433133,
      "loss": 0.0009,
      "step": 100280
    },
    {
      "epoch": 46.409069875057845,
      "grad_norm": 0.0019439192255958915,
      "learning_rate": 0.007181860249884313,
      "loss": 0.0001,
      "step": 100290
    },
    {
      "epoch": 46.41369736233225,
      "grad_norm": 0.007220303174108267,
      "learning_rate": 0.007172605275335493,
      "loss": 0.0001,
      "step": 100300
    },
    {
      "epoch": 46.41832484960666,
      "grad_norm": 0.01841704361140728,
      "learning_rate": 0.007163350300786672,
      "loss": 0.0001,
      "step": 100310
    },
    {
      "epoch": 46.422952336881075,
      "grad_norm": 0.007659932132810354,
      "learning_rate": 0.007154095326237853,
      "loss": 0.0001,
      "step": 100320
    },
    {
      "epoch": 46.42757982415549,
      "grad_norm": 0.0012852250365540385,
      "learning_rate": 0.007144840351689033,
      "loss": 0.0001,
      "step": 100330
    },
    {
      "epoch": 46.43220731142989,
      "grad_norm": 0.0216422900557518,
      "learning_rate": 0.007135585377140214,
      "loss": 0.0001,
      "step": 100340
    },
    {
      "epoch": 46.436834798704304,
      "grad_norm": 0.001366815879009664,
      "learning_rate": 0.007126330402591393,
      "loss": 0.0001,
      "step": 100350
    },
    {
      "epoch": 46.441462285978716,
      "grad_norm": 0.052310992032289505,
      "learning_rate": 0.007117075428042573,
      "loss": 0.0001,
      "step": 100360
    },
    {
      "epoch": 46.44608977325312,
      "grad_norm": 0.007709737401455641,
      "learning_rate": 0.007107820453493754,
      "loss": 0.0,
      "step": 100370
    },
    {
      "epoch": 46.45071726052753,
      "grad_norm": 0.003900401294231415,
      "learning_rate": 0.0070985654789449334,
      "loss": 0.0012,
      "step": 100380
    },
    {
      "epoch": 46.455344747801945,
      "grad_norm": 0.002972520189359784,
      "learning_rate": 0.007089310504396113,
      "loss": 0.0001,
      "step": 100390
    },
    {
      "epoch": 46.45997223507635,
      "grad_norm": 0.31707197427749634,
      "learning_rate": 0.007080055529847293,
      "loss": 0.0001,
      "step": 100400
    },
    {
      "epoch": 46.46459972235076,
      "grad_norm": 0.0017172173829749227,
      "learning_rate": 0.007070800555298473,
      "loss": 0.0001,
      "step": 100410
    },
    {
      "epoch": 46.469227209625174,
      "grad_norm": 0.021273665130138397,
      "learning_rate": 0.007061545580749654,
      "loss": 0.0001,
      "step": 100420
    },
    {
      "epoch": 46.473854696899586,
      "grad_norm": 0.003849188331514597,
      "learning_rate": 0.007052290606200833,
      "loss": 0.0,
      "step": 100430
    },
    {
      "epoch": 46.47848218417399,
      "grad_norm": 0.0913674458861351,
      "learning_rate": 0.007043035631652013,
      "loss": 0.0001,
      "step": 100440
    },
    {
      "epoch": 46.4831096714484,
      "grad_norm": 0.007597423158586025,
      "learning_rate": 0.007033780657103194,
      "loss": 0.0001,
      "step": 100450
    },
    {
      "epoch": 46.487737158722815,
      "grad_norm": 0.001011062879115343,
      "learning_rate": 0.007024525682554374,
      "loss": 0.0002,
      "step": 100460
    },
    {
      "epoch": 46.49236464599722,
      "grad_norm": 0.016478797420859337,
      "learning_rate": 0.007015270708005553,
      "loss": 0.0001,
      "step": 100470
    },
    {
      "epoch": 46.49699213327163,
      "grad_norm": 0.01289643719792366,
      "learning_rate": 0.007006015733456734,
      "loss": 0.0001,
      "step": 100480
    },
    {
      "epoch": 46.501619620546045,
      "grad_norm": 0.00705003784969449,
      "learning_rate": 0.006996760758907913,
      "loss": 0.0001,
      "step": 100490
    },
    {
      "epoch": 46.50624710782046,
      "grad_norm": 0.007711128331720829,
      "learning_rate": 0.006987505784359093,
      "loss": 0.0001,
      "step": 100500
    },
    {
      "epoch": 46.51087459509486,
      "grad_norm": 0.003370009595528245,
      "learning_rate": 0.006978250809810273,
      "loss": 0.0001,
      "step": 100510
    },
    {
      "epoch": 46.515502082369274,
      "grad_norm": 0.003386880736798048,
      "learning_rate": 0.006968995835261453,
      "loss": 0.0001,
      "step": 100520
    },
    {
      "epoch": 46.520129569643686,
      "grad_norm": 0.019932376220822334,
      "learning_rate": 0.006959740860712634,
      "loss": 0.0001,
      "step": 100530
    },
    {
      "epoch": 46.52475705691809,
      "grad_norm": 0.0028842731844633818,
      "learning_rate": 0.006950485886163813,
      "loss": 0.0001,
      "step": 100540
    },
    {
      "epoch": 46.5293845441925,
      "grad_norm": 0.0014858344802632928,
      "learning_rate": 0.006941230911614993,
      "loss": 0.0001,
      "step": 100550
    },
    {
      "epoch": 46.534012031466915,
      "grad_norm": 0.006993414368480444,
      "learning_rate": 0.006931975937066174,
      "loss": 0.0001,
      "step": 100560
    },
    {
      "epoch": 46.53863951874132,
      "grad_norm": 0.0020310680847615004,
      "learning_rate": 0.006922720962517354,
      "loss": 0.0002,
      "step": 100570
    },
    {
      "epoch": 46.54326700601573,
      "grad_norm": 0.00565942982211709,
      "learning_rate": 0.006913465987968533,
      "loss": 0.0,
      "step": 100580
    },
    {
      "epoch": 46.547894493290144,
      "grad_norm": 0.03416164219379425,
      "learning_rate": 0.006904211013419714,
      "loss": 0.0002,
      "step": 100590
    },
    {
      "epoch": 46.552521980564556,
      "grad_norm": 0.026358550414443016,
      "learning_rate": 0.006894956038870893,
      "loss": 0.0001,
      "step": 100600
    },
    {
      "epoch": 46.55714946783896,
      "grad_norm": 0.005180008243769407,
      "learning_rate": 0.006885701064322074,
      "loss": 0.0007,
      "step": 100610
    },
    {
      "epoch": 46.56177695511337,
      "grad_norm": 0.0020998779218643904,
      "learning_rate": 0.006876446089773253,
      "loss": 0.0001,
      "step": 100620
    },
    {
      "epoch": 46.566404442387785,
      "grad_norm": 0.00532309478148818,
      "learning_rate": 0.006867191115224433,
      "loss": 0.0001,
      "step": 100630
    },
    {
      "epoch": 46.57103192966219,
      "grad_norm": 0.008223907090723515,
      "learning_rate": 0.006857936140675614,
      "loss": 0.0005,
      "step": 100640
    },
    {
      "epoch": 46.5756594169366,
      "grad_norm": 0.009132827632129192,
      "learning_rate": 0.006848681166126794,
      "loss": 0.0001,
      "step": 100650
    },
    {
      "epoch": 46.580286904211015,
      "grad_norm": 0.0033870371989905834,
      "learning_rate": 0.006839426191577973,
      "loss": 0.0002,
      "step": 100660
    },
    {
      "epoch": 46.58491439148543,
      "grad_norm": 0.005676811560988426,
      "learning_rate": 0.006830171217029154,
      "loss": 0.0002,
      "step": 100670
    },
    {
      "epoch": 46.58954187875983,
      "grad_norm": 0.004425690043717623,
      "learning_rate": 0.006820916242480334,
      "loss": 0.0001,
      "step": 100680
    },
    {
      "epoch": 46.594169366034244,
      "grad_norm": 0.009654366411268711,
      "learning_rate": 0.006811661267931513,
      "loss": 0.0001,
      "step": 100690
    },
    {
      "epoch": 46.598796853308656,
      "grad_norm": 0.0017370627028867602,
      "learning_rate": 0.0068024062933826936,
      "loss": 0.0002,
      "step": 100700
    },
    {
      "epoch": 46.60342434058306,
      "grad_norm": 0.003417020896449685,
      "learning_rate": 0.006793151318833873,
      "loss": 0.0002,
      "step": 100710
    },
    {
      "epoch": 46.60805182785747,
      "grad_norm": 0.0010762820020318031,
      "learning_rate": 0.006783896344285054,
      "loss": 0.0001,
      "step": 100720
    },
    {
      "epoch": 46.612679315131885,
      "grad_norm": 0.0006860286230221391,
      "learning_rate": 0.006774641369736233,
      "loss": 0.0,
      "step": 100730
    },
    {
      "epoch": 46.61730680240629,
      "grad_norm": 0.025633810088038445,
      "learning_rate": 0.006765386395187413,
      "loss": 0.0001,
      "step": 100740
    },
    {
      "epoch": 46.6219342896807,
      "grad_norm": 0.002014745492488146,
      "learning_rate": 0.006756131420638594,
      "loss": 0.0001,
      "step": 100750
    },
    {
      "epoch": 46.626561776955114,
      "grad_norm": 0.023448463529348373,
      "learning_rate": 0.006746876446089774,
      "loss": 0.0001,
      "step": 100760
    },
    {
      "epoch": 46.631189264229526,
      "grad_norm": 0.007526193279772997,
      "learning_rate": 0.006737621471540953,
      "loss": 0.0001,
      "step": 100770
    },
    {
      "epoch": 46.63581675150393,
      "grad_norm": 0.008132444694638252,
      "learning_rate": 0.006728366496992134,
      "loss": 0.0001,
      "step": 100780
    },
    {
      "epoch": 46.64044423877834,
      "grad_norm": 0.0003334977081976831,
      "learning_rate": 0.006719111522443314,
      "loss": 0.0001,
      "step": 100790
    },
    {
      "epoch": 46.645071726052755,
      "grad_norm": 0.0022099940106272697,
      "learning_rate": 0.006709856547894494,
      "loss": 0.0001,
      "step": 100800
    },
    {
      "epoch": 46.64969921332716,
      "grad_norm": 0.01571051776409149,
      "learning_rate": 0.0067006015733456735,
      "loss": 0.0001,
      "step": 100810
    },
    {
      "epoch": 46.65432670060157,
      "grad_norm": 0.0012205179082229733,
      "learning_rate": 0.006691346598796853,
      "loss": 0.0001,
      "step": 100820
    },
    {
      "epoch": 46.658954187875985,
      "grad_norm": 0.013217458501458168,
      "learning_rate": 0.006682091624248034,
      "loss": 0.0001,
      "step": 100830
    },
    {
      "epoch": 46.6635816751504,
      "grad_norm": 0.010730900801718235,
      "learning_rate": 0.006672836649699214,
      "loss": 0.0001,
      "step": 100840
    },
    {
      "epoch": 46.6682091624248,
      "grad_norm": 0.007923743687570095,
      "learning_rate": 0.006663581675150393,
      "loss": 0.0004,
      "step": 100850
    },
    {
      "epoch": 46.672836649699214,
      "grad_norm": 0.03346153348684311,
      "learning_rate": 0.006654326700601574,
      "loss": 0.0002,
      "step": 100860
    },
    {
      "epoch": 46.677464136973626,
      "grad_norm": 0.4364076554775238,
      "learning_rate": 0.006645071726052754,
      "loss": 0.0002,
      "step": 100870
    },
    {
      "epoch": 46.68209162424803,
      "grad_norm": 0.007164767012000084,
      "learning_rate": 0.006635816751503935,
      "loss": 0.0001,
      "step": 100880
    },
    {
      "epoch": 46.68671911152244,
      "grad_norm": 0.0010899209883064032,
      "learning_rate": 0.0066265617769551135,
      "loss": 0.0001,
      "step": 100890
    },
    {
      "epoch": 46.691346598796855,
      "grad_norm": 0.0033845470752567053,
      "learning_rate": 0.006617306802406294,
      "loss": 0.0001,
      "step": 100900
    },
    {
      "epoch": 46.69597408607126,
      "grad_norm": 0.06694172322750092,
      "learning_rate": 0.006608051827857474,
      "loss": 0.0002,
      "step": 100910
    },
    {
      "epoch": 46.70060157334567,
      "grad_norm": 0.0018720373045653105,
      "learning_rate": 0.006598796853308653,
      "loss": 0.0001,
      "step": 100920
    },
    {
      "epoch": 46.705229060620084,
      "grad_norm": 0.005955257918685675,
      "learning_rate": 0.006589541878759833,
      "loss": 0.0001,
      "step": 100930
    },
    {
      "epoch": 46.709856547894496,
      "grad_norm": 0.08283990621566772,
      "learning_rate": 0.006580286904211014,
      "loss": 0.0001,
      "step": 100940
    },
    {
      "epoch": 46.7144840351689,
      "grad_norm": 0.007522301282733679,
      "learning_rate": 0.006571031929662194,
      "loss": 0.0001,
      "step": 100950
    },
    {
      "epoch": 46.71911152244331,
      "grad_norm": 0.0025765972677618265,
      "learning_rate": 0.006561776955113373,
      "loss": 0.0001,
      "step": 100960
    },
    {
      "epoch": 46.723739009717725,
      "grad_norm": 0.02845725230872631,
      "learning_rate": 0.0065525219805645536,
      "loss": 0.0002,
      "step": 100970
    },
    {
      "epoch": 46.72836649699213,
      "grad_norm": 0.00929815974086523,
      "learning_rate": 0.006543267006015734,
      "loss": 0.0002,
      "step": 100980
    },
    {
      "epoch": 46.73299398426654,
      "grad_norm": 0.00320905982516706,
      "learning_rate": 0.006534012031466915,
      "loss": 0.0,
      "step": 100990
    },
    {
      "epoch": 46.737621471540955,
      "grad_norm": 0.02606245130300522,
      "learning_rate": 0.0065247570569180935,
      "loss": 0.0002,
      "step": 101000
    },
    {
      "epoch": 46.74224895881537,
      "grad_norm": 0.0022905890364199877,
      "learning_rate": 0.006515502082369274,
      "loss": 0.0001,
      "step": 101010
    },
    {
      "epoch": 46.74687644608977,
      "grad_norm": 0.0026840840000659227,
      "learning_rate": 0.006506247107820454,
      "loss": 0.0004,
      "step": 101020
    },
    {
      "epoch": 46.751503933364184,
      "grad_norm": 0.007053942419588566,
      "learning_rate": 0.006496992133271634,
      "loss": 0.0003,
      "step": 101030
    },
    {
      "epoch": 46.756131420638596,
      "grad_norm": 0.005797480698674917,
      "learning_rate": 0.006487737158722813,
      "loss": 0.0001,
      "step": 101040
    },
    {
      "epoch": 46.760758907913,
      "grad_norm": 0.0009863210143521428,
      "learning_rate": 0.006478482184173994,
      "loss": 0.0001,
      "step": 101050
    },
    {
      "epoch": 46.76538639518741,
      "grad_norm": 0.030202407389879227,
      "learning_rate": 0.006469227209625174,
      "loss": 0.0001,
      "step": 101060
    },
    {
      "epoch": 46.770013882461825,
      "grad_norm": 0.0010400223545730114,
      "learning_rate": 0.006459972235076355,
      "loss": 0.0001,
      "step": 101070
    },
    {
      "epoch": 46.77464136973623,
      "grad_norm": 0.006336891558021307,
      "learning_rate": 0.0064507172605275335,
      "loss": 0.0001,
      "step": 101080
    },
    {
      "epoch": 46.77926885701064,
      "grad_norm": 0.03257071226835251,
      "learning_rate": 0.006441462285978714,
      "loss": 0.0001,
      "step": 101090
    },
    {
      "epoch": 46.783896344285054,
      "grad_norm": 0.0013985694386065006,
      "learning_rate": 0.006432207311429895,
      "loss": 0.0001,
      "step": 101100
    },
    {
      "epoch": 46.788523831559466,
      "grad_norm": 0.0009783462155610323,
      "learning_rate": 0.006422952336881074,
      "loss": 0.0,
      "step": 101110
    },
    {
      "epoch": 46.79315131883387,
      "grad_norm": 0.004079793114215136,
      "learning_rate": 0.006413697362332254,
      "loss": 0.0,
      "step": 101120
    },
    {
      "epoch": 46.79777880610828,
      "grad_norm": 0.006833991501480341,
      "learning_rate": 0.006404442387783434,
      "loss": 0.0001,
      "step": 101130
    },
    {
      "epoch": 46.802406293382695,
      "grad_norm": 0.006174762733280659,
      "learning_rate": 0.006395187413234614,
      "loss": 0.0001,
      "step": 101140
    },
    {
      "epoch": 46.8070337806571,
      "grad_norm": 0.002807806944474578,
      "learning_rate": 0.006385932438685793,
      "loss": 0.0001,
      "step": 101150
    },
    {
      "epoch": 46.81166126793151,
      "grad_norm": 0.008403562940657139,
      "learning_rate": 0.0063766774641369735,
      "loss": 0.0,
      "step": 101160
    },
    {
      "epoch": 46.816288755205925,
      "grad_norm": 0.1163003146648407,
      "learning_rate": 0.006367422489588154,
      "loss": 0.0001,
      "step": 101170
    },
    {
      "epoch": 46.82091624248034,
      "grad_norm": 0.0013443174539133906,
      "learning_rate": 0.006358167515039335,
      "loss": 0.0001,
      "step": 101180
    },
    {
      "epoch": 46.82554372975474,
      "grad_norm": 0.0014317885506898165,
      "learning_rate": 0.006348912540490513,
      "loss": 0.0001,
      "step": 101190
    },
    {
      "epoch": 46.830171217029154,
      "grad_norm": 0.005182971712201834,
      "learning_rate": 0.006339657565941694,
      "loss": 0.0,
      "step": 101200
    },
    {
      "epoch": 46.834798704303566,
      "grad_norm": 0.04843677952885628,
      "learning_rate": 0.0063304025913928745,
      "loss": 0.0001,
      "step": 101210
    },
    {
      "epoch": 46.83942619157797,
      "grad_norm": 0.003855262417346239,
      "learning_rate": 0.006321147616844054,
      "loss": 0.0,
      "step": 101220
    },
    {
      "epoch": 46.84405367885238,
      "grad_norm": 0.007957483641803265,
      "learning_rate": 0.006311892642295234,
      "loss": 0.0001,
      "step": 101230
    },
    {
      "epoch": 46.848681166126795,
      "grad_norm": 0.0025122028309851885,
      "learning_rate": 0.0063026376677464135,
      "loss": 0.0001,
      "step": 101240
    },
    {
      "epoch": 46.8533086534012,
      "grad_norm": 0.4130062460899353,
      "learning_rate": 0.006293382693197594,
      "loss": 0.0009,
      "step": 101250
    },
    {
      "epoch": 46.85793614067561,
      "grad_norm": 0.007812043651938438,
      "learning_rate": 0.006284127718648775,
      "loss": 0.0001,
      "step": 101260
    },
    {
      "epoch": 46.862563627950024,
      "grad_norm": 0.005317063070833683,
      "learning_rate": 0.0062748727440999534,
      "loss": 0.0001,
      "step": 101270
    },
    {
      "epoch": 46.867191115224436,
      "grad_norm": 0.0032662709709256887,
      "learning_rate": 0.006265617769551134,
      "loss": 0.0001,
      "step": 101280
    },
    {
      "epoch": 46.87181860249884,
      "grad_norm": 0.07152760028839111,
      "learning_rate": 0.0062563627950023145,
      "loss": 0.0001,
      "step": 101290
    },
    {
      "epoch": 46.87644608977325,
      "grad_norm": 0.005158054176717997,
      "learning_rate": 0.006247107820453494,
      "loss": 0.0001,
      "step": 101300
    },
    {
      "epoch": 46.881073577047665,
      "grad_norm": 0.012904572300612926,
      "learning_rate": 0.006237852845904675,
      "loss": 0.0001,
      "step": 101310
    },
    {
      "epoch": 46.88570106432207,
      "grad_norm": 0.000688315078150481,
      "learning_rate": 0.0062285978713558544,
      "loss": 0.0002,
      "step": 101320
    },
    {
      "epoch": 46.89032855159648,
      "grad_norm": 0.10369274020195007,
      "learning_rate": 0.006219342896807034,
      "loss": 0.0002,
      "step": 101330
    },
    {
      "epoch": 46.894956038870895,
      "grad_norm": 0.008403636515140533,
      "learning_rate": 0.006210087922258214,
      "loss": 0.0001,
      "step": 101340
    },
    {
      "epoch": 46.89958352614531,
      "grad_norm": 0.0025043340865522623,
      "learning_rate": 0.0062008329477093935,
      "loss": 0.0001,
      "step": 101350
    },
    {
      "epoch": 46.90421101341971,
      "grad_norm": 0.0014202139573171735,
      "learning_rate": 0.006191577973160574,
      "loss": 0.0001,
      "step": 101360
    },
    {
      "epoch": 46.908838500694124,
      "grad_norm": 0.003533888841047883,
      "learning_rate": 0.006182322998611754,
      "loss": 0.0001,
      "step": 101370
    },
    {
      "epoch": 46.913465987968536,
      "grad_norm": 0.014477789402008057,
      "learning_rate": 0.006173068024062934,
      "loss": 0.0001,
      "step": 101380
    },
    {
      "epoch": 46.91809347524294,
      "grad_norm": 0.0037848104257136583,
      "learning_rate": 0.006163813049514114,
      "loss": 0.0001,
      "step": 101390
    },
    {
      "epoch": 46.92272096251735,
      "grad_norm": 0.008395051583647728,
      "learning_rate": 0.0061545580749652945,
      "loss": 0.0,
      "step": 101400
    },
    {
      "epoch": 46.927348449791765,
      "grad_norm": 0.001959344604983926,
      "learning_rate": 0.006145303100416474,
      "loss": 0.0003,
      "step": 101410
    },
    {
      "epoch": 46.93197593706617,
      "grad_norm": 0.005449485499411821,
      "learning_rate": 0.006136048125867655,
      "loss": 0.0001,
      "step": 101420
    },
    {
      "epoch": 46.93660342434058,
      "grad_norm": 0.003863362595438957,
      "learning_rate": 0.006126793151318834,
      "loss": 0.0,
      "step": 101430
    },
    {
      "epoch": 46.941230911614994,
      "grad_norm": 0.0018798558739945292,
      "learning_rate": 0.006117538176770014,
      "loss": 0.0,
      "step": 101440
    },
    {
      "epoch": 46.945858398889406,
      "grad_norm": 0.05440451204776764,
      "learning_rate": 0.006108283202221194,
      "loss": 0.0001,
      "step": 101450
    },
    {
      "epoch": 46.95048588616381,
      "grad_norm": 0.0034002247266471386,
      "learning_rate": 0.006099028227672374,
      "loss": 0.0002,
      "step": 101460
    },
    {
      "epoch": 46.95511337343822,
      "grad_norm": 0.005878262687474489,
      "learning_rate": 0.006089773253123554,
      "loss": 0.0,
      "step": 101470
    },
    {
      "epoch": 46.959740860712635,
      "grad_norm": 0.0534951388835907,
      "learning_rate": 0.0060805182785747345,
      "loss": 0.0001,
      "step": 101480
    },
    {
      "epoch": 46.96436834798704,
      "grad_norm": 0.013315118849277496,
      "learning_rate": 0.006071263304025914,
      "loss": 0.0001,
      "step": 101490
    },
    {
      "epoch": 46.96899583526145,
      "grad_norm": 0.03932672739028931,
      "learning_rate": 0.006062008329477095,
      "loss": 0.0,
      "step": 101500
    },
    {
      "epoch": 46.973623322535865,
      "grad_norm": 0.0010351687669754028,
      "learning_rate": 0.006052753354928274,
      "loss": 0.0001,
      "step": 101510
    },
    {
      "epoch": 46.97825080981027,
      "grad_norm": 0.0143967866897583,
      "learning_rate": 0.006043498380379455,
      "loss": 0.0004,
      "step": 101520
    },
    {
      "epoch": 46.98287829708468,
      "grad_norm": 0.15228502452373505,
      "learning_rate": 0.006034243405830635,
      "loss": 0.0001,
      "step": 101530
    },
    {
      "epoch": 46.987505784359094,
      "grad_norm": 0.009415688924491405,
      "learning_rate": 0.006024988431281814,
      "loss": 0.0007,
      "step": 101540
    },
    {
      "epoch": 46.992133271633506,
      "grad_norm": 0.01626298576593399,
      "learning_rate": 0.006015733456732994,
      "loss": 0.0,
      "step": 101550
    },
    {
      "epoch": 46.99676075890791,
      "grad_norm": 0.0034092129208147526,
      "learning_rate": 0.006006478482184174,
      "loss": 0.0002,
      "step": 101560
    },
    {
      "epoch": 47.0,
      "eval_accuracy_branch1": 0.991757818517948,
      "eval_accuracy_branch2": 0.4995378215991373,
      "eval_f1_branch1": 0.9925333142204494,
      "eval_f1_branch2": 0.4991287581211813,
      "eval_loss": 0.013969456776976585,
      "eval_precision_branch1": 0.992720706047853,
      "eval_precision_branch2": 0.4995363067990239,
      "eval_recall_branch1": 0.9924348948398509,
      "eval_recall_branch2": 0.4995378215991373,
      "eval_runtime": 29.0467,
      "eval_samples_per_second": 446.935,
      "eval_steps_per_second": 55.875,
      "step": 101567
    },
    {
      "epoch": 47.00138824618232,
      "grad_norm": 0.00197696965187788,
      "learning_rate": 0.005997223507635354,
      "loss": 0.0002,
      "step": 101570
    },
    {
      "epoch": 47.006015733456735,
      "grad_norm": 0.0030884172301739454,
      "learning_rate": 0.005987968533086534,
      "loss": 0.0003,
      "step": 101580
    },
    {
      "epoch": 47.01064322073114,
      "grad_norm": 0.004076843149960041,
      "learning_rate": 0.005978713558537714,
      "loss": 0.0001,
      "step": 101590
    },
    {
      "epoch": 47.01527070800555,
      "grad_norm": 0.002107280772179365,
      "learning_rate": 0.005969458583988894,
      "loss": 0.0001,
      "step": 101600
    },
    {
      "epoch": 47.019898195279964,
      "grad_norm": 0.012099573388695717,
      "learning_rate": 0.005960203609440075,
      "loss": 0.0001,
      "step": 101610
    },
    {
      "epoch": 47.024525682554376,
      "grad_norm": 0.019092751666903496,
      "learning_rate": 0.005950948634891254,
      "loss": 0.0001,
      "step": 101620
    },
    {
      "epoch": 47.02915316982878,
      "grad_norm": 0.0026073320768773556,
      "learning_rate": 0.005941693660342435,
      "loss": 0.0002,
      "step": 101630
    },
    {
      "epoch": 47.03378065710319,
      "grad_norm": 0.002501261653378606,
      "learning_rate": 0.0059324386857936146,
      "loss": 0.0001,
      "step": 101640
    },
    {
      "epoch": 47.038408144377605,
      "grad_norm": 0.043922264128923416,
      "learning_rate": 0.005923183711244794,
      "loss": 0.0001,
      "step": 101650
    },
    {
      "epoch": 47.04303563165201,
      "grad_norm": 0.010732937604188919,
      "learning_rate": 0.005913928736695974,
      "loss": 0.0,
      "step": 101660
    },
    {
      "epoch": 47.04766311892642,
      "grad_norm": 0.024472836405038834,
      "learning_rate": 0.0059046737621471545,
      "loss": 0.0001,
      "step": 101670
    },
    {
      "epoch": 47.052290606200835,
      "grad_norm": 0.0035735159181058407,
      "learning_rate": 0.005895418787598334,
      "loss": 0.0001,
      "step": 101680
    },
    {
      "epoch": 47.05691809347524,
      "grad_norm": 0.0030017776880413294,
      "learning_rate": 0.005886163813049515,
      "loss": 0.0002,
      "step": 101690
    },
    {
      "epoch": 47.06154558074965,
      "grad_norm": 0.012380238622426987,
      "learning_rate": 0.005876908838500694,
      "loss": 0.0002,
      "step": 101700
    },
    {
      "epoch": 47.066173068024064,
      "grad_norm": 0.0022147379349917173,
      "learning_rate": 0.005867653863951875,
      "loss": 0.0001,
      "step": 101710
    },
    {
      "epoch": 47.070800555298476,
      "grad_norm": 0.31426090002059937,
      "learning_rate": 0.005858398889403055,
      "loss": 0.0002,
      "step": 101720
    },
    {
      "epoch": 47.07542804257288,
      "grad_norm": 0.0011982779251411557,
      "learning_rate": 0.005849143914854235,
      "loss": 0.0001,
      "step": 101730
    },
    {
      "epoch": 47.08005552984729,
      "grad_norm": 0.019576119258999825,
      "learning_rate": 0.005839888940305415,
      "loss": 0.0001,
      "step": 101740
    },
    {
      "epoch": 47.084683017121705,
      "grad_norm": 0.01812826469540596,
      "learning_rate": 0.0058306339657565945,
      "loss": 0.0001,
      "step": 101750
    },
    {
      "epoch": 47.08931050439611,
      "grad_norm": 0.011518428102135658,
      "learning_rate": 0.005821378991207774,
      "loss": 0.0001,
      "step": 101760
    },
    {
      "epoch": 47.09393799167052,
      "grad_norm": 0.021658798679709435,
      "learning_rate": 0.005812124016658955,
      "loss": 0.0001,
      "step": 101770
    },
    {
      "epoch": 47.098565478944934,
      "grad_norm": 0.005552852526307106,
      "learning_rate": 0.005802869042110134,
      "loss": 0.0002,
      "step": 101780
    },
    {
      "epoch": 47.103192966219346,
      "grad_norm": 0.03190359100699425,
      "learning_rate": 0.005793614067561314,
      "loss": 0.0001,
      "step": 101790
    },
    {
      "epoch": 47.10782045349375,
      "grad_norm": 0.005809935741126537,
      "learning_rate": 0.005784359093012495,
      "loss": 0.0001,
      "step": 101800
    },
    {
      "epoch": 47.11244794076816,
      "grad_norm": 0.002394892740994692,
      "learning_rate": 0.005775104118463674,
      "loss": 0.0001,
      "step": 101810
    },
    {
      "epoch": 47.117075428042575,
      "grad_norm": 0.002993085188791156,
      "learning_rate": 0.005765849143914855,
      "loss": 0.0002,
      "step": 101820
    },
    {
      "epoch": 47.12170291531698,
      "grad_norm": 0.19666112959384918,
      "learning_rate": 0.0057565941693660345,
      "loss": 0.0001,
      "step": 101830
    },
    {
      "epoch": 47.12633040259139,
      "grad_norm": 0.0015565943904221058,
      "learning_rate": 0.005747339194817215,
      "loss": 0.0008,
      "step": 101840
    },
    {
      "epoch": 47.130957889865805,
      "grad_norm": 0.008331801742315292,
      "learning_rate": 0.005738084220268395,
      "loss": 0.0001,
      "step": 101850
    },
    {
      "epoch": 47.13558537714021,
      "grad_norm": 0.008925137110054493,
      "learning_rate": 0.005728829245719574,
      "loss": 0.0001,
      "step": 101860
    },
    {
      "epoch": 47.14021286441462,
      "grad_norm": 0.14928291738033295,
      "learning_rate": 0.005719574271170754,
      "loss": 0.0001,
      "step": 101870
    },
    {
      "epoch": 47.144840351689034,
      "grad_norm": 0.0013593726325780153,
      "learning_rate": 0.005710319296621935,
      "loss": 0.0001,
      "step": 101880
    },
    {
      "epoch": 47.149467838963446,
      "grad_norm": 0.0008176783449016511,
      "learning_rate": 0.005701064322073114,
      "loss": 0.0,
      "step": 101890
    },
    {
      "epoch": 47.15409532623785,
      "grad_norm": 0.027884507551789284,
      "learning_rate": 0.005691809347524295,
      "loss": 0.0001,
      "step": 101900
    },
    {
      "epoch": 47.15872281351226,
      "grad_norm": 0.02547571063041687,
      "learning_rate": 0.0056825543729754745,
      "loss": 0.0001,
      "step": 101910
    },
    {
      "epoch": 47.163350300786675,
      "grad_norm": 0.028190169483423233,
      "learning_rate": 0.005673299398426655,
      "loss": 0.0002,
      "step": 101920
    },
    {
      "epoch": 47.16797778806108,
      "grad_norm": 0.01790599338710308,
      "learning_rate": 0.005664044423877835,
      "loss": 0.0001,
      "step": 101930
    },
    {
      "epoch": 47.17260527533549,
      "grad_norm": 0.04543222486972809,
      "learning_rate": 0.005654789449329015,
      "loss": 0.0001,
      "step": 101940
    },
    {
      "epoch": 47.177232762609904,
      "grad_norm": 0.014055334031581879,
      "learning_rate": 0.005645534474780195,
      "loss": 0.0001,
      "step": 101950
    },
    {
      "epoch": 47.181860249884316,
      "grad_norm": 0.26325100660324097,
      "learning_rate": 0.005636279500231375,
      "loss": 0.0002,
      "step": 101960
    },
    {
      "epoch": 47.18648773715872,
      "grad_norm": 0.00845180731266737,
      "learning_rate": 0.005627024525682554,
      "loss": 0.0001,
      "step": 101970
    },
    {
      "epoch": 47.19111522443313,
      "grad_norm": 0.004288848489522934,
      "learning_rate": 0.005617769551133735,
      "loss": 0.0007,
      "step": 101980
    },
    {
      "epoch": 47.195742711707545,
      "grad_norm": 0.0015137831214815378,
      "learning_rate": 0.005608514576584915,
      "loss": 0.0002,
      "step": 101990
    },
    {
      "epoch": 47.20037019898195,
      "grad_norm": 0.19795796275138855,
      "learning_rate": 0.005599259602036094,
      "loss": 0.0001,
      "step": 102000
    },
    {
      "epoch": 47.20499768625636,
      "grad_norm": 0.008111811242997646,
      "learning_rate": 0.005590004627487275,
      "loss": 0.0001,
      "step": 102010
    },
    {
      "epoch": 47.209625173530775,
      "grad_norm": 0.06159200891852379,
      "learning_rate": 0.0055807496529384545,
      "loss": 0.0001,
      "step": 102020
    },
    {
      "epoch": 47.21425266080518,
      "grad_norm": 0.014295586384832859,
      "learning_rate": 0.005571494678389635,
      "loss": 0.0,
      "step": 102030
    },
    {
      "epoch": 47.21888014807959,
      "grad_norm": 0.005455879494547844,
      "learning_rate": 0.005562239703840815,
      "loss": 0.0044,
      "step": 102040
    },
    {
      "epoch": 47.223507635354004,
      "grad_norm": 0.056186314672231674,
      "learning_rate": 0.005552984729291995,
      "loss": 0.0001,
      "step": 102050
    },
    {
      "epoch": 47.228135122628416,
      "grad_norm": 0.0007098749629221857,
      "learning_rate": 0.005543729754743175,
      "loss": 0.0,
      "step": 102060
    },
    {
      "epoch": 47.23276260990282,
      "grad_norm": 0.0025351811200380325,
      "learning_rate": 0.005534474780194355,
      "loss": 0.0003,
      "step": 102070
    },
    {
      "epoch": 47.23739009717723,
      "grad_norm": 0.0077769760973751545,
      "learning_rate": 0.005525219805645534,
      "loss": 0.0002,
      "step": 102080
    },
    {
      "epoch": 47.242017584451645,
      "grad_norm": 0.0016371512319892645,
      "learning_rate": 0.005515964831096715,
      "loss": 0.0001,
      "step": 102090
    },
    {
      "epoch": 47.24664507172605,
      "grad_norm": 0.01059785857796669,
      "learning_rate": 0.0055067098565478945,
      "loss": 0.0001,
      "step": 102100
    },
    {
      "epoch": 47.25127255900046,
      "grad_norm": 0.02887580543756485,
      "learning_rate": 0.005497454881999075,
      "loss": 0.0001,
      "step": 102110
    },
    {
      "epoch": 47.255900046274874,
      "grad_norm": 0.008299126289784908,
      "learning_rate": 0.005488199907450255,
      "loss": 0.0001,
      "step": 102120
    },
    {
      "epoch": 47.260527533549286,
      "grad_norm": 0.019808970391750336,
      "learning_rate": 0.005478944932901435,
      "loss": 0.0002,
      "step": 102130
    },
    {
      "epoch": 47.26515502082369,
      "grad_norm": 0.012822364456951618,
      "learning_rate": 0.005469689958352615,
      "loss": 0.0001,
      "step": 102140
    },
    {
      "epoch": 47.2697825080981,
      "grad_norm": 0.047116003930568695,
      "learning_rate": 0.0054604349838037955,
      "loss": 0.0001,
      "step": 102150
    },
    {
      "epoch": 47.274409995372515,
      "grad_norm": 0.08665747195482254,
      "learning_rate": 0.005451180009254975,
      "loss": 0.0001,
      "step": 102160
    },
    {
      "epoch": 47.27903748264692,
      "grad_norm": 0.0031353251542896032,
      "learning_rate": 0.005441925034706155,
      "loss": 0.0001,
      "step": 102170
    },
    {
      "epoch": 47.28366496992133,
      "grad_norm": 0.0007511479198001325,
      "learning_rate": 0.0054326700601573345,
      "loss": 0.0002,
      "step": 102180
    },
    {
      "epoch": 47.288292457195745,
      "grad_norm": 0.0019160493975505233,
      "learning_rate": 0.005423415085608515,
      "loss": 0.0001,
      "step": 102190
    },
    {
      "epoch": 47.29291994447015,
      "grad_norm": 0.22598744928836823,
      "learning_rate": 0.005414160111059695,
      "loss": 0.0002,
      "step": 102200
    },
    {
      "epoch": 47.29754743174456,
      "grad_norm": 0.0038079170044511557,
      "learning_rate": 0.005404905136510874,
      "loss": 0.0,
      "step": 102210
    },
    {
      "epoch": 47.302174919018974,
      "grad_norm": 0.0022177151404321194,
      "learning_rate": 0.005395650161962055,
      "loss": 0.0,
      "step": 102220
    },
    {
      "epoch": 47.306802406293386,
      "grad_norm": 0.004813942592591047,
      "learning_rate": 0.005386395187413235,
      "loss": 0.0001,
      "step": 102230
    },
    {
      "epoch": 47.31142989356779,
      "grad_norm": 0.005169977899640799,
      "learning_rate": 0.005377140212864415,
      "loss": 0.0001,
      "step": 102240
    },
    {
      "epoch": 47.3160573808422,
      "grad_norm": 0.009884711354970932,
      "learning_rate": 0.005367885238315595,
      "loss": 0.0001,
      "step": 102250
    },
    {
      "epoch": 47.320684868116615,
      "grad_norm": 0.015282189473509789,
      "learning_rate": 0.005358630263766775,
      "loss": 0.0001,
      "step": 102260
    },
    {
      "epoch": 47.32531235539102,
      "grad_norm": 0.002550574019551277,
      "learning_rate": 0.005349375289217955,
      "loss": 0.0001,
      "step": 102270
    },
    {
      "epoch": 47.32993984266543,
      "grad_norm": 0.018045369535684586,
      "learning_rate": 0.005340120314669135,
      "loss": 0.0,
      "step": 102280
    },
    {
      "epoch": 47.334567329939844,
      "grad_norm": 0.001188029651530087,
      "learning_rate": 0.0053308653401203145,
      "loss": 0.0001,
      "step": 102290
    },
    {
      "epoch": 47.33919481721425,
      "grad_norm": 0.010891691781580448,
      "learning_rate": 0.005321610365571495,
      "loss": 0.0001,
      "step": 102300
    },
    {
      "epoch": 47.34382230448866,
      "grad_norm": 0.008307280018925667,
      "learning_rate": 0.005312355391022675,
      "loss": 0.0001,
      "step": 102310
    },
    {
      "epoch": 47.34844979176307,
      "grad_norm": 0.01585143245756626,
      "learning_rate": 0.005303100416473855,
      "loss": 0.0001,
      "step": 102320
    },
    {
      "epoch": 47.353077279037485,
      "grad_norm": 0.012232553213834763,
      "learning_rate": 0.005293845441925035,
      "loss": 0.0002,
      "step": 102330
    },
    {
      "epoch": 47.35770476631189,
      "grad_norm": 0.01664639636874199,
      "learning_rate": 0.0052845904673762155,
      "loss": 0.0001,
      "step": 102340
    },
    {
      "epoch": 47.3623322535863,
      "grad_norm": 0.002385642845183611,
      "learning_rate": 0.005275335492827395,
      "loss": 0.0001,
      "step": 102350
    },
    {
      "epoch": 47.366959740860715,
      "grad_norm": 0.0077887955121695995,
      "learning_rate": 0.005266080518278576,
      "loss": 0.0001,
      "step": 102360
    },
    {
      "epoch": 47.37158722813512,
      "grad_norm": 0.06110353767871857,
      "learning_rate": 0.005256825543729755,
      "loss": 0.0002,
      "step": 102370
    },
    {
      "epoch": 47.37621471540953,
      "grad_norm": 0.011693551205098629,
      "learning_rate": 0.005247570569180935,
      "loss": 0.0001,
      "step": 102380
    },
    {
      "epoch": 47.380842202683944,
      "grad_norm": 0.0886669009923935,
      "learning_rate": 0.005238315594632115,
      "loss": 0.0002,
      "step": 102390
    },
    {
      "epoch": 47.385469689958356,
      "grad_norm": 0.008212700486183167,
      "learning_rate": 0.005229060620083295,
      "loss": 0.0001,
      "step": 102400
    },
    {
      "epoch": 47.39009717723276,
      "grad_norm": 0.0015963632613420486,
      "learning_rate": 0.005219805645534475,
      "loss": 0.0001,
      "step": 102410
    },
    {
      "epoch": 47.39472466450717,
      "grad_norm": 0.0009825662709772587,
      "learning_rate": 0.0052105506709856555,
      "loss": 0.0001,
      "step": 102420
    },
    {
      "epoch": 47.399352151781585,
      "grad_norm": 0.0010629353346303105,
      "learning_rate": 0.005201295696436835,
      "loss": 0.0001,
      "step": 102430
    },
    {
      "epoch": 47.40397963905599,
      "grad_norm": 0.016392000019550323,
      "learning_rate": 0.005192040721888015,
      "loss": 0.0001,
      "step": 102440
    },
    {
      "epoch": 47.4086071263304,
      "grad_norm": 0.06347489356994629,
      "learning_rate": 0.005182785747339195,
      "loss": 0.0001,
      "step": 102450
    },
    {
      "epoch": 47.413234613604814,
      "grad_norm": 0.0031186705455183983,
      "learning_rate": 0.005173530772790375,
      "loss": 0.0001,
      "step": 102460
    },
    {
      "epoch": 47.41786210087922,
      "grad_norm": 0.005441967397928238,
      "learning_rate": 0.005164275798241556,
      "loss": 0.0001,
      "step": 102470
    },
    {
      "epoch": 47.42248958815363,
      "grad_norm": 0.3941587805747986,
      "learning_rate": 0.005155020823692735,
      "loss": 0.0003,
      "step": 102480
    },
    {
      "epoch": 47.42711707542804,
      "grad_norm": 0.0015081212623044848,
      "learning_rate": 0.005145765849143915,
      "loss": 0.0001,
      "step": 102490
    },
    {
      "epoch": 47.431744562702455,
      "grad_norm": 0.001865182537585497,
      "learning_rate": 0.005136510874595095,
      "loss": 0.0001,
      "step": 102500
    },
    {
      "epoch": 47.43637204997686,
      "grad_norm": 0.01933327503502369,
      "learning_rate": 0.005127255900046275,
      "loss": 0.0008,
      "step": 102510
    },
    {
      "epoch": 47.44099953725127,
      "grad_norm": 0.040746599435806274,
      "learning_rate": 0.005118000925497455,
      "loss": 0.0002,
      "step": 102520
    },
    {
      "epoch": 47.445627024525685,
      "grad_norm": 0.01999029703438282,
      "learning_rate": 0.005108745950948635,
      "loss": 0.0001,
      "step": 102530
    },
    {
      "epoch": 47.45025451180009,
      "grad_norm": 0.001992041477933526,
      "learning_rate": 0.005099490976399815,
      "loss": 0.0001,
      "step": 102540
    },
    {
      "epoch": 47.4548819990745,
      "grad_norm": 0.007670984603464603,
      "learning_rate": 0.005090236001850996,
      "loss": 0.0001,
      "step": 102550
    },
    {
      "epoch": 47.459509486348914,
      "grad_norm": 0.002585030859336257,
      "learning_rate": 0.005080981027302175,
      "loss": 0.0002,
      "step": 102560
    },
    {
      "epoch": 47.464136973623326,
      "grad_norm": 0.005898821167647839,
      "learning_rate": 0.005071726052753356,
      "loss": 0.0001,
      "step": 102570
    },
    {
      "epoch": 47.46876446089773,
      "grad_norm": 0.008884011767804623,
      "learning_rate": 0.0050624710782045355,
      "loss": 0.0001,
      "step": 102580
    },
    {
      "epoch": 47.47339194817214,
      "grad_norm": 0.02595454640686512,
      "learning_rate": 0.005053216103655715,
      "loss": 0.0001,
      "step": 102590
    },
    {
      "epoch": 47.478019435446555,
      "grad_norm": 0.0033195221330970526,
      "learning_rate": 0.005043961129106895,
      "loss": 0.0,
      "step": 102600
    },
    {
      "epoch": 47.48264692272096,
      "grad_norm": 0.0052376799285411835,
      "learning_rate": 0.0050347061545580754,
      "loss": 0.0001,
      "step": 102610
    },
    {
      "epoch": 47.48727440999537,
      "grad_norm": 0.03570769354701042,
      "learning_rate": 0.005025451180009255,
      "loss": 0.0001,
      "step": 102620
    },
    {
      "epoch": 47.491901897269784,
      "grad_norm": 0.001026302226819098,
      "learning_rate": 0.005016196205460436,
      "loss": 0.0001,
      "step": 102630
    },
    {
      "epoch": 47.49652938454419,
      "grad_norm": 4.0006608963012695,
      "learning_rate": 0.005006941230911615,
      "loss": 0.0026,
      "step": 102640
    },
    {
      "epoch": 47.5011568718186,
      "grad_norm": 0.01818091981112957,
      "learning_rate": 0.004997686256362795,
      "loss": 0.0001,
      "step": 102650
    },
    {
      "epoch": 47.50578435909301,
      "grad_norm": 0.006966459099203348,
      "learning_rate": 0.004988431281813976,
      "loss": 0.0001,
      "step": 102660
    },
    {
      "epoch": 47.510411846367425,
      "grad_norm": 0.02184026688337326,
      "learning_rate": 0.004979176307265155,
      "loss": 0.0001,
      "step": 102670
    },
    {
      "epoch": 47.51503933364183,
      "grad_norm": 0.006213654298335314,
      "learning_rate": 0.004969921332716336,
      "loss": 0.0001,
      "step": 102680
    },
    {
      "epoch": 47.51966682091624,
      "grad_norm": 0.0023082257248461246,
      "learning_rate": 0.0049606663581675155,
      "loss": 0.0001,
      "step": 102690
    },
    {
      "epoch": 47.524294308190655,
      "grad_norm": 0.004479920957237482,
      "learning_rate": 0.004951411383618695,
      "loss": 0.0,
      "step": 102700
    },
    {
      "epoch": 47.52892179546506,
      "grad_norm": 0.1593811810016632,
      "learning_rate": 0.004942156409069875,
      "loss": 0.0002,
      "step": 102710
    },
    {
      "epoch": 47.53354928273947,
      "grad_norm": 0.14072784781455994,
      "learning_rate": 0.004932901434521055,
      "loss": 0.0001,
      "step": 102720
    },
    {
      "epoch": 47.538176770013884,
      "grad_norm": 0.009375517256557941,
      "learning_rate": 0.004923646459972235,
      "loss": 0.0001,
      "step": 102730
    },
    {
      "epoch": 47.542804257288296,
      "grad_norm": 0.04915996268391609,
      "learning_rate": 0.004914391485423416,
      "loss": 0.0001,
      "step": 102740
    },
    {
      "epoch": 47.5474317445627,
      "grad_norm": 0.009294984862208366,
      "learning_rate": 0.004905136510874595,
      "loss": 0.0001,
      "step": 102750
    },
    {
      "epoch": 47.55205923183711,
      "grad_norm": 0.007847931236028671,
      "learning_rate": 0.004895881536325776,
      "loss": 0.0005,
      "step": 102760
    },
    {
      "epoch": 47.556686719111525,
      "grad_norm": 0.011445957235991955,
      "learning_rate": 0.0048866265617769555,
      "loss": 0.0001,
      "step": 102770
    },
    {
      "epoch": 47.56131420638593,
      "grad_norm": 0.033361636102199554,
      "learning_rate": 0.004877371587228136,
      "loss": 0.0001,
      "step": 102780
    },
    {
      "epoch": 47.56594169366034,
      "grad_norm": 0.016466442495584488,
      "learning_rate": 0.004868116612679316,
      "loss": 0.0001,
      "step": 102790
    },
    {
      "epoch": 47.570569180934754,
      "grad_norm": 0.06609897315502167,
      "learning_rate": 0.004858861638130495,
      "loss": 0.0001,
      "step": 102800
    },
    {
      "epoch": 47.57519666820916,
      "grad_norm": 0.0013574573677033186,
      "learning_rate": 0.004849606663581675,
      "loss": 0.0001,
      "step": 102810
    },
    {
      "epoch": 47.57982415548357,
      "grad_norm": 0.0031949516851454973,
      "learning_rate": 0.004840351689032856,
      "loss": 0.0,
      "step": 102820
    },
    {
      "epoch": 47.58445164275798,
      "grad_norm": 0.007935767993330956,
      "learning_rate": 0.004831096714484035,
      "loss": 0.0,
      "step": 102830
    },
    {
      "epoch": 47.589079130032395,
      "grad_norm": 0.05224937945604324,
      "learning_rate": 0.004821841739935216,
      "loss": 0.0001,
      "step": 102840
    },
    {
      "epoch": 47.5937066173068,
      "grad_norm": 0.0038753491826355457,
      "learning_rate": 0.0048125867653863955,
      "loss": 0.0001,
      "step": 102850
    },
    {
      "epoch": 47.59833410458121,
      "grad_norm": 0.010432866401970387,
      "learning_rate": 0.004803331790837575,
      "loss": 0.0001,
      "step": 102860
    },
    {
      "epoch": 47.602961591855625,
      "grad_norm": 0.02457459270954132,
      "learning_rate": 0.004794076816288756,
      "loss": 0.0001,
      "step": 102870
    },
    {
      "epoch": 47.60758907913003,
      "grad_norm": 0.0022672386839985847,
      "learning_rate": 0.004784821841739935,
      "loss": 0.0001,
      "step": 102880
    },
    {
      "epoch": 47.61221656640444,
      "grad_norm": 0.03854962810873985,
      "learning_rate": 0.004775566867191116,
      "loss": 0.0001,
      "step": 102890
    },
    {
      "epoch": 47.616844053678854,
      "grad_norm": 0.5078067183494568,
      "learning_rate": 0.004766311892642296,
      "loss": 0.0002,
      "step": 102900
    },
    {
      "epoch": 47.621471540953266,
      "grad_norm": 0.0028773965314030647,
      "learning_rate": 0.004757056918093475,
      "loss": 0.0001,
      "step": 102910
    },
    {
      "epoch": 47.62609902822767,
      "grad_norm": 0.010710449889302254,
      "learning_rate": 0.004747801943544655,
      "loss": 0.0001,
      "step": 102920
    },
    {
      "epoch": 47.63072651550208,
      "grad_norm": 0.01274521928280592,
      "learning_rate": 0.0047385469689958356,
      "loss": 0.0001,
      "step": 102930
    },
    {
      "epoch": 47.635354002776495,
      "grad_norm": 0.009256978519260883,
      "learning_rate": 0.004729291994447015,
      "loss": 0.0001,
      "step": 102940
    },
    {
      "epoch": 47.6399814900509,
      "grad_norm": 0.0025426128413528204,
      "learning_rate": 0.004720037019898196,
      "loss": 0.0001,
      "step": 102950
    },
    {
      "epoch": 47.64460897732531,
      "grad_norm": 0.01375645026564598,
      "learning_rate": 0.0047107820453493755,
      "loss": 0.0001,
      "step": 102960
    },
    {
      "epoch": 47.649236464599724,
      "grad_norm": 0.007861935533583164,
      "learning_rate": 0.004701527070800556,
      "loss": 0.0001,
      "step": 102970
    },
    {
      "epoch": 47.65386395187413,
      "grad_norm": 0.18131940066814423,
      "learning_rate": 0.004692272096251736,
      "loss": 0.0002,
      "step": 102980
    },
    {
      "epoch": 47.65849143914854,
      "grad_norm": 0.035229574888944626,
      "learning_rate": 0.004683017121702915,
      "loss": 0.0,
      "step": 102990
    },
    {
      "epoch": 47.66311892642295,
      "grad_norm": 0.0009290245361626148,
      "learning_rate": 0.004673762147154095,
      "loss": 0.0001,
      "step": 103000
    },
    {
      "epoch": 47.667746413697365,
      "grad_norm": 0.005379443988204002,
      "learning_rate": 0.004664507172605276,
      "loss": 0.0006,
      "step": 103010
    },
    {
      "epoch": 47.67237390097177,
      "grad_norm": 0.023757753893733025,
      "learning_rate": 0.004655252198056455,
      "loss": 0.0001,
      "step": 103020
    },
    {
      "epoch": 47.67700138824618,
      "grad_norm": 0.016835343092679977,
      "learning_rate": 0.004645997223507636,
      "loss": 0.0001,
      "step": 103030
    },
    {
      "epoch": 47.681628875520595,
      "grad_norm": 0.010301548056304455,
      "learning_rate": 0.0046367422489588155,
      "loss": 0.0001,
      "step": 103040
    },
    {
      "epoch": 47.686256362795,
      "grad_norm": 1.8060345649719238,
      "learning_rate": 0.004627487274409996,
      "loss": 0.0004,
      "step": 103050
    },
    {
      "epoch": 47.69088385006941,
      "grad_norm": 0.0019117385381832719,
      "learning_rate": 0.004618232299861176,
      "loss": 0.0,
      "step": 103060
    },
    {
      "epoch": 47.695511337343824,
      "grad_norm": 0.004845448303967714,
      "learning_rate": 0.004608977325312355,
      "loss": 0.0001,
      "step": 103070
    },
    {
      "epoch": 47.700138824618236,
      "grad_norm": 0.010271914303302765,
      "learning_rate": 0.004599722350763536,
      "loss": 0.0001,
      "step": 103080
    },
    {
      "epoch": 47.70476631189264,
      "grad_norm": 0.0036836806684732437,
      "learning_rate": 0.004590467376214716,
      "loss": 0.0001,
      "step": 103090
    },
    {
      "epoch": 47.70939379916705,
      "grad_norm": 0.007178012747317553,
      "learning_rate": 0.004581212401665895,
      "loss": 0.0001,
      "step": 103100
    },
    {
      "epoch": 47.714021286441465,
      "grad_norm": 0.01985650695860386,
      "learning_rate": 0.004571957427117075,
      "loss": 0.0001,
      "step": 103110
    },
    {
      "epoch": 47.71864877371587,
      "grad_norm": 0.017332252115011215,
      "learning_rate": 0.0045627024525682555,
      "loss": 0.0001,
      "step": 103120
    },
    {
      "epoch": 47.72327626099028,
      "grad_norm": 0.010856500826776028,
      "learning_rate": 0.004553447478019435,
      "loss": 0.0001,
      "step": 103130
    },
    {
      "epoch": 47.727903748264694,
      "grad_norm": 0.006701300852000713,
      "learning_rate": 0.004544192503470616,
      "loss": 0.0001,
      "step": 103140
    },
    {
      "epoch": 47.7325312355391,
      "grad_norm": 0.001638836576603353,
      "learning_rate": 0.004534937528921795,
      "loss": 0.0001,
      "step": 103150
    },
    {
      "epoch": 47.73715872281351,
      "grad_norm": 0.002716503571718931,
      "learning_rate": 0.004525682554372976,
      "loss": 0.0001,
      "step": 103160
    },
    {
      "epoch": 47.74178621008792,
      "grad_norm": 0.009927468374371529,
      "learning_rate": 0.004516427579824156,
      "loss": 0.0001,
      "step": 103170
    },
    {
      "epoch": 47.746413697362335,
      "grad_norm": 0.36541539430618286,
      "learning_rate": 0.004507172605275336,
      "loss": 0.0001,
      "step": 103180
    },
    {
      "epoch": 47.75104118463674,
      "grad_norm": 0.0016768289497122169,
      "learning_rate": 0.004497917630726516,
      "loss": 0.0,
      "step": 103190
    },
    {
      "epoch": 47.75566867191115,
      "grad_norm": 0.17219240963459015,
      "learning_rate": 0.0044886626561776955,
      "loss": 0.0001,
      "step": 103200
    },
    {
      "epoch": 47.760296159185565,
      "grad_norm": 0.0038361093029379845,
      "learning_rate": 0.004479407681628875,
      "loss": 0.0001,
      "step": 103210
    },
    {
      "epoch": 47.76492364645997,
      "grad_norm": 0.00538929458707571,
      "learning_rate": 0.004470152707080056,
      "loss": 0.0001,
      "step": 103220
    },
    {
      "epoch": 47.76955113373438,
      "grad_norm": 0.004351083189249039,
      "learning_rate": 0.0044608977325312354,
      "loss": 0.0001,
      "step": 103230
    },
    {
      "epoch": 47.774178621008794,
      "grad_norm": 0.019766638055443764,
      "learning_rate": 0.004451642757982416,
      "loss": 0.0002,
      "step": 103240
    },
    {
      "epoch": 47.7788061082832,
      "grad_norm": 0.009824089705944061,
      "learning_rate": 0.004442387783433596,
      "loss": 0.0001,
      "step": 103250
    },
    {
      "epoch": 47.78343359555761,
      "grad_norm": 0.014852357096970081,
      "learning_rate": 0.004433132808884776,
      "loss": 0.0001,
      "step": 103260
    },
    {
      "epoch": 47.78806108283202,
      "grad_norm": 0.001890624058432877,
      "learning_rate": 0.004423877834335956,
      "loss": 0.0005,
      "step": 103270
    },
    {
      "epoch": 47.792688570106435,
      "grad_norm": 0.004932408686727285,
      "learning_rate": 0.0044146228597871364,
      "loss": 0.0001,
      "step": 103280
    },
    {
      "epoch": 47.79731605738084,
      "grad_norm": 0.008998836390674114,
      "learning_rate": 0.004405367885238316,
      "loss": 0.0006,
      "step": 103290
    },
    {
      "epoch": 47.80194354465525,
      "grad_norm": 0.01176428236067295,
      "learning_rate": 0.004396112910689496,
      "loss": 0.0,
      "step": 103300
    },
    {
      "epoch": 47.806571031929664,
      "grad_norm": 0.001959801884368062,
      "learning_rate": 0.0043868579361406755,
      "loss": 0.0001,
      "step": 103310
    },
    {
      "epoch": 47.81119851920407,
      "grad_norm": 0.0029218271374702454,
      "learning_rate": 0.004377602961591855,
      "loss": 0.0001,
      "step": 103320
    },
    {
      "epoch": 47.81582600647848,
      "grad_norm": 0.003419737098738551,
      "learning_rate": 0.004368347987043036,
      "loss": 0.0001,
      "step": 103330
    },
    {
      "epoch": 47.82045349375289,
      "grad_norm": 0.001896130619570613,
      "learning_rate": 0.004359093012494215,
      "loss": 0.0,
      "step": 103340
    },
    {
      "epoch": 47.825080981027305,
      "grad_norm": 0.0021655315067619085,
      "learning_rate": 0.004349838037945396,
      "loss": 0.0,
      "step": 103350
    },
    {
      "epoch": 47.82970846830171,
      "grad_norm": 0.0010087171103805304,
      "learning_rate": 0.004340583063396576,
      "loss": 0.0001,
      "step": 103360
    },
    {
      "epoch": 47.83433595557612,
      "grad_norm": 0.03850048780441284,
      "learning_rate": 0.004331328088847756,
      "loss": 0.0001,
      "step": 103370
    },
    {
      "epoch": 47.838963442850535,
      "grad_norm": 0.005538588855415583,
      "learning_rate": 0.004322073114298936,
      "loss": 0.0,
      "step": 103380
    },
    {
      "epoch": 47.84359093012494,
      "grad_norm": 0.0022735048551112413,
      "learning_rate": 0.004312818139750116,
      "loss": 0.0001,
      "step": 103390
    },
    {
      "epoch": 47.84821841739935,
      "grad_norm": 0.006910885218530893,
      "learning_rate": 0.004303563165201296,
      "loss": 0.0001,
      "step": 103400
    },
    {
      "epoch": 47.852845904673764,
      "grad_norm": 0.0026991453487426043,
      "learning_rate": 0.004294308190652476,
      "loss": 0.0001,
      "step": 103410
    },
    {
      "epoch": 47.85747339194817,
      "grad_norm": 0.017394984140992165,
      "learning_rate": 0.004285053216103655,
      "loss": 0.0001,
      "step": 103420
    },
    {
      "epoch": 47.86210087922258,
      "grad_norm": 0.0991012379527092,
      "learning_rate": 0.004275798241554836,
      "loss": 0.0001,
      "step": 103430
    },
    {
      "epoch": 47.86672836649699,
      "grad_norm": 0.001905911136418581,
      "learning_rate": 0.004266543267006016,
      "loss": 0.0001,
      "step": 103440
    },
    {
      "epoch": 47.871355853771405,
      "grad_norm": 0.026691848412156105,
      "learning_rate": 0.004257288292457196,
      "loss": 0.0001,
      "step": 103450
    },
    {
      "epoch": 47.87598334104581,
      "grad_norm": 0.005279138218611479,
      "learning_rate": 0.004248033317908376,
      "loss": 0.0001,
      "step": 103460
    },
    {
      "epoch": 47.88061082832022,
      "grad_norm": 0.005296939518302679,
      "learning_rate": 0.004238778343359556,
      "loss": 0.0002,
      "step": 103470
    },
    {
      "epoch": 47.885238315594634,
      "grad_norm": 0.02070492133498192,
      "learning_rate": 0.004229523368810736,
      "loss": 0.0001,
      "step": 103480
    },
    {
      "epoch": 47.88986580286904,
      "grad_norm": 0.4300410747528076,
      "learning_rate": 0.004220268394261917,
      "loss": 0.0002,
      "step": 103490
    },
    {
      "epoch": 47.89449329014345,
      "grad_norm": 0.013544444926083088,
      "learning_rate": 0.004211013419713096,
      "loss": 0.0001,
      "step": 103500
    },
    {
      "epoch": 47.89912077741786,
      "grad_norm": 0.003314428962767124,
      "learning_rate": 0.004201758445164276,
      "loss": 0.0001,
      "step": 103510
    },
    {
      "epoch": 47.903748264692275,
      "grad_norm": 0.0013964457903057337,
      "learning_rate": 0.004192503470615456,
      "loss": 0.0001,
      "step": 103520
    },
    {
      "epoch": 47.90837575196668,
      "grad_norm": 0.004817466251552105,
      "learning_rate": 0.004183248496066635,
      "loss": 0.0001,
      "step": 103530
    },
    {
      "epoch": 47.91300323924109,
      "grad_norm": 0.013703110627830029,
      "learning_rate": 0.004173993521517816,
      "loss": 0.0,
      "step": 103540
    },
    {
      "epoch": 47.917630726515505,
      "grad_norm": 3.499480962753296,
      "learning_rate": 0.0041647385469689956,
      "loss": 0.0019,
      "step": 103550
    },
    {
      "epoch": 47.92225821378991,
      "grad_norm": 0.0037301103584468365,
      "learning_rate": 0.004155483572420176,
      "loss": 0.0001,
      "step": 103560
    },
    {
      "epoch": 47.92688570106432,
      "grad_norm": 0.009671235457062721,
      "learning_rate": 0.004146228597871356,
      "loss": 0.0001,
      "step": 103570
    },
    {
      "epoch": 47.931513188338734,
      "grad_norm": 0.09695770591497421,
      "learning_rate": 0.004136973623322536,
      "loss": 0.0001,
      "step": 103580
    },
    {
      "epoch": 47.93614067561314,
      "grad_norm": 0.0036162042524665594,
      "learning_rate": 0.004127718648773716,
      "loss": 0.0001,
      "step": 103590
    },
    {
      "epoch": 47.94076816288755,
      "grad_norm": 0.023866329342126846,
      "learning_rate": 0.0041184636742248966,
      "loss": 0.0001,
      "step": 103600
    },
    {
      "epoch": 47.94539565016196,
      "grad_norm": 0.006743696518242359,
      "learning_rate": 0.004109208699676076,
      "loss": 0.0002,
      "step": 103610
    },
    {
      "epoch": 47.950023137436375,
      "grad_norm": 0.013784680515527725,
      "learning_rate": 0.004099953725127256,
      "loss": 0.0001,
      "step": 103620
    },
    {
      "epoch": 47.95465062471078,
      "grad_norm": 1.5079162120819092,
      "learning_rate": 0.004090698750578436,
      "loss": 0.0003,
      "step": 103630
    },
    {
      "epoch": 47.95927811198519,
      "grad_norm": 0.01796199195086956,
      "learning_rate": 0.004081443776029616,
      "loss": 0.0001,
      "step": 103640
    },
    {
      "epoch": 47.963905599259604,
      "grad_norm": 0.01714598946273327,
      "learning_rate": 0.004072188801480796,
      "loss": 0.0002,
      "step": 103650
    },
    {
      "epoch": 47.96853308653401,
      "grad_norm": 0.016803305596113205,
      "learning_rate": 0.004062933826931976,
      "loss": 0.0001,
      "step": 103660
    },
    {
      "epoch": 47.97316057380842,
      "grad_norm": 0.012953212484717369,
      "learning_rate": 0.004053678852383156,
      "loss": 0.0001,
      "step": 103670
    },
    {
      "epoch": 47.97778806108283,
      "grad_norm": 0.033752571791410446,
      "learning_rate": 0.004044423877834337,
      "loss": 0.0002,
      "step": 103680
    },
    {
      "epoch": 47.982415548357245,
      "grad_norm": 0.01014640647917986,
      "learning_rate": 0.004035168903285516,
      "loss": 0.0002,
      "step": 103690
    },
    {
      "epoch": 47.98704303563165,
      "grad_norm": 0.007816733792424202,
      "learning_rate": 0.004025913928736697,
      "loss": 0.0001,
      "step": 103700
    },
    {
      "epoch": 47.99167052290606,
      "grad_norm": 0.02663765288889408,
      "learning_rate": 0.0040166589541878765,
      "loss": 0.0003,
      "step": 103710
    },
    {
      "epoch": 47.996298010180475,
      "grad_norm": 0.030345741659402847,
      "learning_rate": 0.004007403979639056,
      "loss": 0.0001,
      "step": 103720
    },
    {
      "epoch": 48.0,
      "eval_accuracy_branch1": 0.9889077183792944,
      "eval_accuracy_branch2": 0.5001540594669542,
      "eval_f1_branch1": 0.9902554585009608,
      "eval_f1_branch2": 0.5001201008480403,
      "eval_loss": 0.019300593063235283,
      "eval_precision_branch1": 0.9906179918009008,
      "eval_precision_branch2": 0.5001541013415625,
      "eval_recall_branch1": 0.9900413910790685,
      "eval_recall_branch2": 0.5001540594669542,
      "eval_runtime": 29.0314,
      "eval_samples_per_second": 447.171,
      "eval_steps_per_second": 55.905,
      "step": 103728
    },
    {
      "epoch": 48.00092549745488,
      "grad_norm": 0.004438640084117651,
      "learning_rate": 0.003998149005090236,
      "loss": 0.0002,
      "step": 103730
    },
    {
      "epoch": 48.00555298472929,
      "grad_norm": 0.10268183052539825,
      "learning_rate": 0.0039888940305414155,
      "loss": 0.0001,
      "step": 103740
    },
    {
      "epoch": 48.010180472003704,
      "grad_norm": 0.011643582955002785,
      "learning_rate": 0.003979639055992596,
      "loss": 0.0005,
      "step": 103750
    },
    {
      "epoch": 48.01480795927811,
      "grad_norm": 0.0035209818743169308,
      "learning_rate": 0.003970384081443776,
      "loss": 0.0001,
      "step": 103760
    },
    {
      "epoch": 48.01943544655252,
      "grad_norm": 0.04120349884033203,
      "learning_rate": 0.003961129106894956,
      "loss": 0.0001,
      "step": 103770
    },
    {
      "epoch": 48.02406293382693,
      "grad_norm": 0.015477052889764309,
      "learning_rate": 0.003951874132346136,
      "loss": 0.0001,
      "step": 103780
    },
    {
      "epoch": 48.028690421101345,
      "grad_norm": 0.0027927986811846495,
      "learning_rate": 0.0039426191577973165,
      "loss": 0.0003,
      "step": 103790
    },
    {
      "epoch": 48.03331790837575,
      "grad_norm": 0.004664808977395296,
      "learning_rate": 0.003933364183248496,
      "loss": 0.0001,
      "step": 103800
    },
    {
      "epoch": 48.03794539565016,
      "grad_norm": 0.06352211534976959,
      "learning_rate": 0.003924109208699677,
      "loss": 0.0002,
      "step": 103810
    },
    {
      "epoch": 48.042572882924574,
      "grad_norm": 0.005744475871324539,
      "learning_rate": 0.003914854234150856,
      "loss": 0.0001,
      "step": 103820
    },
    {
      "epoch": 48.04720037019898,
      "grad_norm": 0.08596048504114151,
      "learning_rate": 0.0039055992596020365,
      "loss": 0.0001,
      "step": 103830
    },
    {
      "epoch": 48.05182785747339,
      "grad_norm": 0.058308348059654236,
      "learning_rate": 0.003896344285053216,
      "loss": 0.0001,
      "step": 103840
    },
    {
      "epoch": 48.0564553447478,
      "grad_norm": 0.2900913953781128,
      "learning_rate": 0.0038870893105043963,
      "loss": 0.0002,
      "step": 103850
    },
    {
      "epoch": 48.061082832022215,
      "grad_norm": 0.0018592976266518235,
      "learning_rate": 0.003877834335955576,
      "loss": 0.0,
      "step": 103860
    },
    {
      "epoch": 48.06571031929662,
      "grad_norm": 0.0008872904581949115,
      "learning_rate": 0.0038685793614067565,
      "loss": 0.0,
      "step": 103870
    },
    {
      "epoch": 48.07033780657103,
      "grad_norm": 0.0005784837994724512,
      "learning_rate": 0.0038593243868579362,
      "loss": 0.0001,
      "step": 103880
    },
    {
      "epoch": 48.074965293845445,
      "grad_norm": 0.09374018758535385,
      "learning_rate": 0.0038500694123091168,
      "loss": 0.0007,
      "step": 103890
    },
    {
      "epoch": 48.07959278111985,
      "grad_norm": 0.10898426175117493,
      "learning_rate": 0.0038408144377602964,
      "loss": 0.0002,
      "step": 103900
    },
    {
      "epoch": 48.08422026839426,
      "grad_norm": 0.0487796925008297,
      "learning_rate": 0.0038315594632114766,
      "loss": 0.0001,
      "step": 103910
    },
    {
      "epoch": 48.088847755668674,
      "grad_norm": 0.004554264713078737,
      "learning_rate": 0.0038223044886626562,
      "loss": 0.0001,
      "step": 103920
    },
    {
      "epoch": 48.09347524294308,
      "grad_norm": 0.023307984694838524,
      "learning_rate": 0.003813049514113837,
      "loss": 0.0004,
      "step": 103930
    },
    {
      "epoch": 48.09810273021749,
      "grad_norm": 0.03838256746530533,
      "learning_rate": 0.0038037945395650165,
      "loss": 0.0001,
      "step": 103940
    },
    {
      "epoch": 48.1027302174919,
      "grad_norm": 0.0030619229655712843,
      "learning_rate": 0.003794539565016196,
      "loss": 0.0,
      "step": 103950
    },
    {
      "epoch": 48.107357704766315,
      "grad_norm": 0.005849864799529314,
      "learning_rate": 0.0037852845904673763,
      "loss": 0.0001,
      "step": 103960
    },
    {
      "epoch": 48.11198519204072,
      "grad_norm": 0.01940120756626129,
      "learning_rate": 0.003776029615918556,
      "loss": 0.0001,
      "step": 103970
    },
    {
      "epoch": 48.11661267931513,
      "grad_norm": 0.0007381229661405087,
      "learning_rate": 0.0037667746413697365,
      "loss": 0.0,
      "step": 103980
    },
    {
      "epoch": 48.121240166589544,
      "grad_norm": 0.0023846025578677654,
      "learning_rate": 0.003757519666820916,
      "loss": 0.0001,
      "step": 103990
    },
    {
      "epoch": 48.12586765386395,
      "grad_norm": 0.006363870110362768,
      "learning_rate": 0.0037482646922720967,
      "loss": 0.0001,
      "step": 104000
    },
    {
      "epoch": 48.13049514113836,
      "grad_norm": 0.013845114037394524,
      "learning_rate": 0.0037390097177232764,
      "loss": 0.0,
      "step": 104010
    },
    {
      "epoch": 48.13512262841277,
      "grad_norm": 0.0026754862628877163,
      "learning_rate": 0.0037297547431744565,
      "loss": 0.0,
      "step": 104020
    },
    {
      "epoch": 48.13975011568718,
      "grad_norm": 0.013243853114545345,
      "learning_rate": 0.003720499768625636,
      "loss": 0.0001,
      "step": 104030
    },
    {
      "epoch": 48.14437760296159,
      "grad_norm": 0.004596926271915436,
      "learning_rate": 0.0037112447940768167,
      "loss": 0.0001,
      "step": 104040
    },
    {
      "epoch": 48.149005090236,
      "grad_norm": 0.0009236412006430328,
      "learning_rate": 0.0037019898195279964,
      "loss": 0.0001,
      "step": 104050
    },
    {
      "epoch": 48.153632577510415,
      "grad_norm": 0.0027839289978146553,
      "learning_rate": 0.0036927348449791765,
      "loss": 0.0001,
      "step": 104060
    },
    {
      "epoch": 48.15826006478482,
      "grad_norm": 0.03748002275824547,
      "learning_rate": 0.003683479870430356,
      "loss": 0.0003,
      "step": 104070
    },
    {
      "epoch": 48.16288755205923,
      "grad_norm": 0.011125992052257061,
      "learning_rate": 0.0036742248958815367,
      "loss": 0.0001,
      "step": 104080
    },
    {
      "epoch": 48.167515039333644,
      "grad_norm": 0.0010720518184825778,
      "learning_rate": 0.0036649699213327164,
      "loss": 0.0001,
      "step": 104090
    },
    {
      "epoch": 48.17214252660805,
      "grad_norm": 0.00240113353356719,
      "learning_rate": 0.003655714946783897,
      "loss": 0.0001,
      "step": 104100
    },
    {
      "epoch": 48.17677001388246,
      "grad_norm": 0.048317231237888336,
      "learning_rate": 0.0036464599722350766,
      "loss": 0.0002,
      "step": 104110
    },
    {
      "epoch": 48.18139750115687,
      "grad_norm": 0.04112282395362854,
      "learning_rate": 0.0036372049976862567,
      "loss": 0.0001,
      "step": 104120
    },
    {
      "epoch": 48.186024988431285,
      "grad_norm": 0.005638439208269119,
      "learning_rate": 0.0036279500231374364,
      "loss": 0.0001,
      "step": 104130
    },
    {
      "epoch": 48.19065247570569,
      "grad_norm": 0.0020262692123651505,
      "learning_rate": 0.003618695048588617,
      "loss": 0.0001,
      "step": 104140
    },
    {
      "epoch": 48.1952799629801,
      "grad_norm": 0.006700635887682438,
      "learning_rate": 0.0036094400740397966,
      "loss": 0.0001,
      "step": 104150
    },
    {
      "epoch": 48.199907450254514,
      "grad_norm": 0.0020948483143001795,
      "learning_rate": 0.0036001850994909763,
      "loss": 0.0001,
      "step": 104160
    },
    {
      "epoch": 48.20453493752892,
      "grad_norm": 0.0038306608330458403,
      "learning_rate": 0.0035909301249421564,
      "loss": 0.0001,
      "step": 104170
    },
    {
      "epoch": 48.20916242480333,
      "grad_norm": 0.0022224471904337406,
      "learning_rate": 0.003581675150393336,
      "loss": 0.0008,
      "step": 104180
    },
    {
      "epoch": 48.21378991207774,
      "grad_norm": 0.0031483431812375784,
      "learning_rate": 0.0035724201758445167,
      "loss": 0.0001,
      "step": 104190
    },
    {
      "epoch": 48.21841739935215,
      "grad_norm": 0.05859699472784996,
      "learning_rate": 0.0035631652012956963,
      "loss": 0.0001,
      "step": 104200
    },
    {
      "epoch": 48.22304488662656,
      "grad_norm": 0.017064156010746956,
      "learning_rate": 0.003553910226746877,
      "loss": 0.0002,
      "step": 104210
    },
    {
      "epoch": 48.22767237390097,
      "grad_norm": 0.0074803708121180534,
      "learning_rate": 0.0035446552521980566,
      "loss": 0.0,
      "step": 104220
    },
    {
      "epoch": 48.232299861175385,
      "grad_norm": 0.00874373223632574,
      "learning_rate": 0.0035354002776492367,
      "loss": 0.0,
      "step": 104230
    },
    {
      "epoch": 48.23692734844979,
      "grad_norm": 0.004864325746893883,
      "learning_rate": 0.0035261453031004164,
      "loss": 0.0008,
      "step": 104240
    },
    {
      "epoch": 48.2415548357242,
      "grad_norm": 0.011493945494294167,
      "learning_rate": 0.003516890328551597,
      "loss": 0.0001,
      "step": 104250
    },
    {
      "epoch": 48.246182322998614,
      "grad_norm": 0.002247466240078211,
      "learning_rate": 0.0035076353540027766,
      "loss": 0.0,
      "step": 104260
    },
    {
      "epoch": 48.25080981027302,
      "grad_norm": 0.016321763396263123,
      "learning_rate": 0.0034983803794539567,
      "loss": 0.0002,
      "step": 104270
    },
    {
      "epoch": 48.25543729754743,
      "grad_norm": 0.0038919695653021336,
      "learning_rate": 0.0034891254049051364,
      "loss": 0.0001,
      "step": 104280
    },
    {
      "epoch": 48.26006478482184,
      "grad_norm": 0.012801886536180973,
      "learning_rate": 0.003479870430356317,
      "loss": 0.0001,
      "step": 104290
    },
    {
      "epoch": 48.264692272096255,
      "grad_norm": 0.02937738411128521,
      "learning_rate": 0.0034706154558074966,
      "loss": 0.0001,
      "step": 104300
    },
    {
      "epoch": 48.26931975937066,
      "grad_norm": 0.9553654193878174,
      "learning_rate": 0.003461360481258677,
      "loss": 0.0004,
      "step": 104310
    },
    {
      "epoch": 48.27394724664507,
      "grad_norm": 0.005690491292625666,
      "learning_rate": 0.003452105506709857,
      "loss": 0.0001,
      "step": 104320
    },
    {
      "epoch": 48.278574733919484,
      "grad_norm": 0.005217886995524168,
      "learning_rate": 0.003442850532161037,
      "loss": 0.0001,
      "step": 104330
    },
    {
      "epoch": 48.28320222119389,
      "grad_norm": 0.03208053484559059,
      "learning_rate": 0.0034335955576122166,
      "loss": 0.0002,
      "step": 104340
    },
    {
      "epoch": 48.2878297084683,
      "grad_norm": 0.009480352513492107,
      "learning_rate": 0.003424340583063397,
      "loss": 0.0001,
      "step": 104350
    },
    {
      "epoch": 48.29245719574271,
      "grad_norm": 0.0034699353855103254,
      "learning_rate": 0.003415085608514577,
      "loss": 0.0001,
      "step": 104360
    },
    {
      "epoch": 48.29708468301712,
      "grad_norm": 0.0027474346570670605,
      "learning_rate": 0.0034058306339657565,
      "loss": 0.0,
      "step": 104370
    },
    {
      "epoch": 48.30171217029153,
      "grad_norm": 0.005563393700867891,
      "learning_rate": 0.0033965756594169366,
      "loss": 0.0001,
      "step": 104380
    },
    {
      "epoch": 48.30633965756594,
      "grad_norm": 0.001463110325857997,
      "learning_rate": 0.0033873206848681163,
      "loss": 0.0,
      "step": 104390
    },
    {
      "epoch": 48.310967144840355,
      "grad_norm": 0.0026589706540107727,
      "learning_rate": 0.003378065710319297,
      "loss": 0.0001,
      "step": 104400
    },
    {
      "epoch": 48.31559463211476,
      "grad_norm": 0.005967163946479559,
      "learning_rate": 0.0033688107357704765,
      "loss": 0.0001,
      "step": 104410
    },
    {
      "epoch": 48.32022211938917,
      "grad_norm": 0.0080007528886199,
      "learning_rate": 0.003359555761221657,
      "loss": 0.0002,
      "step": 104420
    },
    {
      "epoch": 48.324849606663584,
      "grad_norm": 0.0027645588852465153,
      "learning_rate": 0.0033503007866728367,
      "loss": 0.0001,
      "step": 104430
    },
    {
      "epoch": 48.32947709393799,
      "grad_norm": 0.001292554079554975,
      "learning_rate": 0.003341045812124017,
      "loss": 0.0,
      "step": 104440
    },
    {
      "epoch": 48.3341045812124,
      "grad_norm": 0.0010385116329416633,
      "learning_rate": 0.0033317908375751965,
      "loss": 0.001,
      "step": 104450
    },
    {
      "epoch": 48.33873206848681,
      "grad_norm": 0.003868549130856991,
      "learning_rate": 0.003322535863026377,
      "loss": 0.0002,
      "step": 104460
    },
    {
      "epoch": 48.343359555761225,
      "grad_norm": 0.0008222661563195288,
      "learning_rate": 0.0033132808884775568,
      "loss": 0.0005,
      "step": 104470
    },
    {
      "epoch": 48.34798704303563,
      "grad_norm": 0.016638681292533875,
      "learning_rate": 0.003304025913928737,
      "loss": 0.0003,
      "step": 104480
    },
    {
      "epoch": 48.35261453031004,
      "grad_norm": 0.004860213492065668,
      "learning_rate": 0.0032947709393799166,
      "loss": 0.0001,
      "step": 104490
    },
    {
      "epoch": 48.357242017584454,
      "grad_norm": 0.006903511472046375,
      "learning_rate": 0.003285515964831097,
      "loss": 0.0001,
      "step": 104500
    },
    {
      "epoch": 48.36186950485886,
      "grad_norm": 0.0054708728566765785,
      "learning_rate": 0.0032762609902822768,
      "loss": 0.0001,
      "step": 104510
    },
    {
      "epoch": 48.36649699213327,
      "grad_norm": 0.029900841414928436,
      "learning_rate": 0.0032670060157334573,
      "loss": 0.0001,
      "step": 104520
    },
    {
      "epoch": 48.37112447940768,
      "grad_norm": 0.0027186227962374687,
      "learning_rate": 0.003257751041184637,
      "loss": 0.0001,
      "step": 104530
    },
    {
      "epoch": 48.37575196668209,
      "grad_norm": 0.0023913029581308365,
      "learning_rate": 0.003248496066635817,
      "loss": 0.0,
      "step": 104540
    },
    {
      "epoch": 48.3803794539565,
      "grad_norm": 0.0037176660262048244,
      "learning_rate": 0.003239241092086997,
      "loss": 0.0001,
      "step": 104550
    },
    {
      "epoch": 48.38500694123091,
      "grad_norm": 0.06724928319454193,
      "learning_rate": 0.0032299861175381773,
      "loss": 0.0002,
      "step": 104560
    },
    {
      "epoch": 48.389634428505325,
      "grad_norm": 0.159326434135437,
      "learning_rate": 0.003220731142989357,
      "loss": 0.0002,
      "step": 104570
    },
    {
      "epoch": 48.39426191577973,
      "grad_norm": 0.009287387132644653,
      "learning_rate": 0.003211476168440537,
      "loss": 0.0001,
      "step": 104580
    },
    {
      "epoch": 48.39888940305414,
      "grad_norm": 0.012747074477374554,
      "learning_rate": 0.003202221193891717,
      "loss": 0.0,
      "step": 104590
    },
    {
      "epoch": 48.403516890328554,
      "grad_norm": 0.026112932711839676,
      "learning_rate": 0.0031929662193428965,
      "loss": 0.0001,
      "step": 104600
    },
    {
      "epoch": 48.40814437760296,
      "grad_norm": 0.0014374064048752189,
      "learning_rate": 0.003183711244794077,
      "loss": 0.0,
      "step": 104610
    },
    {
      "epoch": 48.41277186487737,
      "grad_norm": 0.005028332117944956,
      "learning_rate": 0.0031744562702452567,
      "loss": 0.0001,
      "step": 104620
    },
    {
      "epoch": 48.41739935215178,
      "grad_norm": 0.020559845492243767,
      "learning_rate": 0.0031652012956964373,
      "loss": 0.0001,
      "step": 104630
    },
    {
      "epoch": 48.422026839426195,
      "grad_norm": 1.5921176671981812,
      "learning_rate": 0.003155946321147617,
      "loss": 0.0003,
      "step": 104640
    },
    {
      "epoch": 48.4266543267006,
      "grad_norm": 0.03180413320660591,
      "learning_rate": 0.003146691346598797,
      "loss": 0.0,
      "step": 104650
    },
    {
      "epoch": 48.43128181397501,
      "grad_norm": 0.052405621856451035,
      "learning_rate": 0.0031374363720499767,
      "loss": 0.0001,
      "step": 104660
    },
    {
      "epoch": 48.435909301249424,
      "grad_norm": 0.09894368052482605,
      "learning_rate": 0.0031281813975011573,
      "loss": 0.0001,
      "step": 104670
    },
    {
      "epoch": 48.44053678852383,
      "grad_norm": 0.009744376875460148,
      "learning_rate": 0.0031189264229523374,
      "loss": 0.0,
      "step": 104680
    },
    {
      "epoch": 48.44516427579824,
      "grad_norm": 0.03685285151004791,
      "learning_rate": 0.003109671448403517,
      "loss": 0.0,
      "step": 104690
    },
    {
      "epoch": 48.44979176307265,
      "grad_norm": 0.0023152802605181932,
      "learning_rate": 0.0031004164738546967,
      "loss": 0.0,
      "step": 104700
    },
    {
      "epoch": 48.45441925034706,
      "grad_norm": 0.0033884302247315645,
      "learning_rate": 0.003091161499305877,
      "loss": 0.0001,
      "step": 104710
    },
    {
      "epoch": 48.45904673762147,
      "grad_norm": 0.14673009514808655,
      "learning_rate": 0.003081906524757057,
      "loss": 0.0001,
      "step": 104720
    },
    {
      "epoch": 48.46367422489588,
      "grad_norm": 0.007393083069473505,
      "learning_rate": 0.003072651550208237,
      "loss": 0.0003,
      "step": 104730
    },
    {
      "epoch": 48.468301712170295,
      "grad_norm": 0.00260463310405612,
      "learning_rate": 0.003063396575659417,
      "loss": 0.0001,
      "step": 104740
    },
    {
      "epoch": 48.4729291994447,
      "grad_norm": 0.0049126544035971165,
      "learning_rate": 0.003054141601110597,
      "loss": 0.0001,
      "step": 104750
    },
    {
      "epoch": 48.47755668671911,
      "grad_norm": 0.005261933896690607,
      "learning_rate": 0.003044886626561777,
      "loss": 0.0,
      "step": 104760
    },
    {
      "epoch": 48.482184173993524,
      "grad_norm": 0.053707875311374664,
      "learning_rate": 0.003035631652012957,
      "loss": 0.0001,
      "step": 104770
    },
    {
      "epoch": 48.48681166126793,
      "grad_norm": 0.009658190421760082,
      "learning_rate": 0.003026376677464137,
      "loss": 0.0001,
      "step": 104780
    },
    {
      "epoch": 48.49143914854234,
      "grad_norm": 0.0035421366337686777,
      "learning_rate": 0.0030171217029153173,
      "loss": 0.0002,
      "step": 104790
    },
    {
      "epoch": 48.49606663581675,
      "grad_norm": 0.00707288971170783,
      "learning_rate": 0.003007866728366497,
      "loss": 0.0,
      "step": 104800
    },
    {
      "epoch": 48.500694123091165,
      "grad_norm": 0.003375530941411853,
      "learning_rate": 0.002998611753817677,
      "loss": 0.0,
      "step": 104810
    },
    {
      "epoch": 48.50532161036557,
      "grad_norm": 0.007537281606346369,
      "learning_rate": 0.002989356779268857,
      "loss": 0.0001,
      "step": 104820
    },
    {
      "epoch": 48.50994909763998,
      "grad_norm": 0.005140988156199455,
      "learning_rate": 0.0029801018047200373,
      "loss": 0.0001,
      "step": 104830
    },
    {
      "epoch": 48.514576584914394,
      "grad_norm": 0.0012266458943486214,
      "learning_rate": 0.0029708468301712174,
      "loss": 0.0001,
      "step": 104840
    },
    {
      "epoch": 48.5192040721888,
      "grad_norm": 0.000957256241235882,
      "learning_rate": 0.002961591855622397,
      "loss": 0.0001,
      "step": 104850
    },
    {
      "epoch": 48.52383155946321,
      "grad_norm": 0.004801925737410784,
      "learning_rate": 0.0029523368810735772,
      "loss": 0.0001,
      "step": 104860
    },
    {
      "epoch": 48.52845904673762,
      "grad_norm": 0.0019613413605839014,
      "learning_rate": 0.0029430819065247573,
      "loss": 0.0001,
      "step": 104870
    },
    {
      "epoch": 48.53308653401203,
      "grad_norm": 0.0014953728532418609,
      "learning_rate": 0.0029338269319759375,
      "loss": 0.0,
      "step": 104880
    },
    {
      "epoch": 48.53771402128644,
      "grad_norm": 0.022260194644331932,
      "learning_rate": 0.0029245719574271176,
      "loss": 0.0001,
      "step": 104890
    },
    {
      "epoch": 48.54234150856085,
      "grad_norm": 0.012213964015245438,
      "learning_rate": 0.0029153169828782972,
      "loss": 0.0001,
      "step": 104900
    },
    {
      "epoch": 48.546968995835265,
      "grad_norm": 0.006199713330715895,
      "learning_rate": 0.0029060620083294774,
      "loss": 0.0,
      "step": 104910
    },
    {
      "epoch": 48.55159648310967,
      "grad_norm": 0.007055521477013826,
      "learning_rate": 0.002896807033780657,
      "loss": 0.0003,
      "step": 104920
    },
    {
      "epoch": 48.55622397038408,
      "grad_norm": 0.00832358282059431,
      "learning_rate": 0.002887552059231837,
      "loss": 0.0001,
      "step": 104930
    },
    {
      "epoch": 48.560851457658494,
      "grad_norm": 0.007597262971103191,
      "learning_rate": 0.0028782970846830173,
      "loss": 0.0,
      "step": 104940
    },
    {
      "epoch": 48.5654789449329,
      "grad_norm": 0.0036145560443401337,
      "learning_rate": 0.0028690421101341974,
      "loss": 0.0001,
      "step": 104950
    },
    {
      "epoch": 48.57010643220731,
      "grad_norm": 0.029177729040384293,
      "learning_rate": 0.002859787135585377,
      "loss": 0.0001,
      "step": 104960
    },
    {
      "epoch": 48.57473391948172,
      "grad_norm": 0.05204901471734047,
      "learning_rate": 0.002850532161036557,
      "loss": 0.0,
      "step": 104970
    },
    {
      "epoch": 48.579361406756135,
      "grad_norm": 0.0014041680842638016,
      "learning_rate": 0.0028412771864877373,
      "loss": 0.0002,
      "step": 104980
    },
    {
      "epoch": 48.58398889403054,
      "grad_norm": 0.005321221891790628,
      "learning_rate": 0.0028320222119389174,
      "loss": 0.0001,
      "step": 104990
    },
    {
      "epoch": 48.58861638130495,
      "grad_norm": 0.011227321811020374,
      "learning_rate": 0.0028227672373900975,
      "loss": 0.0003,
      "step": 105000
    },
    {
      "epoch": 48.593243868579364,
      "grad_norm": 0.02290276437997818,
      "learning_rate": 0.002813512262841277,
      "loss": 0.0,
      "step": 105010
    },
    {
      "epoch": 48.59787135585377,
      "grad_norm": 0.011660107411444187,
      "learning_rate": 0.0028042572882924573,
      "loss": 0.0001,
      "step": 105020
    },
    {
      "epoch": 48.60249884312818,
      "grad_norm": 0.011118879541754723,
      "learning_rate": 0.0027950023137436374,
      "loss": 0.0001,
      "step": 105030
    },
    {
      "epoch": 48.60712633040259,
      "grad_norm": 0.002394498558714986,
      "learning_rate": 0.0027857473391948175,
      "loss": 0.0,
      "step": 105040
    },
    {
      "epoch": 48.611753817677,
      "grad_norm": 0.08172925561666489,
      "learning_rate": 0.0027764923646459976,
      "loss": 0.0001,
      "step": 105050
    },
    {
      "epoch": 48.61638130495141,
      "grad_norm": 0.0023601774591952562,
      "learning_rate": 0.0027672373900971773,
      "loss": 0.0001,
      "step": 105060
    },
    {
      "epoch": 48.62100879222582,
      "grad_norm": 0.004126683343201876,
      "learning_rate": 0.0027579824155483574,
      "loss": 0.0001,
      "step": 105070
    },
    {
      "epoch": 48.625636279500235,
      "grad_norm": 0.005733179394155741,
      "learning_rate": 0.0027487274409995375,
      "loss": 0.0001,
      "step": 105080
    },
    {
      "epoch": 48.63026376677464,
      "grad_norm": 0.000914035364985466,
      "learning_rate": 0.0027394724664507176,
      "loss": 0.0084,
      "step": 105090
    },
    {
      "epoch": 48.63489125404905,
      "grad_norm": 0.004892885219305754,
      "learning_rate": 0.0027302174919018977,
      "loss": 0.0,
      "step": 105100
    },
    {
      "epoch": 48.639518741323464,
      "grad_norm": 0.004679740872234106,
      "learning_rate": 0.0027209625173530774,
      "loss": 0.0002,
      "step": 105110
    },
    {
      "epoch": 48.64414622859787,
      "grad_norm": 0.0033088442869484425,
      "learning_rate": 0.0027117075428042575,
      "loss": 0.0001,
      "step": 105120
    },
    {
      "epoch": 48.64877371587228,
      "grad_norm": 0.028232524171471596,
      "learning_rate": 0.002702452568255437,
      "loss": 0.0002,
      "step": 105130
    },
    {
      "epoch": 48.65340120314669,
      "grad_norm": 0.020074043422937393,
      "learning_rate": 0.0026931975937066173,
      "loss": 0.0001,
      "step": 105140
    },
    {
      "epoch": 48.6580286904211,
      "grad_norm": 0.024521781131625175,
      "learning_rate": 0.0026839426191577974,
      "loss": 0.0002,
      "step": 105150
    },
    {
      "epoch": 48.66265617769551,
      "grad_norm": 0.003130704164505005,
      "learning_rate": 0.0026746876446089776,
      "loss": 0.0,
      "step": 105160
    },
    {
      "epoch": 48.66728366496992,
      "grad_norm": 0.00417283596470952,
      "learning_rate": 0.0026654326700601572,
      "loss": 0.0001,
      "step": 105170
    },
    {
      "epoch": 48.671911152244334,
      "grad_norm": 0.006276222411543131,
      "learning_rate": 0.0026561776955113373,
      "loss": 0.0,
      "step": 105180
    },
    {
      "epoch": 48.67653863951874,
      "grad_norm": 0.018724732100963593,
      "learning_rate": 0.0026469227209625175,
      "loss": 0.0001,
      "step": 105190
    },
    {
      "epoch": 48.68116612679315,
      "grad_norm": 0.009810966439545155,
      "learning_rate": 0.0026376677464136976,
      "loss": 0.0001,
      "step": 105200
    },
    {
      "epoch": 48.68579361406756,
      "grad_norm": 0.0016365901101380587,
      "learning_rate": 0.0026284127718648777,
      "loss": 0.0001,
      "step": 105210
    },
    {
      "epoch": 48.69042110134197,
      "grad_norm": 0.007156056817620993,
      "learning_rate": 0.0026191577973160574,
      "loss": 0.0001,
      "step": 105220
    },
    {
      "epoch": 48.69504858861638,
      "grad_norm": 0.019197652116417885,
      "learning_rate": 0.0026099028227672375,
      "loss": 0.0001,
      "step": 105230
    },
    {
      "epoch": 48.69967607589079,
      "grad_norm": 0.01794869638979435,
      "learning_rate": 0.0026006478482184176,
      "loss": 0.0001,
      "step": 105240
    },
    {
      "epoch": 48.704303563165205,
      "grad_norm": 0.004240605980157852,
      "learning_rate": 0.0025913928736695977,
      "loss": 0.0003,
      "step": 105250
    },
    {
      "epoch": 48.70893105043961,
      "grad_norm": 0.0031734039075672626,
      "learning_rate": 0.002582137899120778,
      "loss": 0.0003,
      "step": 105260
    },
    {
      "epoch": 48.71355853771402,
      "grad_norm": 0.0036815940402448177,
      "learning_rate": 0.0025728829245719575,
      "loss": 0.0001,
      "step": 105270
    },
    {
      "epoch": 48.718186024988434,
      "grad_norm": 0.004475629422813654,
      "learning_rate": 0.0025636279500231376,
      "loss": 0.0002,
      "step": 105280
    },
    {
      "epoch": 48.72281351226284,
      "grad_norm": 0.058168165385723114,
      "learning_rate": 0.0025543729754743177,
      "loss": 0.0001,
      "step": 105290
    },
    {
      "epoch": 48.72744099953725,
      "grad_norm": 0.013106286525726318,
      "learning_rate": 0.002545118000925498,
      "loss": 0.0,
      "step": 105300
    },
    {
      "epoch": 48.73206848681166,
      "grad_norm": 0.002984635764732957,
      "learning_rate": 0.002535863026376678,
      "loss": 0.0,
      "step": 105310
    },
    {
      "epoch": 48.73669597408607,
      "grad_norm": 0.04996585100889206,
      "learning_rate": 0.0025266080518278576,
      "loss": 0.0001,
      "step": 105320
    },
    {
      "epoch": 48.74132346136048,
      "grad_norm": 0.0027743878308683634,
      "learning_rate": 0.0025173530772790377,
      "loss": 0.0001,
      "step": 105330
    },
    {
      "epoch": 48.74595094863489,
      "grad_norm": 0.0014275879366323352,
      "learning_rate": 0.002508098102730218,
      "loss": 0.0001,
      "step": 105340
    },
    {
      "epoch": 48.750578435909304,
      "grad_norm": 0.003370443591848016,
      "learning_rate": 0.0024988431281813975,
      "loss": 0.0001,
      "step": 105350
    },
    {
      "epoch": 48.75520592318371,
      "grad_norm": 0.07601820677518845,
      "learning_rate": 0.0024895881536325776,
      "loss": 0.0001,
      "step": 105360
    },
    {
      "epoch": 48.75983341045812,
      "grad_norm": 0.01727650873363018,
      "learning_rate": 0.0024803331790837577,
      "loss": 0.0001,
      "step": 105370
    },
    {
      "epoch": 48.76446089773253,
      "grad_norm": 0.004089971538633108,
      "learning_rate": 0.0024710782045349374,
      "loss": 0.0002,
      "step": 105380
    },
    {
      "epoch": 48.76908838500694,
      "grad_norm": 0.07641084492206573,
      "learning_rate": 0.0024618232299861175,
      "loss": 0.0001,
      "step": 105390
    },
    {
      "epoch": 48.77371587228135,
      "grad_norm": 0.0012084502959623933,
      "learning_rate": 0.0024525682554372976,
      "loss": 0.0001,
      "step": 105400
    },
    {
      "epoch": 48.77834335955576,
      "grad_norm": 0.08736037462949753,
      "learning_rate": 0.0024433132808884778,
      "loss": 0.0001,
      "step": 105410
    },
    {
      "epoch": 48.782970846830175,
      "grad_norm": 0.013864166103303432,
      "learning_rate": 0.002434058306339658,
      "loss": 0.001,
      "step": 105420
    },
    {
      "epoch": 48.78759833410458,
      "grad_norm": 0.0022285066079348326,
      "learning_rate": 0.0024248033317908375,
      "loss": 0.0001,
      "step": 105430
    },
    {
      "epoch": 48.79222582137899,
      "grad_norm": 0.0015868080081418157,
      "learning_rate": 0.0024155483572420177,
      "loss": 0.0,
      "step": 105440
    },
    {
      "epoch": 48.796853308653404,
      "grad_norm": 0.005767457652837038,
      "learning_rate": 0.0024062933826931978,
      "loss": 0.0001,
      "step": 105450
    },
    {
      "epoch": 48.80148079592781,
      "grad_norm": 0.0022136433981359005,
      "learning_rate": 0.002397038408144378,
      "loss": 0.0002,
      "step": 105460
    },
    {
      "epoch": 48.80610828320222,
      "grad_norm": 0.006676699500530958,
      "learning_rate": 0.002387783433595558,
      "loss": 0.0,
      "step": 105470
    },
    {
      "epoch": 48.81073577047663,
      "grad_norm": 0.04103979468345642,
      "learning_rate": 0.0023785284590467377,
      "loss": 0.0,
      "step": 105480
    },
    {
      "epoch": 48.81536325775104,
      "grad_norm": 0.006574276369065046,
      "learning_rate": 0.0023692734844979178,
      "loss": 0.0001,
      "step": 105490
    },
    {
      "epoch": 48.81999074502545,
      "grad_norm": 0.0023283688351511955,
      "learning_rate": 0.002360018509949098,
      "loss": 0.0001,
      "step": 105500
    },
    {
      "epoch": 48.82461823229986,
      "grad_norm": 0.0044921524822711945,
      "learning_rate": 0.002350763535400278,
      "loss": 0.0001,
      "step": 105510
    },
    {
      "epoch": 48.829245719574274,
      "grad_norm": 0.012214562855660915,
      "learning_rate": 0.0023415085608514577,
      "loss": 0.0004,
      "step": 105520
    },
    {
      "epoch": 48.83387320684868,
      "grad_norm": 0.0020830254070460796,
      "learning_rate": 0.002332253586302638,
      "loss": 0.0001,
      "step": 105530
    },
    {
      "epoch": 48.83850069412309,
      "grad_norm": 0.000829239608719945,
      "learning_rate": 0.002322998611753818,
      "loss": 0.0001,
      "step": 105540
    },
    {
      "epoch": 48.8431281813975,
      "grad_norm": 0.007442586589604616,
      "learning_rate": 0.002313743637204998,
      "loss": 0.0001,
      "step": 105550
    },
    {
      "epoch": 48.84775566867191,
      "grad_norm": 0.01076856255531311,
      "learning_rate": 0.0023044886626561777,
      "loss": 0.0002,
      "step": 105560
    },
    {
      "epoch": 48.85238315594632,
      "grad_norm": 0.003141120309010148,
      "learning_rate": 0.002295233688107358,
      "loss": 0.0001,
      "step": 105570
    },
    {
      "epoch": 48.85701064322073,
      "grad_norm": 0.018430931493639946,
      "learning_rate": 0.0022859787135585375,
      "loss": 0.0001,
      "step": 105580
    },
    {
      "epoch": 48.861638130495145,
      "grad_norm": 0.013423001393675804,
      "learning_rate": 0.0022767237390097176,
      "loss": 0.0003,
      "step": 105590
    },
    {
      "epoch": 48.86626561776955,
      "grad_norm": 0.007647051941603422,
      "learning_rate": 0.0022674687644608977,
      "loss": 0.0001,
      "step": 105600
    },
    {
      "epoch": 48.87089310504396,
      "grad_norm": 0.002604959299787879,
      "learning_rate": 0.002258213789912078,
      "loss": 0.0,
      "step": 105610
    },
    {
      "epoch": 48.875520592318374,
      "grad_norm": 0.03452096879482269,
      "learning_rate": 0.002248958815363258,
      "loss": 0.0003,
      "step": 105620
    },
    {
      "epoch": 48.88014807959278,
      "grad_norm": 0.007269646972417831,
      "learning_rate": 0.0022397038408144376,
      "loss": 0.0001,
      "step": 105630
    },
    {
      "epoch": 48.88477556686719,
      "grad_norm": 0.533481240272522,
      "learning_rate": 0.0022304488662656177,
      "loss": 0.0003,
      "step": 105640
    },
    {
      "epoch": 48.8894030541416,
      "grad_norm": 0.036019232124090195,
      "learning_rate": 0.002221193891716798,
      "loss": 0.0001,
      "step": 105650
    },
    {
      "epoch": 48.89403054141601,
      "grad_norm": 0.031094759702682495,
      "learning_rate": 0.002211938917167978,
      "loss": 0.0,
      "step": 105660
    },
    {
      "epoch": 48.89865802869042,
      "grad_norm": 0.0045306384563446045,
      "learning_rate": 0.002202683942619158,
      "loss": 0.0001,
      "step": 105670
    },
    {
      "epoch": 48.90328551596483,
      "grad_norm": 0.002630307339131832,
      "learning_rate": 0.0021934289680703377,
      "loss": 0.0,
      "step": 105680
    },
    {
      "epoch": 48.907913003239244,
      "grad_norm": 0.009148265235126019,
      "learning_rate": 0.002184173993521518,
      "loss": 0.0001,
      "step": 105690
    },
    {
      "epoch": 48.91254049051365,
      "grad_norm": 0.00989043340086937,
      "learning_rate": 0.002174919018972698,
      "loss": 0.0001,
      "step": 105700
    },
    {
      "epoch": 48.91716797778806,
      "grad_norm": 0.008352951146662235,
      "learning_rate": 0.002165664044423878,
      "loss": 0.0001,
      "step": 105710
    },
    {
      "epoch": 48.92179546506247,
      "grad_norm": 0.002772762905806303,
      "learning_rate": 0.002156409069875058,
      "loss": 0.0001,
      "step": 105720
    },
    {
      "epoch": 48.92642295233688,
      "grad_norm": 0.0074187652207911015,
      "learning_rate": 0.002147154095326238,
      "loss": 0.0001,
      "step": 105730
    },
    {
      "epoch": 48.93105043961129,
      "grad_norm": 0.00406884727999568,
      "learning_rate": 0.002137899120777418,
      "loss": 0.0002,
      "step": 105740
    },
    {
      "epoch": 48.9356779268857,
      "grad_norm": 0.0451161190867424,
      "learning_rate": 0.002128644146228598,
      "loss": 0.0,
      "step": 105750
    },
    {
      "epoch": 48.94030541416011,
      "grad_norm": 0.003911131992936134,
      "learning_rate": 0.002119389171679778,
      "loss": 0.0002,
      "step": 105760
    },
    {
      "epoch": 48.94493290143452,
      "grad_norm": 0.0018721550004556775,
      "learning_rate": 0.0021101341971309583,
      "loss": 0.0001,
      "step": 105770
    },
    {
      "epoch": 48.94956038870893,
      "grad_norm": 0.0008682585321366787,
      "learning_rate": 0.002100879222582138,
      "loss": 0.0001,
      "step": 105780
    },
    {
      "epoch": 48.954187875983344,
      "grad_norm": 0.0299424696713686,
      "learning_rate": 0.0020916242480333177,
      "loss": 0.0001,
      "step": 105790
    },
    {
      "epoch": 48.95881536325775,
      "grad_norm": 0.022355256602168083,
      "learning_rate": 0.0020823692734844978,
      "loss": 0.0001,
      "step": 105800
    },
    {
      "epoch": 48.96344285053216,
      "grad_norm": 0.05071333423256874,
      "learning_rate": 0.002073114298935678,
      "loss": 0.0002,
      "step": 105810
    },
    {
      "epoch": 48.96807033780657,
      "grad_norm": 0.010831573978066444,
      "learning_rate": 0.002063859324386858,
      "loss": 0.0001,
      "step": 105820
    },
    {
      "epoch": 48.97269782508098,
      "grad_norm": 0.010008743032813072,
      "learning_rate": 0.002054604349838038,
      "loss": 0.0,
      "step": 105830
    },
    {
      "epoch": 48.97732531235539,
      "grad_norm": 0.01283964142203331,
      "learning_rate": 0.002045349375289218,
      "loss": 0.0,
      "step": 105840
    },
    {
      "epoch": 48.9819527996298,
      "grad_norm": 0.0062320781871676445,
      "learning_rate": 0.002036094400740398,
      "loss": 0.0001,
      "step": 105850
    },
    {
      "epoch": 48.986580286904214,
      "grad_norm": 0.009116853587329388,
      "learning_rate": 0.002026839426191578,
      "loss": 0.0001,
      "step": 105860
    },
    {
      "epoch": 48.99120777417862,
      "grad_norm": 0.0027903704904019833,
      "learning_rate": 0.002017584451642758,
      "loss": 0.0001,
      "step": 105870
    },
    {
      "epoch": 48.99583526145303,
      "grad_norm": 0.029138395562767982,
      "learning_rate": 0.0020083294770939382,
      "loss": 0.0001,
      "step": 105880
    },
    {
      "epoch": 49.0,
      "eval_accuracy_branch1": 0.9889847481127715,
      "eval_accuracy_branch2": 0.5004621784008627,
      "eval_f1_branch1": 0.9895755616213598,
      "eval_f1_branch2": 0.5003775078898387,
      "eval_loss": 0.022443076595664024,
      "eval_precision_branch1": 0.9894973122271117,
      "eval_precision_branch2": 0.5004624919129823,
      "eval_recall_branch1": 0.9898302729404861,
      "eval_recall_branch2": 0.5004621784008627,
      "eval_runtime": 28.9488,
      "eval_samples_per_second": 448.447,
      "eval_steps_per_second": 56.065,
      "step": 105889
    },
    {
      "epoch": 49.00046274872744,
      "grad_norm": 0.0023547960445284843,
      "learning_rate": 0.001999074502545118,
      "loss": 0.0414,
      "step": 105890
    },
    {
      "epoch": 49.00509023600185,
      "grad_norm": 0.3479999899864197,
      "learning_rate": 0.001989819527996298,
      "loss": 0.0002,
      "step": 105900
    },
    {
      "epoch": 49.00971772327626,
      "grad_norm": 0.001571531523950398,
      "learning_rate": 0.001980564553447478,
      "loss": 0.0005,
      "step": 105910
    },
    {
      "epoch": 49.01434521055067,
      "grad_norm": 0.0072942087426781654,
      "learning_rate": 0.0019713095788986583,
      "loss": 0.0001,
      "step": 105920
    },
    {
      "epoch": 49.01897269782508,
      "grad_norm": 0.030488189309835434,
      "learning_rate": 0.0019620546043498384,
      "loss": 0.0001,
      "step": 105930
    },
    {
      "epoch": 49.02360018509949,
      "grad_norm": 0.009212939068675041,
      "learning_rate": 0.0019527996298010183,
      "loss": 0.0001,
      "step": 105940
    },
    {
      "epoch": 49.0282276723739,
      "grad_norm": 0.0018117123981937766,
      "learning_rate": 0.0019435446552521982,
      "loss": 0.0002,
      "step": 105950
    },
    {
      "epoch": 49.032855159648314,
      "grad_norm": 0.0011562445433810353,
      "learning_rate": 0.0019342896807033783,
      "loss": 0.0001,
      "step": 105960
    },
    {
      "epoch": 49.03748264692272,
      "grad_norm": 0.0023691754322499037,
      "learning_rate": 0.0019250347061545584,
      "loss": 0.0001,
      "step": 105970
    },
    {
      "epoch": 49.04211013419713,
      "grad_norm": 0.005592921748757362,
      "learning_rate": 0.0019157797316057383,
      "loss": 0.0,
      "step": 105980
    },
    {
      "epoch": 49.04673762147154,
      "grad_norm": 0.010843816213309765,
      "learning_rate": 0.0019065247570569184,
      "loss": 0.0001,
      "step": 105990
    },
    {
      "epoch": 49.05136510874595,
      "grad_norm": 0.0024120095185935497,
      "learning_rate": 0.001897269782508098,
      "loss": 0.0001,
      "step": 106000
    },
    {
      "epoch": 49.05599259602036,
      "grad_norm": 0.047554586082696915,
      "learning_rate": 0.001888014807959278,
      "loss": 0.0001,
      "step": 106010
    },
    {
      "epoch": 49.06062008329477,
      "grad_norm": 0.00681673176586628,
      "learning_rate": 0.001878759833410458,
      "loss": 0.0,
      "step": 106020
    },
    {
      "epoch": 49.065247570569184,
      "grad_norm": 0.008484018966555595,
      "learning_rate": 0.0018695048588616382,
      "loss": 0.0,
      "step": 106030
    },
    {
      "epoch": 49.06987505784359,
      "grad_norm": 0.0016014601569622755,
      "learning_rate": 0.001860249884312818,
      "loss": 0.0001,
      "step": 106040
    },
    {
      "epoch": 49.074502545118,
      "grad_norm": 0.03470299020409584,
      "learning_rate": 0.0018509949097639982,
      "loss": 0.0001,
      "step": 106050
    },
    {
      "epoch": 49.07913003239241,
      "grad_norm": 0.0030145575292408466,
      "learning_rate": 0.001841739935215178,
      "loss": 0.0,
      "step": 106060
    },
    {
      "epoch": 49.08375751966682,
      "grad_norm": 0.011742272414267063,
      "learning_rate": 0.0018324849606663582,
      "loss": 0.0,
      "step": 106070
    },
    {
      "epoch": 49.08838500694123,
      "grad_norm": 0.004625845700502396,
      "learning_rate": 0.0018232299861175383,
      "loss": 0.0001,
      "step": 106080
    },
    {
      "epoch": 49.09301249421564,
      "grad_norm": 0.0015219266060739756,
      "learning_rate": 0.0018139750115687182,
      "loss": 0.0023,
      "step": 106090
    },
    {
      "epoch": 49.09763998149005,
      "grad_norm": 0.010556025430560112,
      "learning_rate": 0.0018047200370198983,
      "loss": 0.0001,
      "step": 106100
    },
    {
      "epoch": 49.10226746876446,
      "grad_norm": 0.008422826416790485,
      "learning_rate": 0.0017954650624710782,
      "loss": 0.0001,
      "step": 106110
    },
    {
      "epoch": 49.10689495603887,
      "grad_norm": 0.0006657165358774364,
      "learning_rate": 0.0017862100879222583,
      "loss": 0.0001,
      "step": 106120
    },
    {
      "epoch": 49.111522443313284,
      "grad_norm": 0.004592760931700468,
      "learning_rate": 0.0017769551133734384,
      "loss": 0.0001,
      "step": 106130
    },
    {
      "epoch": 49.11614993058769,
      "grad_norm": 0.006354017183184624,
      "learning_rate": 0.0017677001388246183,
      "loss": 0.0003,
      "step": 106140
    },
    {
      "epoch": 49.1207774178621,
      "grad_norm": 0.0026246130000799894,
      "learning_rate": 0.0017584451642757984,
      "loss": 0.0001,
      "step": 106150
    },
    {
      "epoch": 49.12540490513651,
      "grad_norm": 0.005387979093939066,
      "learning_rate": 0.0017491901897269783,
      "loss": 0.0,
      "step": 106160
    },
    {
      "epoch": 49.13003239241092,
      "grad_norm": 0.006406567525118589,
      "learning_rate": 0.0017399352151781585,
      "loss": 0.0001,
      "step": 106170
    },
    {
      "epoch": 49.13465987968533,
      "grad_norm": 0.002795260166749358,
      "learning_rate": 0.0017306802406293386,
      "loss": 0.0001,
      "step": 106180
    },
    {
      "epoch": 49.13928736695974,
      "grad_norm": 0.09341083467006683,
      "learning_rate": 0.0017214252660805185,
      "loss": 0.0001,
      "step": 106190
    },
    {
      "epoch": 49.143914854234154,
      "grad_norm": 0.0021640039049088955,
      "learning_rate": 0.0017121702915316986,
      "loss": 0.0032,
      "step": 106200
    },
    {
      "epoch": 49.14854234150856,
      "grad_norm": 0.0023087512236088514,
      "learning_rate": 0.0017029153169828783,
      "loss": 0.0001,
      "step": 106210
    },
    {
      "epoch": 49.15316982878297,
      "grad_norm": 0.0028156309854239225,
      "learning_rate": 0.0016936603424340581,
      "loss": 0.0001,
      "step": 106220
    },
    {
      "epoch": 49.15779731605738,
      "grad_norm": 0.014394630677998066,
      "learning_rate": 0.0016844053678852383,
      "loss": 0.0002,
      "step": 106230
    },
    {
      "epoch": 49.16242480333179,
      "grad_norm": 0.007949303835630417,
      "learning_rate": 0.0016751503933364184,
      "loss": 0.0001,
      "step": 106240
    },
    {
      "epoch": 49.1670522906062,
      "grad_norm": 0.0044232746586203575,
      "learning_rate": 0.0016658954187875983,
      "loss": 0.0001,
      "step": 106250
    },
    {
      "epoch": 49.17167977788061,
      "grad_norm": 1.0712640285491943,
      "learning_rate": 0.0016566404442387784,
      "loss": 0.0003,
      "step": 106260
    },
    {
      "epoch": 49.17630726515502,
      "grad_norm": 0.005995175335556269,
      "learning_rate": 0.0016473854696899583,
      "loss": 0.0001,
      "step": 106270
    },
    {
      "epoch": 49.18093475242943,
      "grad_norm": 0.0010013183346018195,
      "learning_rate": 0.0016381304951411384,
      "loss": 0.0,
      "step": 106280
    },
    {
      "epoch": 49.18556223970384,
      "grad_norm": 0.03644447773694992,
      "learning_rate": 0.0016288755205923185,
      "loss": 0.0001,
      "step": 106290
    },
    {
      "epoch": 49.190189726978254,
      "grad_norm": 0.031876061111688614,
      "learning_rate": 0.0016196205460434984,
      "loss": 0.0001,
      "step": 106300
    },
    {
      "epoch": 49.19481721425266,
      "grad_norm": 0.01122814230620861,
      "learning_rate": 0.0016103655714946785,
      "loss": 0.0001,
      "step": 106310
    },
    {
      "epoch": 49.19944470152707,
      "grad_norm": 0.005646042060106993,
      "learning_rate": 0.0016011105969458584,
      "loss": 0.0003,
      "step": 106320
    },
    {
      "epoch": 49.20407218880148,
      "grad_norm": 0.004822553135454655,
      "learning_rate": 0.0015918556223970385,
      "loss": 0.0,
      "step": 106330
    },
    {
      "epoch": 49.20869967607589,
      "grad_norm": 0.9901242852210999,
      "learning_rate": 0.0015826006478482186,
      "loss": 0.0002,
      "step": 106340
    },
    {
      "epoch": 49.2133271633503,
      "grad_norm": 0.010310947895050049,
      "learning_rate": 0.0015733456732993985,
      "loss": 0.0001,
      "step": 106350
    },
    {
      "epoch": 49.21795465062471,
      "grad_norm": 0.0022941301576793194,
      "learning_rate": 0.0015640906987505786,
      "loss": 0.0001,
      "step": 106360
    },
    {
      "epoch": 49.222582137899124,
      "grad_norm": 0.001798782148398459,
      "learning_rate": 0.0015548357242017585,
      "loss": 0.0001,
      "step": 106370
    },
    {
      "epoch": 49.22720962517353,
      "grad_norm": 0.0010846274672076106,
      "learning_rate": 0.0015455807496529384,
      "loss": 0.0008,
      "step": 106380
    },
    {
      "epoch": 49.23183711244794,
      "grad_norm": 0.0024916508700698614,
      "learning_rate": 0.0015363257751041185,
      "loss": 0.0002,
      "step": 106390
    },
    {
      "epoch": 49.23646459972235,
      "grad_norm": 0.006364371627569199,
      "learning_rate": 0.0015270708005552984,
      "loss": 0.0001,
      "step": 106400
    },
    {
      "epoch": 49.24109208699676,
      "grad_norm": 0.005729435011744499,
      "learning_rate": 0.0015178158260064785,
      "loss": 0.0001,
      "step": 106410
    },
    {
      "epoch": 49.24571957427117,
      "grad_norm": 0.028150901198387146,
      "learning_rate": 0.0015085608514576587,
      "loss": 0.0,
      "step": 106420
    },
    {
      "epoch": 49.25034706154558,
      "grad_norm": 0.006365788634866476,
      "learning_rate": 0.0014993058769088386,
      "loss": 0.0001,
      "step": 106430
    },
    {
      "epoch": 49.25497454881999,
      "grad_norm": 0.008327807299792767,
      "learning_rate": 0.0014900509023600187,
      "loss": 0.0,
      "step": 106440
    },
    {
      "epoch": 49.2596020360944,
      "grad_norm": 0.005976437125355005,
      "learning_rate": 0.0014807959278111986,
      "loss": 0.0001,
      "step": 106450
    },
    {
      "epoch": 49.26422952336881,
      "grad_norm": 0.04849666729569435,
      "learning_rate": 0.0014715409532623787,
      "loss": 0.0001,
      "step": 106460
    },
    {
      "epoch": 49.268857010643224,
      "grad_norm": 0.008718292228877544,
      "learning_rate": 0.0014622859787135588,
      "loss": 0.0001,
      "step": 106470
    },
    {
      "epoch": 49.27348449791763,
      "grad_norm": 0.010507111437618732,
      "learning_rate": 0.0014530310041647387,
      "loss": 0.0,
      "step": 106480
    },
    {
      "epoch": 49.27811198519204,
      "grad_norm": 0.008917507715523243,
      "learning_rate": 0.0014437760296159186,
      "loss": 0.0,
      "step": 106490
    },
    {
      "epoch": 49.28273947246645,
      "grad_norm": 0.0040938579477369785,
      "learning_rate": 0.0014345210550670987,
      "loss": 0.0001,
      "step": 106500
    },
    {
      "epoch": 49.28736695974086,
      "grad_norm": 0.0032738372683525085,
      "learning_rate": 0.0014252660805182786,
      "loss": 0.0001,
      "step": 106510
    },
    {
      "epoch": 49.29199444701527,
      "grad_norm": 0.008253175765275955,
      "learning_rate": 0.0014160111059694587,
      "loss": 0.0001,
      "step": 106520
    },
    {
      "epoch": 49.29662193428968,
      "grad_norm": 0.017567437142133713,
      "learning_rate": 0.0014067561314206386,
      "loss": 0.0001,
      "step": 106530
    },
    {
      "epoch": 49.301249421564094,
      "grad_norm": 0.011018368415534496,
      "learning_rate": 0.0013975011568718187,
      "loss": 0.0003,
      "step": 106540
    },
    {
      "epoch": 49.3058769088385,
      "grad_norm": 0.00969371572136879,
      "learning_rate": 0.0013882461823229988,
      "loss": 0.0001,
      "step": 106550
    },
    {
      "epoch": 49.31050439611291,
      "grad_norm": 0.004241247661411762,
      "learning_rate": 0.0013789912077741787,
      "loss": 0.0001,
      "step": 106560
    },
    {
      "epoch": 49.31513188338732,
      "grad_norm": 0.030234985053539276,
      "learning_rate": 0.0013697362332253588,
      "loss": 0.0001,
      "step": 106570
    },
    {
      "epoch": 49.31975937066173,
      "grad_norm": 0.0045774271711707115,
      "learning_rate": 0.0013604812586765387,
      "loss": 0.0,
      "step": 106580
    },
    {
      "epoch": 49.32438685793614,
      "grad_norm": 0.06042912229895592,
      "learning_rate": 0.0013512262841277186,
      "loss": 0.0001,
      "step": 106590
    },
    {
      "epoch": 49.32901434521055,
      "grad_norm": 0.005885363556444645,
      "learning_rate": 0.0013419713095788987,
      "loss": 0.0003,
      "step": 106600
    },
    {
      "epoch": 49.33364183248496,
      "grad_norm": 0.0025954321026802063,
      "learning_rate": 0.0013327163350300786,
      "loss": 0.0001,
      "step": 106610
    },
    {
      "epoch": 49.33826931975937,
      "grad_norm": 0.0011436401400715113,
      "learning_rate": 0.0013234613604812587,
      "loss": 0.0001,
      "step": 106620
    },
    {
      "epoch": 49.34289680703378,
      "grad_norm": 0.002989106113091111,
      "learning_rate": 0.0013142063859324388,
      "loss": 0.0002,
      "step": 106630
    },
    {
      "epoch": 49.347524294308194,
      "grad_norm": 0.020504381507635117,
      "learning_rate": 0.0013049514113836187,
      "loss": 0.0008,
      "step": 106640
    },
    {
      "epoch": 49.3521517815826,
      "grad_norm": 0.42400896549224854,
      "learning_rate": 0.0012956964368347988,
      "loss": 0.0002,
      "step": 106650
    },
    {
      "epoch": 49.35677926885701,
      "grad_norm": 0.003788173198699951,
      "learning_rate": 0.0012864414622859787,
      "loss": 0.0029,
      "step": 106660
    },
    {
      "epoch": 49.36140675613142,
      "grad_norm": 0.00804431177675724,
      "learning_rate": 0.0012771864877371589,
      "loss": 0.0003,
      "step": 106670
    },
    {
      "epoch": 49.36603424340583,
      "grad_norm": 0.004366519395262003,
      "learning_rate": 0.001267931513188339,
      "loss": 0.0001,
      "step": 106680
    },
    {
      "epoch": 49.37066173068024,
      "grad_norm": 0.0015486212214455009,
      "learning_rate": 0.0012586765386395189,
      "loss": 0.0001,
      "step": 106690
    },
    {
      "epoch": 49.37528921795465,
      "grad_norm": 0.0073038493283092976,
      "learning_rate": 0.0012494215640906988,
      "loss": 0.0001,
      "step": 106700
    },
    {
      "epoch": 49.379916705229064,
      "grad_norm": 0.006533716339617968,
      "learning_rate": 0.0012401665895418789,
      "loss": 0.0001,
      "step": 106710
    },
    {
      "epoch": 49.38454419250347,
      "grad_norm": 0.06571106612682343,
      "learning_rate": 0.0012309116149930588,
      "loss": 0.0001,
      "step": 106720
    },
    {
      "epoch": 49.38917167977788,
      "grad_norm": 0.02663993462920189,
      "learning_rate": 0.0012216566404442389,
      "loss": 0.0001,
      "step": 106730
    },
    {
      "epoch": 49.39379916705229,
      "grad_norm": 0.01382965687662363,
      "learning_rate": 0.0012124016658954188,
      "loss": 0.0001,
      "step": 106740
    },
    {
      "epoch": 49.3984266543267,
      "grad_norm": 0.004648866597563028,
      "learning_rate": 0.0012031466913465989,
      "loss": 0.0001,
      "step": 106750
    },
    {
      "epoch": 49.40305414160111,
      "grad_norm": 0.002248334465548396,
      "learning_rate": 0.001193891716797779,
      "loss": 0.0001,
      "step": 106760
    },
    {
      "epoch": 49.40768162887552,
      "grad_norm": 0.006812435574829578,
      "learning_rate": 0.0011846367422489589,
      "loss": 0.0001,
      "step": 106770
    },
    {
      "epoch": 49.41230911614993,
      "grad_norm": 0.011008336208760738,
      "learning_rate": 0.001175381767700139,
      "loss": 0.0001,
      "step": 106780
    },
    {
      "epoch": 49.41693660342434,
      "grad_norm": 0.004300525411963463,
      "learning_rate": 0.001166126793151319,
      "loss": 0.0001,
      "step": 106790
    },
    {
      "epoch": 49.42156409069875,
      "grad_norm": 0.01616956852376461,
      "learning_rate": 0.001156871818602499,
      "loss": 0.0,
      "step": 106800
    },
    {
      "epoch": 49.426191577973164,
      "grad_norm": 0.0059271156787872314,
      "learning_rate": 0.001147616844053679,
      "loss": 0.0002,
      "step": 106810
    },
    {
      "epoch": 49.43081906524757,
      "grad_norm": 0.1065898984670639,
      "learning_rate": 0.0011383618695048588,
      "loss": 0.0001,
      "step": 106820
    },
    {
      "epoch": 49.43544655252198,
      "grad_norm": 0.006634896155446768,
      "learning_rate": 0.001129106894956039,
      "loss": 0.0001,
      "step": 106830
    },
    {
      "epoch": 49.44007403979639,
      "grad_norm": 0.02022424153983593,
      "learning_rate": 0.0011198519204072188,
      "loss": 0.0001,
      "step": 106840
    },
    {
      "epoch": 49.4447015270708,
      "grad_norm": 0.006077638827264309,
      "learning_rate": 0.001110596945858399,
      "loss": 0.0001,
      "step": 106850
    },
    {
      "epoch": 49.44932901434521,
      "grad_norm": 0.0007629476021975279,
      "learning_rate": 0.001101341971309579,
      "loss": 0.0,
      "step": 106860
    },
    {
      "epoch": 49.45395650161962,
      "grad_norm": 0.12950462102890015,
      "learning_rate": 0.001092086996760759,
      "loss": 0.0002,
      "step": 106870
    },
    {
      "epoch": 49.45858398889403,
      "grad_norm": 0.015381788834929466,
      "learning_rate": 0.001082832022211939,
      "loss": 0.0001,
      "step": 106880
    },
    {
      "epoch": 49.46321147616844,
      "grad_norm": 0.003495096694678068,
      "learning_rate": 0.001073577047663119,
      "loss": 0.0001,
      "step": 106890
    },
    {
      "epoch": 49.46783896344285,
      "grad_norm": 0.010033275000751019,
      "learning_rate": 0.001064322073114299,
      "loss": 0.0001,
      "step": 106900
    },
    {
      "epoch": 49.47246645071726,
      "grad_norm": 0.0022109034471213818,
      "learning_rate": 0.0010550670985654792,
      "loss": 0.0002,
      "step": 106910
    },
    {
      "epoch": 49.47709393799167,
      "grad_norm": 0.008342063054442406,
      "learning_rate": 0.0010458121240166588,
      "loss": 0.0001,
      "step": 106920
    },
    {
      "epoch": 49.48172142526608,
      "grad_norm": 0.004311056807637215,
      "learning_rate": 0.001036557149467839,
      "loss": 0.0,
      "step": 106930
    },
    {
      "epoch": 49.48634891254049,
      "grad_norm": 0.004023159854114056,
      "learning_rate": 0.001027302174919019,
      "loss": 0.0001,
      "step": 106940
    },
    {
      "epoch": 49.4909763998149,
      "grad_norm": 0.030168166384100914,
      "learning_rate": 0.001018047200370199,
      "loss": 0.0002,
      "step": 106950
    },
    {
      "epoch": 49.49560388708931,
      "grad_norm": 0.00924612581729889,
      "learning_rate": 0.001008792225821379,
      "loss": 0.0001,
      "step": 106960
    },
    {
      "epoch": 49.50023137436372,
      "grad_norm": 0.004088771529495716,
      "learning_rate": 0.000999537251272559,
      "loss": 0.0001,
      "step": 106970
    },
    {
      "epoch": 49.504858861638134,
      "grad_norm": 0.0023061493411660194,
      "learning_rate": 0.000990282276723739,
      "loss": 0.0001,
      "step": 106980
    },
    {
      "epoch": 49.50948634891254,
      "grad_norm": 0.0468272902071476,
      "learning_rate": 0.0009810273021749192,
      "loss": 0.0,
      "step": 106990
    },
    {
      "epoch": 49.51411383618695,
      "grad_norm": 0.025981254875659943,
      "learning_rate": 0.0009717723276260991,
      "loss": 0.0002,
      "step": 107000
    },
    {
      "epoch": 49.51874132346136,
      "grad_norm": 0.002975506242364645,
      "learning_rate": 0.0009625173530772792,
      "loss": 0.0001,
      "step": 107010
    },
    {
      "epoch": 49.52336881073577,
      "grad_norm": 0.012073544785380363,
      "learning_rate": 0.0009532623785284592,
      "loss": 0.0002,
      "step": 107020
    },
    {
      "epoch": 49.52799629801018,
      "grad_norm": 0.00801155250519514,
      "learning_rate": 0.000944007403979639,
      "loss": 0.0,
      "step": 107030
    },
    {
      "epoch": 49.53262378528459,
      "grad_norm": 0.01015397347509861,
      "learning_rate": 0.0009347524294308191,
      "loss": 0.0001,
      "step": 107040
    },
    {
      "epoch": 49.537251272559,
      "grad_norm": 0.003766778390854597,
      "learning_rate": 0.0009254974548819991,
      "loss": 0.0001,
      "step": 107050
    },
    {
      "epoch": 49.54187875983341,
      "grad_norm": 0.0021453227382153273,
      "learning_rate": 0.0009162424803331791,
      "loss": 0.0001,
      "step": 107060
    },
    {
      "epoch": 49.54650624710782,
      "grad_norm": 0.013612113893032074,
      "learning_rate": 0.0009069875057843591,
      "loss": 0.0001,
      "step": 107070
    },
    {
      "epoch": 49.55113373438223,
      "grad_norm": 0.1407947838306427,
      "learning_rate": 0.0008977325312355391,
      "loss": 0.0004,
      "step": 107080
    },
    {
      "epoch": 49.55576122165664,
      "grad_norm": 0.0037860176526010036,
      "learning_rate": 0.0008884775566867192,
      "loss": 0.0,
      "step": 107090
    },
    {
      "epoch": 49.56038870893105,
      "grad_norm": 0.03393051028251648,
      "learning_rate": 0.0008792225821378992,
      "loss": 0.0001,
      "step": 107100
    },
    {
      "epoch": 49.56501619620546,
      "grad_norm": 0.0074557154439389706,
      "learning_rate": 0.0008699676075890792,
      "loss": 0.0001,
      "step": 107110
    },
    {
      "epoch": 49.56964368347987,
      "grad_norm": 0.0035821315832436085,
      "learning_rate": 0.0008607126330402592,
      "loss": 0.0001,
      "step": 107120
    },
    {
      "epoch": 49.57427117075428,
      "grad_norm": 0.0041737183928489685,
      "learning_rate": 0.0008514576584914391,
      "loss": 0.0001,
      "step": 107130
    },
    {
      "epoch": 49.57889865802869,
      "grad_norm": 0.0030342894606292248,
      "learning_rate": 0.0008422026839426191,
      "loss": 0.0001,
      "step": 107140
    },
    {
      "epoch": 49.583526145303104,
      "grad_norm": 0.0045135593973100185,
      "learning_rate": 0.0008329477093937991,
      "loss": 0.0001,
      "step": 107150
    },
    {
      "epoch": 49.58815363257751,
      "grad_norm": 0.025543518364429474,
      "learning_rate": 0.0008236927348449791,
      "loss": 0.0001,
      "step": 107160
    },
    {
      "epoch": 49.59278111985192,
      "grad_norm": 0.00135531194973737,
      "learning_rate": 0.0008144377602961593,
      "loss": 0.0001,
      "step": 107170
    },
    {
      "epoch": 49.59740860712633,
      "grad_norm": 0.059617068618535995,
      "learning_rate": 0.0008051827857473393,
      "loss": 0.0001,
      "step": 107180
    },
    {
      "epoch": 49.60203609440074,
      "grad_norm": 0.014467093162238598,
      "learning_rate": 0.0007959278111985193,
      "loss": 0.0001,
      "step": 107190
    },
    {
      "epoch": 49.60666358167515,
      "grad_norm": 0.04383053258061409,
      "learning_rate": 0.0007866728366496993,
      "loss": 0.0001,
      "step": 107200
    },
    {
      "epoch": 49.61129106894956,
      "grad_norm": 0.0016846954822540283,
      "learning_rate": 0.0007774178621008793,
      "loss": 0.0001,
      "step": 107210
    },
    {
      "epoch": 49.61591855622397,
      "grad_norm": 0.005904922261834145,
      "learning_rate": 0.0007681628875520593,
      "loss": 0.0001,
      "step": 107220
    },
    {
      "epoch": 49.62054604349838,
      "grad_norm": 0.0034779072739183903,
      "learning_rate": 0.0007589079130032393,
      "loss": 0.0001,
      "step": 107230
    },
    {
      "epoch": 49.62517353077279,
      "grad_norm": 0.0014059311943128705,
      "learning_rate": 0.0007496529384544193,
      "loss": 0.0,
      "step": 107240
    },
    {
      "epoch": 49.6298010180472,
      "grad_norm": 0.010388493537902832,
      "learning_rate": 0.0007403979639055993,
      "loss": 0.0001,
      "step": 107250
    },
    {
      "epoch": 49.63442850532161,
      "grad_norm": 0.006098568439483643,
      "learning_rate": 0.0007311429893567794,
      "loss": 0.0001,
      "step": 107260
    },
    {
      "epoch": 49.63905599259602,
      "grad_norm": 0.0024304580874741077,
      "learning_rate": 0.0007218880148079593,
      "loss": 0.0001,
      "step": 107270
    },
    {
      "epoch": 49.64368347987043,
      "grad_norm": 0.0017724737990647554,
      "learning_rate": 0.0007126330402591393,
      "loss": 0.0,
      "step": 107280
    },
    {
      "epoch": 49.64831096714484,
      "grad_norm": 0.0039323316887021065,
      "learning_rate": 0.0007033780657103193,
      "loss": 0.0001,
      "step": 107290
    },
    {
      "epoch": 49.65293845441925,
      "grad_norm": 0.011477609165012836,
      "learning_rate": 0.0006941230911614994,
      "loss": 0.0001,
      "step": 107300
    },
    {
      "epoch": 49.65756594169366,
      "grad_norm": 0.010142391547560692,
      "learning_rate": 0.0006848681166126794,
      "loss": 0.0001,
      "step": 107310
    },
    {
      "epoch": 49.662193428968074,
      "grad_norm": 0.038848064839839935,
      "learning_rate": 0.0006756131420638593,
      "loss": 0.0001,
      "step": 107320
    },
    {
      "epoch": 49.66682091624248,
      "grad_norm": 0.0059301420114934444,
      "learning_rate": 0.0006663581675150393,
      "loss": 0.0002,
      "step": 107330
    },
    {
      "epoch": 49.67144840351689,
      "grad_norm": 0.002462207106873393,
      "learning_rate": 0.0006571031929662194,
      "loss": 0.0007,
      "step": 107340
    },
    {
      "epoch": 49.6760758907913,
      "grad_norm": 0.31141939759254456,
      "learning_rate": 0.0006478482184173994,
      "loss": 0.0002,
      "step": 107350
    },
    {
      "epoch": 49.68070337806571,
      "grad_norm": 0.002620227634906769,
      "learning_rate": 0.0006385932438685794,
      "loss": 0.0,
      "step": 107360
    },
    {
      "epoch": 49.68533086534012,
      "grad_norm": 0.002443780889734626,
      "learning_rate": 0.0006293382693197594,
      "loss": 0.0001,
      "step": 107370
    },
    {
      "epoch": 49.68995835261453,
      "grad_norm": 0.01334959827363491,
      "learning_rate": 0.0006200832947709394,
      "loss": 0.0001,
      "step": 107380
    },
    {
      "epoch": 49.69458583988894,
      "grad_norm": 0.015018438920378685,
      "learning_rate": 0.0006108283202221194,
      "loss": 0.0004,
      "step": 107390
    },
    {
      "epoch": 49.69921332716335,
      "grad_norm": 0.0015909619396552444,
      "learning_rate": 0.0006015733456732994,
      "loss": 0.0001,
      "step": 107400
    },
    {
      "epoch": 49.70384081443776,
      "grad_norm": 0.010720979422330856,
      "learning_rate": 0.0005923183711244794,
      "loss": 0.0001,
      "step": 107410
    },
    {
      "epoch": 49.70846830171217,
      "grad_norm": 0.0145947290584445,
      "learning_rate": 0.0005830633965756594,
      "loss": 0.0001,
      "step": 107420
    },
    {
      "epoch": 49.71309578898658,
      "grad_norm": 0.006507198326289654,
      "learning_rate": 0.0005738084220268395,
      "loss": 0.0001,
      "step": 107430
    },
    {
      "epoch": 49.71772327626099,
      "grad_norm": 0.02669261209666729,
      "learning_rate": 0.0005645534474780195,
      "loss": 0.0001,
      "step": 107440
    },
    {
      "epoch": 49.7223507635354,
      "grad_norm": 0.002432446461170912,
      "learning_rate": 0.0005552984729291995,
      "loss": 0.0001,
      "step": 107450
    },
    {
      "epoch": 49.72697825080981,
      "grad_norm": 0.003616764908656478,
      "learning_rate": 0.0005460434983803795,
      "loss": 0.0,
      "step": 107460
    },
    {
      "epoch": 49.73160573808422,
      "grad_norm": 0.0025736442767083645,
      "learning_rate": 0.0005367885238315595,
      "loss": 0.0,
      "step": 107470
    },
    {
      "epoch": 49.73623322535863,
      "grad_norm": 0.00330837513320148,
      "learning_rate": 0.0005275335492827396,
      "loss": 0.0001,
      "step": 107480
    },
    {
      "epoch": 49.74086071263304,
      "grad_norm": 0.009331192821264267,
      "learning_rate": 0.0005182785747339195,
      "loss": 0.0001,
      "step": 107490
    },
    {
      "epoch": 49.74548819990745,
      "grad_norm": 0.013801559805870056,
      "learning_rate": 0.0005090236001850995,
      "loss": 0.0004,
      "step": 107500
    },
    {
      "epoch": 49.75011568718186,
      "grad_norm": 0.015733839944005013,
      "learning_rate": 0.0004997686256362795,
      "loss": 0.0001,
      "step": 107510
    },
    {
      "epoch": 49.75474317445627,
      "grad_norm": 0.0028500251937657595,
      "learning_rate": 0.0004905136510874596,
      "loss": 0.0,
      "step": 107520
    },
    {
      "epoch": 49.75937066173068,
      "grad_norm": 0.02057211473584175,
      "learning_rate": 0.0004812586765386396,
      "loss": 0.0001,
      "step": 107530
    },
    {
      "epoch": 49.76399814900509,
      "grad_norm": 0.05257758870720863,
      "learning_rate": 0.0004720037019898195,
      "loss": 0.0003,
      "step": 107540
    },
    {
      "epoch": 49.7686256362795,
      "grad_norm": 0.014213343150913715,
      "learning_rate": 0.00046274872744099955,
      "loss": 0.0,
      "step": 107550
    },
    {
      "epoch": 49.77325312355391,
      "grad_norm": 0.0041126674041152,
      "learning_rate": 0.00045349375289217955,
      "loss": 0.0001,
      "step": 107560
    },
    {
      "epoch": 49.77788061082832,
      "grad_norm": 0.004926994442939758,
      "learning_rate": 0.0004442387783433596,
      "loss": 0.0001,
      "step": 107570
    },
    {
      "epoch": 49.78250809810273,
      "grad_norm": 0.0030242041684687138,
      "learning_rate": 0.0004349838037945396,
      "loss": 0.0001,
      "step": 107580
    },
    {
      "epoch": 49.78713558537714,
      "grad_norm": 0.004426783882081509,
      "learning_rate": 0.00042572882924571956,
      "loss": 0.0001,
      "step": 107590
    },
    {
      "epoch": 49.79176307265155,
      "grad_norm": 0.002406330546364188,
      "learning_rate": 0.00041647385469689957,
      "loss": 0.0002,
      "step": 107600
    },
    {
      "epoch": 49.79639055992596,
      "grad_norm": 0.013261834159493446,
      "learning_rate": 0.0004072188801480796,
      "loss": 0.0001,
      "step": 107610
    },
    {
      "epoch": 49.80101804720037,
      "grad_norm": 0.0014242226025089622,
      "learning_rate": 0.00039796390559925963,
      "loss": 0.0001,
      "step": 107620
    },
    {
      "epoch": 49.80564553447478,
      "grad_norm": 0.004854581318795681,
      "learning_rate": 0.00038870893105043963,
      "loss": 0.0001,
      "step": 107630
    },
    {
      "epoch": 49.81027302174919,
      "grad_norm": 0.038765840232372284,
      "learning_rate": 0.00037945395650161964,
      "loss": 0.0002,
      "step": 107640
    },
    {
      "epoch": 49.8149005090236,
      "grad_norm": 0.008626594208180904,
      "learning_rate": 0.00037019898195279964,
      "loss": 0.0001,
      "step": 107650
    },
    {
      "epoch": 49.81952799629801,
      "grad_norm": 0.01217509713023901,
      "learning_rate": 0.00036094400740397964,
      "loss": 0.0002,
      "step": 107660
    },
    {
      "epoch": 49.82415548357242,
      "grad_norm": 0.006954814773052931,
      "learning_rate": 0.00035168903285515965,
      "loss": 0.0,
      "step": 107670
    },
    {
      "epoch": 49.82878297084683,
      "grad_norm": 0.03390798717737198,
      "learning_rate": 0.0003424340583063397,
      "loss": 0.0001,
      "step": 107680
    },
    {
      "epoch": 49.83341045812124,
      "grad_norm": 0.0018497996497899294,
      "learning_rate": 0.00033317908375751965,
      "loss": 0.0,
      "step": 107690
    },
    {
      "epoch": 49.83803794539565,
      "grad_norm": 0.012647117488086224,
      "learning_rate": 0.0003239241092086997,
      "loss": 0.0001,
      "step": 107700
    },
    {
      "epoch": 49.84266543267006,
      "grad_norm": 0.0336521714925766,
      "learning_rate": 0.0003146691346598797,
      "loss": 0.0001,
      "step": 107710
    },
    {
      "epoch": 49.84729291994447,
      "grad_norm": 0.001795070944353938,
      "learning_rate": 0.0003054141601110597,
      "loss": 0.0001,
      "step": 107720
    },
    {
      "epoch": 49.85192040721888,
      "grad_norm": 0.0019373663235455751,
      "learning_rate": 0.0002961591855622397,
      "loss": 0.0001,
      "step": 107730
    },
    {
      "epoch": 49.85654789449329,
      "grad_norm": 0.05181588605046272,
      "learning_rate": 0.0002869042110134197,
      "loss": 0.0001,
      "step": 107740
    },
    {
      "epoch": 49.8611753817677,
      "grad_norm": 0.00805156771093607,
      "learning_rate": 0.00027764923646459973,
      "loss": 0.0,
      "step": 107750
    },
    {
      "epoch": 49.86580286904211,
      "grad_norm": 0.01978067122399807,
      "learning_rate": 0.00026839426191577973,
      "loss": 0.0003,
      "step": 107760
    },
    {
      "epoch": 49.87043035631652,
      "grad_norm": 0.0026034193579107523,
      "learning_rate": 0.00025913928736695974,
      "loss": 0.0,
      "step": 107770
    },
    {
      "epoch": 49.87505784359093,
      "grad_norm": 0.00282750534825027,
      "learning_rate": 0.00024988431281813974,
      "loss": 0.0001,
      "step": 107780
    },
    {
      "epoch": 49.87968533086534,
      "grad_norm": 0.008492865599691868,
      "learning_rate": 0.0002406293382693198,
      "loss": 0.0001,
      "step": 107790
    },
    {
      "epoch": 49.88431281813975,
      "grad_norm": 1.2068673372268677,
      "learning_rate": 0.00023137436372049977,
      "loss": 0.0004,
      "step": 107800
    },
    {
      "epoch": 49.88894030541416,
      "grad_norm": 0.021705755963921547,
      "learning_rate": 0.0002221193891716798,
      "loss": 0.0001,
      "step": 107810
    },
    {
      "epoch": 49.89356779268857,
      "grad_norm": 0.0010921297362074256,
      "learning_rate": 0.00021286441462285978,
      "loss": 0.0001,
      "step": 107820
    },
    {
      "epoch": 49.89819527996298,
      "grad_norm": 0.021608522161841393,
      "learning_rate": 0.0002036094400740398,
      "loss": 0.0001,
      "step": 107830
    },
    {
      "epoch": 49.90282276723739,
      "grad_norm": 0.008240769617259502,
      "learning_rate": 0.00019435446552521982,
      "loss": 0.0002,
      "step": 107840
    },
    {
      "epoch": 49.9074502545118,
      "grad_norm": 0.17848537862300873,
      "learning_rate": 0.00018509949097639982,
      "loss": 0.0001,
      "step": 107850
    },
    {
      "epoch": 49.91207774178621,
      "grad_norm": 0.003304768120869994,
      "learning_rate": 0.00017584451642757982,
      "loss": 0.0001,
      "step": 107860
    },
    {
      "epoch": 49.91670522906062,
      "grad_norm": 0.0017041447572410107,
      "learning_rate": 0.00016658954187875983,
      "loss": 0.0001,
      "step": 107870
    },
    {
      "epoch": 49.92133271633503,
      "grad_norm": 0.0005719683249481022,
      "learning_rate": 0.00015733456732993986,
      "loss": 0.0006,
      "step": 107880
    },
    {
      "epoch": 49.92596020360944,
      "grad_norm": 0.0028026814106851816,
      "learning_rate": 0.00014807959278111986,
      "loss": 0.0003,
      "step": 107890
    },
    {
      "epoch": 49.93058769088385,
      "grad_norm": 0.03030424192547798,
      "learning_rate": 0.00013882461823229986,
      "loss": 0.0001,
      "step": 107900
    },
    {
      "epoch": 49.93521517815826,
      "grad_norm": 0.003814093768596649,
      "learning_rate": 0.00012956964368347987,
      "loss": 0.0001,
      "step": 107910
    },
    {
      "epoch": 49.93984266543267,
      "grad_norm": 0.006017489358782768,
      "learning_rate": 0.0001203146691346599,
      "loss": 0.0001,
      "step": 107920
    },
    {
      "epoch": 49.94447015270708,
      "grad_norm": 0.0025871952529996634,
      "learning_rate": 0.0001110596945858399,
      "loss": 0.0001,
      "step": 107930
    },
    {
      "epoch": 49.94909763998149,
      "grad_norm": 0.04706280678510666,
      "learning_rate": 0.0001018047200370199,
      "loss": 0.0001,
      "step": 107940
    },
    {
      "epoch": 49.9537251272559,
      "grad_norm": 0.002237903419882059,
      "learning_rate": 9.254974548819991e-05,
      "loss": 0.0,
      "step": 107950
    },
    {
      "epoch": 49.95835261453031,
      "grad_norm": 0.006558069959282875,
      "learning_rate": 8.329477093937991e-05,
      "loss": 0.0,
      "step": 107960
    },
    {
      "epoch": 49.96298010180472,
      "grad_norm": 0.04557402431964874,
      "learning_rate": 7.403979639055993e-05,
      "loss": 0.0001,
      "step": 107970
    },
    {
      "epoch": 49.96760758907913,
      "grad_norm": 0.01460967119783163,
      "learning_rate": 6.478482184173993e-05,
      "loss": 0.0001,
      "step": 107980
    },
    {
      "epoch": 49.97223507635354,
      "grad_norm": 0.006982495542615652,
      "learning_rate": 5.552984729291995e-05,
      "loss": 0.0001,
      "step": 107990
    },
    {
      "epoch": 49.97686256362795,
      "grad_norm": 0.40383830666542053,
      "learning_rate": 4.6274872744099955e-05,
      "loss": 0.0004,
      "step": 108000
    },
    {
      "epoch": 49.98149005090236,
      "grad_norm": 0.001463782275095582,
      "learning_rate": 3.7019898195279965e-05,
      "loss": 0.0,
      "step": 108010
    },
    {
      "epoch": 49.98611753817677,
      "grad_norm": 0.01726166531443596,
      "learning_rate": 2.7764923646459976e-05,
      "loss": 0.0001,
      "step": 108020
    },
    {
      "epoch": 49.99074502545118,
      "grad_norm": 0.003940068185329437,
      "learning_rate": 1.8509949097639983e-05,
      "loss": 0.0001,
      "step": 108030
    },
    {
      "epoch": 49.99537251272559,
      "grad_norm": 0.002191853476688266,
      "learning_rate": 9.254974548819991e-06,
      "loss": 0.0001,
      "step": 108040
    },
    {
      "epoch": 50.0,
      "grad_norm": 138.84605407714844,
      "learning_rate": 0.0,
      "loss": 0.8573,
      "step": 108050
    },
    {
      "epoch": 50.0,
      "eval_accuracy_branch1": 0.9916037590509937,
      "eval_accuracy_branch2": 0.5003081189339085,
      "eval_f1_branch1": 0.9925094277110177,
      "eval_f1_branch2": 0.4996154671950436,
      "eval_loss": 0.01563028246164322,
      "eval_precision_branch1": 0.9927985044566267,
      "eval_precision_branch2": 0.5003098344736413,
      "eval_recall_branch1": 0.9923105786050495,
      "eval_recall_branch2": 0.5003081189339085,
      "eval_runtime": 28.9178,
      "eval_samples_per_second": 448.927,
      "eval_steps_per_second": 56.125,
      "step": 108050
    }
  ],
  "logging_steps": 10,
  "max_steps": 108050,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
