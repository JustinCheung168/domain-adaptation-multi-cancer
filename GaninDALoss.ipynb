{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHmpRkWg5CAaEg7x6whoX6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":119,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AeQrKEOuUH0R","executionInfo":{"status":"ok","timestamp":1753232097808,"user_tz":300,"elapsed":45,"user":{"displayName":"Justin Cheung","userId":"09708893114482318552"}},"outputId":"bc2d6822-fa6e-4a9f-8554-5d1af68abac2"},"outputs":[{"output_type":"stream","name":"stdout","text":["y_pred = X W^T + b; L(y_pred, y_true, d)\n","X=\n"," tensor([[-0.5300, -1.3035,  0.4438,  1.2221,  1.0395,  0.9608],\n","        [ 0.4214,  0.7452, -1.8389, -1.2497, -0.2485,  0.1428],\n","        [-1.0509,  0.3527, -0.0916,  0.0341, -0.8986,  0.1022],\n","        [-0.6627, -0.1350, -0.3983, -1.7892,  1.2785,  1.3351],\n","        [-0.3066,  1.0382,  1.2762,  0.0419, -1.2794, -1.8432],\n","        [ 0.8633, -1.7786, -0.8080, -0.8735,  0.9367, -1.2319],\n","        [ 1.5287, -0.2759, -0.8625, -0.1915, -0.4807, -1.4154],\n","        [ 0.0934, -0.2420,  1.0251,  1.3822,  2.1080,  0.2562],\n","        [ 0.4913,  1.0152, -0.0184, -0.8487, -1.6520, -1.1392],\n","        [ 0.6818,  0.4731, -0.4292, -1.0216, -0.5285,  1.6272]],\n","       requires_grad=True)\n","y_true= tensor([0, 1, 2, 0, 1, 2, 2, 1, 2, 1])\n","y_pred=\n"," tensor([[-0.3210, -0.3229,  0.7304],\n","        [-0.6966, -0.3994, -1.3843],\n","        [-0.4325, -1.0315, -0.0327],\n","        [-1.0576, -0.1506, -0.4659],\n","        [ 0.3075, -0.5426,  0.5685],\n","        [-0.2938,  1.2050, -0.1878],\n","        [ 0.1525,  0.4897, -0.6814],\n","        [-0.0564, -0.1489,  1.1359],\n","        [ 0.0688, -0.3645, -0.6709],\n","        [-0.4269, -0.4746, -1.2784]], grad_fn=<AddmmBackward0>)\n","d= tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1])\n","W=\n"," tensor([[ 0.2577,  0.0009,  0.2120,  0.1517, -0.1139, -0.1095],\n","        [ 0.3562, -0.3750,  0.1043, -0.2599,  0.1723, -0.2085],\n","        [-0.3194, -0.0878,  0.3797,  0.2606,  0.2474, -0.2426]])\n","b= tensor([-0.2390, -0.3304, -0.0644])\n","dL/dX=\n"," tensor([[-0.0320, -0.0129,  0.0077, -0.0020,  0.0272, -0.0099],\n","        [-0.0154,  0.0183,  0.0086,  0.0236, -0.0087,  0.0029],\n","        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n","        [-0.0151, -0.0207,  0.0007, -0.0156,  0.0258, -0.0092],\n","        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n","        [ 0.0537, -0.0111, -0.0195, -0.0296, -0.0165,  0.0064],\n","        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n","Row indices of X.grad with all zeros:\n","\t (tensor([2, 4, 5, 7, 8, 9]),)\n","Samples for which d is 1:\n","\t (tensor([2, 4, 5, 7, 8, 9]),)\n"]}],"source":["import torch\n","import torch.nn\n","import torch.optim\n","import numpy as np\n","\n","torch.manual_seed(69)\n","\n","n = 10 # Batch size\n","p = 6 # Number of input features\n","c = 3 # Number of output features (number of organ classes)\n","\n","def L_y(y_pred: torch.tensor, y_true: torch.tensor, d: torch.tensor):\n","  losses = torch.nn.functional.cross_entropy(y_pred, y_true, reduction='none')\n","  losses = losses * (1-d) # Set losses of unlabeled instances to 0\n","  return torch.mean(losses)\n","\n","X = torch.randn(n, p, requires_grad=True) # Batch of input features\n","y_true = torch.randint(0, c, (n,)) # Batch of class labels (organ class outputs)\n","d = torch.randint(0, 2, (n,)) # Batch of domain labels\n","\n","linear_layer = torch.nn.Linear(p, c)\n","y_pred = linear_layer(X)\n","L = L_y(y_pred, y_true, d)\n","\n","print(\"y_pred = X W^T + b; L(y_pred, y_true, d)\")\n","print(\"X=\\n\", X)\n","print(\"y_true=\", y_true)\n","print(\"y_pred=\\n\", y_pred)\n","print(\"d=\", d)\n","print(\"W=\\n\", linear_layer.weight.data)\n","print(\"b=\", linear_layer.bias.data)\n","\n","L.backward() # Calculate the gradients\n","print(\"dL/dX=\\n\", X.grad)\n","\n","zero_gradient_rows = torch.where(torch.sum(torch.abs(X.grad), dim=1) == 0)\n","print(\"Row indices of X.grad with all zeros:\\n\\t\", zero_gradient_rows)\n","print(\"Samples for which d is 1:\\n\\t\", torch.where(d == 1))"]},{"cell_type":"code","source":[],"metadata":{"id":"Mt6rgVTjuXW-"},"execution_count":null,"outputs":[]}]}